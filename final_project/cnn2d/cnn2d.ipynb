{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.cnn2d.model import build_train_cnn, full_cnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from final_project.cnn2d.config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2502, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2502, 7)\n",
      "Shape of a given window (prior to preprocessing): (6, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.89197166e+00 1.17762692e+00 1.49940968e-01 7.31404959e-01\n",
      " 9.55312869e+00 9.44510035e-03 0.00000000e+00 1.18063754e-03\n",
      " 2.59740260e-02 1.18063754e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.67958644  1.49942966  0.35701355  1.17399741 10.40055847  1.36100371\n",
      "  1.          0.03434012  0.15905778  0.03434012]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building CNN Architecture ======\n",
      "====== Done Building CNN Architecture ======\n",
      "Epoch 1/2000, Train Loss: 10.847939062185786, Val Loss: 11.473070292245774, Val MAE: 1.9841138124465942\n",
      "Epoch 2/2000, Train Loss: 10.824047087109811, Val Loss: 11.448764344056448, Val MAE: 1.9793323278427124\n",
      "Epoch 3/2000, Train Loss: 10.800746595573694, Val Loss: 11.424900009518577, Val MAE: 1.9746402502059937\n",
      "Epoch 4/2000, Train Loss: 10.777272792057527, Val Loss: 11.400924906844185, Val MAE: 1.9699381589889526\n",
      "Epoch 5/2000, Train Loss: 10.753539890765472, Val Loss: 11.377027017729622, Val MAE: 1.9652284383773804\n",
      "Epoch 6/2000, Train Loss: 10.730152241770067, Val Loss: 11.35284724973497, Val MAE: 1.960510015487671\n",
      "Epoch 7/2000, Train Loss: 10.706282929741949, Val Loss: 11.32880999928429, Val MAE: 1.9558156728744507\n",
      "Epoch 8/2000, Train Loss: 10.682763173650788, Val Loss: 11.304545958836874, Val MAE: 1.9511139392852783\n",
      "Epoch 9/2000, Train Loss: 10.659009436458055, Val Loss: 11.280301111085075, Val MAE: 1.9464492797851562\n",
      "Epoch 10/2000, Train Loss: 10.635490331394212, Val Loss: 11.256579345180874, Val MAE: 1.9419264793395996\n",
      "Epoch 11/2000, Train Loss: 10.612000456565863, Val Loss: 11.232331287293206, Val MAE: 1.937323808670044\n",
      "Epoch 12/2000, Train Loss: 10.588584722349438, Val Loss: 11.208412579127721, Val MAE: 1.9327993392944336\n",
      "Epoch 13/2000, Train Loss: 10.565055892901293, Val Loss: 11.184484240554628, Val MAE: 1.9283117055892944\n",
      "Epoch 14/2000, Train Loss: 10.541421192825597, Val Loss: 11.160671100729989, Val MAE: 1.9239161014556885\n",
      "Epoch 15/2000, Train Loss: 10.51801344033525, Val Loss: 11.136628071467081, Val MAE: 1.919493556022644\n",
      "Epoch 16/2000, Train Loss: 10.494425382197155, Val Loss: 11.11276011807578, Val MAE: 1.9152061939239502\n",
      "Epoch 17/2000, Train Loss: 10.470932500486818, Val Loss: 11.088603791736421, Val MAE: 1.9108858108520508\n",
      "Epoch 18/2000, Train Loss: 10.447459956150297, Val Loss: 11.064575374126434, Val MAE: 1.906650185585022\n",
      "Epoch 19/2000, Train Loss: 10.424108238919665, Val Loss: 11.04091462351027, Val MAE: 1.9024782180786133\n",
      "Epoch 20/2000, Train Loss: 10.400634968650694, Val Loss: 11.016915363924843, Val MAE: 1.8982961177825928\n",
      "Epoch 21/2000, Train Loss: 10.377432431757702, Val Loss: 10.993041481290545, Val MAE: 1.8942028284072876\n",
      "Epoch 22/2000, Train Loss: 10.353802009765452, Val Loss: 10.969215330623445, Val MAE: 1.8901853561401367\n",
      "Epoch 23/2000, Train Loss: 10.33021377541955, Val Loss: 10.94513053553445, Val MAE: 1.8862733840942383\n",
      "Epoch 24/2000, Train Loss: 10.306881978750566, Val Loss: 10.921228323663984, Val MAE: 1.8824633359909058\n",
      "Epoch 25/2000, Train Loss: 10.283727695642641, Val Loss: 10.897352681273507, Val MAE: 1.8787615299224854\n",
      "Epoch 26/2000, Train Loss: 10.26012693978163, Val Loss: 10.874125744615283, Val MAE: 1.875212550163269\n",
      "Epoch 27/2000, Train Loss: 10.2368819878367, Val Loss: 10.85034430878503, Val MAE: 1.8716245889663696\n",
      "Epoch 28/2000, Train Loss: 10.213486395702711, Val Loss: 10.82629629827681, Val MAE: 1.8680652379989624\n",
      "Epoch 29/2000, Train Loss: 10.190017835377638, Val Loss: 10.802402223859515, Val MAE: 1.8645670413970947\n",
      "Epoch 30/2000, Train Loss: 10.166764040087443, Val Loss: 10.778872356528328, Val MAE: 1.861162781715393\n",
      "Epoch 31/2000, Train Loss: 10.143417970922332, Val Loss: 10.755064876306625, Val MAE: 1.8578519821166992\n",
      "Epoch 32/2000, Train Loss: 10.12028205209794, Val Loss: 10.731376815409888, Val MAE: 1.8546295166015625\n",
      "Epoch 33/2000, Train Loss: 10.096920072612036, Val Loss: 10.707515262422108, Val MAE: 1.8515039682388306\n",
      "Epoch 34/2000, Train Loss: 10.073606142708545, Val Loss: 10.683832767463866, Val MAE: 1.8486026525497437\n",
      "Epoch 35/2000, Train Loss: 10.050195341554112, Val Loss: 10.660447869982038, Val MAE: 1.8458532094955444\n",
      "Epoch 36/2000, Train Loss: 10.02707487571727, Val Loss: 10.63668399765378, Val MAE: 1.8431999683380127\n",
      "Epoch 37/2000, Train Loss: 10.00394736358578, Val Loss: 10.61282088359197, Val MAE: 1.8407201766967773\n",
      "Epoch 38/2000, Train Loss: 9.980577351848565, Val Loss: 10.589699069658915, Val MAE: 1.838472604751587\n",
      "Epoch 39/2000, Train Loss: 9.957082970354218, Val Loss: 10.565539010933467, Val MAE: 1.8362637758255005\n",
      "Epoch 40/2000, Train Loss: 9.933464529820322, Val Loss: 10.541732935678391, Val MAE: 1.8342430591583252\n",
      "Epoch 41/2000, Train Loss: 9.910328723816004, Val Loss: 10.518024770986466, Val MAE: 1.8323527574539185\n",
      "Epoch 42/2000, Train Loss: 9.886722769152126, Val Loss: 10.493876777944111, Val MAE: 1.8305631875991821\n",
      "Epoch 43/2000, Train Loss: 9.863423167230044, Val Loss: 10.470276256402334, Val MAE: 1.8289580345153809\n",
      "Epoch 44/2000, Train Loss: 9.84033972789942, Val Loss: 10.447061413810367, Val MAE: 1.8274681568145752\n",
      "Epoch 45/2000, Train Loss: 9.817226467751313, Val Loss: 10.423136251313347, Val MAE: 1.8260477781295776\n",
      "Epoch 46/2000, Train Loss: 9.793948094498456, Val Loss: 10.39950936748868, Val MAE: 1.824754238128662\n",
      "Epoch 47/2000, Train Loss: 9.770847046156696, Val Loss: 10.376507517837343, Val MAE: 1.8235517740249634\n",
      "Epoch 48/2000, Train Loss: 9.74767850249039, Val Loss: 10.352652152379354, Val MAE: 1.8223692178726196\n",
      "Epoch 49/2000, Train Loss: 9.724498684887154, Val Loss: 10.32855696053732, Val MAE: 1.8212312459945679\n",
      "Epoch 50/2000, Train Loss: 9.701153534591954, Val Loss: 10.3056201338768, Val MAE: 1.8201467990875244\n",
      "Epoch 51/2000, Train Loss: 9.678310840859568, Val Loss: 10.282005622273399, Val MAE: 1.8190706968307495\n",
      "Epoch 52/2000, Train Loss: 9.655151168113031, Val Loss: 10.258559823036194, Val MAE: 1.8180749416351318\n",
      "Epoch 53/2000, Train Loss: 9.63189216027475, Val Loss: 10.234315298852467, Val MAE: 1.8171063661575317\n",
      "Epoch 54/2000, Train Loss: 9.608691007024984, Val Loss: 10.211584392048064, Val MAE: 1.8162140846252441\n",
      "Epoch 55/2000, Train Loss: 9.585563285730789, Val Loss: 10.18819429477056, Val MAE: 1.8153283596038818\n",
      "Epoch 56/2000, Train Loss: 9.562339836518754, Val Loss: 10.164549935431708, Val MAE: 1.8144007921218872\n",
      "Epoch 57/2000, Train Loss: 9.538988697175752, Val Loss: 10.140962688695817, Val MAE: 1.8135263919830322\n",
      "Epoch 58/2000, Train Loss: 9.51569875625024, Val Loss: 10.116500604720343, Val MAE: 1.81256902217865\n",
      "Epoch 59/2000, Train Loss: 9.492397227643744, Val Loss: 10.093938520976476, Val MAE: 1.8116827011108398\n",
      "Epoch 60/2000, Train Loss: 9.469196735887836, Val Loss: 10.070875962575277, Val MAE: 1.8107715845108032\n",
      "Epoch 61/2000, Train Loss: 9.446321409411087, Val Loss: 10.047000890686398, Val MAE: 1.809827208518982\n",
      "Epoch 62/2000, Train Loss: 9.42325449242074, Val Loss: 10.024106482664743, Val MAE: 1.808911681175232\n",
      "Epoch 63/2000, Train Loss: 9.400498469558523, Val Loss: 10.000876639570508, Val MAE: 1.807981252670288\n",
      "Epoch 64/2000, Train Loss: 9.377450895914743, Val Loss: 9.977172806149436, Val MAE: 1.8070762157440186\n",
      "Epoch 65/2000, Train Loss: 9.354198061360627, Val Loss: 9.95460874126071, Val MAE: 1.8061679601669312\n",
      "Epoch 66/2000, Train Loss: 9.331563981867307, Val Loss: 9.931339567615872, Val MAE: 1.805220127105713\n",
      "Epoch 67/2000, Train Loss: 9.308247540666287, Val Loss: 9.908313211940584, Val MAE: 1.8042856454849243\n",
      "Epoch 68/2000, Train Loss: 9.28498972958671, Val Loss: 9.884618543443226, Val MAE: 1.8033288717269897\n",
      "Epoch 69/2000, Train Loss: 9.261809385377699, Val Loss: 9.861779627345857, Val MAE: 1.8024004697799683\n",
      "Epoch 70/2000, Train Loss: 9.238905950392924, Val Loss: 9.838399075326466, Val MAE: 1.801434874534607\n",
      "Epoch 71/2000, Train Loss: 9.216036921663916, Val Loss: 9.81583576259159, Val MAE: 1.8004934787750244\n",
      "Epoch 72/2000, Train Loss: 9.193141900938219, Val Loss: 9.792908058280037, Val MAE: 1.799560546875\n",
      "Epoch 73/2000, Train Loss: 9.170224621534684, Val Loss: 9.769782074860164, Val MAE: 1.7985801696777344\n",
      "Epoch 74/2000, Train Loss: 9.14717038293147, Val Loss: 9.746683938162667, Val MAE: 1.7976382970809937\n",
      "Epoch 75/2000, Train Loss: 9.124013575243177, Val Loss: 9.723270248799096, Val MAE: 1.7966541051864624\n",
      "Epoch 76/2000, Train Loss: 9.10091508157835, Val Loss: 9.699930342889967, Val MAE: 1.7956644296646118\n",
      "Epoch 77/2000, Train Loss: 9.077814083341483, Val Loss: 9.677048390819913, Val MAE: 1.7947100400924683\n",
      "Epoch 78/2000, Train Loss: 9.054938526853014, Val Loss: 9.653985601095926, Val MAE: 1.7937153577804565\n",
      "Epoch 79/2000, Train Loss: 9.03230798563937, Val Loss: 9.631276278268723, Val MAE: 1.792755126953125\n",
      "Epoch 80/2000, Train Loss: 9.009837170077649, Val Loss: 9.609232720874605, Val MAE: 1.791797399520874\n",
      "Epoch 81/2000, Train Loss: 8.987598967316792, Val Loss: 9.585813113621303, Val MAE: 1.790809988975525\n",
      "Epoch 82/2000, Train Loss: 8.964846781850365, Val Loss: 9.564105595861163, Val MAE: 1.7898272275924683\n",
      "Epoch 83/2000, Train Loss: 8.942381450588512, Val Loss: 9.54131855141549, Val MAE: 1.7888554334640503\n",
      "Epoch 84/2000, Train Loss: 8.919733768457755, Val Loss: 9.518863276356743, Val MAE: 1.7878508567810059\n",
      "Epoch 85/2000, Train Loss: 8.897423188928489, Val Loss: 9.496200716211682, Val MAE: 1.7868634462356567\n",
      "Epoch 86/2000, Train Loss: 8.875116872854731, Val Loss: 9.473828051771436, Val MAE: 1.7858442068099976\n",
      "Epoch 87/2000, Train Loss: 8.852700606051554, Val Loss: 9.451572486332484, Val MAE: 1.7848336696624756\n",
      "Epoch 88/2000, Train Loss: 8.83015415530615, Val Loss: 9.428755873725528, Val MAE: 1.783790946006775\n",
      "Epoch 89/2000, Train Loss: 8.807524986176295, Val Loss: 9.406408996809096, Val MAE: 1.7827582359313965\n",
      "Epoch 90/2000, Train Loss: 8.785603056505469, Val Loss: 9.38405851806913, Val MAE: 1.7817251682281494\n",
      "Epoch 91/2000, Train Loss: 8.763320308141884, Val Loss: 9.361960114467712, Val MAE: 1.7807155847549438\n",
      "Epoch 92/2000, Train Loss: 8.741068883742534, Val Loss: 9.33910474323091, Val MAE: 1.779624581336975\n",
      "Epoch 93/2000, Train Loss: 8.718710763498153, Val Loss: 9.316856222493309, Val MAE: 1.778592824935913\n",
      "Epoch 94/2000, Train Loss: 8.696457976514765, Val Loss: 9.294688985461281, Val MAE: 1.7775815725326538\n",
      "Epoch 95/2000, Train Loss: 8.674306338361355, Val Loss: 9.272746889364152, Val MAE: 1.776564121246338\n",
      "Epoch 96/2000, Train Loss: 8.652379322791806, Val Loss: 9.25096785454523, Val MAE: 1.7755279541015625\n",
      "Epoch 97/2000, Train Loss: 8.63049337272079, Val Loss: 9.229245621533622, Val MAE: 1.7745344638824463\n",
      "Epoch 98/2000, Train Loss: 8.608730033691579, Val Loss: 9.207477238916216, Val MAE: 1.7735363245010376\n",
      "Epoch 99/2000, Train Loss: 8.586794897598676, Val Loss: 9.185682770751772, Val MAE: 1.7725107669830322\n",
      "Epoch 100/2000, Train Loss: 8.564727910987413, Val Loss: 9.163680397328877, Val MAE: 1.7714847326278687\n",
      "Epoch 101/2000, Train Loss: 8.542570313210211, Val Loss: 9.14199931706701, Val MAE: 1.770453691482544\n",
      "Epoch 102/2000, Train Loss: 8.520804711894693, Val Loss: 9.119553408452443, Val MAE: 1.769390344619751\n",
      "Epoch 103/2000, Train Loss: 8.498878695564647, Val Loss: 9.098288585742315, Val MAE: 1.7684195041656494\n",
      "Epoch 104/2000, Train Loss: 8.477072848923957, Val Loss: 9.07589362348829, Val MAE: 1.767332911491394\n",
      "Epoch 105/2000, Train Loss: 8.45507012402221, Val Loss: 9.054231605359487, Val MAE: 1.7663005590438843\n",
      "Epoch 106/2000, Train Loss: 8.433471726765921, Val Loss: 9.032841986133938, Val MAE: 1.7653340101242065\n",
      "Epoch 107/2000, Train Loss: 8.411927442708707, Val Loss: 9.011124802487236, Val MAE: 1.764304518699646\n",
      "Epoch 108/2000, Train Loss: 8.390569338509327, Val Loss: 8.99003989072073, Val MAE: 1.763356328010559\n",
      "Epoch 109/2000, Train Loss: 8.36952966148998, Val Loss: 8.968350836208888, Val MAE: 1.76236093044281\n",
      "Epoch 110/2000, Train Loss: 8.348261379892971, Val Loss: 8.947746709698723, Val MAE: 1.7614433765411377\n",
      "Epoch 111/2000, Train Loss: 8.326852841168769, Val Loss: 8.925519124383019, Val MAE: 1.7604079246520996\n",
      "Epoch 112/2000, Train Loss: 8.305595897653038, Val Loss: 8.904402065844764, Val MAE: 1.759501338005066\n",
      "Epoch 113/2000, Train Loss: 8.284575013162723, Val Loss: 8.883187885795321, Val MAE: 1.7585283517837524\n",
      "Epoch 114/2000, Train Loss: 8.263574777772632, Val Loss: 8.86287777480625, Val MAE: 1.757606863975525\n",
      "Epoch 115/2000, Train Loss: 8.242779458695642, Val Loss: 8.84164281828063, Val MAE: 1.7566572427749634\n",
      "Epoch 116/2000, Train Loss: 8.221292215947203, Val Loss: 8.820700947727476, Val MAE: 1.7557289600372314\n",
      "Epoch 117/2000, Train Loss: 8.200313656891687, Val Loss: 8.799079450823012, Val MAE: 1.754765510559082\n",
      "Epoch 118/2000, Train Loss: 8.179436492314627, Val Loss: 8.77838178333782, Val MAE: 1.7538328170776367\n",
      "Epoch 119/2000, Train Loss: 8.158783854819488, Val Loss: 8.75785717368126, Val MAE: 1.75295090675354\n",
      "Epoch 120/2000, Train Loss: 8.138193731751866, Val Loss: 8.737061572926384, Val MAE: 1.7520991563796997\n",
      "Epoch 121/2000, Train Loss: 8.117484828480881, Val Loss: 8.716177684920174, Val MAE: 1.7513039112091064\n",
      "Epoch 122/2000, Train Loss: 8.096800250630453, Val Loss: 8.695089961801257, Val MAE: 1.7505302429199219\n",
      "Epoch 123/2000, Train Loss: 8.076265594939082, Val Loss: 8.674571778093066, Val MAE: 1.7497658729553223\n",
      "Epoch 124/2000, Train Loss: 8.056128073814726, Val Loss: 8.654632903280712, Val MAE: 1.7490068674087524\n",
      "Epoch 125/2000, Train Loss: 8.036064914654945, Val Loss: 8.634703183457965, Val MAE: 1.748284101486206\n",
      "Epoch 126/2000, Train Loss: 8.015881604805315, Val Loss: 8.613954901695251, Val MAE: 1.7475550174713135\n",
      "Epoch 127/2000, Train Loss: 7.995664431110592, Val Loss: 8.59430418979554, Val MAE: 1.7468258142471313\n",
      "Epoch 128/2000, Train Loss: 7.975612575816166, Val Loss: 8.574018109412421, Val MAE: 1.7460919618606567\n",
      "Epoch 129/2000, Train Loss: 7.95585223279988, Val Loss: 8.554198827062335, Val MAE: 1.7453827857971191\n",
      "Epoch 130/2000, Train Loss: 7.936014710091401, Val Loss: 8.534607355083738, Val MAE: 1.7447179555892944\n",
      "Epoch 131/2000, Train Loss: 7.916334612245116, Val Loss: 8.514539210569291, Val MAE: 1.7440348863601685\n",
      "Epoch 132/2000, Train Loss: 7.896597967161278, Val Loss: 8.494494460877918, Val MAE: 1.743389368057251\n",
      "Epoch 133/2000, Train Loss: 7.876694465389709, Val Loss: 8.474960259028844, Val MAE: 1.7427797317504883\n",
      "Epoch 134/2000, Train Loss: 7.856924569489085, Val Loss: 8.455425892557416, Val MAE: 1.7421784400939941\n",
      "Epoch 135/2000, Train Loss: 7.837381554255196, Val Loss: 8.435632907208943, Val MAE: 1.7416319847106934\n",
      "Epoch 136/2000, Train Loss: 7.818033864703266, Val Loss: 8.416347639901298, Val MAE: 1.7410776615142822\n",
      "Epoch 137/2000, Train Loss: 7.798772781624948, Val Loss: 8.397082819825126, Val MAE: 1.7405426502227783\n",
      "Epoch 138/2000, Train Loss: 7.779576038271819, Val Loss: 8.377066010520572, Val MAE: 1.7399450540542603\n",
      "Epoch 139/2000, Train Loss: 7.760170284212056, Val Loss: 8.358408767552604, Val MAE: 1.7394485473632812\n",
      "Epoch 140/2000, Train Loss: 7.740950583066524, Val Loss: 8.338358710209528, Val MAE: 1.7388509511947632\n",
      "Epoch 141/2000, Train Loss: 7.721474842561149, Val Loss: 8.319289110955738, Val MAE: 1.738312840461731\n",
      "Epoch 142/2000, Train Loss: 7.702324007731062, Val Loss: 8.299825276647296, Val MAE: 1.737768292427063\n",
      "Epoch 143/2000, Train Loss: 7.68320475831859, Val Loss: 8.28137324253718, Val MAE: 1.7372400760650635\n",
      "Epoch 144/2000, Train Loss: 7.6647221736074, Val Loss: 8.262420501027789, Val MAE: 1.7366420030593872\n",
      "Epoch 145/2000, Train Loss: 7.646124278876946, Val Loss: 8.243733444384166, Val MAE: 1.7361044883728027\n",
      "Epoch 146/2000, Train Loss: 7.627542839400018, Val Loss: 8.225383707455226, Val MAE: 1.7356221675872803\n",
      "Epoch 147/2000, Train Loss: 7.60927589555049, Val Loss: 8.206451347896031, Val MAE: 1.7351056337356567\n",
      "Epoch 148/2000, Train Loss: 7.590879026521916, Val Loss: 8.18835427079882, Val MAE: 1.734602451324463\n",
      "Epoch 149/2000, Train Loss: 7.572753433448135, Val Loss: 8.170411680425916, Val MAE: 1.7341264486312866\n",
      "Epoch 150/2000, Train Loss: 7.554636146184924, Val Loss: 8.151845259325844, Val MAE: 1.7335880994796753\n",
      "Epoch 151/2000, Train Loss: 7.536287383737282, Val Loss: 8.133732523236956, Val MAE: 1.73309326171875\n",
      "Epoch 152/2000, Train Loss: 7.518307034656595, Val Loss: 8.115235533033099, Val MAE: 1.7326278686523438\n",
      "Epoch 153/2000, Train Loss: 7.500334111524402, Val Loss: 8.09745555406525, Val MAE: 1.7321199178695679\n",
      "Epoch 154/2000, Train Loss: 7.482368353559873, Val Loss: 8.079313536485037, Val MAE: 1.7316645383834839\n",
      "Epoch 155/2000, Train Loss: 7.464159909357305, Val Loss: 8.061065537588936, Val MAE: 1.7311869859695435\n",
      "Epoch 156/2000, Train Loss: 7.446328167854815, Val Loss: 8.04276624747685, Val MAE: 1.7306830883026123\n",
      "Epoch 157/2000, Train Loss: 7.428397900631128, Val Loss: 8.025280093862897, Val MAE: 1.7302197217941284\n",
      "Epoch 158/2000, Train Loss: 7.4109352337372485, Val Loss: 8.007599932806832, Val MAE: 1.7297117710113525\n",
      "Epoch 159/2000, Train Loss: 7.393385597949976, Val Loss: 7.990586882545834, Val MAE: 1.7292155027389526\n",
      "Epoch 160/2000, Train Loss: 7.375875727582213, Val Loss: 7.9721343119939165, Val MAE: 1.7287358045578003\n",
      "Epoch 161/2000, Train Loss: 7.358329177239045, Val Loss: 7.955215232712882, Val MAE: 1.7282707691192627\n",
      "Epoch 162/2000, Train Loss: 7.34132230634918, Val Loss: 7.937962120487576, Val MAE: 1.7277710437774658\n",
      "Epoch 163/2000, Train Loss: 7.3242216642383795, Val Loss: 7.920596202214559, Val MAE: 1.7273049354553223\n",
      "Epoch 164/2000, Train Loss: 7.307170338119539, Val Loss: 7.903811806724185, Val MAE: 1.726829171180725\n",
      "Epoch 165/2000, Train Loss: 7.290405537412937, Val Loss: 7.88691771030426, Val MAE: 1.7263894081115723\n",
      "Epoch 166/2000, Train Loss: 7.273480302356026, Val Loss: 7.869856170245579, Val MAE: 1.7259087562561035\n",
      "Epoch 167/2000, Train Loss: 7.256432036923084, Val Loss: 7.852950379962013, Val MAE: 1.7254297733306885\n",
      "Epoch 168/2000, Train Loss: 7.239327095711685, Val Loss: 7.835589073953175, Val MAE: 1.7249300479888916\n",
      "Epoch 169/2000, Train Loss: 7.2228105017765625, Val Loss: 7.818711031050909, Val MAE: 1.7244718074798584\n",
      "Epoch 170/2000, Train Loss: 7.206051283730102, Val Loss: 7.80240984190078, Val MAE: 1.7240056991577148\n",
      "Epoch 171/2000, Train Loss: 7.189792376815516, Val Loss: 7.785369262808845, Val MAE: 1.723547101020813\n",
      "Epoch 172/2000, Train Loss: 7.173061662063276, Val Loss: 7.769164823350453, Val MAE: 1.7230502367019653\n",
      "Epoch 173/2000, Train Loss: 7.156924642696031, Val Loss: 7.752416548274812, Val MAE: 1.7226063013076782\n",
      "Epoch 174/2000, Train Loss: 7.140501731877939, Val Loss: 7.7368239391417735, Val MAE: 1.7221273183822632\n",
      "Epoch 175/2000, Train Loss: 7.124569800071555, Val Loss: 7.72034238917487, Val MAE: 1.7216684818267822\n",
      "Epoch 176/2000, Train Loss: 7.108325385240304, Val Loss: 7.704518777983529, Val MAE: 1.721217393875122\n",
      "Epoch 177/2000, Train Loss: 7.092256993265515, Val Loss: 7.687563399473826, Val MAE: 1.7207728624343872\n",
      "Epoch 178/2000, Train Loss: 7.075963889930413, Val Loss: 7.671597282091777, Val MAE: 1.7203176021575928\n",
      "Epoch 179/2000, Train Loss: 7.060012398384858, Val Loss: 7.656635832218897, Val MAE: 1.7198740243911743\n",
      "Epoch 180/2000, Train Loss: 7.044356570761698, Val Loss: 7.640208284060161, Val MAE: 1.7194043397903442\n",
      "Epoch 181/2000, Train Loss: 7.02832943629815, Val Loss: 7.624311129252116, Val MAE: 1.7190132141113281\n",
      "Epoch 182/2000, Train Loss: 7.012603578177426, Val Loss: 7.608364783582234, Val MAE: 1.718537449836731\n",
      "Epoch 183/2000, Train Loss: 6.996920767893071, Val Loss: 7.59301251456851, Val MAE: 1.7180665731430054\n",
      "Epoch 184/2000, Train Loss: 6.98159570519107, Val Loss: 7.577680116608029, Val MAE: 1.7176679372787476\n",
      "Epoch 185/2000, Train Loss: 6.966200195011198, Val Loss: 7.562582333882649, Val MAE: 1.7172223329544067\n",
      "Epoch 186/2000, Train Loss: 6.950904683434577, Val Loss: 7.547016456013634, Val MAE: 1.716766595840454\n",
      "Epoch 187/2000, Train Loss: 6.935747820794666, Val Loss: 7.531312721116202, Val MAE: 1.7163792848587036\n",
      "Epoch 188/2000, Train Loss: 6.920578961984395, Val Loss: 7.516958903698694, Val MAE: 1.7158896923065186\n",
      "Epoch 189/2000, Train Loss: 6.905676083100699, Val Loss: 7.502146136193049, Val MAE: 1.7154508829116821\n",
      "Epoch 190/2000, Train Loss: 6.8906691232420325, Val Loss: 7.486373262745993, Val MAE: 1.7149988412857056\n",
      "Epoch 191/2000, Train Loss: 6.87543887816296, Val Loss: 7.471771177791414, Val MAE: 1.7145589590072632\n",
      "Epoch 192/2000, Train Loss: 6.86098925519225, Val Loss: 7.457214764186314, Val MAE: 1.7141026258468628\n",
      "Epoch 193/2000, Train Loss: 6.846427367671756, Val Loss: 7.442811903499422, Val MAE: 1.7136437892913818\n",
      "Epoch 194/2000, Train Loss: 6.832065302495055, Val Loss: 7.428091946102324, Val MAE: 1.713232159614563\n",
      "Epoch 195/2000, Train Loss: 6.817309719887372, Val Loss: 7.413610949402764, Val MAE: 1.7128336429595947\n",
      "Epoch 196/2000, Train Loss: 6.802958529489837, Val Loss: 7.398403633208502, Val MAE: 1.7124415636062622\n",
      "Epoch 197/2000, Train Loss: 6.788538966427737, Val Loss: 7.38478513274874, Val MAE: 1.7120285034179688\n",
      "Epoch 198/2000, Train Loss: 6.774489856741492, Val Loss: 7.370581158569881, Val MAE: 1.7116951942443848\n",
      "Epoch 199/2000, Train Loss: 6.760411133046547, Val Loss: 7.356651326020558, Val MAE: 1.7112659215927124\n",
      "Epoch 200/2000, Train Loss: 6.746434253428988, Val Loss: 7.342478306520553, Val MAE: 1.7108757495880127\n",
      "Epoch 201/2000, Train Loss: 6.732449365435602, Val Loss: 7.328546796526227, Val MAE: 1.710471749305725\n",
      "Epoch 202/2000, Train Loss: 6.718463149494445, Val Loss: 7.314598287854876, Val MAE: 1.710119366645813\n",
      "Epoch 203/2000, Train Loss: 6.704714843349161, Val Loss: 7.3005415712084085, Val MAE: 1.7098076343536377\n",
      "Epoch 204/2000, Train Loss: 6.690971428315629, Val Loss: 7.287368831180391, Val MAE: 1.7094292640686035\n",
      "Epoch 205/2000, Train Loss: 6.67749173415901, Val Loss: 7.273655951023102, Val MAE: 1.709075927734375\n",
      "Epoch 206/2000, Train Loss: 6.663587416008598, Val Loss: 7.260214964548747, Val MAE: 1.708706259727478\n",
      "Epoch 207/2000, Train Loss: 6.65029061429087, Val Loss: 7.246306232043675, Val MAE: 1.7083879709243774\n",
      "Epoch 208/2000, Train Loss: 6.636598237983263, Val Loss: 7.232551770550864, Val MAE: 1.7080888748168945\n",
      "Epoch 209/2000, Train Loss: 6.6232107406106415, Val Loss: 7.219134580521357, Val MAE: 1.7078096866607666\n",
      "Epoch 210/2000, Train Loss: 6.610025866243499, Val Loss: 7.206676117011479, Val MAE: 1.7073628902435303\n",
      "Epoch 211/2000, Train Loss: 6.596945693698017, Val Loss: 7.193312185151236, Val MAE: 1.707136631011963\n",
      "Epoch 212/2000, Train Loss: 6.583847213026835, Val Loss: 7.180106225467863, Val MAE: 1.7067770957946777\n",
      "Epoch 213/2000, Train Loss: 6.5708579088972385, Val Loss: 7.167274250870659, Val MAE: 1.7064712047576904\n",
      "Epoch 214/2000, Train Loss: 6.558068933877017, Val Loss: 7.154106395585196, Val MAE: 1.706174612045288\n",
      "Epoch 215/2000, Train Loss: 6.54509607995683, Val Loss: 7.141638582661038, Val MAE: 1.7058383226394653\n",
      "Epoch 216/2000, Train Loss: 6.5322226482655, Val Loss: 7.12852468377068, Val MAE: 1.7055233716964722\n",
      "Epoch 217/2000, Train Loss: 6.519420487588148, Val Loss: 7.115980778421674, Val MAE: 1.7052088975906372\n",
      "Epoch 218/2000, Train Loss: 6.506978127112341, Val Loss: 7.102975371338072, Val MAE: 1.7049015760421753\n",
      "Epoch 219/2000, Train Loss: 6.494223694875311, Val Loss: 7.091062670662289, Val MAE: 1.704556941986084\n",
      "Epoch 220/2000, Train Loss: 6.48198686332057, Val Loss: 7.0784492918423245, Val MAE: 1.704254150390625\n",
      "Epoch 221/2000, Train Loss: 6.469789006982435, Val Loss: 7.066233390853519, Val MAE: 1.7039694786071777\n",
      "Epoch 222/2000, Train Loss: 6.457532026535701, Val Loss: 7.054401604902177, Val MAE: 1.7036420106887817\n",
      "Epoch 223/2000, Train Loss: 6.445536908040767, Val Loss: 7.042402185144878, Val MAE: 1.7033528089523315\n",
      "Epoch 224/2000, Train Loss: 6.433490823118912, Val Loss: 7.030377121198745, Val MAE: 1.703088641166687\n",
      "Epoch 225/2000, Train Loss: 6.421256400970545, Val Loss: 7.017905422619411, Val MAE: 1.7028483152389526\n",
      "Epoch 226/2000, Train Loss: 6.409157919782846, Val Loss: 7.006214581784748, Val MAE: 1.7025645971298218\n",
      "Epoch 227/2000, Train Loss: 6.397520458076165, Val Loss: 6.993862231572469, Val MAE: 1.7023104429244995\n",
      "Epoch 228/2000, Train Loss: 6.385587295456901, Val Loss: 6.9826237587701705, Val MAE: 1.7020944356918335\n",
      "Epoch 229/2000, Train Loss: 6.373957376385945, Val Loss: 6.971155791055589, Val MAE: 1.7019219398498535\n",
      "Epoch 230/2000, Train Loss: 6.362408512906396, Val Loss: 6.959183170681908, Val MAE: 1.701659917831421\n",
      "Epoch 231/2000, Train Loss: 6.350654860309552, Val Loss: 6.947306025595892, Val MAE: 1.7014868259429932\n",
      "Epoch 232/2000, Train Loss: 6.338994547409467, Val Loss: 6.9363964058104015, Val MAE: 1.7012979984283447\n",
      "Epoch 233/2000, Train Loss: 6.327510996497064, Val Loss: 6.92527972800391, Val MAE: 1.701109528541565\n",
      "Epoch 234/2000, Train Loss: 6.316276902708584, Val Loss: 6.913298464956737, Val MAE: 1.7009809017181396\n",
      "Epoch 235/2000, Train Loss: 6.304920743315445, Val Loss: 6.9025939759754, Val MAE: 1.7007955312728882\n",
      "Epoch 236/2000, Train Loss: 6.293931955679179, Val Loss: 6.891535103321075, Val MAE: 1.700681447982788\n",
      "Epoch 237/2000, Train Loss: 6.282598017973691, Val Loss: 6.880249897638957, Val MAE: 1.7005146741867065\n",
      "Epoch 238/2000, Train Loss: 6.271438940287645, Val Loss: 6.869324559257144, Val MAE: 1.7003977298736572\n",
      "Epoch 239/2000, Train Loss: 6.26073207640009, Val Loss: 6.858538116727557, Val MAE: 1.7002646923065186\n",
      "Epoch 240/2000, Train Loss: 6.249813923210284, Val Loss: 6.8482488223484586, Val MAE: 1.7000833749771118\n",
      "Epoch 241/2000, Train Loss: 6.239166723908423, Val Loss: 6.837178088369823, Val MAE: 1.7000318765640259\n",
      "Epoch 242/2000, Train Loss: 6.228600533960232, Val Loss: 6.827044518220992, Val MAE: 1.6998484134674072\n",
      "Epoch 243/2000, Train Loss: 6.218049347989146, Val Loss: 6.816443959871928, Val MAE: 1.6997840404510498\n",
      "Epoch 244/2000, Train Loss: 6.20722917516746, Val Loss: 6.805439125923884, Val MAE: 1.6997190713882446\n",
      "Epoch 245/2000, Train Loss: 6.196745294778068, Val Loss: 6.79519560223534, Val MAE: 1.6996060609817505\n",
      "Epoch 246/2000, Train Loss: 6.186364393140096, Val Loss: 6.784885894684565, Val MAE: 1.6995340585708618\n",
      "Epoch 247/2000, Train Loss: 6.1759829642238, Val Loss: 6.775059455916995, Val MAE: 1.6993647813796997\n",
      "Epoch 248/2000, Train Loss: 6.165993533114284, Val Loss: 6.764849546409788, Val MAE: 1.6992270946502686\n",
      "Epoch 249/2000, Train Loss: 6.155835043055719, Val Loss: 6.754788864226568, Val MAE: 1.6992278099060059\n",
      "Epoch 250/2000, Train Loss: 6.145805882466024, Val Loss: 6.744783730733962, Val MAE: 1.6991535425186157\n",
      "Epoch 251/2000, Train Loss: 6.135931802064979, Val Loss: 6.734640867937179, Val MAE: 1.6991312503814697\n",
      "Epoch 252/2000, Train Loss: 6.125960068911188, Val Loss: 6.725478833629971, Val MAE: 1.6990022659301758\n",
      "Epoch 253/2000, Train Loss: 6.1160332739605385, Val Loss: 6.715566413743155, Val MAE: 1.6990145444869995\n",
      "Epoch 254/2000, Train Loss: 6.106276072299027, Val Loss: 6.705777332896278, Val MAE: 1.6989713907241821\n",
      "Epoch 255/2000, Train Loss: 6.096574213003742, Val Loss: 6.695949213845389, Val MAE: 1.6989586353302002\n",
      "Epoch 256/2000, Train Loss: 6.086961003049304, Val Loss: 6.686934746447063, Val MAE: 1.6988511085510254\n",
      "Epoch 257/2000, Train Loss: 6.077539162844293, Val Loss: 6.677678993770054, Val MAE: 1.69876229763031\n",
      "Epoch 258/2000, Train Loss: 6.067858003258537, Val Loss: 6.668134062063126, Val MAE: 1.6988216638565063\n",
      "Epoch 259/2000, Train Loss: 6.058213184179136, Val Loss: 6.65821297395797, Val MAE: 1.6988683938980103\n",
      "Epoch 260/2000, Train Loss: 6.048772440642665, Val Loss: 6.649209530580611, Val MAE: 1.698911428451538\n",
      "Epoch 261/2000, Train Loss: 6.039418153265132, Val Loss: 6.6398544652121405, Val MAE: 1.699016809463501\n",
      "Epoch 262/2000, Train Loss: 6.0300561961401336, Val Loss: 6.630740001088097, Val MAE: 1.6989786624908447\n",
      "Epoch 263/2000, Train Loss: 6.0206618729364045, Val Loss: 6.62128072977066, Val MAE: 1.6990296840667725\n",
      "Epoch 264/2000, Train Loss: 6.0118661329507495, Val Loss: 6.612761355581737, Val MAE: 1.698991298675537\n",
      "Epoch 265/2000, Train Loss: 6.003037542146755, Val Loss: 6.604679524898529, Val MAE: 1.6988569498062134\n",
      "Epoch 266/2000, Train Loss: 5.994386996469646, Val Loss: 6.595363960379646, Val MAE: 1.6989860534667969\n",
      "Epoch 267/2000, Train Loss: 5.985177694490161, Val Loss: 6.586790947687058, Val MAE: 1.6989718675613403\n",
      "Epoch 268/2000, Train Loss: 5.976298656551054, Val Loss: 6.578030767894926, Val MAE: 1.6990383863449097\n",
      "Epoch 269/2000, Train Loss: 5.967756706081089, Val Loss: 6.569783889112019, Val MAE: 1.6990374326705933\n",
      "Epoch 270/2000, Train Loss: 5.9592920802376, Val Loss: 6.561763261045728, Val MAE: 1.6990220546722412\n",
      "Epoch 271/2000, Train Loss: 5.950923661334895, Val Loss: 6.552968876702445, Val MAE: 1.6991513967514038\n",
      "Epoch 272/2000, Train Loss: 5.942460965370426, Val Loss: 6.5451387876556035, Val MAE: 1.699143886566162\n",
      "Epoch 273/2000, Train Loss: 5.933944760996464, Val Loss: 6.536748610791706, Val MAE: 1.6992714405059814\n",
      "Epoch 274/2000, Train Loss: 5.925585104817228, Val Loss: 6.528805522691636, Val MAE: 1.6993422508239746\n",
      "Epoch 275/2000, Train Loss: 5.917414824532185, Val Loss: 6.520313631920588, Val MAE: 1.6995106935501099\n",
      "Epoch 276/2000, Train Loss: 5.909206972135643, Val Loss: 6.512444481963203, Val MAE: 1.6995131969451904\n",
      "Epoch 277/2000, Train Loss: 5.900881826457251, Val Loss: 6.504686117172241, Val MAE: 1.6995630264282227\n",
      "Epoch 278/2000, Train Loss: 5.8926670914476444, Val Loss: 6.4962149659792585, Val MAE: 1.699729323387146\n",
      "Epoch 279/2000, Train Loss: 5.884433057984109, Val Loss: 6.4885370419138955, Val MAE: 1.6997767686843872\n",
      "Epoch 280/2000, Train Loss: 5.87655365383003, Val Loss: 6.480924870286669, Val MAE: 1.6998127698898315\n",
      "Epoch 281/2000, Train Loss: 5.868541071882369, Val Loss: 6.47328686998004, Val MAE: 1.6998401880264282\n",
      "Epoch 282/2000, Train Loss: 5.8608315122144345, Val Loss: 6.465178234236581, Val MAE: 1.7001497745513916\n",
      "Epoch 283/2000, Train Loss: 5.85264620357239, Val Loss: 6.4575639480636235, Val MAE: 1.7002472877502441\n",
      "Epoch 284/2000, Train Loss: 5.844898996299346, Val Loss: 6.450299572376978, Val MAE: 1.7003122568130493\n",
      "Epoch 285/2000, Train Loss: 5.837459338232559, Val Loss: 6.443115838936397, Val MAE: 1.700336217880249\n",
      "Epoch 286/2000, Train Loss: 5.829801857051123, Val Loss: 6.435123128550393, Val MAE: 1.7006311416625977\n",
      "Epoch 287/2000, Train Loss: 5.822217566001902, Val Loss: 6.428102388268425, Val MAE: 1.7007144689559937\n",
      "Epoch 288/2000, Train Loss: 5.814783938344679, Val Loss: 6.420766785031273, Val MAE: 1.7008525133132935\n",
      "Epoch 289/2000, Train Loss: 5.807450622698484, Val Loss: 6.414091147127605, Val MAE: 1.7007969617843628\n",
      "Epoch 290/2000, Train Loss: 5.800162937812644, Val Loss: 6.406793673833211, Val MAE: 1.7009541988372803\n",
      "Epoch 291/2000, Train Loss: 5.792869156271177, Val Loss: 6.3997650146484375, Val MAE: 1.7010160684585571\n",
      "Epoch 292/2000, Train Loss: 5.7855731159742865, Val Loss: 6.392688561053503, Val MAE: 1.7011232376098633\n",
      "Epoch 293/2000, Train Loss: 5.778500303058597, Val Loss: 6.385550967284611, Val MAE: 1.7012227773666382\n",
      "Epoch 294/2000, Train Loss: 5.771313810550275, Val Loss: 6.378880273728144, Val MAE: 1.7012288570404053\n",
      "Epoch 295/2000, Train Loss: 5.764307163329992, Val Loss: 6.372277958052499, Val MAE: 1.7012096643447876\n",
      "Epoch 296/2000, Train Loss: 5.757443655361073, Val Loss: 6.365726516360328, Val MAE: 1.7011533975601196\n",
      "Epoch 297/2000, Train Loss: 5.750599059466751, Val Loss: 6.35886242559978, Val MAE: 1.7013126611709595\n",
      "Epoch 298/2000, Train Loss: 5.743772345302135, Val Loss: 6.352283846764338, Val MAE: 1.7013590335845947\n",
      "Epoch 299/2000, Train Loss: 5.736851665297752, Val Loss: 6.345755560057504, Val MAE: 1.701430320739746\n",
      "Epoch 300/2000, Train Loss: 5.730209816998588, Val Loss: 6.33935405810674, Val MAE: 1.7014638185501099\n",
      "Epoch 301/2000, Train Loss: 5.723622630109908, Val Loss: 6.33294225306738, Val MAE: 1.701508641242981\n",
      "Epoch 302/2000, Train Loss: 5.717098752601527, Val Loss: 6.327065870875404, Val MAE: 1.7014780044555664\n",
      "Epoch 303/2000, Train Loss: 5.7103416805374945, Val Loss: 6.3204048190798074, Val MAE: 1.7016000747680664\n",
      "Epoch 304/2000, Train Loss: 5.703804274372052, Val Loss: 6.3141201337178545, Val MAE: 1.7016369104385376\n",
      "Epoch 305/2000, Train Loss: 5.697378396987915, Val Loss: 6.3078060519127614, Val MAE: 1.701762080192566\n",
      "Epoch 306/2000, Train Loss: 5.691073729726256, Val Loss: 6.302030835832868, Val MAE: 1.7016501426696777\n",
      "Epoch 307/2000, Train Loss: 5.6844734748765005, Val Loss: 6.295545572326297, Val MAE: 1.7018167972564697\n",
      "Epoch 308/2000, Train Loss: 5.6780001447970845, Val Loss: 6.289438650721595, Val MAE: 1.7018178701400757\n",
      "Epoch 309/2000, Train Loss: 5.6718296016052845, Val Loss: 6.283388864426386, Val MAE: 1.7017873525619507\n",
      "Epoch 310/2000, Train Loss: 5.665631454989671, Val Loss: 6.277298913115547, Val MAE: 1.7019954919815063\n",
      "Epoch 311/2000, Train Loss: 5.6594347772208184, Val Loss: 6.271540692874363, Val MAE: 1.7019667625427246\n",
      "Epoch 312/2000, Train Loss: 5.653493742848653, Val Loss: 6.265437438374474, Val MAE: 1.701987862586975\n",
      "Epoch 313/2000, Train Loss: 5.647329128679671, Val Loss: 6.260331284432184, Val MAE: 1.7018110752105713\n",
      "Epoch 314/2000, Train Loss: 5.641466041209836, Val Loss: 6.2546166351863315, Val MAE: 1.7017854452133179\n",
      "Epoch 315/2000, Train Loss: 5.635643811084656, Val Loss: 6.249074073064895, Val MAE: 1.7016961574554443\n",
      "Epoch 316/2000, Train Loss: 5.629814304737177, Val Loss: 6.243627397787003, Val MAE: 1.7016761302947998\n",
      "Epoch 317/2000, Train Loss: 5.624063729231045, Val Loss: 6.238241258121672, Val MAE: 1.7015734910964966\n",
      "Epoch 318/2000, Train Loss: 5.618280156543124, Val Loss: 6.23300221988133, Val MAE: 1.7013657093048096\n",
      "Epoch 319/2000, Train Loss: 5.612515384959233, Val Loss: 6.227181003207252, Val MAE: 1.7014153003692627\n",
      "Epoch 320/2000, Train Loss: 5.606729122748328, Val Loss: 6.221756884029934, Val MAE: 1.7012910842895508\n",
      "Epoch 321/2000, Train Loss: 5.6010380255318495, Val Loss: 6.216482298714774, Val MAE: 1.7011936902999878\n",
      "Epoch 322/2000, Train Loss: 5.595636787804629, Val Loss: 6.211115734917777, Val MAE: 1.7011853456497192\n",
      "Epoch 323/2000, Train Loss: 5.590127579079697, Val Loss: 6.206577266965594, Val MAE: 1.7008534669876099\n",
      "Epoch 324/2000, Train Loss: 5.584709790597009, Val Loss: 6.201157036281767, Val MAE: 1.7008066177368164\n",
      "Epoch 325/2000, Train Loss: 5.579320402844837, Val Loss: 6.1958788860411875, Val MAE: 1.7007948160171509\n",
      "Epoch 326/2000, Train Loss: 5.573803776914546, Val Loss: 6.191047242709568, Val MAE: 1.7005549669265747\n",
      "Epoch 327/2000, Train Loss: 5.568645587256665, Val Loss: 6.185998124735696, Val MAE: 1.7004603147506714\n",
      "Epoch 328/2000, Train Loss: 5.563456483889366, Val Loss: 6.181464473406474, Val MAE: 1.7002040147781372\n",
      "Epoch 329/2000, Train Loss: 5.558337649017867, Val Loss: 6.176545384384337, Val MAE: 1.7000764608383179\n",
      "Epoch 330/2000, Train Loss: 5.553141237145923, Val Loss: 6.171621734187717, Val MAE: 1.699885368347168\n",
      "Epoch 331/2000, Train Loss: 5.548073896400011, Val Loss: 6.166607748894465, Val MAE: 1.6998354196548462\n",
      "Epoch 332/2000, Train Loss: 5.542726465609918, Val Loss: 6.161973578589303, Val MAE: 1.699647068977356\n",
      "Epoch 333/2000, Train Loss: 5.537579401928212, Val Loss: 6.157120815345219, Val MAE: 1.6995561122894287\n",
      "Epoch 334/2000, Train Loss: 5.532470529607388, Val Loss: 6.15234785420554, Val MAE: 1.6994268894195557\n",
      "Epoch 335/2000, Train Loss: 5.52732288829026, Val Loss: 6.147560982477097, Val MAE: 1.6994038820266724\n",
      "Epoch 336/2000, Train Loss: 5.522267433080418, Val Loss: 6.143157658122835, Val MAE: 1.6990855932235718\n",
      "Epoch 337/2000, Train Loss: 5.51735650013465, Val Loss: 6.138575576600575, Val MAE: 1.6990532875061035\n",
      "Epoch 338/2000, Train Loss: 5.512528720796529, Val Loss: 6.134131204514277, Val MAE: 1.6988505125045776\n",
      "Epoch 339/2000, Train Loss: 5.507723781722893, Val Loss: 6.129485255195981, Val MAE: 1.6987203359603882\n",
      "Epoch 340/2000, Train Loss: 5.502886107845602, Val Loss: 6.125225572358994, Val MAE: 1.698528528213501\n",
      "Epoch 341/2000, Train Loss: 5.498340129684157, Val Loss: 6.120650637717474, Val MAE: 1.6984281539916992\n",
      "Epoch 342/2000, Train Loss: 5.493604041289207, Val Loss: 6.116578465416318, Val MAE: 1.6981687545776367\n",
      "Epoch 343/2000, Train Loss: 5.488964060297806, Val Loss: 6.112272901194436, Val MAE: 1.6980140209197998\n",
      "Epoch 344/2000, Train Loss: 5.48427855144603, Val Loss: 6.107882905574072, Val MAE: 1.697873592376709\n",
      "Epoch 345/2000, Train Loss: 5.479666645503347, Val Loss: 6.103854636351268, Val MAE: 1.6976991891860962\n",
      "Epoch 346/2000, Train Loss: 5.475186204035965, Val Loss: 6.0998721520106, Val MAE: 1.6974018812179565\n",
      "Epoch 347/2000, Train Loss: 5.470580286300401, Val Loss: 6.095494951520648, Val MAE: 1.6973342895507812\n",
      "Epoch 348/2000, Train Loss: 5.465930890297184, Val Loss: 6.091249170757475, Val MAE: 1.6972014904022217\n",
      "Epoch 349/2000, Train Loss: 5.461521427116878, Val Loss: 6.087029644421169, Val MAE: 1.6970301866531372\n",
      "Epoch 350/2000, Train Loss: 5.456994893688745, Val Loss: 6.082855514117649, Val MAE: 1.6969101428985596\n",
      "Epoch 351/2000, Train Loss: 5.452586815622865, Val Loss: 6.078828212760744, Val MAE: 1.6967395544052124\n",
      "Epoch 352/2000, Train Loss: 5.4482248231285215, Val Loss: 6.075028533027286, Val MAE: 1.6966127157211304\n",
      "Epoch 353/2000, Train Loss: 5.444023296426146, Val Loss: 6.071327220825922, Val MAE: 1.696295976638794\n",
      "Epoch 354/2000, Train Loss: 5.439798762330888, Val Loss: 6.067500528835115, Val MAE: 1.696087121963501\n",
      "Epoch 355/2000, Train Loss: 5.43562583829183, Val Loss: 6.063736234392438, Val MAE: 1.6959114074707031\n",
      "Epoch 356/2000, Train Loss: 5.431433713990979, Val Loss: 6.059873910177322, Val MAE: 1.6957716941833496\n",
      "Epoch 357/2000, Train Loss: 5.427323161462803, Val Loss: 6.055941913809095, Val MAE: 1.695635437965393\n",
      "Epoch 358/2000, Train Loss: 5.423069500620509, Val Loss: 6.0524712687446955, Val MAE: 1.6953580379486084\n",
      "Epoch 359/2000, Train Loss: 5.41896563154686, Val Loss: 6.048535619463239, Val MAE: 1.6954737901687622\n",
      "Epoch 360/2000, Train Loss: 5.4147274840533814, Val Loss: 6.0444946374212, Val MAE: 1.695493459701538\n",
      "Epoch 361/2000, Train Loss: 5.410642142356367, Val Loss: 6.041021855104537, Val MAE: 1.695281744003296\n",
      "Epoch 362/2000, Train Loss: 5.4066527631286165, Val Loss: 6.037332699412391, Val MAE: 1.6952896118164062\n",
      "Epoch 363/2000, Train Loss: 5.402796245596472, Val Loss: 6.03385085151309, Val MAE: 1.695082664489746\n",
      "Epoch 364/2000, Train Loss: 5.398735824525777, Val Loss: 6.030433978353228, Val MAE: 1.694901943206787\n",
      "Epoch 365/2000, Train Loss: 5.394858256381725, Val Loss: 6.0271272999899725, Val MAE: 1.6945470571517944\n",
      "Epoch 366/2000, Train Loss: 5.391095802377074, Val Loss: 6.023581817036583, Val MAE: 1.6944806575775146\n",
      "Epoch 367/2000, Train Loss: 5.387323233909768, Val Loss: 6.020438117640359, Val MAE: 1.694183349609375\n",
      "Epoch 368/2000, Train Loss: 5.383578127967286, Val Loss: 6.016867047264462, Val MAE: 1.694165587425232\n",
      "Epoch 369/2000, Train Loss: 5.379743023214623, Val Loss: 6.013551941939762, Val MAE: 1.6939188241958618\n",
      "Epoch 370/2000, Train Loss: 5.37609859229143, Val Loss: 6.009821579569862, Val MAE: 1.6940467357635498\n",
      "Epoch 371/2000, Train Loss: 5.372268149479488, Val Loss: 6.006878214223044, Val MAE: 1.693662166595459\n",
      "Epoch 372/2000, Train Loss: 5.368610271445787, Val Loss: 6.003464446181343, Val MAE: 1.6936434507369995\n",
      "Epoch 373/2000, Train Loss: 5.364958937985268, Val Loss: 6.00027566864377, Val MAE: 1.6934922933578491\n",
      "Epoch 374/2000, Train Loss: 5.361322344442349, Val Loss: 5.99699406396775, Val MAE: 1.6933863162994385\n",
      "Epoch 375/2000, Train Loss: 5.357805102096794, Val Loss: 5.993818178063347, Val MAE: 1.693194031715393\n",
      "Epoch 376/2000, Train Loss: 5.354164564390949, Val Loss: 5.9907244216828115, Val MAE: 1.6930643320083618\n",
      "Epoch 377/2000, Train Loss: 5.3508291157075485, Val Loss: 5.987279290244693, Val MAE: 1.6930456161499023\n",
      "Epoch 378/2000, Train Loss: 5.347084084417991, Val Loss: 5.984573957465944, Val MAE: 1.6926378011703491\n",
      "Epoch 379/2000, Train Loss: 5.343618326361996, Val Loss: 5.981139347666786, Val MAE: 1.6926634311676025\n",
      "Epoch 380/2000, Train Loss: 5.33990717606753, Val Loss: 5.978167159216745, Val MAE: 1.6924067735671997\n",
      "Epoch 381/2000, Train Loss: 5.336426346022924, Val Loss: 5.974826103165036, Val MAE: 1.6923192739486694\n",
      "Epoch 382/2000, Train Loss: 5.332986359535723, Val Loss: 5.972098012765248, Val MAE: 1.6919788122177124\n",
      "Epoch 383/2000, Train Loss: 5.32965444593739, Val Loss: 5.9688369660150435, Val MAE: 1.6919794082641602\n",
      "Epoch 384/2000, Train Loss: 5.326247810476758, Val Loss: 5.966296241396949, Val MAE: 1.6914516687393188\n",
      "Epoch 385/2000, Train Loss: 5.322856784707568, Val Loss: 5.963394182068961, Val MAE: 1.6913601160049438\n",
      "Epoch 386/2000, Train Loss: 5.31958482362992, Val Loss: 5.960559756982894, Val MAE: 1.6911530494689941\n",
      "Epoch 387/2000, Train Loss: 5.316403839921077, Val Loss: 5.957578451860519, Val MAE: 1.6909517049789429\n",
      "Epoch 388/2000, Train Loss: 5.313213488615114, Val Loss: 5.954786045210702, Val MAE: 1.6906934976577759\n",
      "Epoch 389/2000, Train Loss: 5.30988094282083, Val Loss: 5.951945077805292, Val MAE: 1.6904703378677368\n",
      "Epoch 390/2000, Train Loss: 5.306631562740073, Val Loss: 5.949179581233433, Val MAE: 1.6902827024459839\n",
      "Epoch 391/2000, Train Loss: 5.303562165988013, Val Loss: 5.946520379611424, Val MAE: 1.6899045705795288\n",
      "Epoch 392/2000, Train Loss: 5.300439242751709, Val Loss: 5.94371603784107, Val MAE: 1.6896758079528809\n",
      "Epoch 393/2000, Train Loss: 5.297225018986862, Val Loss: 5.941001369839623, Val MAE: 1.6895310878753662\n",
      "Epoch 394/2000, Train Loss: 5.294173079249889, Val Loss: 5.938459884552729, Val MAE: 1.6891908645629883\n",
      "Epoch 395/2000, Train Loss: 5.29118070615868, Val Loss: 5.935980893316723, Val MAE: 1.6887481212615967\n",
      "Epoch 396/2000, Train Loss: 5.288112570435776, Val Loss: 5.932924032211304, Val MAE: 1.6888463497161865\n",
      "Epoch 397/2000, Train Loss: 5.2850427193769445, Val Loss: 5.930523378508432, Val MAE: 1.6884496212005615\n",
      "Epoch 398/2000, Train Loss: 5.282044930249579, Val Loss: 5.928037901719411, Val MAE: 1.6880601644515991\n",
      "Epoch 399/2000, Train Loss: 5.279126971665828, Val Loss: 5.925489079384577, Val MAE: 1.687726378440857\n",
      "Epoch 400/2000, Train Loss: 5.276192792043699, Val Loss: 5.922838213897887, Val MAE: 1.6875410079956055\n",
      "Epoch 401/2000, Train Loss: 5.2732097403118745, Val Loss: 5.92061272973106, Val MAE: 1.6869611740112305\n",
      "Epoch 402/2000, Train Loss: 5.270435875326353, Val Loss: 5.918220520019531, Val MAE: 1.6866847276687622\n",
      "Epoch 403/2000, Train Loss: 5.267501672333151, Val Loss: 5.915578634965987, Val MAE: 1.6864783763885498\n",
      "Epoch 404/2000, Train Loss: 5.264443011478914, Val Loss: 5.913027002697899, Val MAE: 1.6862987279891968\n",
      "Epoch 405/2000, Train Loss: 5.2615443927108485, Val Loss: 5.910568961075374, Val MAE: 1.6860380172729492\n",
      "Epoch 406/2000, Train Loss: 5.25872749737523, Val Loss: 5.908121137391953, Val MAE: 1.6858124732971191\n",
      "Epoch 407/2000, Train Loss: 5.255977336711372, Val Loss: 5.9060372568312145, Val MAE: 1.6853657960891724\n",
      "Epoch 408/2000, Train Loss: 5.253229216560826, Val Loss: 5.9035123984018965, Val MAE: 1.685179591178894\n",
      "Epoch 409/2000, Train Loss: 5.250365985633624, Val Loss: 5.901178005195799, Val MAE: 1.6849008798599243\n",
      "Epoch 410/2000, Train Loss: 5.247663855720811, Val Loss: 5.898995913210369, Val MAE: 1.6844377517700195\n",
      "Epoch 411/2000, Train Loss: 5.245029202973725, Val Loss: 5.8967504643258595, Val MAE: 1.6842317581176758\n",
      "Epoch 412/2000, Train Loss: 5.242332403347086, Val Loss: 5.894461597715106, Val MAE: 1.683791160583496\n",
      "Epoch 413/2000, Train Loss: 5.2396952218161985, Val Loss: 5.892113089561462, Val MAE: 1.6836079359054565\n",
      "Epoch 414/2000, Train Loss: 5.237011155557565, Val Loss: 5.890132410185678, Val MAE: 1.6831415891647339\n",
      "Epoch 415/2000, Train Loss: 5.234408237365809, Val Loss: 5.888169254575457, Val MAE: 1.6825408935546875\n",
      "Epoch 416/2000, Train Loss: 5.231902409003383, Val Loss: 5.88575203645797, Val MAE: 1.682597041130066\n",
      "Epoch 417/2000, Train Loss: 5.229237103159572, Val Loss: 5.8836336107481095, Val MAE: 1.6821216344833374\n",
      "Epoch 418/2000, Train Loss: 5.226631081490994, Val Loss: 5.881671139172146, Val MAE: 1.6817541122436523\n",
      "Epoch 419/2000, Train Loss: 5.224105404529652, Val Loss: 5.879448036352794, Val MAE: 1.681487798690796\n",
      "Epoch 420/2000, Train Loss: 5.2214840941435865, Val Loss: 5.877411198048365, Val MAE: 1.681121826171875\n",
      "Epoch 421/2000, Train Loss: 5.2188807823425964, Val Loss: 5.875243802865346, Val MAE: 1.6809786558151245\n",
      "Epoch 422/2000, Train Loss: 5.216586559212258, Val Loss: 5.873282341730027, Val MAE: 1.6804594993591309\n",
      "Epoch 423/2000, Train Loss: 5.213980744688736, Val Loss: 5.871413619745345, Val MAE: 1.680100440979004\n",
      "Epoch 424/2000, Train Loss: 5.21152951384465, Val Loss: 5.8691114981969195, Val MAE: 1.6799885034561157\n",
      "Epoch 425/2000, Train Loss: 5.209044125251609, Val Loss: 5.867219561622257, Val MAE: 1.6795262098312378\n",
      "Epoch 426/2000, Train Loss: 5.2065747547553185, Val Loss: 5.865405545348213, Val MAE: 1.6790924072265625\n",
      "Epoch 427/2000, Train Loss: 5.204192450419804, Val Loss: 5.863297987551916, Val MAE: 1.6789498329162598\n",
      "Epoch 428/2000, Train Loss: 5.201749121688821, Val Loss: 5.861523140044439, Val MAE: 1.67839777469635\n",
      "Epoch 429/2000, Train Loss: 5.199589091060192, Val Loss: 5.859646754605429, Val MAE: 1.6781200170516968\n",
      "Epoch 430/2000, Train Loss: 5.197178370021126, Val Loss: 5.857916389192853, Val MAE: 1.677587628364563\n",
      "Epoch 431/2000, Train Loss: 5.194946360016407, Val Loss: 5.855899186361404, Val MAE: 1.6773321628570557\n",
      "Epoch 432/2000, Train Loss: 5.192642484632635, Val Loss: 5.854309206917172, Val MAE: 1.6767140626907349\n",
      "Epoch 433/2000, Train Loss: 5.190414204079611, Val Loss: 5.852498031797863, Val MAE: 1.676436424255371\n",
      "Epoch 434/2000, Train Loss: 5.188227773889668, Val Loss: 5.850833262716021, Val MAE: 1.6759670972824097\n",
      "Epoch 435/2000, Train Loss: 5.185902125744625, Val Loss: 5.848855001585824, Val MAE: 1.6758077144622803\n",
      "Epoch 436/2000, Train Loss: 5.183552420526028, Val Loss: 5.847212621143886, Val MAE: 1.6753953695297241\n",
      "Epoch 437/2000, Train Loss: 5.181317118898938, Val Loss: 5.845343552884602, Val MAE: 1.6751482486724854\n",
      "Epoch 438/2000, Train Loss: 5.179202742569873, Val Loss: 5.843546540964217, Val MAE: 1.6748197078704834\n",
      "Epoch 439/2000, Train Loss: 5.177046919407058, Val Loss: 5.8420585450671965, Val MAE: 1.6742455959320068\n",
      "Epoch 440/2000, Train Loss: 5.174852806354948, Val Loss: 5.840346963632674, Val MAE: 1.673949956893921\n",
      "Epoch 441/2000, Train Loss: 5.172659380311522, Val Loss: 5.838476725987026, Val MAE: 1.6735320091247559\n",
      "Epoch 442/2000, Train Loss: 5.170542169357052, Val Loss: 5.837044216337658, Val MAE: 1.6729596853256226\n",
      "Epoch 443/2000, Train Loss: 5.1685489110112695, Val Loss: 5.835506876309712, Val MAE: 1.6726088523864746\n",
      "Epoch 444/2000, Train Loss: 5.166454656672243, Val Loss: 5.83385709070024, Val MAE: 1.6721351146697998\n",
      "Epoch 445/2000, Train Loss: 5.164435145885214, Val Loss: 5.832327624162038, Val MAE: 1.6717129945755005\n",
      "Epoch 446/2000, Train Loss: 5.162382654982326, Val Loss: 5.830739518006642, Val MAE: 1.671553134918213\n",
      "Epoch 447/2000, Train Loss: 5.160343342674804, Val Loss: 5.829202030386243, Val MAE: 1.6710771322250366\n",
      "Epoch 448/2000, Train Loss: 5.158326105103002, Val Loss: 5.827694929781414, Val MAE: 1.6706622838974\n",
      "Epoch 449/2000, Train Loss: 5.156306415754246, Val Loss: 5.82618339572634, Val MAE: 1.6702426671981812\n",
      "Epoch 450/2000, Train Loss: 5.154298953176049, Val Loss: 5.824716221718561, Val MAE: 1.669724464416504\n",
      "Epoch 451/2000, Train Loss: 5.152304243133502, Val Loss: 5.823337177435557, Val MAE: 1.669257402420044\n",
      "Epoch 452/2000, Train Loss: 5.150381638401822, Val Loss: 5.821870653402238, Val MAE: 1.668820858001709\n",
      "Epoch 453/2000, Train Loss: 5.148426416394405, Val Loss: 5.8203893928300765, Val MAE: 1.6684904098510742\n",
      "Epoch 454/2000, Train Loss: 5.146510456445691, Val Loss: 5.819015031769162, Val MAE: 1.6680586338043213\n",
      "Epoch 455/2000, Train Loss: 5.144663604761885, Val Loss: 5.817395638851893, Val MAE: 1.6678991317749023\n",
      "Epoch 456/2000, Train Loss: 5.142780773394199, Val Loss: 5.816157017435346, Val MAE: 1.6673548221588135\n",
      "Epoch 457/2000, Train Loss: 5.141017326010635, Val Loss: 5.815028780982608, Val MAE: 1.666603446006775\n",
      "Epoch 458/2000, Train Loss: 5.139092628306831, Val Loss: 5.81332878839402, Val MAE: 1.6666430234909058\n",
      "Epoch 459/2000, Train Loss: 5.13717207459704, Val Loss: 5.812086423238118, Val MAE: 1.6663233041763306\n",
      "Epoch 460/2000, Train Loss: 5.135406562740611, Val Loss: 5.810892124970754, Val MAE: 1.6657174825668335\n",
      "Epoch 461/2000, Train Loss: 5.133659288476317, Val Loss: 5.809178786618369, Val MAE: 1.6656512022018433\n",
      "Epoch 462/2000, Train Loss: 5.131556679735063, Val Loss: 5.8079381954102285, Val MAE: 1.6651957035064697\n",
      "Epoch 463/2000, Train Loss: 5.12991081306393, Val Loss: 5.806755134037563, Val MAE: 1.664750576019287\n",
      "Epoch 464/2000, Train Loss: 5.128127020067826, Val Loss: 5.805358012517293, Val MAE: 1.6644225120544434\n",
      "Epoch 465/2000, Train Loss: 5.1263214970173046, Val Loss: 5.804141203562419, Val MAE: 1.663927674293518\n",
      "Epoch 466/2000, Train Loss: 5.124629602445702, Val Loss: 5.802945063227699, Val MAE: 1.6634739637374878\n",
      "Epoch 467/2000, Train Loss: 5.122846726470001, Val Loss: 5.801634518873124, Val MAE: 1.6631494760513306\n",
      "Epoch 468/2000, Train Loss: 5.121053707952728, Val Loss: 5.800311429159982, Val MAE: 1.6629021167755127\n",
      "Epoch 469/2000, Train Loss: 5.119322901215977, Val Loss: 5.799130783194587, Val MAE: 1.662309169769287\n",
      "Epoch 470/2000, Train Loss: 5.117663478313279, Val Loss: 5.797972747257778, Val MAE: 1.6618707180023193\n",
      "Epoch 471/2000, Train Loss: 5.115893599512883, Val Loss: 5.796748765877315, Val MAE: 1.6615110635757446\n",
      "Epoch 472/2000, Train Loss: 5.1142430890598485, Val Loss: 5.7955760444913595, Val MAE: 1.6609892845153809\n",
      "Epoch 473/2000, Train Loss: 5.112596057534049, Val Loss: 5.794358188197727, Val MAE: 1.6607820987701416\n",
      "Epoch 474/2000, Train Loss: 5.110962220408516, Val Loss: 5.7931617214566185, Val MAE: 1.6604337692260742\n",
      "Epoch 475/2000, Train Loss: 5.109324032227983, Val Loss: 5.792083850928715, Val MAE: 1.6599763631820679\n",
      "Epoch 476/2000, Train Loss: 5.107739640896344, Val Loss: 5.790986373310997, Val MAE: 1.6594117879867554\n",
      "Epoch 477/2000, Train Loss: 5.106085802167024, Val Loss: 5.78988178003402, Val MAE: 1.659018874168396\n",
      "Epoch 478/2000, Train Loss: 5.1045288331754115, Val Loss: 5.788536452111744, Val MAE: 1.6589257717132568\n",
      "Epoch 479/2000, Train Loss: 5.102841484866122, Val Loss: 5.787503685270037, Val MAE: 1.658286690711975\n",
      "Epoch 480/2000, Train Loss: 5.101383834362703, Val Loss: 5.7864183357783725, Val MAE: 1.6581639051437378\n",
      "Epoch 481/2000, Train Loss: 5.099786681416004, Val Loss: 5.785426525842576, Val MAE: 1.6577117443084717\n",
      "Epoch 482/2000, Train Loss: 5.098278935268332, Val Loss: 5.784433211599078, Val MAE: 1.6571894884109497\n",
      "Epoch 483/2000, Train Loss: 5.096800262063784, Val Loss: 5.783375686123257, Val MAE: 1.6567604541778564\n",
      "Epoch 484/2000, Train Loss: 5.095296546669706, Val Loss: 5.782257134006137, Val MAE: 1.6564382314682007\n",
      "Epoch 485/2000, Train Loss: 5.09380566350832, Val Loss: 5.781299755686805, Val MAE: 1.6557759046554565\n",
      "Epoch 486/2000, Train Loss: 5.0922891444648775, Val Loss: 5.780370556172871, Val MAE: 1.6554155349731445\n",
      "Epoch 487/2000, Train Loss: 5.090722218555187, Val Loss: 5.7793448851222085, Val MAE: 1.6550180912017822\n",
      "Epoch 488/2000, Train Loss: 5.0892597193105935, Val Loss: 5.778352510361445, Val MAE: 1.6548001766204834\n",
      "Epoch 489/2000, Train Loss: 5.087806714018242, Val Loss: 5.7774092725345065, Val MAE: 1.6542928218841553\n",
      "Epoch 490/2000, Train Loss: 5.086412987688869, Val Loss: 5.776436073439462, Val MAE: 1.6538957357406616\n",
      "Epoch 491/2000, Train Loss: 5.08504824779602, Val Loss: 5.775526151770637, Val MAE: 1.6533777713775635\n",
      "Epoch 492/2000, Train Loss: 5.0837125747933545, Val Loss: 5.7748051683108015, Val MAE: 1.6529083251953125\n",
      "Epoch 493/2000, Train Loss: 5.08237088937181, Val Loss: 5.773856821514311, Val MAE: 1.652537226676941\n",
      "Epoch 494/2000, Train Loss: 5.080970449407279, Val Loss: 5.773022861707778, Val MAE: 1.6520229578018188\n",
      "Epoch 495/2000, Train Loss: 5.079507165298139, Val Loss: 5.7719166449138095, Val MAE: 1.6517972946166992\n",
      "Epoch 496/2000, Train Loss: 5.078089623256194, Val Loss: 5.771116898173378, Val MAE: 1.651379942893982\n",
      "Epoch 497/2000, Train Loss: 5.076733716956651, Val Loss: 5.770242688201723, Val MAE: 1.6509521007537842\n",
      "Epoch 498/2000, Train Loss: 5.075388567403946, Val Loss: 5.769341741289411, Val MAE: 1.6505868434906006\n",
      "Epoch 499/2000, Train Loss: 5.074046452391803, Val Loss: 5.7684473139899115, Val MAE: 1.6502580642700195\n",
      "Epoch 500/2000, Train Loss: 5.072746970589642, Val Loss: 5.76755401350203, Val MAE: 1.6497865915298462\n",
      "Epoch 501/2000, Train Loss: 5.0714764537865085, Val Loss: 5.766915568283626, Val MAE: 1.64914071559906\n",
      "Epoch 502/2000, Train Loss: 5.070130928951527, Val Loss: 5.765981589044843, Val MAE: 1.6489650011062622\n",
      "Epoch 503/2000, Train Loss: 5.068777104695189, Val Loss: 5.765105871927171, Val MAE: 1.6486345529556274\n",
      "Epoch 504/2000, Train Loss: 5.0674616311938205, Val Loss: 5.764295177800315, Val MAE: 1.6481999158859253\n",
      "Epoch 505/2000, Train Loss: 5.066199819863431, Val Loss: 5.763401037170773, Val MAE: 1.6480903625488281\n",
      "Epoch 506/2000, Train Loss: 5.0648902706770365, Val Loss: 5.762553938797542, Val MAE: 1.6477032899856567\n",
      "Epoch 507/2000, Train Loss: 5.063608964508444, Val Loss: 5.761925572440738, Val MAE: 1.6471234560012817\n",
      "Epoch 508/2000, Train Loss: 5.062518682819496, Val Loss: 5.7612013731684, Val MAE: 1.6465860605239868\n",
      "Epoch 509/2000, Train Loss: 5.0612895619886045, Val Loss: 5.760378982339587, Val MAE: 1.6461374759674072\n",
      "Epoch 510/2000, Train Loss: 5.060015386519546, Val Loss: 5.75974057118098, Val MAE: 1.6456760168075562\n",
      "Epoch 511/2000, Train Loss: 5.058836274826308, Val Loss: 5.758850608553205, Val MAE: 1.6455258131027222\n",
      "Epoch 512/2000, Train Loss: 5.057639984216945, Val Loss: 5.758227109909058, Val MAE: 1.644862413406372\n",
      "Epoch 513/2000, Train Loss: 5.056419985923175, Val Loss: 5.757575117406391, Val MAE: 1.644471287727356\n",
      "Epoch 514/2000, Train Loss: 5.055181955332817, Val Loss: 5.756689344133649, Val MAE: 1.6441258192062378\n",
      "Epoch 515/2000, Train Loss: 5.054031167615452, Val Loss: 5.756032194410052, Val MAE: 1.6437777280807495\n",
      "Epoch 516/2000, Train Loss: 5.052908028298608, Val Loss: 5.755287116482144, Val MAE: 1.643530011177063\n",
      "Epoch 517/2000, Train Loss: 5.051745600021104, Val Loss: 5.754627795446487, Val MAE: 1.6430602073669434\n",
      "Epoch 518/2000, Train Loss: 5.050682689978811, Val Loss: 5.753896977220263, Val MAE: 1.6428171396255493\n",
      "Epoch 519/2000, Train Loss: 5.04950939112557, Val Loss: 5.753363702978406, Val MAE: 1.6423274278640747\n",
      "Epoch 520/2000, Train Loss: 5.048409790851502, Val Loss: 5.752636438324338, Val MAE: 1.6419107913970947\n",
      "Epoch 521/2000, Train Loss: 5.047317507909282, Val Loss: 5.752084405649276, Val MAE: 1.6414903402328491\n",
      "Epoch 522/2000, Train Loss: 5.046203948378731, Val Loss: 5.751423279444377, Val MAE: 1.641081690788269\n",
      "Epoch 523/2000, Train Loss: 5.045104542303153, Val Loss: 5.750689506530762, Val MAE: 1.6407434940338135\n",
      "Epoch 524/2000, Train Loss: 5.0440673209380025, Val Loss: 5.750118965194339, Val MAE: 1.6404179334640503\n",
      "Epoch 525/2000, Train Loss: 5.042898375157408, Val Loss: 5.749553583917164, Val MAE: 1.6398751735687256\n",
      "Epoch 526/2000, Train Loss: 5.0418510975050825, Val Loss: 5.748953487191882, Val MAE: 1.6394215822219849\n",
      "Epoch 527/2000, Train Loss: 5.040854430837591, Val Loss: 5.748226421219962, Val MAE: 1.639224648475647\n",
      "Epoch 528/2000, Train Loss: 5.039724387302049, Val Loss: 5.747661792096638, Val MAE: 1.6385847330093384\n",
      "Epoch 529/2000, Train Loss: 5.038713385255112, Val Loss: 5.7471715694382075, Val MAE: 1.6381621360778809\n",
      "Epoch 530/2000, Train Loss: 5.037637974210451, Val Loss: 5.74658618370692, Val MAE: 1.6376756429672241\n",
      "Epoch 531/2000, Train Loss: 5.036631348102823, Val Loss: 5.745896594864981, Val MAE: 1.6375045776367188\n",
      "Epoch 532/2000, Train Loss: 5.035537801777526, Val Loss: 5.745287387143998, Val MAE: 1.6372004747390747\n",
      "Epoch 533/2000, Train Loss: 5.034475701147814, Val Loss: 5.744658819266728, Val MAE: 1.636780858039856\n",
      "Epoch 534/2000, Train Loss: 5.033452003059334, Val Loss: 5.744138814154125, Val MAE: 1.636377215385437\n",
      "Epoch 535/2000, Train Loss: 5.032444693642039, Val Loss: 5.743526007447924, Val MAE: 1.635998010635376\n",
      "Epoch 536/2000, Train Loss: 5.031485597909085, Val Loss: 5.743083306721279, Val MAE: 1.6353999376296997\n",
      "Epoch 537/2000, Train Loss: 5.030575527122562, Val Loss: 5.742490944408235, Val MAE: 1.6353285312652588\n",
      "Epoch 538/2000, Train Loss: 5.029566353904175, Val Loss: 5.742034775870187, Val MAE: 1.634814977645874\n",
      "Epoch 539/2000, Train Loss: 5.028638999115765, Val Loss: 5.741479164078122, Val MAE: 1.6344797611236572\n",
      "Epoch 540/2000, Train Loss: 5.02764445736647, Val Loss: 5.741046408812205, Val MAE: 1.6339762210845947\n",
      "Epoch 541/2000, Train Loss: 5.026742909623806, Val Loss: 5.740501540047782, Val MAE: 1.633550763130188\n",
      "Epoch 542/2000, Train Loss: 5.025806351676478, Val Loss: 5.740013724281674, Val MAE: 1.633275032043457\n",
      "Epoch 543/2000, Train Loss: 5.024884200735052, Val Loss: 5.739513238271077, Val MAE: 1.632902979850769\n",
      "Epoch 544/2000, Train Loss: 5.023992273131614, Val Loss: 5.739133820647285, Val MAE: 1.6322991847991943\n",
      "Epoch 545/2000, Train Loss: 5.023035880372622, Val Loss: 5.73860228913171, Val MAE: 1.6321552991867065\n",
      "Epoch 546/2000, Train Loss: 5.022135341453283, Val Loss: 5.738093858673459, Val MAE: 1.6317552328109741\n",
      "Epoch 547/2000, Train Loss: 5.0211726102573415, Val Loss: 5.737672592912402, Val MAE: 1.6313416957855225\n",
      "Epoch 548/2000, Train Loss: 5.020235602038536, Val Loss: 5.7371753965105325, Val MAE: 1.6309759616851807\n",
      "Epoch 549/2000, Train Loss: 5.019384488316282, Val Loss: 5.736601920354934, Val MAE: 1.6306848526000977\n",
      "Epoch 550/2000, Train Loss: 5.018428443012857, Val Loss: 5.736183248815083, Val MAE: 1.630232334136963\n",
      "Epoch 551/2000, Train Loss: 5.017647975590736, Val Loss: 5.73584277573086, Val MAE: 1.6297495365142822\n",
      "Epoch 552/2000, Train Loss: 5.016836731322898, Val Loss: 5.735480654807318, Val MAE: 1.6292041540145874\n",
      "Epoch 553/2000, Train Loss: 5.015866406041241, Val Loss: 5.734984542642321, Val MAE: 1.6289739608764648\n",
      "Epoch 554/2000, Train Loss: 5.015029222571799, Val Loss: 5.734548537504105, Val MAE: 1.6286653280258179\n",
      "Epoch 555/2000, Train Loss: 5.01417409146286, Val Loss: 5.733960174378895, Val MAE: 1.6287431716918945\n",
      "Epoch 556/2000, Train Loss: 5.013279990853981, Val Loss: 5.733543631576357, Val MAE: 1.628237009048462\n",
      "Epoch 557/2000, Train Loss: 5.012461018663199, Val Loss: 5.733084485644386, Val MAE: 1.6280077695846558\n",
      "Epoch 558/2000, Train Loss: 5.01165878991314, Val Loss: 5.73282185622624, Val MAE: 1.6275856494903564\n",
      "Epoch 559/2000, Train Loss: 5.0108455430469325, Val Loss: 5.732372738066173, Val MAE: 1.6272985935211182\n",
      "Epoch 560/2000, Train Loss: 5.010035922564303, Val Loss: 5.732057432333629, Val MAE: 1.6267389059066772\n",
      "Epoch 561/2000, Train Loss: 5.0092863256403355, Val Loss: 5.731628886290959, Val MAE: 1.6263548135757446\n",
      "Epoch 562/2000, Train Loss: 5.008394971721095, Val Loss: 5.731265647070749, Val MAE: 1.6260052919387817\n",
      "Epoch 563/2000, Train Loss: 5.007825482547367, Val Loss: 5.730822441123781, Val MAE: 1.6260550022125244\n",
      "Epoch 564/2000, Train Loss: 5.006902433753182, Val Loss: 5.730602091266995, Val MAE: 1.625372052192688\n",
      "Epoch 565/2000, Train Loss: 5.006159350969559, Val Loss: 5.7302355624380565, Val MAE: 1.624918818473816\n",
      "Epoch 566/2000, Train Loss: 5.005422031215283, Val Loss: 5.729909252552759, Val MAE: 1.624518632888794\n",
      "Epoch 567/2000, Train Loss: 5.004680736949313, Val Loss: 5.729611995674315, Val MAE: 1.6240812540054321\n",
      "Epoch 568/2000, Train Loss: 5.003943410679719, Val Loss: 5.729186188606989, Val MAE: 1.623734474182129\n",
      "Epoch 569/2000, Train Loss: 5.00314998626709, Val Loss: 5.728847188608987, Val MAE: 1.6233526468276978\n",
      "Epoch 570/2000, Train Loss: 5.002397849294127, Val Loss: 5.728564449719021, Val MAE: 1.623016595840454\n",
      "Epoch 571/2000, Train Loss: 5.001768390617855, Val Loss: 5.7281440780276345, Val MAE: 1.622865915298462\n",
      "Epoch 572/2000, Train Loss: 5.001003919770922, Val Loss: 5.72796904189246, Val MAE: 1.6222528219223022\n",
      "Epoch 573/2000, Train Loss: 5.000319796321422, Val Loss: 5.7276769024985175, Val MAE: 1.6218725442886353\n",
      "Epoch 574/2000, Train Loss: 4.999601734037628, Val Loss: 5.727251699992588, Val MAE: 1.6217260360717773\n",
      "Epoch 575/2000, Train Loss: 4.998809609998265, Val Loss: 5.726940944081261, Val MAE: 1.6213327646255493\n",
      "Epoch 576/2000, Train Loss: 4.998113714253112, Val Loss: 5.726594956148238, Val MAE: 1.6210764646530151\n",
      "Epoch 577/2000, Train Loss: 4.997394646676875, Val Loss: 5.726300392832075, Val MAE: 1.6207990646362305\n",
      "Epoch 578/2000, Train Loss: 4.996737533294936, Val Loss: 5.726017682325272, Val MAE: 1.620268702507019\n",
      "Epoch 579/2000, Train Loss: 4.995976775422251, Val Loss: 5.725698246842339, Val MAE: 1.6198917627334595\n",
      "Epoch 580/2000, Train Loss: 4.995277632779228, Val Loss: 5.7253832675161815, Val MAE: 1.6196650266647339\n",
      "Epoch 581/2000, Train Loss: 4.994659146737985, Val Loss: 5.725231355144864, Val MAE: 1.619111180305481\n",
      "Epoch 582/2000, Train Loss: 4.994023706243809, Val Loss: 5.724819680054982, Val MAE: 1.6189720630645752\n",
      "Epoch 583/2000, Train Loss: 4.9933129619644125, Val Loss: 5.724662667229062, Val MAE: 1.618316650390625\n",
      "Epoch 584/2000, Train Loss: 4.992664874524761, Val Loss: 5.724367442585173, Val MAE: 1.6180249452590942\n",
      "Epoch 585/2000, Train Loss: 4.992008213936358, Val Loss: 5.723991771539052, Val MAE: 1.6178429126739502\n",
      "Epoch 586/2000, Train Loss: 4.991376597050719, Val Loss: 5.723635239260537, Val MAE: 1.6177992820739746\n",
      "Epoch 587/2000, Train Loss: 4.990611249872592, Val Loss: 5.723447515850975, Val MAE: 1.617248296737671\n",
      "Epoch 588/2000, Train Loss: 4.989976971711023, Val Loss: 5.723156037784758, Val MAE: 1.6169172525405884\n",
      "Epoch 589/2000, Train Loss: 4.989383349801993, Val Loss: 5.722783784071605, Val MAE: 1.6170574426651\n",
      "Epoch 590/2000, Train Loss: 4.988655820383844, Val Loss: 5.722455504394713, Val MAE: 1.6168569326400757\n",
      "Epoch 591/2000, Train Loss: 4.98809603676305, Val Loss: 5.72226559548151, Val MAE: 1.6161977052688599\n",
      "Epoch 592/2000, Train Loss: 4.987419737410982, Val Loss: 5.721932703540439, Val MAE: 1.6161998510360718\n",
      "Epoch 593/2000, Train Loss: 4.986752031215996, Val Loss: 5.721741043386006, Val MAE: 1.6157448291778564\n",
      "Epoch 594/2000, Train Loss: 4.98613455937847, Val Loss: 5.721383211158571, Val MAE: 1.6157420873641968\n",
      "Epoch 595/2000, Train Loss: 4.985633000332142, Val Loss: 5.721043067319052, Val MAE: 1.6155725717544556\n",
      "Epoch 596/2000, Train Loss: 4.985036139763965, Val Loss: 5.720919118041084, Val MAE: 1.6150768995285034\n",
      "Epoch 597/2000, Train Loss: 4.984298894368375, Val Loss: 5.720710936046782, Val MAE: 1.6147161722183228\n",
      "Epoch 598/2000, Train Loss: 4.983743481642773, Val Loss: 5.720466037591298, Val MAE: 1.6144551038742065\n",
      "Epoch 599/2000, Train Loss: 4.983123137348966, Val Loss: 5.72023313102268, Val MAE: 1.6141626834869385\n",
      "Epoch 600/2000, Train Loss: 4.982539381395778, Val Loss: 5.720106976372855, Val MAE: 1.6135218143463135\n",
      "Epoch 601/2000, Train Loss: 4.982012819756237, Val Loss: 5.71981327022825, Val MAE: 1.6132211685180664\n",
      "Epoch 602/2000, Train Loss: 4.981459582642204, Val Loss: 5.719631740025112, Val MAE: 1.6129724979400635\n",
      "Epoch 603/2000, Train Loss: 4.980837451386015, Val Loss: 5.719368463470822, Val MAE: 1.6125720739364624\n",
      "Epoch 604/2000, Train Loss: 4.980274388080591, Val Loss: 5.71924954085123, Val MAE: 1.6120136976242065\n",
      "Epoch 605/2000, Train Loss: 4.979656956595998, Val Loss: 5.718983772255125, Val MAE: 1.6118957996368408\n",
      "Epoch 606/2000, Train Loss: 4.979110167628451, Val Loss: 5.71864918867747, Val MAE: 1.6117268800735474\n",
      "Epoch 607/2000, Train Loss: 4.978509650075721, Val Loss: 5.718490785076504, Val MAE: 1.6113897562026978\n",
      "Epoch 608/2000, Train Loss: 4.9779882948892915, Val Loss: 5.718211500417619, Val MAE: 1.6114598512649536\n",
      "Epoch 609/2000, Train Loss: 4.97744238158039, Val Loss: 5.718044048263913, Val MAE: 1.611046552658081\n",
      "Epoch 610/2000, Train Loss: 4.976939404464743, Val Loss: 5.717889564377921, Val MAE: 1.6106191873550415\n",
      "Epoch 611/2000, Train Loss: 4.976399823876127, Val Loss: 5.717662754512968, Val MAE: 1.6104565858840942\n",
      "Epoch 612/2000, Train Loss: 4.975812394797214, Val Loss: 5.717514302049365, Val MAE: 1.6101306676864624\n",
      "Epoch 613/2000, Train Loss: 4.975270100473853, Val Loss: 5.717303119954609, Val MAE: 1.6097973585128784\n",
      "Epoch 614/2000, Train Loss: 4.974755842023239, Val Loss: 5.717182170777094, Val MAE: 1.6093937158584595\n",
      "Epoch 615/2000, Train Loss: 4.9741954884172666, Val Loss: 5.716886148566291, Val MAE: 1.6092997789382935\n",
      "Epoch 616/2000, Train Loss: 4.973632665379932, Val Loss: 5.716714708578019, Val MAE: 1.609080195426941\n",
      "Epoch 617/2000, Train Loss: 4.973047269920704, Val Loss: 5.716577387991405, Val MAE: 1.6087286472320557\n",
      "Epoch 618/2000, Train Loss: 4.972553183228745, Val Loss: 5.716410784494309, Val MAE: 1.6083688735961914\n",
      "Epoch 619/2000, Train Loss: 4.972064090884791, Val Loss: 5.7162215539387295, Val MAE: 1.6082258224487305\n",
      "Epoch 620/2000, Train Loss: 4.9715143714536225, Val Loss: 5.716005038647425, Val MAE: 1.6080304384231567\n",
      "Epoch 621/2000, Train Loss: 4.970961896253406, Val Loss: 5.715872256528764, Val MAE: 1.6076300144195557\n",
      "Epoch 622/2000, Train Loss: 4.9705379981079965, Val Loss: 5.715716963722592, Val MAE: 1.6072674989700317\n",
      "Epoch 623/2000, Train Loss: 4.969978504019497, Val Loss: 5.715466828573318, Val MAE: 1.6072665452957153\n",
      "Epoch 624/2000, Train Loss: 4.969449709853937, Val Loss: 5.715289297558012, Val MAE: 1.607023000717163\n",
      "Epoch 625/2000, Train Loss: 4.968996162306943, Val Loss: 5.715224958601452, Val MAE: 1.6064754724502563\n",
      "Epoch 626/2000, Train Loss: 4.968417271908651, Val Loss: 5.715034581366039, Val MAE: 1.606316328048706\n",
      "Epoch 627/2000, Train Loss: 4.967918152365261, Val Loss: 5.714968346414112, Val MAE: 1.6059541702270508\n",
      "Epoch 628/2000, Train Loss: 4.96747911514449, Val Loss: 5.714758926913852, Val MAE: 1.6058411598205566\n",
      "Epoch 629/2000, Train Loss: 4.967037127619906, Val Loss: 5.714730526719775, Val MAE: 1.6050970554351807\n",
      "Epoch 630/2000, Train Loss: 4.966508391882032, Val Loss: 5.714541730426607, Val MAE: 1.6048028469085693\n",
      "Epoch 631/2000, Train Loss: 4.966112828725484, Val Loss: 5.714494126183646, Val MAE: 1.604331374168396\n",
      "Epoch 632/2000, Train Loss: 4.965558276694315, Val Loss: 5.714244385560353, Val MAE: 1.6043381690979004\n",
      "Epoch 633/2000, Train Loss: 4.965164612311402, Val Loss: 5.714195765200115, Val MAE: 1.6038405895233154\n",
      "Epoch 634/2000, Train Loss: 4.964647981949014, Val Loss: 5.713976808956692, Val MAE: 1.6038217544555664\n",
      "Epoch 635/2000, Train Loss: 4.964171695440209, Val Loss: 5.713925622758412, Val MAE: 1.6031581163406372\n",
      "Epoch 636/2000, Train Loss: 4.963665116152743, Val Loss: 5.713725166661399, Val MAE: 1.6031612157821655\n",
      "Epoch 637/2000, Train Loss: 4.963128138328305, Val Loss: 5.713545870213282, Val MAE: 1.6029056310653687\n",
      "Epoch 638/2000, Train Loss: 4.962664704396795, Val Loss: 5.713398967470441, Val MAE: 1.6026771068572998\n",
      "Epoch 639/2000, Train Loss: 4.962240828445499, Val Loss: 5.713270113581703, Val MAE: 1.602452039718628\n",
      "Epoch 640/2000, Train Loss: 4.96179146221898, Val Loss: 5.71320245663325, Val MAE: 1.6019182205200195\n",
      "Epoch 641/2000, Train Loss: 4.961393054348793, Val Loss: 5.712959122090113, Val MAE: 1.6022464036941528\n",
      "Epoch 642/2000, Train Loss: 4.960883067087999, Val Loss: 5.712795686154139, Val MAE: 1.6021239757537842\n",
      "Epoch 643/2000, Train Loss: 4.960475196623163, Val Loss: 5.712686473415012, Val MAE: 1.6018280982971191\n",
      "Epoch 644/2000, Train Loss: 4.959934035544671, Val Loss: 5.7125384750820345, Val MAE: 1.601589560508728\n",
      "Epoch 645/2000, Train Loss: 4.959537036664395, Val Loss: 5.7124677413985845, Val MAE: 1.6013914346694946\n",
      "Epoch 646/2000, Train Loss: 4.95909984491776, Val Loss: 5.712333670684269, Val MAE: 1.6009886264801025\n",
      "Epoch 647/2000, Train Loss: 4.958626711822531, Val Loss: 5.71220850944519, Val MAE: 1.600805401802063\n",
      "Epoch 648/2000, Train Loss: 4.958184060324062, Val Loss: 5.712124577590397, Val MAE: 1.6005445718765259\n",
      "Epoch 649/2000, Train Loss: 4.957766910870421, Val Loss: 5.7120280265808105, Val MAE: 1.6002963781356812\n",
      "Epoch 650/2000, Train Loss: 4.95734157901893, Val Loss: 5.711901664733887, Val MAE: 1.600131630897522\n",
      "Epoch 651/2000, Train Loss: 4.956961192600482, Val Loss: 5.711773645310175, Val MAE: 1.5998808145523071\n",
      "Epoch 652/2000, Train Loss: 4.95648430634621, Val Loss: 5.711696020194462, Val MAE: 1.5996068716049194\n",
      "Epoch 653/2000, Train Loss: 4.9560847564880195, Val Loss: 5.711595447290511, Val MAE: 1.5993833541870117\n",
      "Epoch 654/2000, Train Loss: 4.9556864683315345, Val Loss: 5.7114308050700595, Val MAE: 1.5993051528930664\n",
      "Epoch 655/2000, Train Loss: 4.955199265849943, Val Loss: 5.711339919340043, Val MAE: 1.5989866256713867\n",
      "Epoch 656/2000, Train Loss: 4.95487518202939, Val Loss: 5.711312271299816, Val MAE: 1.5984987020492554\n",
      "Epoch 657/2000, Train Loss: 4.954362813777412, Val Loss: 5.711149255434672, Val MAE: 1.598438024520874\n",
      "Epoch 658/2000, Train Loss: 4.953981727067439, Val Loss: 5.711096363408225, Val MAE: 1.598020315170288\n",
      "Epoch 659/2000, Train Loss: 4.953519753912104, Val Loss: 5.710938036441803, Val MAE: 1.5979357957839966\n",
      "Epoch 660/2000, Train Loss: 4.953148485407002, Val Loss: 5.710824055331094, Val MAE: 1.5978869199752808\n",
      "Epoch 661/2000, Train Loss: 4.952890152319194, Val Loss: 5.710680785633269, Val MAE: 1.5978586673736572\n",
      "Epoch 662/2000, Train Loss: 4.952418167264908, Val Loss: 5.710610792750404, Val MAE: 1.5973587036132812\n",
      "Epoch 663/2000, Train Loss: 4.952020315592649, Val Loss: 5.710579324336279, Val MAE: 1.5970771312713623\n",
      "Epoch 664/2000, Train Loss: 4.951656275642944, Val Loss: 5.710506098611014, Val MAE: 1.596767783164978\n",
      "Epoch 665/2000, Train Loss: 4.9512480455325925, Val Loss: 5.710345342045739, Val MAE: 1.5965890884399414\n",
      "Epoch 666/2000, Train Loss: 4.950853693468447, Val Loss: 5.710316709109715, Val MAE: 1.5963218212127686\n",
      "Epoch 667/2000, Train Loss: 4.9504557467986565, Val Loss: 5.710159088884081, Val MAE: 1.596281886100769\n",
      "Epoch 668/2000, Train Loss: 4.950067425984758, Val Loss: 5.7100291933332175, Val MAE: 1.596065878868103\n",
      "Epoch 669/2000, Train Loss: 4.949651308557378, Val Loss: 5.71001919962111, Val MAE: 1.595686912536621\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 670/2000, Train Loss: 4.949285477273723, Val Loss: 5.70990051258178, Val MAE: 1.595611810684204\n",
      "Epoch 671/2000, Train Loss: 4.9489054040949165, Val Loss: 5.709803308759417, Val MAE: 1.5954399108886719\n",
      "Epoch 672/2000, Train Loss: 4.94855686639697, Val Loss: 5.709757416021256, Val MAE: 1.5950205326080322\n",
      "Epoch 673/2000, Train Loss: 4.948266214981402, Val Loss: 5.709640006224315, Val MAE: 1.594944715499878\n",
      "Epoch 674/2000, Train Loss: 4.947799146595727, Val Loss: 5.7095441818237305, Val MAE: 1.5947903394699097\n",
      "Epoch 675/2000, Train Loss: 4.947401156042123, Val Loss: 5.709477495579493, Val MAE: 1.594579815864563\n",
      "Epoch 676/2000, Train Loss: 4.9471111398489755, Val Loss: 5.70934971457436, Val MAE: 1.5945628881454468\n",
      "Epoch 677/2000, Train Loss: 4.946652988788943, Val Loss: 5.709245579583304, Val MAE: 1.5942684412002563\n",
      "Epoch 678/2000, Train Loss: 4.946307724050137, Val Loss: 5.709186173620678, Val MAE: 1.5940558910369873\n",
      "Epoch 679/2000, Train Loss: 4.945973221102285, Val Loss: 5.709139772823879, Val MAE: 1.5935672521591187\n",
      "Epoch 680/2000, Train Loss: 4.945606721641314, Val Loss: 5.709030795665014, Val MAE: 1.5936086177825928\n",
      "Epoch 681/2000, Train Loss: 4.945225176253003, Val Loss: 5.709022232464382, Val MAE: 1.5931378602981567\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 682/2000, Train Loss: 4.944870227482826, Val Loss: 5.708919993468693, Val MAE: 1.5931527614593506\n",
      "Epoch 683/2000, Train Loss: 4.9445026919603015, Val Loss: 5.7088800839015414, Val MAE: 1.5927879810333252\n",
      "Epoch 684/2000, Train Loss: 4.9441162697519, Val Loss: 5.708855095363798, Val MAE: 1.5925803184509277\n",
      "Epoch 685/2000, Train Loss: 4.943773271670967, Val Loss: 5.708766900357746, Val MAE: 1.5922695398330688\n",
      "Epoch 686/2000, Train Loss: 4.943559220210453, Val Loss: 5.7087269595691135, Val MAE: 1.5921709537506104\n",
      "Epoch 687/2000, Train Loss: 4.943093376536296, Val Loss: 5.708700219790141, Val MAE: 1.5917198657989502\n",
      "Epoch 688/2000, Train Loss: 4.942730259996207, Val Loss: 5.708625906989688, Val MAE: 1.5914201736450195\n",
      "Epoch 689/2000, Train Loss: 4.94245048134216, Val Loss: 5.708582977453868, Val MAE: 1.5911611318588257\n",
      "Epoch 690/2000, Train Loss: 4.942113911315316, Val Loss: 5.708498946258, Val MAE: 1.5910464525222778\n",
      "Epoch 691/2000, Train Loss: 4.941746612193723, Val Loss: 5.708464704808735, Val MAE: 1.5906355381011963\n",
      "Epoch 692/2000, Train Loss: 4.941418003464283, Val Loss: 5.708291337603614, Val MAE: 1.590764045715332\n",
      "Epoch 693/2000, Train Loss: 4.941008410769895, Val Loss: 5.708236379282815, Val MAE: 1.5905202627182007\n",
      "Epoch 694/2000, Train Loss: 4.940712676902416, Val Loss: 5.708132056962876, Val MAE: 1.5904831886291504\n",
      "Epoch 695/2000, Train Loss: 4.940323786944025, Val Loss: 5.708045281115032, Val MAE: 1.5903552770614624\n",
      "Epoch 696/2000, Train Loss: 4.940004005418678, Val Loss: 5.70803758927754, Val MAE: 1.5899596214294434\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 697/2000, Train Loss: 4.939660346726605, Val Loss: 5.708025185834794, Val MAE: 1.5896600484848022\n",
      "Epoch 698/2000, Train Loss: 4.939428621353989, Val Loss: 5.707786120119549, Val MAE: 1.5899981260299683\n",
      "Epoch 699/2000, Train Loss: 4.938953650518936, Val Loss: 5.707778411252158, Val MAE: 1.5897703170776367\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 700/2000, Train Loss: 4.938617387510657, Val Loss: 5.707770341918582, Val MAE: 1.5892750024795532\n",
      "Epoch 701/2000, Train Loss: 4.938331258145643, Val Loss: 5.7076646117936995, Val MAE: 1.589215636253357\n",
      "Epoch 702/2000, Train Loss: 4.937972659957089, Val Loss: 5.707662000542595, Val MAE: 1.5889126062393188\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 703/2000, Train Loss: 4.937710481571042, Val Loss: 5.7075502673784895, Val MAE: 1.588824987411499\n",
      "Epoch 704/2000, Train Loss: 4.937320449624647, Val Loss: 5.707582825706119, Val MAE: 1.5882644653320312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 705/2000, Train Loss: 4.937056225344896, Val Loss: 5.707452850682395, Val MAE: 1.5883803367614746\n",
      "Epoch 706/2000, Train Loss: 4.936724342647493, Val Loss: 5.707439496403649, Val MAE: 1.58791184425354\n",
      "Epoch 707/2000, Train Loss: 4.936385777840662, Val Loss: 5.7073666879108975, Val MAE: 1.5878757238388062\n",
      "Epoch 708/2000, Train Loss: 4.936055726493867, Val Loss: 5.707326344081333, Val MAE: 1.5876225233078003\n",
      "Epoch 709/2000, Train Loss: 4.935732962550498, Val Loss: 5.707261156468165, Val MAE: 1.5874027013778687\n",
      "Epoch 710/2000, Train Loss: 4.9354188975225215, Val Loss: 5.707197152432942, Val MAE: 1.5873342752456665\n",
      "Epoch 711/2000, Train Loss: 4.935070545279929, Val Loss: 5.707131417024703, Val MAE: 1.5872217416763306\n",
      "Epoch 712/2000, Train Loss: 4.934760273259517, Val Loss: 5.707065758251009, Val MAE: 1.587090253829956\n",
      "Epoch 713/2000, Train Loss: 4.934424902051057, Val Loss: 5.707012460345314, Val MAE: 1.5867725610733032\n",
      "Epoch 714/2000, Train Loss: 4.934050159242493, Val Loss: 5.706919162046342, Val MAE: 1.5867469310760498\n",
      "Epoch 715/2000, Train Loss: 4.933750714838757, Val Loss: 5.706844097092038, Val MAE: 1.5866193771362305\n",
      "Epoch 716/2000, Train Loss: 4.9334743846791085, Val Loss: 5.706816054525829, Val MAE: 1.586423397064209\n",
      "Epoch 717/2000, Train Loss: 4.9331572079355865, Val Loss: 5.706750321955908, Val MAE: 1.586340069770813\n",
      "Epoch 718/2000, Train Loss: 4.93278276651299, Val Loss: 5.706699538798559, Val MAE: 1.5862212181091309\n",
      "Epoch 719/2000, Train Loss: 4.932544145328538, Val Loss: 5.70665861311413, Val MAE: 1.5860052108764648\n",
      "Epoch 720/2000, Train Loss: 4.932299034215499, Val Loss: 5.706677232469831, Val MAE: 1.5855941772460938\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 721/2000, Train Loss: 4.931973815132432, Val Loss: 5.706485529740651, Val MAE: 1.5858116149902344\n",
      "Epoch 722/2000, Train Loss: 4.931604394119112, Val Loss: 5.706510325272878, Val MAE: 1.5854074954986572\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 723/2000, Train Loss: 4.931292702348007, Val Loss: 5.706361659935543, Val MAE: 1.585456132888794\n",
      "Epoch 724/2000, Train Loss: 4.930937251856364, Val Loss: 5.706406695502145, Val MAE: 1.584908127784729\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 725/2000, Train Loss: 4.930647335536725, Val Loss: 5.706339038553692, Val MAE: 1.5847992897033691\n",
      "Epoch 726/2000, Train Loss: 4.930395939286236, Val Loss: 5.706258030164809, Val MAE: 1.5846946239471436\n",
      "Epoch 727/2000, Train Loss: 4.930158727596106, Val Loss: 5.706186572710673, Val MAE: 1.5845434665679932\n",
      "Epoch 728/2000, Train Loss: 4.929718081470269, Val Loss: 5.706182565007891, Val MAE: 1.5841816663742065\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 729/2000, Train Loss: 4.929440664471625, Val Loss: 5.706162367548261, Val MAE: 1.5840158462524414\n",
      "Epoch 730/2000, Train Loss: 4.929111104085516, Val Loss: 5.706086505027044, Val MAE: 1.5839121341705322\n",
      "Epoch 731/2000, Train Loss: 4.928818074537097, Val Loss: 5.706021626790364, Val MAE: 1.583837628364563\n",
      "Epoch 732/2000, Train Loss: 4.928695873077565, Val Loss: 5.706069568792979, Val MAE: 1.583355188369751\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 733/2000, Train Loss: 4.928250965177592, Val Loss: 5.705975901512873, Val MAE: 1.5833722352981567\n",
      "Epoch 734/2000, Train Loss: 4.928060600216197, Val Loss: 5.705791754381997, Val MAE: 1.58370041847229\n",
      "Epoch 735/2000, Train Loss: 4.927742287369474, Val Loss: 5.705824182147071, Val MAE: 1.5832414627075195\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 736/2000, Train Loss: 4.9273855630702466, Val Loss: 5.705797933396839, Val MAE: 1.582988977432251\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 737/2000, Train Loss: 4.92710702879977, Val Loss: 5.705773015817006, Val MAE: 1.5828142166137695\n",
      "Epoch 738/2000, Train Loss: 4.92676413765411, Val Loss: 5.705689785026369, Val MAE: 1.5826104879379272\n",
      "Epoch 739/2000, Train Loss: 4.9265651857567105, Val Loss: 5.705730097634452, Val MAE: 1.5821589231491089\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 740/2000, Train Loss: 4.926223122014314, Val Loss: 5.705611935683659, Val MAE: 1.5822430849075317\n",
      "Epoch 741/2000, Train Loss: 4.925922562944872, Val Loss: 5.705593075071063, Val MAE: 1.5820101499557495\n",
      "Epoch 742/2000, Train Loss: 4.9255941522810796, Val Loss: 5.705491965725308, Val MAE: 1.5818959474563599\n",
      "Epoch 743/2000, Train Loss: 4.925318540067363, Val Loss: 5.705409623327709, Val MAE: 1.581764578819275\n",
      "Epoch 744/2000, Train Loss: 4.925010699983712, Val Loss: 5.705378100985572, Val MAE: 1.5815908908843994\n",
      "Epoch 745/2000, Train Loss: 4.924789767675574, Val Loss: 5.705253944510505, Val MAE: 1.5817548036575317\n",
      "Epoch 746/2000, Train Loss: 4.9245613095454335, Val Loss: 5.705245191142673, Val MAE: 1.5813047885894775\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 747/2000, Train Loss: 4.924167990684509, Val Loss: 5.705204095159258, Val MAE: 1.5812959671020508\n",
      "Epoch 748/2000, Train Loss: 4.923862146221532, Val Loss: 5.705111398583367, Val MAE: 1.5812897682189941\n",
      "Epoch 749/2000, Train Loss: 4.9236138751375655, Val Loss: 5.7051213298525125, Val MAE: 1.5808583498001099\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 750/2000, Train Loss: 4.923358413497215, Val Loss: 5.705013045242855, Val MAE: 1.5809673070907593\n",
      "Epoch 751/2000, Train Loss: 4.923054753641147, Val Loss: 5.7049712765784495, Val MAE: 1.580655813217163\n",
      "Epoch 752/2000, Train Loss: 4.922720106767835, Val Loss: 5.704920896462032, Val MAE: 1.5806034803390503\n",
      "Epoch 753/2000, Train Loss: 4.922446758017386, Val Loss: 5.704915265242259, Val MAE: 1.580296516418457\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 754/2000, Train Loss: 4.922164081013926, Val Loss: 5.704859716551645, Val MAE: 1.580248475074768\n",
      "Epoch 755/2000, Train Loss: 4.921960203872917, Val Loss: 5.704737141018822, Val MAE: 1.5803531408309937\n",
      "Epoch 756/2000, Train Loss: 4.92167087973257, Val Loss: 5.704712070169903, Val MAE: 1.580073356628418\n",
      "Epoch 757/2000, Train Loss: 4.92135696014329, Val Loss: 5.704725302401043, Val MAE: 1.5797193050384521\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 758/2000, Train Loss: 4.921149319418731, Val Loss: 5.7046932606470016, Val MAE: 1.5795847177505493\n",
      "Epoch 759/2000, Train Loss: 4.920830439781437, Val Loss: 5.704701704638345, Val MAE: 1.5791767835617065\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 760/2000, Train Loss: 4.92067836739953, Val Loss: 5.704680508091336, Val MAE: 1.5789538621902466\n",
      "Epoch 761/2000, Train Loss: 4.920273609322789, Val Loss: 5.7046154879388355, Val MAE: 1.5788309574127197\n",
      "Epoch 762/2000, Train Loss: 4.920066846274523, Val Loss: 5.704556825615111, Val MAE: 1.5787707567214966\n",
      "Epoch 763/2000, Train Loss: 4.9197486143018025, Val Loss: 5.704556717759087, Val MAE: 1.5784391164779663\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 764/2000, Train Loss: 4.9195366336193, Val Loss: 5.70444705372765, Val MAE: 1.578691005706787\n",
      "Epoch 765/2000, Train Loss: 4.919210245141862, Val Loss: 5.704464710894085, Val MAE: 1.5782830715179443\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 766/2000, Train Loss: 4.918996520708243, Val Loss: 5.70443840821584, Val MAE: 1.5779772996902466\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 767/2000, Train Loss: 4.9186583533778006, Val Loss: 5.704333458627973, Val MAE: 1.5780844688415527\n",
      "Epoch 768/2000, Train Loss: 4.918386207817304, Val Loss: 5.704292209375472, Val MAE: 1.577970266342163\n",
      "Epoch 769/2000, Train Loss: 4.918171617981409, Val Loss: 5.704224646091461, Val MAE: 1.5779789686203003\n",
      "Epoch 770/2000, Train Loss: 4.917928235655275, Val Loss: 5.7042467679296225, Val MAE: 1.5775034427642822\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 771/2000, Train Loss: 4.917624658606789, Val Loss: 5.704214922019413, Val MAE: 1.5773863792419434\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 772/2000, Train Loss: 4.917353315313041, Val Loss: 5.70416948341188, Val MAE: 1.5771605968475342\n",
      "Epoch 773/2000, Train Loss: 4.917137370795558, Val Loss: 5.704093819572812, Val MAE: 1.5771605968475342\n",
      "Epoch 774/2000, Train Loss: 4.9168022462444005, Val Loss: 5.704090212072645, Val MAE: 1.576863408088684\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 775/2000, Train Loss: 4.9165321694442685, Val Loss: 5.704025881631034, Val MAE: 1.5766754150390625\n",
      "Epoch 776/2000, Train Loss: 4.916289990980635, Val Loss: 5.703965896651859, Val MAE: 1.5766268968582153\n",
      "Epoch 777/2000, Train Loss: 4.91599777185362, Val Loss: 5.703968646980467, Val MAE: 1.5763168334960938\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 778/2000, Train Loss: 4.9157630230375, Val Loss: 5.70391035931451, Val MAE: 1.5762245655059814\n",
      "Epoch 779/2000, Train Loss: 4.9154822444714, Val Loss: 5.7038520971934, Val MAE: 1.5760712623596191\n",
      "Epoch 780/2000, Train Loss: 4.915227250084386, Val Loss: 5.703801541101365, Val MAE: 1.5760160684585571\n",
      "Epoch 781/2000, Train Loss: 4.914957550920452, Val Loss: 5.70374089763278, Val MAE: 1.5758920907974243\n",
      "Epoch 782/2000, Train Loss: 4.914722428755632, Val Loss: 5.703669706980388, Val MAE: 1.5759624242782593\n",
      "Epoch 783/2000, Train Loss: 4.914461353770096, Val Loss: 5.703669655890692, Val MAE: 1.5756574869155884\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 784/2000, Train Loss: 4.914144237219699, Val Loss: 5.703497003941309, Val MAE: 1.5759080648422241\n",
      "Epoch 785/2000, Train Loss: 4.914087648620391, Val Loss: 5.703383130686624, Val MAE: 1.5761191844940186\n",
      "Epoch 786/2000, Train Loss: 4.913728481959893, Val Loss: 5.703439789158957, Val MAE: 1.5755040645599365\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 787/2000, Train Loss: 4.913413461744365, Val Loss: 5.703366940929776, Val MAE: 1.575547695159912\n",
      "Epoch 788/2000, Train Loss: 4.913148314727547, Val Loss: 5.70334655046463, Val MAE: 1.5754332542419434\n",
      "Epoch 789/2000, Train Loss: 4.912903273895866, Val Loss: 5.703332886809394, Val MAE: 1.5751157999038696\n",
      "Epoch 790/2000, Train Loss: 4.912641063900021, Val Loss: 5.703230182329814, Val MAE: 1.5752712488174438\n",
      "Epoch 791/2000, Train Loss: 4.912412489418923, Val Loss: 5.7031037864230925, Val MAE: 1.5754117965698242\n",
      "Epoch 792/2000, Train Loss: 4.912156605417873, Val Loss: 5.703102344558353, Val MAE: 1.5752476453781128\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 793/2000, Train Loss: 4.91184140462297, Val Loss: 5.70306840964726, Val MAE: 1.5750010013580322\n",
      "Epoch 794/2000, Train Loss: 4.911625605039771, Val Loss: 5.702963434514546, Val MAE: 1.5751858949661255\n",
      "Epoch 795/2000, Train Loss: 4.911378530588405, Val Loss: 5.702918066864922, Val MAE: 1.5750634670257568\n",
      "Epoch 796/2000, Train Loss: 4.911093064188453, Val Loss: 5.702872253599621, Val MAE: 1.5749295949935913\n",
      "Epoch 797/2000, Train Loss: 4.910870062111127, Val Loss: 5.7028624131566, Val MAE: 1.5749584436416626\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 798/2000, Train Loss: 4.910589956598322, Val Loss: 5.702831364813305, Val MAE: 1.574812889099121\n",
      "Epoch 799/2000, Train Loss: 4.910361956810245, Val Loss: 5.7027210508074075, Val MAE: 1.574858546257019\n",
      "Epoch 800/2000, Train Loss: 4.910130765777717, Val Loss: 5.702680570738656, Val MAE: 1.5747289657592773\n",
      "Epoch 801/2000, Train Loss: 4.909885378247088, Val Loss: 5.702630318346477, Val MAE: 1.5745311975479126\n",
      "Epoch 802/2000, Train Loss: 4.9096356473957705, Val Loss: 5.702656964461009, Val MAE: 1.574403166770935\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 803/2000, Train Loss: 4.909364105111621, Val Loss: 5.702657270999182, Val MAE: 1.5741121768951416\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 804/2000, Train Loss: 4.909153002777961, Val Loss: 5.702564458052318, Val MAE: 1.5742579698562622\n",
      "Epoch 805/2000, Train Loss: 4.908873642785593, Val Loss: 5.702566893327804, Val MAE: 1.574013113975525\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 806/2000, Train Loss: 4.908721588898781, Val Loss: 5.7025685878027055, Val MAE: 1.573712706565857\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 807/2000, Train Loss: 4.908551480941611, Val Loss: 5.702464132081895, Val MAE: 1.5738581418991089\n",
      "Epoch 808/2000, Train Loss: 4.908159027482962, Val Loss: 5.702483841351101, Val MAE: 1.5734957456588745\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 809/2000, Train Loss: 4.907965479179565, Val Loss: 5.702550533271971, Val MAE: 1.573084831237793\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 810/2000, Train Loss: 4.907719273156945, Val Loss: 5.702485195228031, Val MAE: 1.5732206106185913\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 811/2000, Train Loss: 4.907487668170579, Val Loss: 5.702387860843113, Val MAE: 1.5730699300765991\n",
      "Epoch 812/2000, Train Loss: 4.907179827750654, Val Loss: 5.702290779068356, Val MAE: 1.573123574256897\n",
      "Epoch 813/2000, Train Loss: 4.906980706202799, Val Loss: 5.702196385179247, Val MAE: 1.5732091665267944\n",
      "Epoch 814/2000, Train Loss: 4.906721413724009, Val Loss: 5.70224192028954, Val MAE: 1.5729398727416992\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 815/2000, Train Loss: 4.906482338737197, Val Loss: 5.702094852924347, Val MAE: 1.5731035470962524\n",
      "Epoch 816/2000, Train Loss: 4.906235774582297, Val Loss: 5.702093064785004, Val MAE: 1.57308030128479\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 817/2000, Train Loss: 4.906004276363066, Val Loss: 5.70201035340627, Val MAE: 1.573003888130188\n",
      "Epoch 818/2000, Train Loss: 4.905765110414018, Val Loss: 5.702051826885769, Val MAE: 1.5725576877593994\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 819/2000, Train Loss: 4.905513413029766, Val Loss: 5.701988311040969, Val MAE: 1.5725561380386353\n",
      "Epoch 820/2000, Train Loss: 4.905296280959093, Val Loss: 5.701923651354654, Val MAE: 1.572568416595459\n",
      "Epoch 821/2000, Train Loss: 4.905112652583586, Val Loss: 5.701836131867909, Val MAE: 1.5726231336593628\n",
      "Epoch 822/2000, Train Loss: 4.904792794892078, Val Loss: 5.701841135819753, Val MAE: 1.5723412036895752\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 823/2000, Train Loss: 4.904590120772885, Val Loss: 5.7018600560369945, Val MAE: 1.5719804763793945\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 824/2000, Train Loss: 4.904309352126882, Val Loss: 5.701786807605198, Val MAE: 1.5719233751296997\n",
      "Epoch 825/2000, Train Loss: 4.904190635815662, Val Loss: 5.701805713630858, Val MAE: 1.5716650485992432\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 826/2000, Train Loss: 4.90388367307203, Val Loss: 5.701698093187241, Val MAE: 1.5717086791992188\n",
      "Epoch 827/2000, Train Loss: 4.903640313948832, Val Loss: 5.701611357075827, Val MAE: 1.5718663930892944\n",
      "Epoch 828/2000, Train Loss: 4.903370539123148, Val Loss: 5.701600117342813, Val MAE: 1.571562647819519\n",
      "Epoch 829/2000, Train Loss: 4.903151089785298, Val Loss: 5.701526661713918, Val MAE: 1.5715481042861938\n",
      "Epoch 830/2000, Train Loss: 4.90286244626442, Val Loss: 5.701518212045942, Val MAE: 1.5713987350463867\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 831/2000, Train Loss: 4.902653916093963, Val Loss: 5.701514343420665, Val MAE: 1.571394443511963\n",
      "Epoch 832/2000, Train Loss: 4.902424804246308, Val Loss: 5.701395122777848, Val MAE: 1.5715585947036743\n",
      "Epoch 833/2000, Train Loss: 4.902180610321809, Val Loss: 5.701324661572774, Val MAE: 1.5714852809906006\n",
      "Epoch 834/2000, Train Loss: 4.901971210377509, Val Loss: 5.70134250890641, Val MAE: 1.571114182472229\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 835/2000, Train Loss: 4.901822032141585, Val Loss: 5.701263498692286, Val MAE: 1.5712236166000366\n",
      "Epoch 836/2000, Train Loss: 4.90149662289532, Val Loss: 5.701277372382936, Val MAE: 1.5710264444351196\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 837/2000, Train Loss: 4.901280229283321, Val Loss: 5.701190468810854, Val MAE: 1.5709482431411743\n",
      "Epoch 838/2000, Train Loss: 4.901029279437153, Val Loss: 5.701169541903904, Val MAE: 1.5706859827041626\n",
      "Epoch 839/2000, Train Loss: 4.900755882263184, Val Loss: 5.701169805867331, Val MAE: 1.5705660581588745\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 840/2000, Train Loss: 4.900527047169393, Val Loss: 5.701111938272204, Val MAE: 1.5704389810562134\n",
      "Epoch 841/2000, Train Loss: 4.900285233227255, Val Loss: 5.701032522178831, Val MAE: 1.5704823732376099\n",
      "Epoch 842/2000, Train Loss: 4.900214382892603, Val Loss: 5.701067268848419, Val MAE: 1.5700514316558838\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 843/2000, Train Loss: 4.899826371451191, Val Loss: 5.700912205945878, Val MAE: 1.5704487562179565\n",
      "Epoch 844/2000, Train Loss: 4.899637171848873, Val Loss: 5.700936541670845, Val MAE: 1.5702345371246338\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 845/2000, Train Loss: 4.899342420575313, Val Loss: 5.700935088452839, Val MAE: 1.5700113773345947\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 846/2000, Train Loss: 4.899120382933085, Val Loss: 5.700880604130881, Val MAE: 1.5698041915893555\n",
      "Epoch 847/2000, Train Loss: 4.898903607649594, Val Loss: 5.700891114416576, Val MAE: 1.5697472095489502\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 848/2000, Train Loss: 4.89868295949336, Val Loss: 5.700861831506093, Val MAE: 1.5696313381195068\n",
      "Epoch 849/2000, Train Loss: 4.898502170619238, Val Loss: 5.70073386033376, Val MAE: 1.5696778297424316\n",
      "Epoch 850/2000, Train Loss: 4.898267226091393, Val Loss: 5.7007172618593485, Val MAE: 1.5695730447769165\n",
      "Epoch 851/2000, Train Loss: 4.898070309831326, Val Loss: 5.700745185216268, Val MAE: 1.569188117980957\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 852/2000, Train Loss: 4.897769428947244, Val Loss: 5.700763804571969, Val MAE: 1.5690407752990723\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 853/2000, Train Loss: 4.897520971230963, Val Loss: 5.700627389408293, Val MAE: 1.5691733360290527\n",
      "Epoch 854/2000, Train Loss: 4.897343324168949, Val Loss: 5.700636108716329, Val MAE: 1.569063425064087\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 855/2000, Train Loss: 4.897106034799423, Val Loss: 5.700515846411387, Val MAE: 1.5690869092941284\n",
      "Epoch 856/2000, Train Loss: 4.896897667049521, Val Loss: 5.700493892033895, Val MAE: 1.5691226720809937\n",
      "Epoch 857/2000, Train Loss: 4.89668670931387, Val Loss: 5.700416987850552, Val MAE: 1.5689526796340942\n",
      "Epoch 858/2000, Train Loss: 4.896495736368284, Val Loss: 5.700450162092845, Val MAE: 1.5687543153762817\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 859/2000, Train Loss: 4.896289306904264, Val Loss: 5.700290748051235, Val MAE: 1.5690245628356934\n",
      "Epoch 860/2000, Train Loss: 4.896031365576181, Val Loss: 5.700275719165802, Val MAE: 1.5688514709472656\n",
      "Epoch 861/2000, Train Loss: 4.895785272541772, Val Loss: 5.70023668096179, Val MAE: 1.568589687347412\n",
      "Epoch 862/2000, Train Loss: 4.895564254819926, Val Loss: 5.700229395003546, Val MAE: 1.5684759616851807\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 863/2000, Train Loss: 4.895368383700824, Val Loss: 5.700225444067092, Val MAE: 1.568285584449768\n",
      "Epoch 864/2000, Train Loss: 4.895095479505186, Val Loss: 5.700127865586962, Val MAE: 1.5684683322906494\n",
      "Epoch 865/2000, Train Loss: 4.894859533383917, Val Loss: 5.699942920889173, Val MAE: 1.5687440633773804\n",
      "Epoch 866/2000, Train Loss: 4.894643273104734, Val Loss: 5.6999480951400034, Val MAE: 1.5686143636703491\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 867/2000, Train Loss: 4.894444031170629, Val Loss: 5.699909908430917, Val MAE: 1.568453073501587\n",
      "Epoch 868/2000, Train Loss: 4.894219452302446, Val Loss: 5.6998481182825, Val MAE: 1.568488359451294\n",
      "Epoch 869/2000, Train Loss: 4.894035662178933, Val Loss: 5.699747162205832, Val MAE: 1.568517804145813\n",
      "Epoch 870/2000, Train Loss: 4.893756680831586, Val Loss: 5.699766792002178, Val MAE: 1.5682885646820068\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 871/2000, Train Loss: 4.893537233006971, Val Loss: 5.699715418475015, Val MAE: 1.5682311058044434\n",
      "Epoch 872/2000, Train Loss: 4.8933557112227035, Val Loss: 5.699620275270371, Val MAE: 1.568317174911499\n",
      "Epoch 873/2000, Train Loss: 4.893148325730446, Val Loss: 5.699619517439888, Val MAE: 1.5681402683258057\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 874/2000, Train Loss: 4.892894960761239, Val Loss: 5.699569608483996, Val MAE: 1.568223476409912\n",
      "Epoch 875/2000, Train Loss: 4.892707786371745, Val Loss: 5.699590498492832, Val MAE: 1.5678869485855103\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 876/2000, Train Loss: 4.8924443913447675, Val Loss: 5.6995197875159125, Val MAE: 1.5679502487182617\n",
      "Epoch 877/2000, Train Loss: 4.892236561633973, Val Loss: 5.699430885769072, Val MAE: 1.5680567026138306\n",
      "Epoch 878/2000, Train Loss: 4.891993777371932, Val Loss: 5.699389185224261, Val MAE: 1.567960262298584\n",
      "Epoch 879/2000, Train Loss: 4.891749680630747, Val Loss: 5.699344470387413, Val MAE: 1.5678867101669312\n",
      "Epoch 880/2000, Train Loss: 4.891602564681905, Val Loss: 5.699377167792547, Val MAE: 1.5675302743911743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 881/2000, Train Loss: 4.8913688558785635, Val Loss: 5.699275056521098, Val MAE: 1.5676512718200684\n",
      "Epoch 882/2000, Train Loss: 4.891182202041906, Val Loss: 5.699306933652787, Val MAE: 1.5672937631607056\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 883/2000, Train Loss: 4.890902611365271, Val Loss: 5.699209633327666, Val MAE: 1.5673404932022095\n",
      "Epoch 884/2000, Train Loss: 4.8906990926928176, Val Loss: 5.699171443780263, Val MAE: 1.567262053489685\n",
      "Epoch 885/2000, Train Loss: 4.890525161463383, Val Loss: 5.699155168873923, Val MAE: 1.5670970678329468\n",
      "Epoch 886/2000, Train Loss: 4.890280336856169, Val Loss: 5.699088247049422, Val MAE: 1.5671775341033936\n",
      "Epoch 887/2000, Train Loss: 4.890102688112783, Val Loss: 5.699078443504515, Val MAE: 1.5670499801635742\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 888/2000, Train Loss: 4.889826488595755, Val Loss: 5.699013630549113, Val MAE: 1.5670980215072632\n",
      "Epoch 889/2000, Train Loss: 4.889635695388859, Val Loss: 5.699037727855501, Val MAE: 1.5668280124664307\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 890/2000, Train Loss: 4.889422574063451, Val Loss: 5.698930005232493, Val MAE: 1.5668413639068604\n",
      "Epoch 891/2000, Train Loss: 4.889223448144028, Val Loss: 5.698829724675133, Val MAE: 1.5670392513275146\n",
      "Epoch 892/2000, Train Loss: 4.889050108085062, Val Loss: 5.698741294088817, Val MAE: 1.567084550857544\n",
      "Epoch 893/2000, Train Loss: 4.888814560953417, Val Loss: 5.6987344565845675, Val MAE: 1.5669549703598022\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 894/2000, Train Loss: 4.888653494239022, Val Loss: 5.698728814011528, Val MAE: 1.5669344663619995\n",
      "Epoch 895/2000, Train Loss: 4.88837733726071, Val Loss: 5.698744813601176, Val MAE: 1.5665779113769531\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 896/2000, Train Loss: 4.888201301962095, Val Loss: 5.6986898836635405, Val MAE: 1.566442608833313\n",
      "Epoch 897/2000, Train Loss: 4.8879704638495935, Val Loss: 5.698646142369225, Val MAE: 1.5665061473846436\n",
      "Epoch 898/2000, Train Loss: 4.887763867425313, Val Loss: 5.698672533035278, Val MAE: 1.5663259029388428\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 899/2000, Train Loss: 4.88760970644285, Val Loss: 5.698637369133177, Val MAE: 1.5662671327590942\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 900/2000, Train Loss: 4.887328308795504, Val Loss: 5.698623251347315, Val MAE: 1.5660643577575684\n",
      "Epoch 901/2000, Train Loss: 4.887127602554343, Val Loss: 5.698580324649811, Val MAE: 1.5660009384155273\n",
      "Epoch 902/2000, Train Loss: 4.886940971584347, Val Loss: 5.698531108243125, Val MAE: 1.5659880638122559\n",
      "Epoch 903/2000, Train Loss: 4.886754166424191, Val Loss: 5.698503900141943, Val MAE: 1.5659940242767334\n",
      "Epoch 904/2000, Train Loss: 4.886572246322847, Val Loss: 5.698466652915592, Val MAE: 1.5660070180892944\n",
      "Epoch 905/2000, Train Loss: 4.886326025840425, Val Loss: 5.6984769730340865, Val MAE: 1.5657227039337158\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 906/2000, Train Loss: 4.886113280469844, Val Loss: 5.698414195151556, Val MAE: 1.5657545328140259\n",
      "Epoch 907/2000, Train Loss: 4.885884650839737, Val Loss: 5.698431542941502, Val MAE: 1.5656099319458008\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 908/2000, Train Loss: 4.885699403639069, Val Loss: 5.698428545679365, Val MAE: 1.565382480621338\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 909/2000, Train Loss: 4.885549095689157, Val Loss: 5.698380646251497, Val MAE: 1.5653921365737915\n",
      "Epoch 910/2000, Train Loss: 4.885251084234886, Val Loss: 5.698336907795498, Val MAE: 1.565349817276001\n",
      "Epoch 911/2000, Train Loss: 4.885028939825523, Val Loss: 5.698308280536106, Val MAE: 1.5652879476547241\n",
      "Epoch 912/2000, Train Loss: 4.884862602513667, Val Loss: 5.698305794170925, Val MAE: 1.5650783777236938\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 913/2000, Train Loss: 4.884625716000921, Val Loss: 5.698240251768203, Val MAE: 1.5651090145111084\n",
      "Epoch 914/2000, Train Loss: 4.884463654250453, Val Loss: 5.698247929414113, Val MAE: 1.565284013748169\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 915/2000, Train Loss: 4.88423803965699, Val Loss: 5.698197319394066, Val MAE: 1.56509268283844\n",
      "Epoch 916/2000, Train Loss: 4.884039629329579, Val Loss: 5.698142497312455, Val MAE: 1.5650579929351807\n",
      "Epoch 917/2000, Train Loss: 4.8839192134873315, Val Loss: 5.69819222177778, Val MAE: 1.5648012161254883\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 918/2000, Train Loss: 4.883631220993774, Val Loss: 5.698146113327572, Val MAE: 1.5646209716796875\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 919/2000, Train Loss: 4.883493014215919, Val Loss: 5.698147274198986, Val MAE: 1.5645681619644165\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 920/2000, Train Loss: 4.883250830708169, Val Loss: 5.698096065294175, Val MAE: 1.5643926858901978\n",
      "Epoch 921/2000, Train Loss: 4.883012779677032, Val Loss: 5.698087649685996, Val MAE: 1.5643980503082275\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 922/2000, Train Loss: 4.882885273151909, Val Loss: 5.698050470579238, Val MAE: 1.564271092414856\n",
      "Epoch 923/2000, Train Loss: 4.882648544015602, Val Loss: 5.698046732516516, Val MAE: 1.564051628112793\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 924/2000, Train Loss: 4.88245428028833, Val Loss: 5.698056206816719, Val MAE: 1.563995361328125\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 925/2000, Train Loss: 4.882282681458423, Val Loss: 5.697937590735299, Val MAE: 1.564252257347107\n",
      "Epoch 926/2000, Train Loss: 4.882024328530423, Val Loss: 5.697918812433879, Val MAE: 1.564099669456482\n",
      "Epoch 927/2000, Train Loss: 4.8818399872531, Val Loss: 5.6978427313622975, Val MAE: 1.5640407800674438\n",
      "Epoch 928/2000, Train Loss: 4.881614080941559, Val Loss: 5.697819181850979, Val MAE: 1.5639179944992065\n",
      "Epoch 929/2000, Train Loss: 4.881398868829811, Val Loss: 5.697798345770154, Val MAE: 1.5638536214828491\n",
      "Epoch 930/2000, Train Loss: 4.881188166662735, Val Loss: 5.697737742038, Val MAE: 1.5638930797576904\n",
      "Epoch 931/2000, Train Loss: 4.881028668332335, Val Loss: 5.697619991643088, Val MAE: 1.5640168190002441\n",
      "Epoch 932/2000, Train Loss: 4.880777138412755, Val Loss: 5.697630553018479, Val MAE: 1.5637452602386475\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 933/2000, Train Loss: 4.880624068640856, Val Loss: 5.6976737805775235, Val MAE: 1.5636541843414307\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 934/2000, Train Loss: 4.8803713721852375, Val Loss: 5.69761001496088, Val MAE: 1.5637266635894775\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 935/2000, Train Loss: 4.880206677069617, Val Loss: 5.697608504976545, Val MAE: 1.563604474067688\n",
      "Epoch 936/2000, Train Loss: 4.880018805583206, Val Loss: 5.697418439955938, Val MAE: 1.563904881477356\n",
      "Epoch 937/2000, Train Loss: 4.87985593240251, Val Loss: 5.6974598708606905, Val MAE: 1.563579797744751\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 938/2000, Train Loss: 4.879565023568857, Val Loss: 5.697424860227676, Val MAE: 1.5634381771087646\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 939/2000, Train Loss: 4.879374039021803, Val Loss: 5.697439684754326, Val MAE: 1.5632905960083008\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 940/2000, Train Loss: 4.879201968398855, Val Loss: 5.697455914247604, Val MAE: 1.5630629062652588\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 941/2000, Train Loss: 4.878971724651428, Val Loss: 5.697428061848595, Val MAE: 1.5630360841751099\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 942/2000, Train Loss: 4.878793498524826, Val Loss: 5.69735567626499, Val MAE: 1.5632318258285522\n",
      "Epoch 943/2000, Train Loss: 4.878557972578471, Val Loss: 5.697300499393826, Val MAE: 1.5631579160690308\n",
      "Epoch 944/2000, Train Loss: 4.878393454511344, Val Loss: 5.697299207959857, Val MAE: 1.5629477500915527\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 945/2000, Train Loss: 4.878250790415765, Val Loss: 5.697281417392549, Val MAE: 1.562873125076294\n",
      "Epoch 946/2000, Train Loss: 4.878067558003414, Val Loss: 5.697304001876286, Val MAE: 1.5626646280288696\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 947/2000, Train Loss: 4.877805471420288, Val Loss: 5.6972344773156305, Val MAE: 1.5626318454742432\n",
      "Epoch 948/2000, Train Loss: 4.877572812886097, Val Loss: 5.697139135428837, Val MAE: 1.5627858638763428\n",
      "Epoch 949/2000, Train Loss: 4.87736860955888, Val Loss: 5.697111149628957, Val MAE: 1.5627092123031616\n",
      "Epoch 950/2000, Train Loss: 4.877139652565605, Val Loss: 5.697038945697603, Val MAE: 1.5627005100250244\n",
      "Epoch 951/2000, Train Loss: 4.876998842519161, Val Loss: 5.697043112346104, Val MAE: 1.562654733657837\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 952/2000, Train Loss: 4.8767799150792435, Val Loss: 5.69707282100405, Val MAE: 1.562451720237732\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 953/2000, Train Loss: 4.876629806708213, Val Loss: 5.696952237969353, Val MAE: 1.5625641345977783\n",
      "Epoch 954/2000, Train Loss: 4.876378228533251, Val Loss: 5.6969493088268095, Val MAE: 1.5623220205307007\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 955/2000, Train Loss: 4.876250288886648, Val Loss: 5.696972915104458, Val MAE: 1.5621225833892822\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 956/2000, Train Loss: 4.875993662727905, Val Loss: 5.696922327790942, Val MAE: 1.5620633363723755\n",
      "Epoch 957/2000, Train Loss: 4.875829258867984, Val Loss: 5.696871394202823, Val MAE: 1.5620675086975098\n",
      "Epoch 958/2000, Train Loss: 4.875601810191683, Val Loss: 5.696879182543073, Val MAE: 1.5618809461593628\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 959/2000, Train Loss: 4.87542431916101, Val Loss: 5.696716529982431, Val MAE: 1.562238097190857\n",
      "Epoch 960/2000, Train Loss: 4.875247846873759, Val Loss: 5.696617983636402, Val MAE: 1.5624473094940186\n",
      "Epoch 961/2000, Train Loss: 4.874979137365505, Val Loss: 5.696604243346623, Val MAE: 1.5623270273208618\n",
      "Epoch 962/2000, Train Loss: 4.8748565856761426, Val Loss: 5.696641067663829, Val MAE: 1.5621302127838135\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 963/2000, Train Loss: 4.874612349885475, Val Loss: 5.696517132577442, Val MAE: 1.5622888803482056\n",
      "Epoch 964/2000, Train Loss: 4.87446062168046, Val Loss: 5.696427563826243, Val MAE: 1.5623921155929565\n",
      "Epoch 965/2000, Train Loss: 4.874188473597904, Val Loss: 5.6964774784587675, Val MAE: 1.5621657371520996\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 966/2000, Train Loss: 4.8740067421465225, Val Loss: 5.696464056060428, Val MAE: 1.562062382698059\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 967/2000, Train Loss: 4.8738509714183085, Val Loss: 5.6964587824685236, Val MAE: 1.5620287656784058\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 968/2000, Train Loss: 4.8736691622875306, Val Loss: 5.69641527675447, Val MAE: 1.5620585680007935\n",
      "Epoch 969/2000, Train Loss: 4.8734873506346945, Val Loss: 5.696369108699617, Val MAE: 1.5619381666183472\n",
      "Epoch 970/2000, Train Loss: 4.8732597225980125, Val Loss: 5.696335415045421, Val MAE: 1.561814785003662\n",
      "Epoch 971/2000, Train Loss: 4.873032224195128, Val Loss: 5.696286627224514, Val MAE: 1.5616956949234009\n",
      "Epoch 972/2000, Train Loss: 4.872850639863815, Val Loss: 5.6962556299709135, Val MAE: 1.5618951320648193\n",
      "Epoch 973/2000, Train Loss: 4.872646597443918, Val Loss: 5.69624433347157, Val MAE: 1.5617127418518066\n",
      "Epoch 974/2000, Train Loss: 4.872501906286006, Val Loss: 5.696274643852597, Val MAE: 1.5615761280059814\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 975/2000, Train Loss: 4.8724615758834, Val Loss: 5.696171712307703, Val MAE: 1.5618962049484253\n",
      "Epoch 976/2000, Train Loss: 4.8721024172934895, Val Loss: 5.69624358131772, Val MAE: 1.5614945888519287\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 977/2000, Train Loss: 4.871893943280864, Val Loss: 5.696165127413614, Val MAE: 1.5618494749069214\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 978/2000, Train Loss: 4.871688720285472, Val Loss: 5.696121195952098, Val MAE: 1.5617380142211914\n",
      "Epoch 979/2000, Train Loss: 4.871514159634352, Val Loss: 5.696066325619107, Val MAE: 1.561800241470337\n",
      "Epoch 980/2000, Train Loss: 4.871301998708749, Val Loss: 5.696039214020684, Val MAE: 1.5617603063583374\n",
      "Epoch 981/2000, Train Loss: 4.871139183535394, Val Loss: 5.695992855798631, Val MAE: 1.5618226528167725\n",
      "Epoch 982/2000, Train Loss: 4.870951885901318, Val Loss: 5.69598183461598, Val MAE: 1.5616627931594849\n",
      "Epoch 983/2000, Train Loss: 4.870740651075527, Val Loss: 5.696072067533221, Val MAE: 1.5612620115280151\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 984/2000, Train Loss: 4.870634339928459, Val Loss: 5.695994558788481, Val MAE: 1.5612291097640991\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 985/2000, Train Loss: 4.870347281941574, Val Loss: 5.695940290178571, Val MAE: 1.5613843202590942\n",
      "Epoch 986/2000, Train Loss: 4.870158112099544, Val Loss: 5.695900806358883, Val MAE: 1.561327576637268\n",
      "Epoch 987/2000, Train Loss: 4.8699664464286085, Val Loss: 5.695854663848877, Val MAE: 1.5613901615142822\n",
      "Epoch 988/2000, Train Loss: 4.869765102106694, Val Loss: 5.695805279981522, Val MAE: 1.5614745616912842\n",
      "Epoch 989/2000, Train Loss: 4.86967212760398, Val Loss: 5.695804343337104, Val MAE: 1.5614900588989258\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 990/2000, Train Loss: 4.869390025125404, Val Loss: 5.695758703209105, Val MAE: 1.561497449874878\n",
      "Epoch 991/2000, Train Loss: 4.869229244076147, Val Loss: 5.69571259192058, Val MAE: 1.5614606142044067\n",
      "Epoch 992/2000, Train Loss: 4.869007339262323, Val Loss: 5.695677598317464, Val MAE: 1.5612821578979492\n",
      "Epoch 993/2000, Train Loss: 4.868797252477476, Val Loss: 5.695687276976449, Val MAE: 1.5612213611602783\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 994/2000, Train Loss: 4.868658522792865, Val Loss: 5.695557602814266, Val MAE: 1.5614558458328247\n",
      "Epoch 995/2000, Train Loss: 4.868445659389953, Val Loss: 5.695590419428689, Val MAE: 1.5611032247543335\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 996/2000, Train Loss: 4.868258059949565, Val Loss: 5.695532177175794, Val MAE: 1.5610589981079102\n",
      "Epoch 997/2000, Train Loss: 4.868025484138887, Val Loss: 5.695486383778708, Val MAE: 1.561199426651001\n",
      "Epoch 998/2000, Train Loss: 4.867868621191286, Val Loss: 5.695536179201944, Val MAE: 1.5609620809555054\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 999/2000, Train Loss: 4.8676392568015245, Val Loss: 5.695509405363174, Val MAE: 1.5608937740325928\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1000/2000, Train Loss: 4.867579236185265, Val Loss: 5.695406297842662, Val MAE: 1.56122624874115\n",
      "Epoch 1001/2000, Train Loss: 4.867310337690776, Val Loss: 5.695456484953563, Val MAE: 1.5606756210327148\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1002/2000, Train Loss: 4.867089737621786, Val Loss: 5.69545256523859, Val MAE: 1.5608011484146118\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1003/2000, Train Loss: 4.8670114300651175, Val Loss: 5.69536448660351, Val MAE: 1.560903787612915\n",
      "Epoch 1004/2000, Train Loss: 4.8667890053710074, Val Loss: 5.695394833882649, Val MAE: 1.560541033744812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1005/2000, Train Loss: 4.86654029511262, Val Loss: 5.695319629850841, Val MAE: 1.560718297958374\n",
      "Epoch 1006/2000, Train Loss: 4.866371710983083, Val Loss: 5.695265389624096, Val MAE: 1.5605392456054688\n",
      "Epoch 1007/2000, Train Loss: 4.866164097160479, Val Loss: 5.695187614077613, Val MAE: 1.5607354640960693\n",
      "Epoch 1008/2000, Train Loss: 4.866012623010805, Val Loss: 5.695226737431118, Val MAE: 1.5607695579528809\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1009/2000, Train Loss: 4.865790401426458, Val Loss: 5.6950893714314414, Val MAE: 1.560854196548462\n",
      "Epoch 1010/2000, Train Loss: 4.865592070459815, Val Loss: 5.695105169500623, Val MAE: 1.5607274770736694\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1011/2000, Train Loss: 4.865482086187021, Val Loss: 5.695093819073269, Val MAE: 1.5605835914611816\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1012/2000, Train Loss: 4.865230359883167, Val Loss: 5.694943904876709, Val MAE: 1.5610358715057373\n",
      "Epoch 1013/2000, Train Loss: 4.865073052548891, Val Loss: 5.694915967328208, Val MAE: 1.5607815980911255\n",
      "Epoch 1014/2000, Train Loss: 4.864865148017033, Val Loss: 5.69492065622693, Val MAE: 1.5606772899627686\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1015/2000, Train Loss: 4.864672381047301, Val Loss: 5.694952490783873, Val MAE: 1.5604512691497803\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1016/2000, Train Loss: 4.864588204492803, Val Loss: 5.694869654519217, Val MAE: 1.5607534646987915\n",
      "Epoch 1017/2000, Train Loss: 4.864302819639402, Val Loss: 5.694885807377951, Val MAE: 1.5605340003967285\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1018/2000, Train Loss: 4.864133027780207, Val Loss: 5.694925714106787, Val MAE: 1.5602874755859375\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1019/2000, Train Loss: 4.864060031678344, Val Loss: 5.694799619061606, Val MAE: 1.5607030391693115\n",
      "Epoch 1020/2000, Train Loss: 4.863717435949781, Val Loss: 5.694871956393833, Val MAE: 1.5602489709854126\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1021/2000, Train Loss: 4.863609265541324, Val Loss: 5.6948310903140476, Val MAE: 1.5602611303329468\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1022/2000, Train Loss: 4.863407608484179, Val Loss: 5.694885202816555, Val MAE: 1.5599581003189087\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1023/2000, Train Loss: 4.863291494600864, Val Loss: 5.694792844000316, Val MAE: 1.5600814819335938\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1024/2000, Train Loss: 4.863033320907141, Val Loss: 5.694805298532758, Val MAE: 1.5599908828735352\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1025/2000, Train Loss: 4.8628849251818425, Val Loss: 5.694791402135577, Val MAE: 1.5599206686019897\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1026/2000, Train Loss: 4.862719908419718, Val Loss: 5.694834743227277, Val MAE: 1.5597748756408691\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1027/2000, Train Loss: 4.862498571237152, Val Loss: 5.694696446259816, Val MAE: 1.5598838329315186\n",
      "Epoch 1028/2000, Train Loss: 4.862359057360543, Val Loss: 5.694742608638037, Val MAE: 1.5597399473190308\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1029/2000, Train Loss: 4.862201198368045, Val Loss: 5.694714719340915, Val MAE: 1.5596212148666382\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1030/2000, Train Loss: 4.861951510223582, Val Loss: 5.6947127637409025, Val MAE: 1.5595743656158447\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1031/2000, Train Loss: 4.861763655550893, Val Loss: 5.694707782495589, Val MAE: 1.5595128536224365\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1032/2000, Train Loss: 4.861634249075175, Val Loss: 5.6946401198705034, Val MAE: 1.5595426559448242\n",
      "Epoch 1033/2000, Train Loss: 4.861420294461701, Val Loss: 5.694656902835483, Val MAE: 1.5594751834869385\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1034/2000, Train Loss: 4.86121441682404, Val Loss: 5.6945883972304205, Val MAE: 1.5594769716262817\n",
      "Epoch 1035/2000, Train Loss: 4.861103207335318, Val Loss: 5.694580262615567, Val MAE: 1.5592927932739258\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1036/2000, Train Loss: 4.860948861906324, Val Loss: 5.694593006656284, Val MAE: 1.5592775344848633\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1037/2000, Train Loss: 4.860711043240825, Val Loss: 5.694597624597096, Val MAE: 1.5590990781784058\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1038/2000, Train Loss: 4.860552345243597, Val Loss: 5.694528732981, Val MAE: 1.5589845180511475\n",
      "Epoch 1039/2000, Train Loss: 4.860359803914015, Val Loss: 5.69449767328444, Val MAE: 1.5590318441390991\n",
      "Epoch 1040/2000, Train Loss: 4.860231812144873, Val Loss: 5.694395425773802, Val MAE: 1.5593221187591553\n",
      "Epoch 1041/2000, Train Loss: 4.859996791619004, Val Loss: 5.6943161601112005, Val MAE: 1.5592081546783447\n",
      "Epoch 1042/2000, Train Loss: 4.859902642845939, Val Loss: 5.694368640581767, Val MAE: 1.5588139295578003\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1043/2000, Train Loss: 4.859617726927247, Val Loss: 5.694275728293827, Val MAE: 1.5591531991958618\n",
      "Epoch 1044/2000, Train Loss: 4.859502617831963, Val Loss: 5.69435917763483, Val MAE: 1.5589007139205933\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1045/2000, Train Loss: 4.859283027595122, Val Loss: 5.694262110051655, Val MAE: 1.559046745300293\n",
      "Epoch 1046/2000, Train Loss: 4.859147733794617, Val Loss: 5.6942368774186995, Val MAE: 1.5590275526046753\n",
      "Epoch 1047/2000, Train Loss: 4.8589443689677205, Val Loss: 5.694225421973637, Val MAE: 1.5589120388031006\n",
      "Epoch 1048/2000, Train Loss: 4.858868952026152, Val Loss: 5.694262967223213, Val MAE: 1.558711290359497\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1049/2000, Train Loss: 4.858570331410729, Val Loss: 5.694207398664384, Val MAE: 1.5586344003677368\n",
      "Epoch 1050/2000, Train Loss: 4.858455119247329, Val Loss: 5.694145781653268, Val MAE: 1.5588164329528809\n",
      "Epoch 1051/2000, Train Loss: 4.858279608200569, Val Loss: 5.694174882911501, Val MAE: 1.5585130453109741\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1052/2000, Train Loss: 4.85813247266374, Val Loss: 5.694155579521542, Val MAE: 1.558382272720337\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1053/2000, Train Loss: 4.857941308041722, Val Loss: 5.694056127752576, Val MAE: 1.558622121810913\n",
      "Epoch 1054/2000, Train Loss: 4.8577677483955455, Val Loss: 5.694018809568314, Val MAE: 1.5585076808929443\n",
      "Epoch 1055/2000, Train Loss: 4.857573421112405, Val Loss: 5.693988274960291, Val MAE: 1.5586613416671753\n",
      "Epoch 1056/2000, Train Loss: 4.857336484158493, Val Loss: 5.693923760028112, Val MAE: 1.5585123300552368\n",
      "Epoch 1057/2000, Train Loss: 4.857225702578998, Val Loss: 5.693933518159957, Val MAE: 1.558416485786438\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1058/2000, Train Loss: 4.857050827090932, Val Loss: 5.6938342821030385, Val MAE: 1.5585359334945679\n",
      "Epoch 1059/2000, Train Loss: 4.856800320118204, Val Loss: 5.693790415922801, Val MAE: 1.5585432052612305\n",
      "Epoch 1060/2000, Train Loss: 4.856686983525501, Val Loss: 5.69369774205344, Val MAE: 1.5587300062179565\n",
      "Epoch 1061/2000, Train Loss: 4.856475113141015, Val Loss: 5.693808867817833, Val MAE: 1.558457851409912\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1062/2000, Train Loss: 4.856329800295729, Val Loss: 5.6936794718106585, Val MAE: 1.5585635900497437\n",
      "Epoch 1063/2000, Train Loss: 4.856154355410965, Val Loss: 5.6937161428587775, Val MAE: 1.5583871603012085\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1064/2000, Train Loss: 4.855954314993198, Val Loss: 5.693698525428772, Val MAE: 1.5582777261734009\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1065/2000, Train Loss: 4.855770423819215, Val Loss: 5.693667329493023, Val MAE: 1.5582504272460938\n",
      "Epoch 1066/2000, Train Loss: 4.855625981168115, Val Loss: 5.693670377844856, Val MAE: 1.5581305027008057\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1067/2000, Train Loss: 4.855451543845646, Val Loss: 5.693687129588354, Val MAE: 1.5580602884292603\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1068/2000, Train Loss: 4.855292739249419, Val Loss: 5.693671788488116, Val MAE: 1.5579164028167725\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1069/2000, Train Loss: 4.855128440265091, Val Loss: 5.693583939756666, Val MAE: 1.557782530784607\n",
      "Epoch 1070/2000, Train Loss: 4.85503939311158, Val Loss: 5.6935276956785295, Val MAE: 1.5579702854156494\n",
      "Epoch 1071/2000, Train Loss: 4.854872451346415, Val Loss: 5.693639735380809, Val MAE: 1.557431936264038\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1072/2000, Train Loss: 4.854614320359546, Val Loss: 5.693499681495485, Val MAE: 1.557671308517456\n",
      "Epoch 1073/2000, Train Loss: 4.8544859783605725, Val Loss: 5.693439560277121, Val MAE: 1.5576623678207397\n",
      "Epoch 1074/2000, Train Loss: 4.8542405607333805, Val Loss: 5.693488433247521, Val MAE: 1.557545781135559\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1075/2000, Train Loss: 4.854116733723198, Val Loss: 5.6934261208488826, Val MAE: 1.5575138330459595\n",
      "Epoch 1076/2000, Train Loss: 4.853873979053309, Val Loss: 5.693351856299809, Val MAE: 1.557529091835022\n",
      "Epoch 1077/2000, Train Loss: 4.853681448316372, Val Loss: 5.69331001951581, Val MAE: 1.5574517250061035\n",
      "Epoch 1078/2000, Train Loss: 4.853529057818845, Val Loss: 5.693292765390305, Val MAE: 1.5574308633804321\n",
      "Epoch 1079/2000, Train Loss: 4.853429865601705, Val Loss: 5.693331207547869, Val MAE: 1.5572545528411865\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1080/2000, Train Loss: 4.853209861411026, Val Loss: 5.693268284911201, Val MAE: 1.5572702884674072\n",
      "Epoch 1081/2000, Train Loss: 4.853052662693395, Val Loss: 5.6931430867740085, Val MAE: 1.5575617551803589\n",
      "Epoch 1082/2000, Train Loss: 4.852828692986363, Val Loss: 5.693113227685292, Val MAE: 1.5574871301651\n",
      "Epoch 1083/2000, Train Loss: 4.852708969869465, Val Loss: 5.693016932124183, Val MAE: 1.557721495628357\n",
      "Epoch 1084/2000, Train Loss: 4.852601223166806, Val Loss: 5.693043444837842, Val MAE: 1.5574171543121338\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1085/2000, Train Loss: 4.852408894698946, Val Loss: 5.692950001784733, Val MAE: 1.557583212852478\n",
      "Epoch 1086/2000, Train Loss: 4.852206172996919, Val Loss: 5.692957727682023, Val MAE: 1.557494044303894\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1087/2000, Train Loss: 4.852040030556101, Val Loss: 5.69286922329948, Val MAE: 1.5575940608978271\n",
      "Epoch 1088/2000, Train Loss: 4.851835138538829, Val Loss: 5.692912643864041, Val MAE: 1.5572949647903442\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1089/2000, Train Loss: 4.851671276038726, Val Loss: 5.692884868099576, Val MAE: 1.5572545528411865\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1090/2000, Train Loss: 4.851494884625477, Val Loss: 5.6928153577305025, Val MAE: 1.5573904514312744\n",
      "Epoch 1091/2000, Train Loss: 4.85143038449738, Val Loss: 5.6928499681609015, Val MAE: 1.5573217868804932\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1092/2000, Train Loss: 4.851125825107181, Val Loss: 5.692819853623708, Val MAE: 1.5572377443313599\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1093/2000, Train Loss: 4.851035027645202, Val Loss: 5.692751086893535, Val MAE: 1.5572575330734253\n",
      "Epoch 1094/2000, Train Loss: 4.850773992928531, Val Loss: 5.692772765954335, Val MAE: 1.5570831298828125\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1095/2000, Train Loss: 4.850634395151448, Val Loss: 5.692749083042145, Val MAE: 1.5569097995758057\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1096/2000, Train Loss: 4.850432778951648, Val Loss: 5.692727787154062, Val MAE: 1.556952714920044\n",
      "Epoch 1097/2000, Train Loss: 4.850327358595575, Val Loss: 5.692757396470933, Val MAE: 1.55667245388031\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1098/2000, Train Loss: 4.850147704647694, Val Loss: 5.692726651827495, Val MAE: 1.5566729307174683\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1099/2000, Train Loss: 4.849950267162242, Val Loss: 5.692672973587399, Val MAE: 1.5566926002502441\n",
      "Epoch 1100/2000, Train Loss: 4.849767654671823, Val Loss: 5.692667223158336, Val MAE: 1.5567681789398193\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1101/2000, Train Loss: 4.849604350387294, Val Loss: 5.692687781084151, Val MAE: 1.5566164255142212\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1102/2000, Train Loss: 4.849441043748842, Val Loss: 5.692594247204917, Val MAE: 1.5567530393600464\n",
      "Epoch 1103/2000, Train Loss: 4.849263339183898, Val Loss: 5.692627117747352, Val MAE: 1.5564225912094116\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1104/2000, Train Loss: 4.849112022410663, Val Loss: 5.692550298713503, Val MAE: 1.5565556287765503\n",
      "Epoch 1105/2000, Train Loss: 4.848939054103092, Val Loss: 5.692450160071964, Val MAE: 1.5567082166671753\n",
      "Epoch 1106/2000, Train Loss: 4.848785505308251, Val Loss: 5.692357205209278, Val MAE: 1.5566891431808472\n",
      "Epoch 1107/2000, Train Loss: 4.848659400375, Val Loss: 5.692335333142962, Val MAE: 1.556778907775879\n",
      "Epoch 1108/2000, Train Loss: 4.848463058807954, Val Loss: 5.692349695024037, Val MAE: 1.5563766956329346\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1109/2000, Train Loss: 4.8483234359784255, Val Loss: 5.692263889880407, Val MAE: 1.5565564632415771\n",
      "Epoch 1110/2000, Train Loss: 4.848090260590418, Val Loss: 5.692211750007811, Val MAE: 1.5565505027770996\n",
      "Epoch 1111/2000, Train Loss: 4.847898715137594, Val Loss: 5.692170029594784, Val MAE: 1.556268572807312\n",
      "Epoch 1112/2000, Train Loss: 4.847730168838932, Val Loss: 5.692129986626761, Val MAE: 1.5562115907669067\n",
      "Epoch 1113/2000, Train Loss: 4.847657463950735, Val Loss: 5.692183715956552, Val MAE: 1.556022047996521\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1114/2000, Train Loss: 4.847459026048819, Val Loss: 5.692092452730451, Val MAE: 1.5562001466751099\n",
      "Epoch 1115/2000, Train Loss: 4.847258645825729, Val Loss: 5.692028258528028, Val MAE: 1.5561784505844116\n",
      "Epoch 1116/2000, Train Loss: 4.847091370812592, Val Loss: 5.692033529281616, Val MAE: 1.556174874305725\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1117/2000, Train Loss: 4.84691337335261, Val Loss: 5.691979967412495, Val MAE: 1.5562139749526978\n",
      "Epoch 1118/2000, Train Loss: 4.846704434608707, Val Loss: 5.691890253907158, Val MAE: 1.5564168691635132\n",
      "Epoch 1119/2000, Train Loss: 4.8465676489266425, Val Loss: 5.691838287171864, Val MAE: 1.5562098026275635\n",
      "Epoch 1120/2000, Train Loss: 4.846387072577968, Val Loss: 5.691798493975685, Val MAE: 1.5564193725585938\n",
      "Epoch 1121/2000, Train Loss: 4.846194788498334, Val Loss: 5.69180318855104, Val MAE: 1.5562636852264404\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1122/2000, Train Loss: 4.846078510513091, Val Loss: 5.691783743245261, Val MAE: 1.556094765663147\n",
      "Epoch 1123/2000, Train Loss: 4.8459005288580075, Val Loss: 5.69170469045639, Val MAE: 1.5562925338745117\n",
      "Epoch 1124/2000, Train Loss: 4.845706921202171, Val Loss: 5.691645290170397, Val MAE: 1.556266188621521\n",
      "Epoch 1125/2000, Train Loss: 4.84554646465775, Val Loss: 5.691640595595042, Val MAE: 1.556174874305725\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1126/2000, Train Loss: 4.845427065919585, Val Loss: 5.69160338810512, Val MAE: 1.5561610460281372\n",
      "Epoch 1127/2000, Train Loss: 4.845252756675645, Val Loss: 5.691617352621896, Val MAE: 1.5559511184692383\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1128/2000, Train Loss: 4.845051579818403, Val Loss: 5.691525964509873, Val MAE: 1.5560081005096436\n",
      "Epoch 1129/2000, Train Loss: 4.844908207865124, Val Loss: 5.691472320329575, Val MAE: 1.5562822818756104\n",
      "Epoch 1130/2000, Train Loss: 4.844746816982167, Val Loss: 5.691372034095583, Val MAE: 1.5562235116958618\n",
      "Epoch 1131/2000, Train Loss: 4.844600523140266, Val Loss: 5.691297860372634, Val MAE: 1.556449294090271\n",
      "Epoch 1132/2000, Train Loss: 4.844479922010801, Val Loss: 5.691322698479607, Val MAE: 1.556352138519287\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1133/2000, Train Loss: 4.844263218690041, Val Loss: 5.691357425280979, Val MAE: 1.5561637878417969\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1134/2000, Train Loss: 4.84407790566365, Val Loss: 5.691349330402556, Val MAE: 1.556109070777893\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1135/2000, Train Loss: 4.843968477168439, Val Loss: 5.6913300809406095, Val MAE: 1.5559275150299072\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1136/2000, Train Loss: 4.843831201870787, Val Loss: 5.691248473666963, Val MAE: 1.5562406778335571\n",
      "Epoch 1137/2000, Train Loss: 4.8435741667182555, Val Loss: 5.691302313691094, Val MAE: 1.5559508800506592\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1138/2000, Train Loss: 4.8434758879065685, Val Loss: 5.6912654638290405, Val MAE: 1.5558451414108276\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1139/2000, Train Loss: 4.843340946353541, Val Loss: 5.6912046528997875, Val MAE: 1.5561374425888062\n",
      "Epoch 1140/2000, Train Loss: 4.843107818043955, Val Loss: 5.691108839852469, Val MAE: 1.5561305284500122\n",
      "Epoch 1141/2000, Train Loss: 4.843053603878478, Val Loss: 5.691137819063096, Val MAE: 1.5558794736862183\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1142/2000, Train Loss: 4.842756192674085, Val Loss: 5.691094023840768, Val MAE: 1.5559568405151367\n",
      "Epoch 1143/2000, Train Loss: 4.842611287981902, Val Loss: 5.691033434300196, Val MAE: 1.5560004711151123\n",
      "Epoch 1144/2000, Train Loss: 4.842460557335018, Val Loss: 5.691003254481724, Val MAE: 1.5559515953063965\n",
      "Epoch 1145/2000, Train Loss: 4.842325248906575, Val Loss: 5.690952652976627, Val MAE: 1.555969476699829\n",
      "Epoch 1146/2000, Train Loss: 4.842113338169157, Val Loss: 5.690965331736065, Val MAE: 1.5557926893234253\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1147/2000, Train Loss: 4.841969496440484, Val Loss: 5.690899559429714, Val MAE: 1.5558496713638306\n",
      "Epoch 1148/2000, Train Loss: 4.841811260148063, Val Loss: 5.690855077334812, Val MAE: 1.5558326244354248\n",
      "Epoch 1149/2000, Train Loss: 4.84170817387289, Val Loss: 5.690899627549308, Val MAE: 1.555532693862915\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1150/2000, Train Loss: 4.841468381444557, Val Loss: 5.690750369003841, Val MAE: 1.555838942527771\n",
      "Epoch 1151/2000, Train Loss: 4.841323510716093, Val Loss: 5.690682348750887, Val MAE: 1.5560003519058228\n",
      "Epoch 1152/2000, Train Loss: 4.84115551557124, Val Loss: 5.690691613015675, Val MAE: 1.5557515621185303\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1153/2000, Train Loss: 4.841002853027688, Val Loss: 5.690670498779842, Val MAE: 1.5556811094284058\n",
      "Epoch 1154/2000, Train Loss: 4.840905497793755, Val Loss: 5.690709883258457, Val MAE: 1.5555086135864258\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1155/2000, Train Loss: 4.840731786571874, Val Loss: 5.6906607037498835, Val MAE: 1.5554320812225342\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1156/2000, Train Loss: 4.840507334479156, Val Loss: 5.690619721299126, Val MAE: 1.5554158687591553\n",
      "Epoch 1157/2000, Train Loss: 4.840364309897375, Val Loss: 5.690636586575281, Val MAE: 1.5554108619689941\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1158/2000, Train Loss: 4.840210452570734, Val Loss: 5.690586390949431, Val MAE: 1.5554300546646118\n",
      "Epoch 1159/2000, Train Loss: 4.840047344670477, Val Loss: 5.690512398878734, Val MAE: 1.5555156469345093\n",
      "Epoch 1160/2000, Train Loss: 4.839883564893887, Val Loss: 5.690536706220536, Val MAE: 1.5554108619689941\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1161/2000, Train Loss: 4.839698552076504, Val Loss: 5.690503270853133, Val MAE: 1.5554158687591553\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1162/2000, Train Loss: 4.839542392951309, Val Loss: 5.690466188249134, Val MAE: 1.5553568601608276\n",
      "Epoch 1163/2000, Train Loss: 4.839384311008857, Val Loss: 5.690383351984478, Val MAE: 1.5554758310317993\n",
      "Epoch 1164/2000, Train Loss: 4.839233324457123, Val Loss: 5.690379906268347, Val MAE: 1.5553131103515625\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1165/2000, Train Loss: 4.839093262116899, Val Loss: 5.69032420147033, Val MAE: 1.5552949905395508\n",
      "Epoch 1166/2000, Train Loss: 4.8389280632621645, Val Loss: 5.690347830454509, Val MAE: 1.5551968812942505\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1167/2000, Train Loss: 4.838812058668043, Val Loss: 5.690265181518736, Val MAE: 1.555116891860962\n",
      "Epoch 1168/2000, Train Loss: 4.838619031421893, Val Loss: 5.690181400094714, Val MAE: 1.5550380945205688\n",
      "Epoch 1169/2000, Train Loss: 4.838451490751947, Val Loss: 5.690152032034738, Val MAE: 1.5550439357757568\n",
      "Epoch 1170/2000, Train Loss: 4.83828984211463, Val Loss: 5.69013706275395, Val MAE: 1.5550180673599243\n",
      "Epoch 1171/2000, Train Loss: 4.8381047729712785, Val Loss: 5.690106428804851, Val MAE: 1.554858922958374\n",
      "Epoch 1172/2000, Train Loss: 4.837958910233211, Val Loss: 5.6900918285051985, Val MAE: 1.5547401905059814\n",
      "Epoch 1173/2000, Train Loss: 4.837827059378577, Val Loss: 5.690015758786883, Val MAE: 1.554834246635437\n",
      "Epoch 1174/2000, Train Loss: 4.837651026097609, Val Loss: 5.689967013540722, Val MAE: 1.5548491477966309\n",
      "Epoch 1175/2000, Train Loss: 4.837536984001128, Val Loss: 5.689913037277403, Val MAE: 1.5548771619796753\n",
      "Epoch 1176/2000, Train Loss: 4.837356727449447, Val Loss: 5.689932099410465, Val MAE: 1.5549029111862183\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1177/2000, Train Loss: 4.837208232018776, Val Loss: 5.689924926984878, Val MAE: 1.554730772972107\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1178/2000, Train Loss: 4.837042048552477, Val Loss: 5.689828673998515, Val MAE: 1.55477774143219\n",
      "Epoch 1179/2000, Train Loss: 4.836879435648198, Val Loss: 5.689802743139721, Val MAE: 1.5548526048660278\n",
      "Epoch 1180/2000, Train Loss: 4.836756878410307, Val Loss: 5.689720633484068, Val MAE: 1.5549112558364868\n",
      "Epoch 1181/2000, Train Loss: 4.836548638108419, Val Loss: 5.689728668757847, Val MAE: 1.554905891418457\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1182/2000, Train Loss: 4.836364714988364, Val Loss: 5.689632288047245, Val MAE: 1.5549490451812744\n",
      "Epoch 1183/2000, Train Loss: 4.836216482860581, Val Loss: 5.689657937912714, Val MAE: 1.5547325611114502\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1184/2000, Train Loss: 4.836073308468538, Val Loss: 5.689629849933443, Val MAE: 1.5546979904174805\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1185/2000, Train Loss: 4.835992149640878, Val Loss: 5.6895245881307694, Val MAE: 1.5549581050872803\n",
      "Epoch 1186/2000, Train Loss: 4.835882244392578, Val Loss: 5.689608139651162, Val MAE: 1.5543622970581055\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1187/2000, Train Loss: 4.835630932715111, Val Loss: 5.689508426757086, Val MAE: 1.5546048879623413\n",
      "Epoch 1188/2000, Train Loss: 4.83549994373187, Val Loss: 5.689452179840633, Val MAE: 1.5548059940338135\n",
      "Epoch 1189/2000, Train Loss: 4.835324156940067, Val Loss: 5.689439699763343, Val MAE: 1.554856777191162\n",
      "Epoch 1190/2000, Train Loss: 4.835198977770355, Val Loss: 5.689305067062378, Val MAE: 1.5550090074539185\n",
      "Epoch 1191/2000, Train Loss: 4.834972224887907, Val Loss: 5.6892972475006465, Val MAE: 1.554822325706482\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1192/2000, Train Loss: 4.834873660831424, Val Loss: 5.6892450366701395, Val MAE: 1.554901123046875\n",
      "Epoch 1193/2000, Train Loss: 4.834691408827207, Val Loss: 5.689097296623957, Val MAE: 1.5552018880844116\n",
      "Epoch 1194/2000, Train Loss: 4.834517572427166, Val Loss: 5.689172988846188, Val MAE: 1.5548855066299438\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1195/2000, Train Loss: 4.834375331701109, Val Loss: 5.689046240988231, Val MAE: 1.5548665523529053\n",
      "Epoch 1196/2000, Train Loss: 4.8341943753623156, Val Loss: 5.689031345503671, Val MAE: 1.55500066280365\n",
      "Epoch 1197/2000, Train Loss: 4.834027579540258, Val Loss: 5.688976767517271, Val MAE: 1.555113673210144\n",
      "Epoch 1198/2000, Train Loss: 4.833861638780037, Val Loss: 5.68901560987745, Val MAE: 1.554833173751831\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1199/2000, Train Loss: 4.833777244151563, Val Loss: 5.688984751701355, Val MAE: 1.5546566247940063\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1200/2000, Train Loss: 4.833647368152656, Val Loss: 5.689017897560483, Val MAE: 1.554419755935669\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1201/2000, Train Loss: 4.833429488207624, Val Loss: 5.688913271540687, Val MAE: 1.554781198501587\n",
      "Epoch 1202/2000, Train Loss: 4.833257205731778, Val Loss: 5.688894385383243, Val MAE: 1.5546108484268188\n",
      "Epoch 1203/2000, Train Loss: 4.83313335787258, Val Loss: 5.688788533210754, Val MAE: 1.5548186302185059\n",
      "Epoch 1204/2000, Train Loss: 4.832960693725242, Val Loss: 5.688752849896749, Val MAE: 1.5546458959579468\n",
      "Epoch 1205/2000, Train Loss: 4.832781471217469, Val Loss: 5.688701998619806, Val MAE: 1.5547548532485962\n",
      "Epoch 1206/2000, Train Loss: 4.832639518678609, Val Loss: 5.688682121889932, Val MAE: 1.5547239780426025\n",
      "Epoch 1207/2000, Train Loss: 4.832509519014775, Val Loss: 5.688549263136728, Val MAE: 1.555031418800354\n",
      "Epoch 1208/2000, Train Loss: 4.832392459191455, Val Loss: 5.688618679841359, Val MAE: 1.5546631813049316\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1209/2000, Train Loss: 4.832231980957332, Val Loss: 5.6885615927832465, Val MAE: 1.554581642150879\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1210/2000, Train Loss: 4.832033001989841, Val Loss: 5.688508263656071, Val MAE: 1.554587960243225\n",
      "Epoch 1211/2000, Train Loss: 4.831852649936219, Val Loss: 5.68844812540781, Val MAE: 1.5547734498977661\n",
      "Epoch 1212/2000, Train Loss: 4.831738198930017, Val Loss: 5.6884205256189615, Val MAE: 1.5546233654022217\n",
      "Epoch 1213/2000, Train Loss: 4.831542884008841, Val Loss: 5.6883688767751055, Val MAE: 1.5546444654464722\n",
      "Epoch 1214/2000, Train Loss: 4.8314029750097625, Val Loss: 5.688318885508037, Val MAE: 1.5546324253082275\n",
      "Epoch 1215/2000, Train Loss: 4.831264083240869, Val Loss: 5.688205332983108, Val MAE: 1.5547852516174316\n",
      "Epoch 1216/2000, Train Loss: 4.83112661660306, Val Loss: 5.688191938967932, Val MAE: 1.5548019409179688\n",
      "Epoch 1217/2000, Train Loss: 4.830918503076637, Val Loss: 5.688103877362751, Val MAE: 1.5548030138015747\n",
      "Epoch 1218/2000, Train Loss: 4.830810411692674, Val Loss: 5.688136370409103, Val MAE: 1.5545320510864258\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1219/2000, Train Loss: 4.830633571016771, Val Loss: 5.68808475846336, Val MAE: 1.5546784400939941\n",
      "Epoch 1220/2000, Train Loss: 4.830484094169143, Val Loss: 5.68805056810379, Val MAE: 1.5546399354934692\n",
      "Epoch 1221/2000, Train Loss: 4.830382986364647, Val Loss: 5.68791344336101, Val MAE: 1.5549256801605225\n",
      "Epoch 1222/2000, Train Loss: 4.8303115162762, Val Loss: 5.6879922066416055, Val MAE: 1.5545800924301147\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1223/2000, Train Loss: 4.83004389007606, Val Loss: 5.6879132617087595, Val MAE: 1.5547568798065186\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1224/2000, Train Loss: 4.829944144077967, Val Loss: 5.687972176642645, Val MAE: 1.554368495941162\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1225/2000, Train Loss: 4.829802515140205, Val Loss: 5.687913219134013, Val MAE: 1.5544902086257935\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1226/2000, Train Loss: 4.829601937524018, Val Loss: 5.687866256350563, Val MAE: 1.554417610168457\n",
      "Epoch 1227/2000, Train Loss: 4.829416874938018, Val Loss: 5.687889604341416, Val MAE: 1.5544010400772095\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1228/2000, Train Loss: 4.829396270057883, Val Loss: 5.68779608749208, Val MAE: 1.554394006729126\n",
      "Epoch 1229/2000, Train Loss: 4.829178253081016, Val Loss: 5.687779596873692, Val MAE: 1.554438591003418\n",
      "Epoch 1230/2000, Train Loss: 4.829047417607058, Val Loss: 5.687689616566613, Val MAE: 1.5546497106552124\n",
      "Epoch 1231/2000, Train Loss: 4.828825045876173, Val Loss: 5.687740084670839, Val MAE: 1.5544110536575317\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1232/2000, Train Loss: 4.828693656894485, Val Loss: 5.687691464310601, Val MAE: 1.5544853210449219\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1233/2000, Train Loss: 4.828581765273058, Val Loss: 5.687654407251449, Val MAE: 1.5545799732208252\n",
      "Epoch 1234/2000, Train Loss: 4.828422555802403, Val Loss: 5.687636659258888, Val MAE: 1.5542668104171753\n",
      "Epoch 1235/2000, Train Loss: 4.828236682794999, Val Loss: 5.687667026406243, Val MAE: 1.5542795658111572\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1236/2000, Train Loss: 4.828077246507233, Val Loss: 5.687617438180106, Val MAE: 1.5543177127838135\n",
      "Epoch 1237/2000, Train Loss: 4.827910967202718, Val Loss: 5.687553933688572, Val MAE: 1.5542387962341309\n",
      "Epoch 1238/2000, Train Loss: 4.827774799088665, Val Loss: 5.687578436874208, Val MAE: 1.5542495250701904\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1239/2000, Train Loss: 4.827656641665568, Val Loss: 5.6876051142102195, Val MAE: 1.5540337562561035\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1240/2000, Train Loss: 4.827465616933718, Val Loss: 5.687514211450305, Val MAE: 1.554266095161438\n",
      "Epoch 1241/2000, Train Loss: 4.827295780013747, Val Loss: 5.687509593509493, Val MAE: 1.554080605506897\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1242/2000, Train Loss: 4.827175106080866, Val Loss: 5.687526705719176, Val MAE: 1.5540460348129272\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1243/2000, Train Loss: 4.827020770235694, Val Loss: 5.687517818950472, Val MAE: 1.553853154182434\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1244/2000, Train Loss: 4.826852779882751, Val Loss: 5.6873567161105925, Val MAE: 1.5542958974838257\n",
      "Epoch 1245/2000, Train Loss: 4.826812840987664, Val Loss: 5.687422644524347, Val MAE: 1.5539727210998535\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1246/2000, Train Loss: 4.826605176723895, Val Loss: 5.687374097960336, Val MAE: 1.553959608078003\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1247/2000, Train Loss: 4.826519983275484, Val Loss: 5.687405467033386, Val MAE: 1.5538179874420166\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1248/2000, Train Loss: 4.826248927580453, Val Loss: 5.687351905164265, Val MAE: 1.5539870262145996\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1249/2000, Train Loss: 4.826097406352357, Val Loss: 5.68726567711149, Val MAE: 1.5540709495544434\n",
      "Epoch 1250/2000, Train Loss: 4.825943325403211, Val Loss: 5.687265486944289, Val MAE: 1.5539270639419556\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1251/2000, Train Loss: 4.825814561547951, Val Loss: 5.6872793691498895, Val MAE: 1.5537534952163696\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1252/2000, Train Loss: 4.82574397249854, Val Loss: 5.687225574538822, Val MAE: 1.5537703037261963\n",
      "Epoch 1253/2000, Train Loss: 4.825551495061102, Val Loss: 5.687114814917247, Val MAE: 1.5540369749069214\n",
      "Epoch 1254/2000, Train Loss: 4.825462137526282, Val Loss: 5.687108028502691, Val MAE: 1.5536866188049316\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1255/2000, Train Loss: 4.825214660386273, Val Loss: 5.686986406644185, Val MAE: 1.5540330410003662\n",
      "Epoch 1256/2000, Train Loss: 4.825030162068785, Val Loss: 5.687029790310633, Val MAE: 1.553760290145874\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1257/2000, Train Loss: 4.82489410240324, Val Loss: 5.686956740560985, Val MAE: 1.5538091659545898\n",
      "Epoch 1258/2000, Train Loss: 4.8247060684962735, Val Loss: 5.686913811025166, Val MAE: 1.5538787841796875\n",
      "Epoch 1259/2000, Train Loss: 4.824637912392448, Val Loss: 5.686893630595434, Val MAE: 1.5540215969085693\n",
      "Epoch 1260/2000, Train Loss: 4.824405435446455, Val Loss: 5.686877645197368, Val MAE: 1.5538471937179565\n",
      "Epoch 1261/2000, Train Loss: 4.824308499630819, Val Loss: 5.686815065996988, Val MAE: 1.5540087223052979\n",
      "Epoch 1262/2000, Train Loss: 4.824143786692653, Val Loss: 5.686769442898886, Val MAE: 1.5540430545806885\n",
      "Epoch 1263/2000, Train Loss: 4.823968241682174, Val Loss: 5.686715747628893, Val MAE: 1.5539671182632446\n",
      "Epoch 1264/2000, Train Loss: 4.823788941495005, Val Loss: 5.686732519240606, Val MAE: 1.55392587184906\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1265/2000, Train Loss: 4.823690725818844, Val Loss: 5.68671746197201, Val MAE: 1.553809404373169\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1266/2000, Train Loss: 4.823528744437967, Val Loss: 5.686625616891043, Val MAE: 1.5540012121200562\n",
      "Epoch 1267/2000, Train Loss: 4.823335534594795, Val Loss: 5.686523241656167, Val MAE: 1.5540196895599365\n",
      "Epoch 1268/2000, Train Loss: 4.823200045586978, Val Loss: 5.686503217333839, Val MAE: 1.5541777610778809\n",
      "Epoch 1269/2000, Train Loss: 4.82302564088313, Val Loss: 5.686481336752574, Val MAE: 1.5541093349456787\n",
      "Epoch 1270/2000, Train Loss: 4.822870847032168, Val Loss: 5.6864354354994635, Val MAE: 1.5539703369140625\n",
      "Epoch 1271/2000, Train Loss: 4.822876213299707, Val Loss: 5.686487839335487, Val MAE: 1.5536596775054932\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1272/2000, Train Loss: 4.8226281066539425, Val Loss: 5.686335864521208, Val MAE: 1.5539990663528442\n",
      "Epoch 1273/2000, Train Loss: 4.8224282172234005, Val Loss: 5.686379983311608, Val MAE: 1.5537843704223633\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1274/2000, Train Loss: 4.822310724783011, Val Loss: 5.686337340445745, Val MAE: 1.553770899772644\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1275/2000, Train Loss: 4.822133715969887, Val Loss: 5.686287908327012, Val MAE: 1.553766131401062\n",
      "Epoch 1276/2000, Train Loss: 4.821982659136795, Val Loss: 5.686264077822368, Val MAE: 1.5537973642349243\n",
      "Epoch 1277/2000, Train Loss: 4.821830050733429, Val Loss: 5.686212511289687, Val MAE: 1.5537796020507812\n",
      "Epoch 1278/2000, Train Loss: 4.821734215880315, Val Loss: 5.686249315738678, Val MAE: 1.553712010383606\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1279/2000, Train Loss: 4.821686084246938, Val Loss: 5.686205040840876, Val MAE: 1.5539582967758179\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1280/2000, Train Loss: 4.8214852017979695, Val Loss: 5.6861375996044705, Val MAE: 1.554175615310669\n",
      "Epoch 1281/2000, Train Loss: 4.821239347014004, Val Loss: 5.686107976096017, Val MAE: 1.5540430545806885\n",
      "Epoch 1282/2000, Train Loss: 4.82113066765418, Val Loss: 5.6861050469534735, Val MAE: 1.5538150072097778\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1283/2000, Train Loss: 4.820979577025506, Val Loss: 5.686079079196567, Val MAE: 1.5538686513900757\n",
      "Epoch 1284/2000, Train Loss: 4.820887702812765, Val Loss: 5.686021069685618, Val MAE: 1.5539321899414062\n",
      "Epoch 1285/2000, Train Loss: 4.820719004013642, Val Loss: 5.68589061498642, Val MAE: 1.5541160106658936\n",
      "Epoch 1286/2000, Train Loss: 4.820500661691254, Val Loss: 5.685907397951398, Val MAE: 1.553975224494934\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1287/2000, Train Loss: 4.820427488709034, Val Loss: 5.6858288588978, Val MAE: 1.5541502237319946\n",
      "Epoch 1288/2000, Train Loss: 4.820218684643045, Val Loss: 5.6858978016035895, Val MAE: 1.5537623167037964\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1289/2000, Train Loss: 4.82033627594812, Val Loss: 5.685793626876104, Val MAE: 1.5540663003921509\n",
      "Epoch 1290/2000, Train Loss: 4.819912996211408, Val Loss: 5.685694833596547, Val MAE: 1.5539706945419312\n",
      "Epoch 1291/2000, Train Loss: 4.819766855038104, Val Loss: 5.685735753604344, Val MAE: 1.5537785291671753\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1292/2000, Train Loss: 4.8196642572015564, Val Loss: 5.6856914928981235, Val MAE: 1.5537737607955933\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1293/2000, Train Loss: 4.81950624268213, Val Loss: 5.685618048622494, Val MAE: 1.5538628101348877\n",
      "Epoch 1294/2000, Train Loss: 4.819348495332748, Val Loss: 5.685645234017145, Val MAE: 1.5536645650863647\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1295/2000, Train Loss: 4.8192304774635435, Val Loss: 5.685629975228083, Val MAE: 1.553562879562378\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1296/2000, Train Loss: 4.819106329311269, Val Loss: 5.685626325153169, Val MAE: 1.5534013509750366\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1297/2000, Train Loss: 4.819039792705154, Val Loss: 5.685580511887868, Val MAE: 1.5537320375442505\n",
      "Epoch 1298/2000, Train Loss: 4.81875878631312, Val Loss: 5.685642571676345, Val MAE: 1.5534241199493408\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1299/2000, Train Loss: 4.818642893950593, Val Loss: 5.685604850451152, Val MAE: 1.553296446800232\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1300/2000, Train Loss: 4.81856107711792, Val Loss: 5.6855344743955705, Val MAE: 1.5534348487854004\n",
      "Epoch 1301/2000, Train Loss: 4.818408625754718, Val Loss: 5.685577690601349, Val MAE: 1.5530939102172852\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1302/2000, Train Loss: 4.818196246923277, Val Loss: 5.685519014086042, Val MAE: 1.5532053709030151\n",
      "Epoch 1303/2000, Train Loss: 4.818021543607725, Val Loss: 5.685473470460801, Val MAE: 1.553394079208374\n",
      "Epoch 1304/2000, Train Loss: 4.817879168371892, Val Loss: 5.685485249473935, Val MAE: 1.5532137155532837\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1305/2000, Train Loss: 4.817735869726442, Val Loss: 5.685381670792897, Val MAE: 1.5534250736236572\n",
      "Epoch 1306/2000, Train Loss: 4.817623884284446, Val Loss: 5.685404340426127, Val MAE: 1.55316960811615\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1307/2000, Train Loss: 4.817455011295163, Val Loss: 5.685330629348755, Val MAE: 1.5533207654953003\n",
      "Epoch 1308/2000, Train Loss: 4.817312239928037, Val Loss: 5.685293756780171, Val MAE: 1.5533065795898438\n",
      "Epoch 1309/2000, Train Loss: 4.817169926398564, Val Loss: 5.685276108128684, Val MAE: 1.5533027648925781\n",
      "Epoch 1310/2000, Train Loss: 4.817043461483523, Val Loss: 5.685244378589449, Val MAE: 1.5532604455947876\n",
      "Epoch 1311/2000, Train Loss: 4.817004696774718, Val Loss: 5.685287398951394, Val MAE: 1.553001046180725\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1312/2000, Train Loss: 4.816812925513608, Val Loss: 5.685262935502188, Val MAE: 1.5528619289398193\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1313/2000, Train Loss: 4.81656568174806, Val Loss: 5.6851660552478975, Val MAE: 1.5531450510025024\n",
      "Epoch 1314/2000, Train Loss: 4.816525227932735, Val Loss: 5.685106158256531, Val MAE: 1.5535169839859009\n",
      "Epoch 1315/2000, Train Loss: 4.816293826056132, Val Loss: 5.685138100669498, Val MAE: 1.5533162355422974\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1316/2000, Train Loss: 4.816110925042243, Val Loss: 5.685132185618083, Val MAE: 1.5533101558685303\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1317/2000, Train Loss: 4.815969400580746, Val Loss: 5.6850315020197915, Val MAE: 1.5533002614974976\n",
      "Epoch 1318/2000, Train Loss: 4.815857668689679, Val Loss: 5.685002460366204, Val MAE: 1.5533207654953003\n",
      "Epoch 1319/2000, Train Loss: 4.815751596297464, Val Loss: 5.685031050727481, Val MAE: 1.5530061721801758\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1320/2000, Train Loss: 4.815545212566769, Val Loss: 5.684907552741823, Val MAE: 1.553370475769043\n",
      "Epoch 1321/2000, Train Loss: 4.815483709316496, Val Loss: 5.684966725962503, Val MAE: 1.5529725551605225\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1322/2000, Train Loss: 4.815228183027383, Val Loss: 5.684831403550648, Val MAE: 1.5532373189926147\n",
      "Epoch 1323/2000, Train Loss: 4.81516051864086, Val Loss: 5.684766911325001, Val MAE: 1.5534021854400635\n",
      "Epoch 1324/2000, Train Loss: 4.81498608501741, Val Loss: 5.684822962397621, Val MAE: 1.553314208984375\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1325/2000, Train Loss: 4.814847292449477, Val Loss: 5.684814055760701, Val MAE: 1.5532546043395996\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1326/2000, Train Loss: 4.814687667175476, Val Loss: 5.684773964541299, Val MAE: 1.553173303604126\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1327/2000, Train Loss: 4.814496322142221, Val Loss: 5.6847396578107565, Val MAE: 1.5531318187713623\n",
      "Epoch 1328/2000, Train Loss: 4.814474416552545, Val Loss: 5.684688301313491, Val MAE: 1.5531423091888428\n",
      "Epoch 1329/2000, Train Loss: 4.814208236836916, Val Loss: 5.68465541941779, Val MAE: 1.5530614852905273\n",
      "Epoch 1330/2000, Train Loss: 4.81417303730974, Val Loss: 5.684697054681324, Val MAE: 1.5528644323349\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1331/2000, Train Loss: 4.813908135773264, Val Loss: 5.684598945436024, Val MAE: 1.552987813949585\n",
      "Epoch 1332/2000, Train Loss: 4.813786545828805, Val Loss: 5.684568839413779, Val MAE: 1.5530027151107788\n",
      "Epoch 1333/2000, Train Loss: 4.813664085269815, Val Loss: 5.684628983338674, Val MAE: 1.5529524087905884\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1334/2000, Train Loss: 4.813502461012395, Val Loss: 5.684489480086735, Val MAE: 1.5530763864517212\n",
      "Epoch 1335/2000, Train Loss: 4.813408767891199, Val Loss: 5.684393510932014, Val MAE: 1.553349256515503\n",
      "Epoch 1336/2000, Train Loss: 4.8132594398450115, Val Loss: 5.684456595352718, Val MAE: 1.5530312061309814\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1337/2000, Train Loss: 4.813072891262253, Val Loss: 5.68436092422122, Val MAE: 1.5532469749450684\n",
      "Epoch 1338/2000, Train Loss: 4.812987660823655, Val Loss: 5.684325098991394, Val MAE: 1.5530420541763306\n",
      "Epoch 1339/2000, Train Loss: 4.8127618347135686, Val Loss: 5.684342273644039, Val MAE: 1.552996277809143\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1340/2000, Train Loss: 4.812630386043167, Val Loss: 5.684324457531884, Val MAE: 1.5529067516326904\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1341/2000, Train Loss: 4.812527807878674, Val Loss: 5.684258741991861, Val MAE: 1.5529091358184814\n",
      "Epoch 1342/2000, Train Loss: 4.812329825964902, Val Loss: 5.684311571575346, Val MAE: 1.5526628494262695\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1343/2000, Train Loss: 4.812170107038468, Val Loss: 5.684267220043001, Val MAE: 1.5527547597885132\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1344/2000, Train Loss: 4.81208169678875, Val Loss: 5.684202168669019, Val MAE: 1.552857518196106\n",
      "Epoch 1345/2000, Train Loss: 4.811910451719556, Val Loss: 5.684159264678047, Val MAE: 1.552966833114624\n",
      "Epoch 1346/2000, Train Loss: 4.811809497424342, Val Loss: 5.684212693146297, Val MAE: 1.5527420043945312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1347/2000, Train Loss: 4.811595569040274, Val Loss: 5.68417748666945, Val MAE: 1.5526130199432373\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1348/2000, Train Loss: 4.811582111337121, Val Loss: 5.684159667718978, Val MAE: 1.5525003671646118\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1349/2000, Train Loss: 4.8113545577852275, Val Loss: 5.684136203357151, Val MAE: 1.5526602268218994\n",
      "Epoch 1350/2000, Train Loss: 4.811188679656121, Val Loss: 5.68411344005948, Val MAE: 1.5526944398880005\n",
      "Epoch 1351/2000, Train Loss: 4.811030171317678, Val Loss: 5.684045121783302, Val MAE: 1.55281662940979\n",
      "Epoch 1352/2000, Train Loss: 4.810915538050728, Val Loss: 5.684042655286335, Val MAE: 1.5526273250579834\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1353/2000, Train Loss: 4.810801754885567, Val Loss: 5.683953024092174, Val MAE: 1.552789568901062\n",
      "Epoch 1354/2000, Train Loss: 4.810607260137754, Val Loss: 5.683928336415972, Val MAE: 1.552858591079712\n",
      "Epoch 1355/2000, Train Loss: 4.810480889211421, Val Loss: 5.68396323351633, Val MAE: 1.5528295040130615\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1356/2000, Train Loss: 4.810307962770018, Val Loss: 5.683936669712975, Val MAE: 1.552756667137146\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1357/2000, Train Loss: 4.810164388968007, Val Loss: 5.683838180133274, Val MAE: 1.5528401136398315\n",
      "Epoch 1358/2000, Train Loss: 4.810021202722289, Val Loss: 5.6838184368042715, Val MAE: 1.5528239011764526\n",
      "Epoch 1359/2000, Train Loss: 4.809918613628877, Val Loss: 5.683797362304869, Val MAE: 1.5526903867721558\n",
      "Epoch 1360/2000, Train Loss: 4.8097410235653815, Val Loss: 5.683834904716129, Val MAE: 1.552685022354126\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1361/2000, Train Loss: 4.809551260535236, Val Loss: 5.683795418058123, Val MAE: 1.5525591373443604\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1362/2000, Train Loss: 4.8094426859249015, Val Loss: 5.6838087638219195, Val MAE: 1.552384853363037\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1363/2000, Train Loss: 4.809340947773964, Val Loss: 5.683838886874063, Val MAE: 1.5522239208221436\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1364/2000, Train Loss: 4.809146245376684, Val Loss: 5.683795225052607, Val MAE: 1.5523394346237183\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1365/2000, Train Loss: 4.809025355219337, Val Loss: 5.6837115827060884, Val MAE: 1.5524159669876099\n",
      "Epoch 1366/2000, Train Loss: 4.808884835209597, Val Loss: 5.683708017780667, Val MAE: 1.5524790287017822\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1367/2000, Train Loss: 4.808727159486671, Val Loss: 5.683692452453432, Val MAE: 1.55233633518219\n",
      "Epoch 1368/2000, Train Loss: 4.808587002317054, Val Loss: 5.683604339758555, Val MAE: 1.5525211095809937\n",
      "Epoch 1369/2000, Train Loss: 4.808435326402715, Val Loss: 5.683607700325194, Val MAE: 1.552337646484375\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1370/2000, Train Loss: 4.8082868343347895, Val Loss: 5.683643891697838, Val MAE: 1.5522626638412476\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1371/2000, Train Loss: 4.8080982334691145, Val Loss: 5.6836297398521785, Val MAE: 1.5521823167800903\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1372/2000, Train Loss: 4.808002927911971, Val Loss: 5.68359485126677, Val MAE: 1.5521563291549683\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1373/2000, Train Loss: 4.807866198387738, Val Loss: 5.683517413479941, Val MAE: 1.5521483421325684\n",
      "Epoch 1374/2000, Train Loss: 4.807699610383286, Val Loss: 5.683532572927929, Val MAE: 1.5521256923675537\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1375/2000, Train Loss: 4.807571616932771, Val Loss: 5.68350395986012, Val MAE: 1.5520952939987183\n",
      "Epoch 1376/2000, Train Loss: 4.807456911198679, Val Loss: 5.683508171921685, Val MAE: 1.5521302223205566\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1377/2000, Train Loss: 4.807479738684736, Val Loss: 5.683446381773267, Val MAE: 1.552248477935791\n",
      "Epoch 1378/2000, Train Loss: 4.807180779945363, Val Loss: 5.68348703497932, Val MAE: 1.5519212484359741\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1379/2000, Train Loss: 4.8070206198268615, Val Loss: 5.683345028332302, Val MAE: 1.5520844459533691\n",
      "Epoch 1380/2000, Train Loss: 4.8069012699745945, Val Loss: 5.683344369842892, Val MAE: 1.5521730184555054\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1381/2000, Train Loss: 4.806731256510542, Val Loss: 5.683328566097078, Val MAE: 1.5522689819335938\n",
      "Epoch 1382/2000, Train Loss: 4.806589874797547, Val Loss: 5.683292445682344, Val MAE: 1.5521459579467773\n",
      "Epoch 1383/2000, Train Loss: 4.80646843768982, Val Loss: 5.683235000996363, Val MAE: 1.5522435903549194\n",
      "Epoch 1384/2000, Train Loss: 4.806327195026307, Val Loss: 5.683308184146881, Val MAE: 1.5519009828567505\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1385/2000, Train Loss: 4.806153922894778, Val Loss: 5.683246172609783, Val MAE: 1.5519347190856934\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1386/2000, Train Loss: 4.806114698498811, Val Loss: 5.683109601338704, Val MAE: 1.5523420572280884\n",
      "Epoch 1387/2000, Train Loss: 4.805890665740321, Val Loss: 5.683137896515074, Val MAE: 1.552046298980713\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1388/2000, Train Loss: 4.805805998225138, Val Loss: 5.683049468767075, Val MAE: 1.5523834228515625\n",
      "Epoch 1389/2000, Train Loss: 4.805563471044909, Val Loss: 5.683054682754335, Val MAE: 1.5522654056549072\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1390/2000, Train Loss: 4.805545064054524, Val Loss: 5.683113660131182, Val MAE: 1.5519787073135376\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1391/2000, Train Loss: 4.805349670234957, Val Loss: 5.682963490486145, Val MAE: 1.5523113012313843\n",
      "Epoch 1392/2000, Train Loss: 4.8051952132048825, Val Loss: 5.68304287251972, Val MAE: 1.5520899295806885\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1393/2000, Train Loss: 4.80504491450252, Val Loss: 5.68296682266962, Val MAE: 1.5522222518920898\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1394/2000, Train Loss: 4.80487222681597, Val Loss: 5.682962951206026, Val MAE: 1.552081823348999\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1395/2000, Train Loss: 4.804764721726497, Val Loss: 5.682967773505619, Val MAE: 1.5519020557403564\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1396/2000, Train Loss: 4.804595561895115, Val Loss: 5.682954893225715, Val MAE: 1.5520269870758057\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1397/2000, Train Loss: 4.804539011967367, Val Loss: 5.682959868794396, Val MAE: 1.5516804456710815\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1398/2000, Train Loss: 4.804317478400527, Val Loss: 5.682792365550995, Val MAE: 1.5520668029785156\n",
      "Epoch 1399/2000, Train Loss: 4.804206558275963, Val Loss: 5.682860925084069, Val MAE: 1.5520355701446533\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1400/2000, Train Loss: 4.804055908671219, Val Loss: 5.682838962191627, Val MAE: 1.5520403385162354\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1401/2000, Train Loss: 4.803923383586329, Val Loss: 5.682730955736978, Val MAE: 1.5520521402359009\n",
      "Epoch 1402/2000, Train Loss: 4.803756403233672, Val Loss: 5.682711740334828, Val MAE: 1.5520811080932617\n",
      "Epoch 1403/2000, Train Loss: 4.803652772950521, Val Loss: 5.682787548928034, Val MAE: 1.5518468618392944\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1404/2000, Train Loss: 4.80353013214843, Val Loss: 5.682634989420573, Val MAE: 1.5521750450134277\n",
      "Epoch 1405/2000, Train Loss: 4.803317687743474, Val Loss: 5.682629718666985, Val MAE: 1.5521433353424072\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1406/2000, Train Loss: 4.803208875050834, Val Loss: 5.682551494666508, Val MAE: 1.552314043045044\n",
      "Epoch 1407/2000, Train Loss: 4.803023589598948, Val Loss: 5.682586275395893, Val MAE: 1.5521982908248901\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1408/2000, Train Loss: 4.802926789722254, Val Loss: 5.682597571895236, Val MAE: 1.5520479679107666\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1409/2000, Train Loss: 4.802786142432639, Val Loss: 5.682544333594186, Val MAE: 1.5519710779190063\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1410/2000, Train Loss: 4.802619405458609, Val Loss: 5.682608561856406, Val MAE: 1.551904559135437\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1411/2000, Train Loss: 4.802595673898716, Val Loss: 5.682606677214305, Val MAE: 1.55154550075531\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1412/2000, Train Loss: 4.802368335562465, Val Loss: 5.682520809627714, Val MAE: 1.551816463470459\n",
      "Epoch 1413/2000, Train Loss: 4.802202277862807, Val Loss: 5.682478206498282, Val MAE: 1.5519623756408691\n",
      "Epoch 1414/2000, Train Loss: 4.80215127377315, Val Loss: 5.6825880919184, Val MAE: 1.551647424697876\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1415/2000, Train Loss: 4.8019655789912, Val Loss: 5.682388575304122, Val MAE: 1.552004098892212\n",
      "Epoch 1416/2000, Train Loss: 4.801799381401374, Val Loss: 5.682336991741543, Val MAE: 1.552091121673584\n",
      "Epoch 1417/2000, Train Loss: 4.801719576750891, Val Loss: 5.68221697637013, Val MAE: 1.5522727966308594\n",
      "Epoch 1418/2000, Train Loss: 4.80151257259385, Val Loss: 5.682225917066846, Val MAE: 1.552088737487793\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1419/2000, Train Loss: 4.801412235025963, Val Loss: 5.68225709313438, Val MAE: 1.5519925355911255\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1420/2000, Train Loss: 4.801259601065739, Val Loss: 5.682244116351718, Val MAE: 1.5519908666610718\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1421/2000, Train Loss: 4.801097053903114, Val Loss: 5.6821652338618325, Val MAE: 1.5521045923233032\n",
      "Epoch 1422/2000, Train Loss: 4.800996223188758, Val Loss: 5.682258603118715, Val MAE: 1.551740050315857\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1423/2000, Train Loss: 4.800856611456286, Val Loss: 5.682164717288244, Val MAE: 1.5520373582839966\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1424/2000, Train Loss: 4.8007405480141365, Val Loss: 5.682188181650071, Val MAE: 1.5518461465835571\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1425/2000, Train Loss: 4.800569463684125, Val Loss: 5.68221378326416, Val MAE: 1.5516613721847534\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1426/2000, Train Loss: 4.800393937841289, Val Loss: 5.68211263134366, Val MAE: 1.5520055294036865\n",
      "Epoch 1427/2000, Train Loss: 4.800241120275221, Val Loss: 5.68209718806403, Val MAE: 1.5518903732299805\n",
      "Epoch 1428/2000, Train Loss: 4.800113810975057, Val Loss: 5.68212556271326, Val MAE: 1.5518308877944946\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1429/2000, Train Loss: 4.799992453396237, Val Loss: 5.682033101717631, Val MAE: 1.5519059896469116\n",
      "Epoch 1430/2000, Train Loss: 4.799866892891306, Val Loss: 5.682012075469608, Val MAE: 1.5516759157180786\n",
      "Epoch 1431/2000, Train Loss: 4.799787896476444, Val Loss: 5.681981520993369, Val MAE: 1.551814079284668\n",
      "Epoch 1432/2000, Train Loss: 4.799660161452838, Val Loss: 5.682037396090371, Val MAE: 1.551513671875\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1433/2000, Train Loss: 4.799458595189793, Val Loss: 5.681916954971495, Val MAE: 1.5516964197158813\n",
      "Epoch 1434/2000, Train Loss: 4.799347573342209, Val Loss: 5.681861951237633, Val MAE: 1.5517737865447998\n",
      "Epoch 1435/2000, Train Loss: 4.799222009306398, Val Loss: 5.68182738338198, Val MAE: 1.5518074035644531\n",
      "Epoch 1436/2000, Train Loss: 4.799021738372502, Val Loss: 5.681749199117933, Val MAE: 1.5519700050354004\n",
      "Epoch 1437/2000, Train Loss: 4.798867139560715, Val Loss: 5.681758673418136, Val MAE: 1.5517570972442627\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1438/2000, Train Loss: 4.798803994618619, Val Loss: 5.681672516323271, Val MAE: 1.5520048141479492\n",
      "Epoch 1439/2000, Train Loss: 4.798636718932933, Val Loss: 5.681687403292883, Val MAE: 1.551817774772644\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1440/2000, Train Loss: 4.798548035964643, Val Loss: 5.681664977754865, Val MAE: 1.552038311958313\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1441/2000, Train Loss: 4.798319628948217, Val Loss: 5.681672201270149, Val MAE: 1.5518004894256592\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1442/2000, Train Loss: 4.798208735725607, Val Loss: 5.681643474669683, Val MAE: 1.5516825914382935\n",
      "Epoch 1443/2000, Train Loss: 4.798065228590003, Val Loss: 5.681667583329337, Val MAE: 1.5515347719192505\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1444/2000, Train Loss: 4.797939309952794, Val Loss: 5.681651487236931, Val MAE: 1.5515198707580566\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1445/2000, Train Loss: 4.797820875439556, Val Loss: 5.681556321325756, Val MAE: 1.5517138242721558\n",
      "Epoch 1446/2000, Train Loss: 4.797641460919078, Val Loss: 5.68154110511144, Val MAE: 1.5517123937606812\n",
      "Epoch 1447/2000, Train Loss: 4.797574686566932, Val Loss: 5.681552486760276, Val MAE: 1.5514585971832275\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1448/2000, Train Loss: 4.7974649654288894, Val Loss: 5.681498905022939, Val MAE: 1.5515307188034058\n",
      "Epoch 1449/2000, Train Loss: 4.7972564204623565, Val Loss: 5.681489864985148, Val MAE: 1.5515965223312378\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1450/2000, Train Loss: 4.797087636753938, Val Loss: 5.681531925996144, Val MAE: 1.5514686107635498\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1451/2000, Train Loss: 4.796972881419366, Val Loss: 5.681485431534903, Val MAE: 1.5515414476394653\n",
      "Epoch 1452/2000, Train Loss: 4.796838835701451, Val Loss: 5.681506366956802, Val MAE: 1.5513674020767212\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1453/2000, Train Loss: 4.79667653939956, Val Loss: 5.68139831508909, Val MAE: 1.5514318943023682\n",
      "Epoch 1454/2000, Train Loss: 4.796597343910228, Val Loss: 5.681364439782643, Val MAE: 1.5513912439346313\n",
      "Epoch 1455/2000, Train Loss: 4.79642917659286, Val Loss: 5.6814349464007785, Val MAE: 1.5511707067489624\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1456/2000, Train Loss: 4.796384371890672, Val Loss: 5.681442995866139, Val MAE: 1.5511058568954468\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1457/2000, Train Loss: 4.79625857498481, Val Loss: 5.681396640482403, Val MAE: 1.551213264465332\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1458/2000, Train Loss: 4.7960426511482055, Val Loss: 5.6813777429716925, Val MAE: 1.5511318445205688\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1459/2000, Train Loss: 4.795928749278167, Val Loss: 5.68143417721703, Val MAE: 1.5509694814682007\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1460/2000, Train Loss: 4.795736567089689, Val Loss: 5.681357633499872, Val MAE: 1.5512001514434814\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1461/2000, Train Loss: 4.795594446749882, Val Loss: 5.681327064832051, Val MAE: 1.551264762878418\n",
      "Epoch 1462/2000, Train Loss: 4.795522752534519, Val Loss: 5.6812726230848405, Val MAE: 1.551167607307434\n",
      "Epoch 1463/2000, Train Loss: 4.795350715370878, Val Loss: 5.681263920806703, Val MAE: 1.551297903060913\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1464/2000, Train Loss: 4.795258223421987, Val Loss: 5.681251134191241, Val MAE: 1.5511828660964966\n",
      "Epoch 1465/2000, Train Loss: 4.79509744657616, Val Loss: 5.681201770192101, Val MAE: 1.551316261291504\n",
      "Epoch 1466/2000, Train Loss: 4.794911315982533, Val Loss: 5.6812590928304765, Val MAE: 1.5511553287506104\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1467/2000, Train Loss: 4.794793241955498, Val Loss: 5.681253841945103, Val MAE: 1.5510362386703491\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1468/2000, Train Loss: 4.7946678559770035, Val Loss: 5.681247929732005, Val MAE: 1.550873875617981\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1469/2000, Train Loss: 4.794501533293085, Val Loss: 5.681259759834835, Val MAE: 1.5508390665054321\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1470/2000, Train Loss: 4.794325392236158, Val Loss: 5.681161801020305, Val MAE: 1.5511243343353271\n",
      "Epoch 1471/2000, Train Loss: 4.794267281154315, Val Loss: 5.681215777283623, Val MAE: 1.5509917736053467\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1472/2000, Train Loss: 4.794134482678304, Val Loss: 5.6811388447171165, Val MAE: 1.5512197017669678\n",
      "Epoch 1473/2000, Train Loss: 4.793970707609556, Val Loss: 5.681121797788711, Val MAE: 1.551114559173584\n",
      "Epoch 1474/2000, Train Loss: 4.793869812834918, Val Loss: 5.681124996571314, Val MAE: 1.550933599472046\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1475/2000, Train Loss: 4.793654343641359, Val Loss: 5.681137445427122, Val MAE: 1.550965666770935\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1476/2000, Train Loss: 4.793621397152942, Val Loss: 5.680943472044809, Val MAE: 1.5512782335281372\n",
      "Epoch 1477/2000, Train Loss: 4.7935318546671795, Val Loss: 5.681067339011601, Val MAE: 1.550824522972107\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1478/2000, Train Loss: 4.793266960192467, Val Loss: 5.681074922993069, Val MAE: 1.550719976425171\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1479/2000, Train Loss: 4.793121467104079, Val Loss: 5.68104711884544, Val MAE: 1.5508085489273071\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1480/2000, Train Loss: 4.792971243811259, Val Loss: 5.681024102937608, Val MAE: 1.5508702993392944\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1481/2000, Train Loss: 4.792886873562682, Val Loss: 5.681079955328078, Val MAE: 1.5506795644760132\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1482/2000, Train Loss: 4.792745728755031, Val Loss: 5.6809854337147305, Val MAE: 1.5506755113601685\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1483/2000, Train Loss: 4.792563814370353, Val Loss: 5.680905844484057, Val MAE: 1.5508447885513306\n",
      "Epoch 1484/2000, Train Loss: 4.7924469700989505, Val Loss: 5.680906843571436, Val MAE: 1.5507597923278809\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1485/2000, Train Loss: 4.792339652146204, Val Loss: 5.680827410448165, Val MAE: 1.5507805347442627\n",
      "Epoch 1486/2000, Train Loss: 4.792147050623497, Val Loss: 5.680770263785408, Val MAE: 1.5509105920791626\n",
      "Epoch 1487/2000, Train Loss: 4.792125373700442, Val Loss: 5.680776615937551, Val MAE: 1.5508019924163818\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1488/2000, Train Loss: 4.791927901578723, Val Loss: 5.680720039776394, Val MAE: 1.5508662462234497\n",
      "Epoch 1489/2000, Train Loss: 4.79175264172897, Val Loss: 5.68079028526942, Val MAE: 1.5505781173706055\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1490/2000, Train Loss: 4.791694478188314, Val Loss: 5.680761527447474, Val MAE: 1.5504813194274902\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1491/2000, Train Loss: 4.791528734737122, Val Loss: 5.680703367505755, Val MAE: 1.5506460666656494\n",
      "Epoch 1492/2000, Train Loss: 4.791441637302824, Val Loss: 5.680700634207044, Val MAE: 1.5504989624023438\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1493/2000, Train Loss: 4.7912727377478594, Val Loss: 5.680682275976453, Val MAE: 1.5504004955291748\n",
      "Epoch 1494/2000, Train Loss: 4.791128455835942, Val Loss: 5.680590683505649, Val MAE: 1.550644874572754\n",
      "Epoch 1495/2000, Train Loss: 4.790991110593207, Val Loss: 5.680614712692442, Val MAE: 1.5506670475006104\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1496/2000, Train Loss: 4.7908427637958395, Val Loss: 5.6804987362452914, Val MAE: 1.550736427307129\n",
      "Epoch 1497/2000, Train Loss: 4.790768155258028, Val Loss: 5.680453351565769, Val MAE: 1.550868034362793\n",
      "Epoch 1498/2000, Train Loss: 4.790563203383904, Val Loss: 5.680400252342224, Val MAE: 1.550769567489624\n",
      "Epoch 1499/2000, Train Loss: 4.79045771896755, Val Loss: 5.680379719961257, Val MAE: 1.550804853439331\n",
      "Epoch 1500/2000, Train Loss: 4.790264373773916, Val Loss: 5.680329407964434, Val MAE: 1.5508713722229004\n",
      "Epoch 1501/2000, Train Loss: 4.790139293132615, Val Loss: 5.680331829048338, Val MAE: 1.5508172512054443\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1502/2000, Train Loss: 4.790017973384669, Val Loss: 5.6802794734636946, Val MAE: 1.550828456878662\n",
      "Epoch 1503/2000, Train Loss: 4.789897218577784, Val Loss: 5.680230152039301, Val MAE: 1.5509538650512695\n",
      "Epoch 1504/2000, Train Loss: 4.789739452060759, Val Loss: 5.680212352957044, Val MAE: 1.550840139389038\n",
      "Epoch 1505/2000, Train Loss: 4.789621053192612, Val Loss: 5.680172877652304, Val MAE: 1.5507173538208008\n",
      "Epoch 1506/2000, Train Loss: 4.789467539286243, Val Loss: 5.680121208940234, Val MAE: 1.550964593887329\n",
      "Epoch 1507/2000, Train Loss: 4.789323524292165, Val Loss: 5.680118052732377, Val MAE: 1.5509068965911865\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1508/2000, Train Loss: 4.789210423091571, Val Loss: 5.680085738499959, Val MAE: 1.5508586168289185\n",
      "Epoch 1509/2000, Train Loss: 4.789086345893203, Val Loss: 5.680052782808032, Val MAE: 1.5508692264556885\n",
      "Epoch 1510/2000, Train Loss: 4.788928266640947, Val Loss: 5.679988895143781, Val MAE: 1.5509334802627563\n",
      "Epoch 1511/2000, Train Loss: 4.788831715859546, Val Loss: 5.680012245972951, Val MAE: 1.5509313344955444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1512/2000, Train Loss: 4.788671870157648, Val Loss: 5.679971604120164, Val MAE: 1.5508055686950684\n",
      "Epoch 1513/2000, Train Loss: 4.788547565301483, Val Loss: 5.679937368347531, Val MAE: 1.5508055686950684\n",
      "Epoch 1514/2000, Train Loss: 4.788399814046152, Val Loss: 5.679969517957597, Val MAE: 1.5507116317749023\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1515/2000, Train Loss: 4.788276378459419, Val Loss: 5.679976667676653, Val MAE: 1.550628900527954\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1516/2000, Train Loss: 4.7881530861255985, Val Loss: 5.679899499529884, Val MAE: 1.5505567789077759\n",
      "Epoch 1517/2000, Train Loss: 4.788000880242739, Val Loss: 5.679859081904094, Val MAE: 1.5506724119186401\n",
      "Epoch 1518/2000, Train Loss: 4.7879002676023585, Val Loss: 5.679755060445695, Val MAE: 1.550923466682434\n",
      "Epoch 1519/2000, Train Loss: 4.78774635983455, Val Loss: 5.679704589503152, Val MAE: 1.550997018814087\n",
      "Epoch 1520/2000, Train Loss: 4.787681147813461, Val Loss: 5.679625451564789, Val MAE: 1.5512553453445435\n",
      "Epoch 1521/2000, Train Loss: 4.78749788026043, Val Loss: 5.679737726847331, Val MAE: 1.5508204698562622\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1522/2000, Train Loss: 4.787375698304816, Val Loss: 5.679775592826662, Val MAE: 1.5507663488388062\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1523/2000, Train Loss: 4.787195432673725, Val Loss: 5.67965799286252, Val MAE: 1.5509157180786133\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1524/2000, Train Loss: 4.787062258242888, Val Loss: 5.679646852470579, Val MAE: 1.550911545753479\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1525/2000, Train Loss: 4.786928400838661, Val Loss: 5.679588448433649, Val MAE: 1.5509778261184692\n",
      "Epoch 1526/2000, Train Loss: 4.786808406012015, Val Loss: 5.679610326176598, Val MAE: 1.5510112047195435\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1527/2000, Train Loss: 4.7867253596759145, Val Loss: 5.679553145454044, Val MAE: 1.551021933555603\n",
      "Epoch 1528/2000, Train Loss: 4.786550658714284, Val Loss: 5.6795553508259005, Val MAE: 1.5508337020874023\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1529/2000, Train Loss: 4.786405489374115, Val Loss: 5.679539515858605, Val MAE: 1.5508426427841187\n",
      "Epoch 1530/2000, Train Loss: 4.786270364215243, Val Loss: 5.679511598178318, Val MAE: 1.5509378910064697\n",
      "Epoch 1531/2000, Train Loss: 4.786160378933626, Val Loss: 5.679482550848098, Val MAE: 1.5508921146392822\n",
      "Epoch 1532/2000, Train Loss: 4.786034031882777, Val Loss: 5.67942825669334, Val MAE: 1.5509757995605469\n",
      "Epoch 1533/2000, Train Loss: 4.785916762224206, Val Loss: 5.679368808155968, Val MAE: 1.5511976480484009\n",
      "Epoch 1534/2000, Train Loss: 4.785741420996038, Val Loss: 5.679361737909771, Val MAE: 1.5510480403900146\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1535/2000, Train Loss: 4.785652517767988, Val Loss: 5.67939323470706, Val MAE: 1.5508219003677368\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1536/2000, Train Loss: 4.785490021214667, Val Loss: 5.679384373483204, Val MAE: 1.5507184267044067\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1537/2000, Train Loss: 4.785362188503336, Val Loss: 5.679338398433867, Val MAE: 1.5507341623306274\n",
      "Epoch 1538/2000, Train Loss: 4.785205738837023, Val Loss: 5.679320014658428, Val MAE: 1.550656795501709\n",
      "Epoch 1539/2000, Train Loss: 4.785043209328806, Val Loss: 5.679326690378643, Val MAE: 1.550687551498413\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1540/2000, Train Loss: 4.784920586349261, Val Loss: 5.6792425484884355, Val MAE: 1.550813913345337\n",
      "Epoch 1541/2000, Train Loss: 4.784885792200925, Val Loss: 5.679195137250991, Val MAE: 1.5509876012802124\n",
      "Epoch 1542/2000, Train Loss: 4.78467972423193, Val Loss: 5.679208114033654, Val MAE: 1.5508193969726562\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1543/2000, Train Loss: 4.784670439358322, Val Loss: 5.679125879492078, Val MAE: 1.550974726676941\n",
      "Epoch 1544/2000, Train Loss: 4.784438835044506, Val Loss: 5.679150575683231, Val MAE: 1.5507605075836182\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1545/2000, Train Loss: 4.784342841469855, Val Loss: 5.679104055677142, Val MAE: 1.5509082078933716\n",
      "Epoch 1546/2000, Train Loss: 4.784126906368057, Val Loss: 5.679092586040497, Val MAE: 1.550742745399475\n",
      "Epoch 1547/2000, Train Loss: 4.784079805247706, Val Loss: 5.679196076733725, Val MAE: 1.5504475831985474\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1548/2000, Train Loss: 4.783934672561452, Val Loss: 5.679109686896915, Val MAE: 1.550567626953125\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1549/2000, Train Loss: 4.783770532971209, Val Loss: 5.679109292370932, Val MAE: 1.5505818128585815\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1550/2000, Train Loss: 4.783678807728045, Val Loss: 5.679018230665298, Val MAE: 1.5507246255874634\n",
      "Epoch 1551/2000, Train Loss: 4.7835617906115795, Val Loss: 5.678957893734887, Val MAE: 1.5508226156234741\n",
      "Epoch 1552/2000, Train Loss: 4.78339617343144, Val Loss: 5.678915614173526, Val MAE: 1.550811767578125\n",
      "Epoch 1553/2000, Train Loss: 4.783284747550114, Val Loss: 5.678853091739473, Val MAE: 1.5511051416397095\n",
      "Epoch 1554/2000, Train Loss: 4.783127554549148, Val Loss: 5.678893557616642, Val MAE: 1.5509313344955444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1555/2000, Train Loss: 4.782997746057336, Val Loss: 5.678864694776989, Val MAE: 1.5507829189300537\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1556/2000, Train Loss: 4.782900410075114, Val Loss: 5.678873487881252, Val MAE: 1.5508226156234741\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1557/2000, Train Loss: 4.782739161772519, Val Loss: 5.678870155697777, Val MAE: 1.5509462356567383\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1558/2000, Train Loss: 4.782602762133513, Val Loss: 5.6787906885147095, Val MAE: 1.5509276390075684\n",
      "Epoch 1559/2000, Train Loss: 4.7824665064199685, Val Loss: 5.678686706792741, Val MAE: 1.551114797592163\n",
      "Epoch 1560/2000, Train Loss: 4.782323857120465, Val Loss: 5.678715223357791, Val MAE: 1.5510778427124023\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1561/2000, Train Loss: 4.78218107667393, Val Loss: 5.67867097116652, Val MAE: 1.5511082410812378\n",
      "Epoch 1562/2000, Train Loss: 4.782096405459728, Val Loss: 5.678602630183811, Val MAE: 1.5511409044265747\n",
      "Epoch 1563/2000, Train Loss: 4.781971453611538, Val Loss: 5.678506737663632, Val MAE: 1.5515046119689941\n",
      "Epoch 1564/2000, Train Loss: 4.781829232053124, Val Loss: 5.67855498620442, Val MAE: 1.5511525869369507\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1565/2000, Train Loss: 4.7816800869402325, Val Loss: 5.678529557727632, Val MAE: 1.5512288808822632\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1566/2000, Train Loss: 4.781528347133749, Val Loss: 5.678435671897161, Val MAE: 1.5512702465057373\n",
      "Epoch 1567/2000, Train Loss: 4.781415962197717, Val Loss: 5.678483188152313, Val MAE: 1.5511270761489868\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1568/2000, Train Loss: 4.781292686677618, Val Loss: 5.6784348998750955, Val MAE: 1.5512698888778687\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1569/2000, Train Loss: 4.781137406910088, Val Loss: 5.678392552194142, Val MAE: 1.5512484312057495\n",
      "Epoch 1570/2000, Train Loss: 4.7810548612866315, Val Loss: 5.678365735780625, Val MAE: 1.5511598587036133\n",
      "Epoch 1571/2000, Train Loss: 4.780910616364903, Val Loss: 5.67838925690878, Val MAE: 1.5511176586151123\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1572/2000, Train Loss: 4.780793859524855, Val Loss: 5.678273709047408, Val MAE: 1.551203966140747\n",
      "Epoch 1573/2000, Train Loss: 4.780704624057657, Val Loss: 5.678398572263264, Val MAE: 1.5508853197097778\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1574/2000, Train Loss: 4.780535567462528, Val Loss: 5.678341666857402, Val MAE: 1.5508509874343872\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1575/2000, Train Loss: 4.7804010556682375, Val Loss: 5.678344692502703, Val MAE: 1.5508766174316406\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1576/2000, Train Loss: 4.780281744823806, Val Loss: 5.678314875988733, Val MAE: 1.5508630275726318\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1577/2000, Train Loss: 4.780127889851085, Val Loss: 5.678284576960972, Val MAE: 1.5509324073791504\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1578/2000, Train Loss: 4.780004664099603, Val Loss: 5.6783040676798135, Val MAE: 1.5507756471633911\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1579/2000, Train Loss: 4.77990583270494, Val Loss: 5.6782309753554205, Val MAE: 1.5509015321731567\n",
      "Epoch 1580/2000, Train Loss: 4.779827534690058, Val Loss: 5.678335692201342, Val MAE: 1.5506311655044556\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1581/2000, Train Loss: 4.779635612988169, Val Loss: 5.678305705388387, Val MAE: 1.5506082773208618\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1582/2000, Train Loss: 4.779478841843491, Val Loss: 5.6782744243031456, Val MAE: 1.5507138967514038\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1583/2000, Train Loss: 4.779413529612618, Val Loss: 5.678208484536126, Val MAE: 1.550717830657959\n",
      "Epoch 1584/2000, Train Loss: 4.779270677875901, Val Loss: 5.678282343205952, Val MAE: 1.5505353212356567\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1585/2000, Train Loss: 4.779084021012773, Val Loss: 5.678261637687683, Val MAE: 1.5505187511444092\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1586/2000, Train Loss: 4.779017576209581, Val Loss: 5.678191616421654, Val MAE: 1.5505621433258057\n",
      "Epoch 1587/2000, Train Loss: 4.7788525892077445, Val Loss: 5.678209679467337, Val MAE: 1.5505149364471436\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1588/2000, Train Loss: 4.778735250276638, Val Loss: 5.678097225370861, Val MAE: 1.5506771802902222\n",
      "Epoch 1589/2000, Train Loss: 4.7786524682522495, Val Loss: 5.678102047670455, Val MAE: 1.550459861755371\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1590/2000, Train Loss: 4.778498757036852, Val Loss: 5.678197406587147, Val MAE: 1.5502901077270508\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1591/2000, Train Loss: 4.778352128770019, Val Loss: 5.678127535751888, Val MAE: 1.5504319667816162\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1592/2000, Train Loss: 4.778229272584149, Val Loss: 5.6781656231199, Val MAE: 1.5502618551254272\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1593/2000, Train Loss: 4.778121754548109, Val Loss: 5.678129281316485, Val MAE: 1.5503501892089844\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1594/2000, Train Loss: 4.778007162306642, Val Loss: 5.6781413441612605, Val MAE: 1.5501753091812134\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1595/2000, Train Loss: 4.777955332999505, Val Loss: 5.678121944268544, Val MAE: 1.550248622894287\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1596/2000, Train Loss: 4.777724434862016, Val Loss: 5.678035480635507, Val MAE: 1.5504229068756104\n",
      "Epoch 1597/2000, Train Loss: 4.777589483893304, Val Loss: 5.678109949543362, Val MAE: 1.5503405332565308\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1598/2000, Train Loss: 4.777493447948074, Val Loss: 5.677983667169299, Val MAE: 1.5502961874008179\n",
      "Epoch 1599/2000, Train Loss: 4.777429785815886, Val Loss: 5.677977068083627, Val MAE: 1.5503301620483398\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1600/2000, Train Loss: 4.777334963653252, Val Loss: 5.677865025543031, Val MAE: 1.5506747961044312\n",
      "Epoch 1601/2000, Train Loss: 4.777218350570528, Val Loss: 5.677994117850349, Val MAE: 1.550066590309143\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1602/2000, Train Loss: 4.776970082176757, Val Loss: 5.677901302065168, Val MAE: 1.5503140687942505\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1603/2000, Train Loss: 4.7768670305378516, Val Loss: 5.677801205998375, Val MAE: 1.5505316257476807\n",
      "Epoch 1604/2000, Train Loss: 4.776711281667476, Val Loss: 5.677829662958781, Val MAE: 1.550418496131897\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1605/2000, Train Loss: 4.776573500276789, Val Loss: 5.677758205504644, Val MAE: 1.550589919090271\n",
      "Epoch 1606/2000, Train Loss: 4.776455651553629, Val Loss: 5.677745506876991, Val MAE: 1.5505502223968506\n",
      "Epoch 1607/2000, Train Loss: 4.776313866143166, Val Loss: 5.677739293802352, Val MAE: 1.550492286682129\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1608/2000, Train Loss: 4.7761881936588475, Val Loss: 5.6777086201168245, Val MAE: 1.5504971742630005\n",
      "Epoch 1609/2000, Train Loss: 4.7760587691924465, Val Loss: 5.677707953112466, Val MAE: 1.5504337549209595\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1610/2000, Train Loss: 4.776007699361137, Val Loss: 5.677720731212979, Val MAE: 1.550337791442871\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1611/2000, Train Loss: 4.775825056269744, Val Loss: 5.677637335799989, Val MAE: 1.5504419803619385\n",
      "Epoch 1612/2000, Train Loss: 4.7757554612475825, Val Loss: 5.677729640688215, Val MAE: 1.5501821041107178\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1613/2000, Train Loss: 4.77559642529454, Val Loss: 5.677632110459464, Val MAE: 1.5504335165023804\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1614/2000, Train Loss: 4.775437147560173, Val Loss: 5.677639930021195, Val MAE: 1.5502290725708008\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1615/2000, Train Loss: 4.775324547240361, Val Loss: 5.677689404714675, Val MAE: 1.5499422550201416\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1616/2000, Train Loss: 4.775275222000854, Val Loss: 5.677719607239678, Val MAE: 1.5498632192611694\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1617/2000, Train Loss: 4.7751006145235175, Val Loss: 5.677716374397278, Val MAE: 1.5497997999191284\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1618/2000, Train Loss: 4.774990101963576, Val Loss: 5.677736969221206, Val MAE: 1.5498077869415283\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1619/2000, Train Loss: 4.774821462159601, Val Loss: 5.6776358769053505, Val MAE: 1.5499253273010254\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1620/2000, Train Loss: 4.774708918691186, Val Loss: 5.677664850439344, Val MAE: 1.5498955249786377\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1621/2000, Train Loss: 4.774599253374699, Val Loss: 5.677556616919381, Val MAE: 1.5501267910003662\n",
      "Epoch 1622/2000, Train Loss: 4.77441830224816, Val Loss: 5.677505776995704, Val MAE: 1.5500673055648804\n",
      "Epoch 1623/2000, Train Loss: 4.7744017316525005, Val Loss: 5.677432684671311, Val MAE: 1.5502283573150635\n",
      "Epoch 1624/2000, Train Loss: 4.774210165855074, Val Loss: 5.67754943881716, Val MAE: 1.5498476028442383\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1625/2000, Train Loss: 4.7740760380861955, Val Loss: 5.677449671995072, Val MAE: 1.5499597787857056\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1626/2000, Train Loss: 4.774056750729323, Val Loss: 5.677550366946629, Val MAE: 1.5495858192443848\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1627/2000, Train Loss: 4.773894766994525, Val Loss: 5.6774621322041465, Val MAE: 1.5498555898666382\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1628/2000, Train Loss: 4.773691792750392, Val Loss: 5.677381790819622, Val MAE: 1.5499135255813599\n",
      "Epoch 1629/2000, Train Loss: 4.773588435438019, Val Loss: 5.677367516926357, Val MAE: 1.5498181581497192\n",
      "Epoch 1630/2000, Train Loss: 4.77345906830641, Val Loss: 5.677347813333784, Val MAE: 1.5497716665267944\n",
      "Epoch 1631/2000, Train Loss: 4.773435783991524, Val Loss: 5.677293663933163, Val MAE: 1.549986720085144\n",
      "Epoch 1632/2000, Train Loss: 4.7732387234697224, Val Loss: 5.677264315741403, Val MAE: 1.549883484840393\n",
      "Epoch 1633/2000, Train Loss: 4.773097082657269, Val Loss: 5.677284368446895, Val MAE: 1.549687385559082\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1634/2000, Train Loss: 4.773043483201472, Val Loss: 5.677222535723732, Val MAE: 1.5498380661010742\n",
      "Epoch 1635/2000, Train Loss: 4.772831028372007, Val Loss: 5.677236471857343, Val MAE: 1.5496906042099\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1636/2000, Train Loss: 4.772735636728607, Val Loss: 5.677207540898096, Val MAE: 1.5497759580612183\n",
      "Epoch 1637/2000, Train Loss: 4.772591578439194, Val Loss: 5.677157257284437, Val MAE: 1.5497374534606934\n",
      "Epoch 1638/2000, Train Loss: 4.7724508761687074, Val Loss: 5.677177599498203, Val MAE: 1.549660086631775\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1639/2000, Train Loss: 4.77233014691868, Val Loss: 5.677150754701524, Val MAE: 1.5496408939361572\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1640/2000, Train Loss: 4.772211772934843, Val Loss: 5.677065866334098, Val MAE: 1.5497522354125977\n",
      "Epoch 1641/2000, Train Loss: 4.7721073207128875, Val Loss: 5.677035956155686, Val MAE: 1.5497156381607056\n",
      "Epoch 1642/2000, Train Loss: 4.771997878178219, Val Loss: 5.676984565598624, Val MAE: 1.5497527122497559\n",
      "Epoch 1643/2000, Train Loss: 4.7718678845673255, Val Loss: 5.677007734775543, Val MAE: 1.5496561527252197\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1644/2000, Train Loss: 4.771759194614857, Val Loss: 5.6769915734018594, Val MAE: 1.5495734214782715\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1645/2000, Train Loss: 4.771700453522848, Val Loss: 5.676876496701014, Val MAE: 1.5499320030212402\n",
      "Epoch 1646/2000, Train Loss: 4.771481544914299, Val Loss: 5.676868586313157, Val MAE: 1.5498123168945312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1647/2000, Train Loss: 4.77137360216364, Val Loss: 5.676874935626984, Val MAE: 1.5496946573257446\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1648/2000, Train Loss: 4.771258389495828, Val Loss: 5.6767356395721436, Val MAE: 1.5499258041381836\n",
      "Epoch 1649/2000, Train Loss: 4.771156726670366, Val Loss: 5.676774515992119, Val MAE: 1.549726963043213\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1650/2000, Train Loss: 4.7710302471273875, Val Loss: 5.676691509428478, Val MAE: 1.549924373626709\n",
      "Epoch 1651/2000, Train Loss: 4.770926066951456, Val Loss: 5.676740470386687, Val MAE: 1.5497820377349854\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1652/2000, Train Loss: 4.77076467539595, Val Loss: 5.676744682448251, Val MAE: 1.5499579906463623\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1653/2000, Train Loss: 4.770658020912676, Val Loss: 5.676764922482627, Val MAE: 1.5497817993164062\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1654/2000, Train Loss: 4.7705208806292125, Val Loss: 5.6766554315884905, Val MAE: 1.5499311685562134\n",
      "Epoch 1655/2000, Train Loss: 4.770349615887963, Val Loss: 5.676702746323177, Val MAE: 1.5498722791671753\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1656/2000, Train Loss: 4.770285364266679, Val Loss: 5.676631481874557, Val MAE: 1.5499666929244995\n",
      "Epoch 1657/2000, Train Loss: 4.770183066555072, Val Loss: 5.676588677224659, Val MAE: 1.55022394657135\n",
      "Epoch 1658/2000, Train Loss: 4.769964814690508, Val Loss: 5.676617418016706, Val MAE: 1.5500034093856812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1659/2000, Train Loss: 4.769904095295958, Val Loss: 5.676529881500063, Val MAE: 1.5499951839447021\n",
      "Epoch 1660/2000, Train Loss: 4.769791607123672, Val Loss: 5.676498234272003, Val MAE: 1.5501174926757812\n",
      "Epoch 1661/2000, Train Loss: 4.769651102918832, Val Loss: 5.676542344547453, Val MAE: 1.5501400232315063\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1662/2000, Train Loss: 4.769550379980434, Val Loss: 5.676464049589066, Val MAE: 1.5503672361373901\n",
      "Epoch 1663/2000, Train Loss: 4.769395681967688, Val Loss: 5.676478295099168, Val MAE: 1.5502679347991943\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1664/2000, Train Loss: 4.769308988735269, Val Loss: 5.676497604165759, Val MAE: 1.5500622987747192\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1665/2000, Train Loss: 4.7691581353482135, Val Loss: 5.676436605907622, Val MAE: 1.5502159595489502\n",
      "Epoch 1666/2000, Train Loss: 4.769087880219324, Val Loss: 5.676337940352304, Val MAE: 1.5503813028335571\n",
      "Epoch 1667/2000, Train Loss: 4.768883832923448, Val Loss: 5.6763689716657, Val MAE: 1.5504282712936401\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1668/2000, Train Loss: 4.768776945401987, Val Loss: 5.676324055308387, Val MAE: 1.550383448600769\n",
      "Epoch 1669/2000, Train Loss: 4.768683547361613, Val Loss: 5.676297573816209, Val MAE: 1.5504168272018433\n",
      "Epoch 1670/2000, Train Loss: 4.76855549329763, Val Loss: 5.67636164313271, Val MAE: 1.5502278804779053\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1671/2000, Train Loss: 4.7685185326171355, Val Loss: 5.67635311683019, Val MAE: 1.5501174926757812\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1672/2000, Train Loss: 4.768318364583219, Val Loss: 5.676367228939419, Val MAE: 1.5500624179840088\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1673/2000, Train Loss: 4.768150329085432, Val Loss: 5.676255748385475, Val MAE: 1.550268292427063\n",
      "Epoch 1674/2000, Train Loss: 4.768027984877399, Val Loss: 5.676252722740173, Val MAE: 1.5501785278320312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1675/2000, Train Loss: 4.7679757480056395, Val Loss: 5.676353085608709, Val MAE: 1.5500071048736572\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1676/2000, Train Loss: 4.767786809520426, Val Loss: 5.676219168163481, Val MAE: 1.5503634214401245\n",
      "Epoch 1677/2000, Train Loss: 4.767703962931344, Val Loss: 5.676248465265546, Val MAE: 1.5501512289047241\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1678/2000, Train Loss: 4.767601019572808, Val Loss: 5.676074567295256, Val MAE: 1.5504636764526367\n",
      "Epoch 1679/2000, Train Loss: 4.767537849746403, Val Loss: 5.676167638528915, Val MAE: 1.550172209739685\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1680/2000, Train Loss: 4.767330265347813, Val Loss: 5.676093254770551, Val MAE: 1.55031418800354\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1681/2000, Train Loss: 4.767208138709344, Val Loss: 5.676074811390468, Val MAE: 1.5503592491149902\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1682/2000, Train Loss: 4.767094789001939, Val Loss: 5.676066869781131, Val MAE: 1.5501905679702759\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1683/2000, Train Loss: 4.7670323932792975, Val Loss: 5.676123812085106, Val MAE: 1.549992561340332\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1684/2000, Train Loss: 4.766864853954449, Val Loss: 5.676191809631529, Val MAE: 1.5497167110443115\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1685/2000, Train Loss: 4.76683090637702, Val Loss: 5.676053722699483, Val MAE: 1.5501680374145508\n",
      "Epoch 1686/2000, Train Loss: 4.766613158798016, Val Loss: 5.6760114175932745, Val MAE: 1.5501019954681396\n",
      "Epoch 1687/2000, Train Loss: 4.766540623854515, Val Loss: 5.676125997588748, Val MAE: 1.5497645139694214\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1688/2000, Train Loss: 4.7663462047684515, Val Loss: 5.676035798731304, Val MAE: 1.5498430728912354\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1689/2000, Train Loss: 4.766249916281115, Val Loss: 5.676036738214039, Val MAE: 1.5497792959213257\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1690/2000, Train Loss: 4.766141592867788, Val Loss: 5.676000555356343, Val MAE: 1.5498868227005005\n",
      "Epoch 1691/2000, Train Loss: 4.766027585743849, Val Loss: 5.675979412737346, Val MAE: 1.5500879287719727\n",
      "Epoch 1692/2000, Train Loss: 4.765865784690141, Val Loss: 5.675966166314625, Val MAE: 1.549798846244812\n",
      "Epoch 1693/2000, Train Loss: 4.7657452300506185, Val Loss: 5.67596419652303, Val MAE: 1.5498396158218384\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1694/2000, Train Loss: 4.765639018608922, Val Loss: 5.675962025210971, Val MAE: 1.5497422218322754\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1695/2000, Train Loss: 4.7655715952471045, Val Loss: 5.675830466406686, Val MAE: 1.5500010251998901\n",
      "Epoch 1696/2000, Train Loss: 4.765407970493031, Val Loss: 5.675917968863533, Val MAE: 1.5497565269470215\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1697/2000, Train Loss: 4.7653516828593485, Val Loss: 5.675887669835772, Val MAE: 1.5499002933502197\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1698/2000, Train Loss: 4.765204502598019, Val Loss: 5.675897598266602, Val MAE: 1.5499552488327026\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1699/2000, Train Loss: 4.76512022078962, Val Loss: 5.6758614267621725, Val MAE: 1.5499765872955322\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1700/2000, Train Loss: 4.764949778407518, Val Loss: 5.6758555088724405, Val MAE: 1.5497163534164429\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1701/2000, Train Loss: 4.764781988221936, Val Loss: 5.675794110411689, Val MAE: 1.5497872829437256\n",
      "Epoch 1702/2000, Train Loss: 4.764679728173066, Val Loss: 5.675757027807689, Val MAE: 1.5498583316802979\n",
      "Epoch 1703/2000, Train Loss: 4.764567755845773, Val Loss: 5.675725022951762, Val MAE: 1.5498230457305908\n",
      "Epoch 1704/2000, Train Loss: 4.764511803477708, Val Loss: 5.675625222069876, Val MAE: 1.550006628036499\n",
      "Epoch 1705/2000, Train Loss: 4.764296226340053, Val Loss: 5.6757157274654935, Val MAE: 1.549599051475525\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1706/2000, Train Loss: 4.7642629096134765, Val Loss: 5.67560506150836, Val MAE: 1.5499305725097656\n",
      "Epoch 1707/2000, Train Loss: 4.764092625616636, Val Loss: 5.675566298621042, Val MAE: 1.5499638319015503\n",
      "Epoch 1708/2000, Train Loss: 4.763928939660614, Val Loss: 5.675588167849041, Val MAE: 1.5498555898666382\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1709/2000, Train Loss: 4.76382715174106, Val Loss: 5.6754789380800155, Val MAE: 1.5499531030654907\n",
      "Epoch 1710/2000, Train Loss: 4.763666486538348, Val Loss: 5.675566531362987, Val MAE: 1.5497992038726807\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1711/2000, Train Loss: 4.763683590128997, Val Loss: 5.675634449436551, Val MAE: 1.549471139907837\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1712/2000, Train Loss: 4.763507503716666, Val Loss: 5.6754977731477645, Val MAE: 1.549727439880371\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1713/2000, Train Loss: 4.763381246451094, Val Loss: 5.675607468400683, Val MAE: 1.5494359731674194\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1714/2000, Train Loss: 4.7632361019952345, Val Loss: 5.675572466282618, Val MAE: 1.5495316982269287\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1715/2000, Train Loss: 4.76312087856664, Val Loss: 5.675566105615525, Val MAE: 1.5494811534881592\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1716/2000, Train Loss: 4.762976092915609, Val Loss: 5.675559114842188, Val MAE: 1.5496762990951538\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1717/2000, Train Loss: 4.762847155542737, Val Loss: 5.675483363015311, Val MAE: 1.5496840476989746\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1718/2000, Train Loss: 4.762733963884319, Val Loss: 5.675406288532984, Val MAE: 1.5497183799743652\n",
      "Epoch 1719/2000, Train Loss: 4.762644772805347, Val Loss: 5.675402811595371, Val MAE: 1.5498123168945312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1720/2000, Train Loss: 4.7625098292346735, Val Loss: 5.675378322601318, Val MAE: 1.5495685338974\n",
      "Epoch 1721/2000, Train Loss: 4.762410536975888, Val Loss: 5.675333812123253, Val MAE: 1.5497033596038818\n",
      "Epoch 1722/2000, Train Loss: 4.762300046105647, Val Loss: 5.6753320296605425, Val MAE: 1.5497363805770874\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1723/2000, Train Loss: 4.762147048632753, Val Loss: 5.675319504170191, Val MAE: 1.5496212244033813\n",
      "Epoch 1724/2000, Train Loss: 4.762011894554278, Val Loss: 5.675336119674501, Val MAE: 1.5495859384536743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1725/2000, Train Loss: 4.761943396123071, Val Loss: 5.675308511370704, Val MAE: 1.5495107173919678\n",
      "Epoch 1726/2000, Train Loss: 4.761854336527406, Val Loss: 5.67530807143166, Val MAE: 1.5492851734161377\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1727/2000, Train Loss: 4.761829472395194, Val Loss: 5.675170427276974, Val MAE: 1.5495775938034058\n",
      "Epoch 1728/2000, Train Loss: 4.761583452668614, Val Loss: 5.675213285854885, Val MAE: 1.5492130517959595\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1729/2000, Train Loss: 4.761521816253662, Val Loss: 5.675253950414204, Val MAE: 1.5493347644805908\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1730/2000, Train Loss: 4.761480569503203, Val Loss: 5.6752755755469915, Val MAE: 1.5490061044692993\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1731/2000, Train Loss: 4.761237992049945, Val Loss: 5.675216921738216, Val MAE: 1.5491628646850586\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1732/2000, Train Loss: 4.761234029896337, Val Loss: 5.675294572398776, Val MAE: 1.5488401651382446\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1733/2000, Train Loss: 4.761016515817897, Val Loss: 5.675136952173142, Val MAE: 1.5492604970932007\n",
      "Epoch 1734/2000, Train Loss: 4.7608878077841945, Val Loss: 5.67515093939645, Val MAE: 1.5491769313812256\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1735/2000, Train Loss: 4.760745545536574, Val Loss: 5.675151958352044, Val MAE: 1.5490366220474243\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1736/2000, Train Loss: 4.76062726570958, Val Loss: 5.675089662983304, Val MAE: 1.5490635633468628\n",
      "Epoch 1737/2000, Train Loss: 4.76050173849582, Val Loss: 5.675089833282289, Val MAE: 1.548977255821228\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1738/2000, Train Loss: 4.760388808458917, Val Loss: 5.675120719841549, Val MAE: 1.5488240718841553\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1739/2000, Train Loss: 4.760274268339988, Val Loss: 5.675042416368212, Val MAE: 1.548916220664978\n",
      "Epoch 1740/2000, Train Loss: 4.760167259087179, Val Loss: 5.674930038906279, Val MAE: 1.5490243434906006\n",
      "Epoch 1741/2000, Train Loss: 4.760048186661326, Val Loss: 5.674971782025837, Val MAE: 1.5489473342895508\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1742/2000, Train Loss: 4.7598864692558855, Val Loss: 5.674898190157754, Val MAE: 1.5490951538085938\n",
      "Epoch 1743/2000, Train Loss: 4.759770773361701, Val Loss: 5.6749001542727155, Val MAE: 1.5489827394485474\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1744/2000, Train Loss: 4.759665667254094, Val Loss: 5.674808417047773, Val MAE: 1.5491503477096558\n",
      "Epoch 1745/2000, Train Loss: 4.759573037264546, Val Loss: 5.674863273189182, Val MAE: 1.549033522605896\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1746/2000, Train Loss: 4.759436105639373, Val Loss: 5.674888227667127, Val MAE: 1.5489124059677124\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1747/2000, Train Loss: 4.759331633745363, Val Loss: 5.674719432989757, Val MAE: 1.549220085144043\n",
      "Epoch 1748/2000, Train Loss: 4.759208337544386, Val Loss: 5.674802115985325, Val MAE: 1.5488520860671997\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1749/2000, Train Loss: 4.759089973985056, Val Loss: 5.674738841397422, Val MAE: 1.5491880178451538\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1750/2000, Train Loss: 4.758946240873027, Val Loss: 5.674640173003787, Val MAE: 1.549246072769165\n",
      "Epoch 1751/2000, Train Loss: 4.758830687937515, Val Loss: 5.6746541403588795, Val MAE: 1.548983097076416\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1752/2000, Train Loss: 4.7586924067169045, Val Loss: 5.6747015828178045, Val MAE: 1.54892897605896\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1753/2000, Train Loss: 4.758616688557505, Val Loss: 5.674623018219357, Val MAE: 1.548953652381897\n",
      "Epoch 1754/2000, Train Loss: 4.758492105602377, Val Loss: 5.674681907608395, Val MAE: 1.5488367080688477\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1755/2000, Train Loss: 4.758359212969523, Val Loss: 5.674609504994892, Val MAE: 1.5489810705184937\n",
      "Epoch 1756/2000, Train Loss: 4.758239860763335, Val Loss: 5.674535124074845, Val MAE: 1.5491602420806885\n",
      "Epoch 1757/2000, Train Loss: 4.758100026669388, Val Loss: 5.674550456660135, Val MAE: 1.548939824104309\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1758/2000, Train Loss: 4.758321055908634, Val Loss: 5.6744756528309415, Val MAE: 1.549338936805725\n",
      "Epoch 1759/2000, Train Loss: 4.757827262616124, Val Loss: 5.674454200835455, Val MAE: 1.5490920543670654\n",
      "Epoch 1760/2000, Train Loss: 4.757745528927306, Val Loss: 5.6744431626229055, Val MAE: 1.5489835739135742\n",
      "Epoch 1761/2000, Train Loss: 4.757657593160825, Val Loss: 5.674430211385091, Val MAE: 1.5488728284835815\n",
      "Epoch 1762/2000, Train Loss: 4.7576501951231105, Val Loss: 5.67438083319437, Val MAE: 1.549062967300415\n",
      "Epoch 1763/2000, Train Loss: 4.757415707255957, Val Loss: 5.67431409302212, Val MAE: 1.5492212772369385\n",
      "Epoch 1764/2000, Train Loss: 4.757348808145994, Val Loss: 5.674389776729402, Val MAE: 1.548905849456787\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1765/2000, Train Loss: 4.757256978657753, Val Loss: 5.674275043464842, Val MAE: 1.5491918325424194\n",
      "Epoch 1766/2000, Train Loss: 4.757034970271403, Val Loss: 5.6742956808635165, Val MAE: 1.549114465713501\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1767/2000, Train Loss: 4.756935706427807, Val Loss: 5.674198014395578, Val MAE: 1.549274206161499\n",
      "Epoch 1768/2000, Train Loss: 4.7568102220890385, Val Loss: 5.67418418611799, Val MAE: 1.5492522716522217\n",
      "Epoch 1769/2000, Train Loss: 4.75672274113374, Val Loss: 5.674110580058325, Val MAE: 1.5492862462997437\n",
      "Epoch 1770/2000, Train Loss: 4.756599341696509, Val Loss: 5.6741757194201154, Val MAE: 1.5490236282348633\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1771/2000, Train Loss: 4.756646410534513, Val Loss: 5.674114221618289, Val MAE: 1.5493260622024536\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1772/2000, Train Loss: 4.75632622346219, Val Loss: 5.674099323295412, Val MAE: 1.5492613315582275\n",
      "Epoch 1773/2000, Train Loss: 4.756233084857548, Val Loss: 5.674115447770982, Val MAE: 1.5490716695785522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1774/2000, Train Loss: 4.756115146012838, Val Loss: 5.6740678662345525, Val MAE: 1.5492398738861084\n",
      "Epoch 1775/2000, Train Loss: 4.7559839907418855, Val Loss: 5.673932285535903, Val MAE: 1.5493850708007812\n",
      "Epoch 1776/2000, Train Loss: 4.755898735923391, Val Loss: 5.673939389841897, Val MAE: 1.549389123916626\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1777/2000, Train Loss: 4.75582875544665, Val Loss: 5.673871730055128, Val MAE: 1.549455165863037\n",
      "Epoch 1778/2000, Train Loss: 4.755620825946415, Val Loss: 5.673917026746841, Val MAE: 1.5492448806762695\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1779/2000, Train Loss: 4.7555133398564475, Val Loss: 5.67389497586659, Val MAE: 1.5493178367614746\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1780/2000, Train Loss: 4.755439286507739, Val Loss: 5.6739515860875445, Val MAE: 1.549095869064331\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1781/2000, Train Loss: 4.755289713990033, Val Loss: 5.673937377475557, Val MAE: 1.5491281747817993\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1782/2000, Train Loss: 4.755159439758118, Val Loss: 5.673905741600763, Val MAE: 1.5492467880249023\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1783/2000, Train Loss: 4.755035988030212, Val Loss: 5.6738924015136, Val MAE: 1.5492039918899536\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1784/2000, Train Loss: 4.754928926486727, Val Loss: 5.6738715540795095, Val MAE: 1.549197793006897\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1785/2000, Train Loss: 4.754899952072014, Val Loss: 5.673815046037946, Val MAE: 1.5495096445083618\n",
      "Epoch 1786/2000, Train Loss: 4.754718993715574, Val Loss: 5.6738421093849905, Val MAE: 1.549391269683838\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1787/2000, Train Loss: 4.754580969702878, Val Loss: 5.673861018248966, Val MAE: 1.5491151809692383\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1788/2000, Train Loss: 4.754455516637632, Val Loss: 5.67388467561631, Val MAE: 1.548952579498291\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1789/2000, Train Loss: 4.7543972702053265, Val Loss: 5.673762846560705, Val MAE: 1.5493059158325195\n",
      "Epoch 1790/2000, Train Loss: 4.7542083724093205, Val Loss: 5.673790156841278, Val MAE: 1.5491663217544556\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1791/2000, Train Loss: 4.754080680260706, Val Loss: 5.673831437315259, Val MAE: 1.5490087270736694\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1792/2000, Train Loss: 4.7539797730439135, Val Loss: 5.673791462466831, Val MAE: 1.5489883422851562\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1793/2000, Train Loss: 4.753872679722494, Val Loss: 5.673757059233529, Val MAE: 1.5491456985473633\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1794/2000, Train Loss: 4.753746247930486, Val Loss: 5.673754811286926, Val MAE: 1.548966884613037\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1795/2000, Train Loss: 4.753630473684357, Val Loss: 5.673740920566377, Val MAE: 1.5490182638168335\n",
      "Epoch 1796/2000, Train Loss: 4.75351594129974, Val Loss: 5.673697712875548, Val MAE: 1.5491578578948975\n",
      "Epoch 1797/2000, Train Loss: 4.753424905082907, Val Loss: 5.673673802898044, Val MAE: 1.5490102767944336\n",
      "Epoch 1798/2000, Train Loss: 4.753309409272015, Val Loss: 5.673691925548372, Val MAE: 1.5489113330841064\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1799/2000, Train Loss: 4.7531239125557105, Val Loss: 5.6736635111627125, Val MAE: 1.5489591360092163\n",
      "Epoch 1800/2000, Train Loss: 4.753027406162536, Val Loss: 5.673608334291549, Val MAE: 1.5488898754119873\n",
      "Epoch 1801/2000, Train Loss: 4.7529501733389825, Val Loss: 5.673645851157961, Val MAE: 1.5488537549972534\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1802/2000, Train Loss: 4.752844594933923, Val Loss: 5.673545564923968, Val MAE: 1.5492918491363525\n",
      "Epoch 1803/2000, Train Loss: 4.75268278875203, Val Loss: 5.673565566539764, Val MAE: 1.5490809679031372\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1804/2000, Train Loss: 4.752591783472446, Val Loss: 5.673500449884505, Val MAE: 1.5492075681686401\n",
      "Epoch 1805/2000, Train Loss: 4.752435978107964, Val Loss: 5.673468408130464, Val MAE: 1.549324631690979\n",
      "Epoch 1806/2000, Train Loss: 4.752352132951928, Val Loss: 5.673544276328314, Val MAE: 1.549102783203125\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1807/2000, Train Loss: 4.752218728004961, Val Loss: 5.673564224016099, Val MAE: 1.5489377975463867\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1808/2000, Train Loss: 4.752117488885296, Val Loss: 5.673418428216662, Val MAE: 1.5491943359375\n",
      "Epoch 1809/2000, Train Loss: 4.751944359838542, Val Loss: 5.673435838449569, Val MAE: 1.5491235256195068\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1810/2000, Train Loss: 4.751862248429458, Val Loss: 5.673397629033952, Val MAE: 1.5491899251937866\n",
      "Epoch 1811/2000, Train Loss: 4.751833822898703, Val Loss: 5.673434555530548, Val MAE: 1.5490548610687256\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1812/2000, Train Loss: 4.75170165493727, Val Loss: 5.673367994172232, Val MAE: 1.5491864681243896\n",
      "Epoch 1813/2000, Train Loss: 4.751599598922918, Val Loss: 5.673231525080545, Val MAE: 1.54935622215271\n",
      "Epoch 1814/2000, Train Loss: 4.751405762011982, Val Loss: 5.6732291494097025, Val MAE: 1.5493015050888062\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1815/2000, Train Loss: 4.751337481822887, Val Loss: 5.673284011227744, Val MAE: 1.5490282773971558\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1816/2000, Train Loss: 4.751155418804906, Val Loss: 5.673277375243959, Val MAE: 1.5491373538970947\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1817/2000, Train Loss: 4.751053326556982, Val Loss: 5.673194374356951, Val MAE: 1.5492688417434692\n",
      "Epoch 1818/2000, Train Loss: 4.750937677741219, Val Loss: 5.67319879645393, Val MAE: 1.5493111610412598\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1819/2000, Train Loss: 4.750833510343716, Val Loss: 5.673150068237668, Val MAE: 1.5492695569992065\n",
      "Epoch 1820/2000, Train Loss: 4.750725398110738, Val Loss: 5.673112349850791, Val MAE: 1.5492953062057495\n",
      "Epoch 1821/2000, Train Loss: 4.750642799019645, Val Loss: 5.673061084179651, Val MAE: 1.5494444370269775\n",
      "Epoch 1822/2000, Train Loss: 4.750480899353458, Val Loss: 5.673104104541597, Val MAE: 1.5491714477539062\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1823/2000, Train Loss: 4.750384686149562, Val Loss: 5.672979383241563, Val MAE: 1.5494940280914307\n",
      "Epoch 1824/2000, Train Loss: 4.750310383831664, Val Loss: 5.673052154836201, Val MAE: 1.5492349863052368\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1825/2000, Train Loss: 4.75014590006453, Val Loss: 5.6729870183127264, Val MAE: 1.5494693517684937\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1826/2000, Train Loss: 4.750082940073376, Val Loss: 5.673065358684177, Val MAE: 1.5491572618484497\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1827/2000, Train Loss: 4.749896350801412, Val Loss: 5.672976238386972, Val MAE: 1.5493485927581787\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1828/2000, Train Loss: 4.749802417496196, Val Loss: 5.67301294917152, Val MAE: 1.5493011474609375\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1829/2000, Train Loss: 4.74965966487637, Val Loss: 5.672905907744453, Val MAE: 1.5494297742843628\n",
      "Epoch 1830/2000, Train Loss: 4.749557813232809, Val Loss: 5.672950520401909, Val MAE: 1.5492064952850342\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1831/2000, Train Loss: 4.749476822878645, Val Loss: 5.672907488686698, Val MAE: 1.549308180809021\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1832/2000, Train Loss: 4.749376486655855, Val Loss: 5.672905254931677, Val MAE: 1.5492346286773682\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1833/2000, Train Loss: 4.74925231799084, Val Loss: 5.672847770509266, Val MAE: 1.549281120300293\n",
      "Epoch 1834/2000, Train Loss: 4.749128572359072, Val Loss: 5.672955697491055, Val MAE: 1.5490018129348755\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1835/2000, Train Loss: 4.749116191406681, Val Loss: 5.672925208296094, Val MAE: 1.5492557287216187\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1836/2000, Train Loss: 4.748893751580221, Val Loss: 5.67292724053065, Val MAE: 1.5491888523101807\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1837/2000, Train Loss: 4.748820386921569, Val Loss: 5.672913684731438, Val MAE: 1.5491048097610474\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1838/2000, Train Loss: 4.748703298178647, Val Loss: 5.67274136202676, Val MAE: 1.5496107339859009\n",
      "Epoch 1839/2000, Train Loss: 4.7485515620711825, Val Loss: 5.6728548010190325, Val MAE: 1.5493818521499634\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1840/2000, Train Loss: 4.748498135124175, Val Loss: 5.672784549849374, Val MAE: 1.5493806600570679\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1841/2000, Train Loss: 4.748316399637499, Val Loss: 5.672891520318531, Val MAE: 1.549222707748413\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1842/2000, Train Loss: 4.748215213649195, Val Loss: 5.67278124888738, Val MAE: 1.549189567565918\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1843/2000, Train Loss: 4.74806913300529, Val Loss: 5.672823150952657, Val MAE: 1.5491803884506226\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1844/2000, Train Loss: 4.74799851974412, Val Loss: 5.672682012830462, Val MAE: 1.5494232177734375\n",
      "Epoch 1845/2000, Train Loss: 4.7478423802909795, Val Loss: 5.6727239773387, Val MAE: 1.5493425130844116\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1846/2000, Train Loss: 4.747726552072802, Val Loss: 5.672695926257542, Val MAE: 1.5493018627166748\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1847/2000, Train Loss: 4.7476077875062, Val Loss: 5.672668621653602, Val MAE: 1.5492398738861084\n",
      "Epoch 1848/2000, Train Loss: 4.747522832308232, Val Loss: 5.67271291641962, Val MAE: 1.5491634607315063\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1849/2000, Train Loss: 4.747447104228737, Val Loss: 5.6726367104621165, Val MAE: 1.5493192672729492\n",
      "Epoch 1850/2000, Train Loss: 4.747312588819495, Val Loss: 5.672733414740789, Val MAE: 1.548995018005371\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1851/2000, Train Loss: 4.7471749838047534, Val Loss: 5.6725884562446955, Val MAE: 1.549236536026001\n",
      "Epoch 1852/2000, Train Loss: 4.747060951292094, Val Loss: 5.672589540481567, Val MAE: 1.5491420030593872\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1853/2000, Train Loss: 4.746995654193571, Val Loss: 5.672493250597091, Val MAE: 1.549391269683838\n",
      "Epoch 1854/2000, Train Loss: 4.746852681061781, Val Loss: 5.67253927957444, Val MAE: 1.5491228103637695\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1855/2000, Train Loss: 4.7467449521816665, Val Loss: 5.672536923771813, Val MAE: 1.549153447151184\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1856/2000, Train Loss: 4.7466007492942435, Val Loss: 5.672532158238547, Val MAE: 1.5491163730621338\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1857/2000, Train Loss: 4.74656038913135, Val Loss: 5.672560603845687, Val MAE: 1.5489015579223633\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1858/2000, Train Loss: 4.746422456249027, Val Loss: 5.672475973765056, Val MAE: 1.5491790771484375\n",
      "Epoch 1859/2000, Train Loss: 4.746291731241223, Val Loss: 5.672397894518716, Val MAE: 1.5492134094238281\n",
      "Epoch 1860/2000, Train Loss: 4.746199865099069, Val Loss: 5.672444536572411, Val MAE: 1.5490366220474243\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1861/2000, Train Loss: 4.746074083661831, Val Loss: 5.6724479255222136, Val MAE: 1.5488760471343994\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1862/2000, Train Loss: 4.745941483251467, Val Loss: 5.672493860835121, Val MAE: 1.5487046241760254\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1863/2000, Train Loss: 4.745801817715084, Val Loss: 5.672444658620017, Val MAE: 1.5487569570541382\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1864/2000, Train Loss: 4.745720313197299, Val Loss: 5.672505461034321, Val MAE: 1.5485246181488037\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1865/2000, Train Loss: 4.745612944635249, Val Loss: 5.672460459527516, Val MAE: 1.5487258434295654\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1866/2000, Train Loss: 4.745510585889829, Val Loss: 5.6723703174364, Val MAE: 1.548755407333374\n",
      "Epoch 1867/2000, Train Loss: 4.745405406306257, Val Loss: 5.672354181607564, Val MAE: 1.5485589504241943\n",
      "Epoch 1868/2000, Train Loss: 4.745248058450912, Val Loss: 5.672298550605774, Val MAE: 1.5487899780273438\n",
      "Epoch 1869/2000, Train Loss: 4.745181988929996, Val Loss: 5.672283283301762, Val MAE: 1.5488150119781494\n",
      "Epoch 1870/2000, Train Loss: 4.745051000451167, Val Loss: 5.672278219745273, Val MAE: 1.54873526096344\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1871/2000, Train Loss: 4.744957701611754, Val Loss: 5.672260778290885, Val MAE: 1.5484466552734375\n",
      "Epoch 1872/2000, Train Loss: 4.744814913346838, Val Loss: 5.672249643575578, Val MAE: 1.5486079454421997\n",
      "Epoch 1873/2000, Train Loss: 4.74471366590438, Val Loss: 5.672284503777822, Val MAE: 1.5485676527023315\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1874/2000, Train Loss: 4.744594182551159, Val Loss: 5.67220101470039, Val MAE: 1.5485639572143555\n",
      "Epoch 1875/2000, Train Loss: 4.744499945337917, Val Loss: 5.672133195967901, Val MAE: 1.548795223236084\n",
      "Epoch 1876/2000, Train Loss: 4.744478743570648, Val Loss: 5.672123733020964, Val MAE: 1.5488361120224\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1877/2000, Train Loss: 4.744251475179481, Val Loss: 5.672058377947126, Val MAE: 1.5488672256469727\n",
      "Epoch 1878/2000, Train Loss: 4.74419350012065, Val Loss: 5.672057472524189, Val MAE: 1.5486936569213867\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1879/2000, Train Loss: 4.74406654878463, Val Loss: 5.672085719449179, Val MAE: 1.5486141443252563\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1880/2000, Train Loss: 4.743948096785122, Val Loss: 5.672043334870112, Val MAE: 1.5487524271011353\n",
      "Epoch 1881/2000, Train Loss: 4.7437660354485125, Val Loss: 5.672060464109693, Val MAE: 1.5485527515411377\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1882/2000, Train Loss: 4.743728173636583, Val Loss: 5.672015439896357, Val MAE: 1.548614501953125\n",
      "Epoch 1883/2000, Train Loss: 4.743613399134368, Val Loss: 5.671939282190232, Val MAE: 1.548643946647644\n",
      "Epoch 1884/2000, Train Loss: 4.743448532518783, Val Loss: 5.67189176593508, Val MAE: 1.548906922340393\n",
      "Epoch 1885/2000, Train Loss: 4.743357108241244, Val Loss: 5.671868957224346, Val MAE: 1.5488651990890503\n",
      "Epoch 1886/2000, Train Loss: 4.74324420914159, Val Loss: 5.671792495818365, Val MAE: 1.548882246017456\n",
      "Epoch 1887/2000, Train Loss: 4.743163505629525, Val Loss: 5.671801632358914, Val MAE: 1.548945665359497\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1888/2000, Train Loss: 4.7429913452213, Val Loss: 5.67180442526227, Val MAE: 1.5489040613174438\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1889/2000, Train Loss: 4.743100523444258, Val Loss: 5.67186712367194, Val MAE: 1.5485583543777466\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1890/2000, Train Loss: 4.742826576797851, Val Loss: 5.671766227199917, Val MAE: 1.5487688779830933\n",
      "Epoch 1891/2000, Train Loss: 4.742684438299225, Val Loss: 5.6718243190220425, Val MAE: 1.5486345291137695\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1892/2000, Train Loss: 4.742626997244207, Val Loss: 5.671663922922952, Val MAE: 1.5490458011627197\n",
      "Epoch 1893/2000, Train Loss: 4.742482140975543, Val Loss: 5.671607639108386, Val MAE: 1.54904305934906\n",
      "Epoch 1894/2000, Train Loss: 4.74233291637578, Val Loss: 5.671592527911777, Val MAE: 1.5490902662277222\n",
      "Epoch 1895/2000, Train Loss: 4.742298692506862, Val Loss: 5.671591381231944, Val MAE: 1.549054503440857\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1896/2000, Train Loss: 4.742227632336959, Val Loss: 5.671613962877364, Val MAE: 1.5490518808364868\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1897/2000, Train Loss: 4.742011856797384, Val Loss: 5.67165413073131, Val MAE: 1.548799753189087\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1898/2000, Train Loss: 4.7419420949830995, Val Loss: 5.671532145568302, Val MAE: 1.5490877628326416\n",
      "Epoch 1899/2000, Train Loss: 4.741783412790769, Val Loss: 5.671511215823037, Val MAE: 1.549107313156128\n",
      "Epoch 1900/2000, Train Loss: 4.74173097684454, Val Loss: 5.671559282711574, Val MAE: 1.5488680601119995\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1901/2000, Train Loss: 4.741617433443056, Val Loss: 5.671529937358129, Val MAE: 1.5488801002502441\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1902/2000, Train Loss: 4.74144264807654, Val Loss: 5.671494117804936, Val MAE: 1.5489394664764404\n",
      "Epoch 1903/2000, Train Loss: 4.741381814684956, Val Loss: 5.671509790988195, Val MAE: 1.5488355159759521\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1904/2000, Train Loss: 4.7412719279317495, Val Loss: 5.671458110922859, Val MAE: 1.5489745140075684\n",
      "Epoch 1905/2000, Train Loss: 4.741207514898734, Val Loss: 5.671512368179503, Val MAE: 1.5487914085388184\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1906/2000, Train Loss: 4.741081183989058, Val Loss: 5.671408749762035, Val MAE: 1.5490175485610962\n",
      "Epoch 1907/2000, Train Loss: 4.740978090315847, Val Loss: 5.671505374567849, Val MAE: 1.5487139225006104\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1908/2000, Train Loss: 4.740817081440655, Val Loss: 5.671439636321295, Val MAE: 1.5489331483840942\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1909/2000, Train Loss: 4.740746506851045, Val Loss: 5.671466353393736, Val MAE: 1.5488172769546509\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1910/2000, Train Loss: 4.740573524672154, Val Loss: 5.671363024484544, Val MAE: 1.5489678382873535\n",
      "Epoch 1911/2000, Train Loss: 4.740543032903093, Val Loss: 5.671350660778227, Val MAE: 1.5490642786026\n",
      "Epoch 1912/2000, Train Loss: 4.7404484547075665, Val Loss: 5.671389877796173, Val MAE: 1.5488001108169556\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1913/2000, Train Loss: 4.740295786783625, Val Loss: 5.671348086425236, Val MAE: 1.5488641262054443\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1914/2000, Train Loss: 4.740173193900643, Val Loss: 5.671344555559612, Val MAE: 1.5490018129348755\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1915/2000, Train Loss: 4.740015066561141, Val Loss: 5.6713409679276605, Val MAE: 1.5489418506622314\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1916/2000, Train Loss: 4.739943796555986, Val Loss: 5.671355068683624, Val MAE: 1.5489650964736938\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1917/2000, Train Loss: 4.739828613312187, Val Loss: 5.6713480693953375, Val MAE: 1.548883318901062\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1918/2000, Train Loss: 4.739707366367657, Val Loss: 5.671398665223803, Val MAE: 1.5486838817596436\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1919/2000, Train Loss: 4.7396332773738585, Val Loss: 5.671376412823086, Val MAE: 1.5486897230148315\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1920/2000, Train Loss: 4.739526971607181, Val Loss: 5.671305080254872, Val MAE: 1.5487338304519653\n",
      "Epoch 1921/2000, Train Loss: 4.739409879501515, Val Loss: 5.671433074133737, Val MAE: 1.5483450889587402\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1922/2000, Train Loss: 4.739284136399563, Val Loss: 5.67136629138674, Val MAE: 1.5485254526138306\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1923/2000, Train Loss: 4.73928059442087, Val Loss: 5.671412845452626, Val MAE: 1.548279881477356\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1924/2000, Train Loss: 4.739061893629927, Val Loss: 5.671379395893642, Val MAE: 1.5484049320220947\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1925/2000, Train Loss: 4.73899136241972, Val Loss: 5.67122867561522, Val MAE: 1.5489171743392944\n",
      "Epoch 1926/2000, Train Loss: 4.738920441467436, Val Loss: 5.67122423081171, Val MAE: 1.5488672256469727\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1927/2000, Train Loss: 4.738750219008819, Val Loss: 5.671314707824162, Val MAE: 1.548545479774475\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1928/2000, Train Loss: 4.738606962061399, Val Loss: 5.671221378303709, Val MAE: 1.5485986471176147\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1929/2000, Train Loss: 4.738576582355795, Val Loss: 5.671350674969809, Val MAE: 1.548364520072937\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1930/2000, Train Loss: 4.738414301690665, Val Loss: 5.67124741418021, Val MAE: 1.5485073328018188\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1931/2000, Train Loss: 4.7382893306748315, Val Loss: 5.671285782541547, Val MAE: 1.5482659339904785\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1932/2000, Train Loss: 4.738232370492265, Val Loss: 5.671199035076868, Val MAE: 1.5482795238494873\n",
      "Epoch 1933/2000, Train Loss: 4.738064521458656, Val Loss: 5.67122391292027, Val MAE: 1.5483907461166382\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1934/2000, Train Loss: 4.737955197628866, Val Loss: 5.671121384416308, Val MAE: 1.548457145690918\n",
      "Epoch 1935/2000, Train Loss: 4.737886475238881, Val Loss: 5.671153656073979, Val MAE: 1.5482995510101318\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1936/2000, Train Loss: 4.737741061427193, Val Loss: 5.671132734843662, Val MAE: 1.548315167427063\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1937/2000, Train Loss: 4.737644574537937, Val Loss: 5.671109542960212, Val MAE: 1.5483155250549316\n",
      "Epoch 1938/2000, Train Loss: 4.737503528258697, Val Loss: 5.671139978227162, Val MAE: 1.548275351524353\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1939/2000, Train Loss: 4.737403751932852, Val Loss: 5.671050446374076, Val MAE: 1.548384189605713\n",
      "Epoch 1940/2000, Train Loss: 4.737323123409987, Val Loss: 5.67102179924647, Val MAE: 1.5484637022018433\n",
      "Epoch 1941/2000, Train Loss: 4.737217821758792, Val Loss: 5.671058251744225, Val MAE: 1.5482666492462158\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1942/2000, Train Loss: 4.7371022315388505, Val Loss: 5.671053770042601, Val MAE: 1.5482213497161865\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1943/2000, Train Loss: 4.737008427026746, Val Loss: 5.671063130810147, Val MAE: 1.5483096837997437\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1944/2000, Train Loss: 4.736891686664145, Val Loss: 5.671010536806924, Val MAE: 1.5483801364898682\n",
      "Epoch 1945/2000, Train Loss: 4.736746141705425, Val Loss: 5.671013494332631, Val MAE: 1.54830002784729\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1946/2000, Train Loss: 4.736649943003365, Val Loss: 5.670971782434554, Val MAE: 1.54840087890625\n",
      "Epoch 1947/2000, Train Loss: 4.736539433133619, Val Loss: 5.670901230403355, Val MAE: 1.5483938455581665\n",
      "Epoch 1948/2000, Train Loss: 4.73642336061206, Val Loss: 5.670934946764083, Val MAE: 1.5484659671783447\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1949/2000, Train Loss: 4.736289614178398, Val Loss: 5.670874161379678, Val MAE: 1.54840886592865\n",
      "Epoch 1950/2000, Train Loss: 4.736188250973464, Val Loss: 5.670802482536861, Val MAE: 1.5485905408859253\n",
      "Epoch 1951/2000, Train Loss: 4.736078965815233, Val Loss: 5.670905493554615, Val MAE: 1.5483790636062622\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1952/2000, Train Loss: 4.73595719579581, Val Loss: 5.6707703073819475, Val MAE: 1.548710823059082\n",
      "Epoch 1953/2000, Train Loss: 4.735823153776914, Val Loss: 5.670757092180706, Val MAE: 1.5486841201782227\n",
      "Epoch 1954/2000, Train Loss: 4.735749701350633, Val Loss: 5.670712033907573, Val MAE: 1.5487213134765625\n",
      "Epoch 1955/2000, Train Loss: 4.735707438378811, Val Loss: 5.6707577875682285, Val MAE: 1.548520803451538\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1956/2000, Train Loss: 4.735622945053789, Val Loss: 5.670632895969209, Val MAE: 1.5488084554672241\n",
      "Epoch 1957/2000, Train Loss: 4.7354215032122875, Val Loss: 5.670656241121746, Val MAE: 1.5487202405929565\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1958/2000, Train Loss: 4.735390659363885, Val Loss: 5.670766265619369, Val MAE: 1.5483907461166382\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1959/2000, Train Loss: 4.735300193888849, Val Loss: 5.670735813322521, Val MAE: 1.548383116722107\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1960/2000, Train Loss: 4.735103806924752, Val Loss: 5.670709223974319, Val MAE: 1.548418641090393\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1961/2000, Train Loss: 4.734998585979424, Val Loss: 5.670648867175693, Val MAE: 1.5484727621078491\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1962/2000, Train Loss: 4.734906402394197, Val Loss: 5.670698395797184, Val MAE: 1.5483125448226929\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1963/2000, Train Loss: 4.734754770195535, Val Loss: 5.670597360247657, Val MAE: 1.5483478307724\n",
      "Epoch 1964/2000, Train Loss: 4.734680050687158, Val Loss: 5.670681249527704, Val MAE: 1.5481585264205933\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1965/2000, Train Loss: 4.734560561617272, Val Loss: 5.670648370470319, Val MAE: 1.5481843948364258\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1966/2000, Train Loss: 4.734568371254904, Val Loss: 5.67065289474669, Val MAE: 1.5480492115020752\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1967/2000, Train Loss: 4.734462766956711, Val Loss: 5.670518792810894, Val MAE: 1.548372507095337\n",
      "Epoch 1968/2000, Train Loss: 4.734204081453289, Val Loss: 5.670549106030237, Val MAE: 1.548404335975647\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1969/2000, Train Loss: 4.734135316356449, Val Loss: 5.670507462251754, Val MAE: 1.5485007762908936\n",
      "Epoch 1970/2000, Train Loss: 4.733933331431723, Val Loss: 5.670470836616698, Val MAE: 1.5484830141067505\n",
      "Epoch 1971/2000, Train Loss: 4.733854573267303, Val Loss: 5.670454680919647, Val MAE: 1.5485469102859497\n",
      "Epoch 1972/2000, Train Loss: 4.733795413849888, Val Loss: 5.67052279767536, Val MAE: 1.5483042001724243\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1973/2000, Train Loss: 4.733782169681006, Val Loss: 5.670384957676842, Val MAE: 1.5486817359924316\n",
      "Epoch 1974/2000, Train Loss: 4.733585519750297, Val Loss: 5.670464206309545, Val MAE: 1.5484216213226318\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1975/2000, Train Loss: 4.733388323037345, Val Loss: 5.670464660440173, Val MAE: 1.5484411716461182\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1976/2000, Train Loss: 4.733314958714968, Val Loss: 5.670335971173786, Val MAE: 1.548596739768982\n",
      "Epoch 1977/2000, Train Loss: 4.733218314701478, Val Loss: 5.670403003692627, Val MAE: 1.5484633445739746\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1978/2000, Train Loss: 4.733113119397076, Val Loss: 5.670362705276126, Val MAE: 1.5484224557876587\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1979/2000, Train Loss: 4.732995087068071, Val Loss: 5.67033984263738, Val MAE: 1.5485271215438843\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1980/2000, Train Loss: 4.732930596187521, Val Loss: 5.670287362166813, Val MAE: 1.5486990213394165\n",
      "Epoch 1981/2000, Train Loss: 4.732731669510706, Val Loss: 5.670300134590694, Val MAE: 1.5485389232635498\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1982/2000, Train Loss: 4.732672192313943, Val Loss: 5.670392587071373, Val MAE: 1.5483728647232056\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1983/2000, Train Loss: 4.732580128442417, Val Loss: 5.670233082203638, Val MAE: 1.548755407333374\n",
      "Epoch 1984/2000, Train Loss: 4.7324068499216745, Val Loss: 5.670230039528438, Val MAE: 1.5486319065093994\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1985/2000, Train Loss: 4.7323688591484965, Val Loss: 5.670215243384952, Val MAE: 1.5486199855804443\n",
      "Epoch 1986/2000, Train Loss: 4.732189731301979, Val Loss: 5.670236374650683, Val MAE: 1.5485155582427979\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1987/2000, Train Loss: 4.732120749980673, Val Loss: 5.670215427875519, Val MAE: 1.5484346151351929\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1988/2000, Train Loss: 4.732060661100702, Val Loss: 5.670277615388234, Val MAE: 1.548209547996521\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1989/2000, Train Loss: 4.731898036090544, Val Loss: 5.670209626356761, Val MAE: 1.5483685731887817\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1990/2000, Train Loss: 4.731809542781039, Val Loss: 5.670236008507865, Val MAE: 1.5482221841812134\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1991/2000, Train Loss: 4.731649083378285, Val Loss: 5.670202612876892, Val MAE: 1.5484037399291992\n",
      "Epoch 1992/2000, Train Loss: 4.731533275168436, Val Loss: 5.670144651617322, Val MAE: 1.5484739542007446\n",
      "Epoch 1993/2000, Train Loss: 4.731481780118095, Val Loss: 5.67008288985207, Val MAE: 1.5486772060394287\n",
      "Epoch 1994/2000, Train Loss: 4.731318632537454, Val Loss: 5.670042767411187, Val MAE: 1.5486363172531128\n",
      "Epoch 1995/2000, Train Loss: 4.731248913430024, Val Loss: 5.670140186945598, Val MAE: 1.5484777688980103\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1996/2000, Train Loss: 4.731240782314027, Val Loss: 5.670040462698255, Val MAE: 1.548731803894043\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1997/2000, Train Loss: 4.731022484716139, Val Loss: 5.66987232083366, Val MAE: 1.5489650964736938\n",
      "Epoch 1998/2000, Train Loss: 4.730976337438242, Val Loss: 5.669840457893553, Val MAE: 1.5488535165786743\n",
      "Epoch 1999/2000, Train Loss: 4.730830583316819, Val Loss: 5.66984315429415, Val MAE: 1.5487924814224243\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 2000/2000, Train Loss: 4.730699791390402, Val Loss: 5.66986239240283, Val MAE: 1.5488122701644897\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Test Loss (MSE): 5.128900527954102\n",
      "Test Mean Absolute Error (MAE): 1.4705641670966323\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(CNNModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (conv2d): Conv2d(1, 6, kernel_size=(2, 2), stride=(1, 1))\n",
       "   (dense1): Linear(in_features=271, out_features=64, bias=True)\n",
       "   (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=271, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'test_mae': 1.4705641670966323})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 266\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_cnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=6,\n",
    "                kernel_size=2,\n",
    "                num_filters=64,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = ['total_points', 'ict_index', 'clean_sheets', 'goals_conceded', 'bps', 'matchup_difficulty', 'goals_scored', 'assists', 'yellow_cards', 'red_cards'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpremier.cnn.experiment import gridsearch_cnn\n",
    "\n",
    "#gridsearch_cnn(epochs=100, verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized)  {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
