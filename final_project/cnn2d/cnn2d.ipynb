{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.cnn2d.model import build_train_cnn, full_cnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from final_project.cnn2d.config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2502, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 9).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2502, 7)\n",
      "Shape of a given window (prior to preprocessing): (6, 9)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.94211124e+00 4.79057889e+01 0.00000000e+00 1.70261067e-03\n",
      " 1.45289444e-01 9.96594779e+00 2.66742338e-02 1.13507378e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[2.73976222e+00 4.48207323e+01 1.00000000e+00 4.12275610e-02\n",
      " 3.52392425e-01 1.07559860e+01 1.61129510e-01 3.36717298e-02]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building CNN Architecture ======\n",
      "====== Done Building CNN Architecture ======\n",
      "Epoch 1/2000, Train Loss: 11.866855933053657, Val Loss: 8.849685135239362, Val MAE: 1.585715413093567\n",
      "Epoch 2/2000, Train Loss: 11.856927807004352, Val Loss: 8.842698648831565, Val MAE: 1.5848805904388428\n",
      "Epoch 3/2000, Train Loss: 11.846918737548663, Val Loss: 8.835782386973046, Val MAE: 1.5840386152267456\n",
      "Epoch 4/2000, Train Loss: 11.837180389963303, Val Loss: 8.82882673425754, Val MAE: 1.5832027196884155\n",
      "Epoch 5/2000, Train Loss: 11.827497647931166, Val Loss: 8.821973066444611, Val MAE: 1.5823637247085571\n",
      "Epoch 6/2000, Train Loss: 11.817971029630995, Val Loss: 8.815286939701535, Val MAE: 1.5815705060958862\n",
      "Epoch 7/2000, Train Loss: 11.808436651242799, Val Loss: 8.80859997318251, Val MAE: 1.580766201019287\n",
      "Epoch 8/2000, Train Loss: 11.798833518371168, Val Loss: 8.801664979208818, Val MAE: 1.5799314975738525\n",
      "Epoch 9/2000, Train Loss: 11.789064327120943, Val Loss: 8.794776226242814, Val MAE: 1.5791029930114746\n",
      "Epoch 10/2000, Train Loss: 11.77931123834452, Val Loss: 8.787960435134565, Val MAE: 1.5782819986343384\n",
      "Epoch 11/2000, Train Loss: 11.769953894971023, Val Loss: 8.781364195542336, Val MAE: 1.5774986743927002\n",
      "Epoch 12/2000, Train Loss: 11.760864886565809, Val Loss: 8.774877942139172, Val MAE: 1.5767303705215454\n",
      "Epoch 13/2000, Train Loss: 11.751800811436315, Val Loss: 8.768485536427956, Val MAE: 1.5759376287460327\n",
      "Epoch 14/2000, Train Loss: 11.742594695641195, Val Loss: 8.761799162812714, Val MAE: 1.5751374959945679\n",
      "Epoch 15/2000, Train Loss: 11.733463302871979, Val Loss: 8.755421778811582, Val MAE: 1.5743449926376343\n",
      "Epoch 16/2000, Train Loss: 11.724424077531518, Val Loss: 8.749002462925066, Val MAE: 1.5735670328140259\n",
      "Epoch 17/2000, Train Loss: 11.715369060210975, Val Loss: 8.742505513025286, Val MAE: 1.572807788848877\n",
      "Epoch 18/2000, Train Loss: 11.705994935986793, Val Loss: 8.735728210938442, Val MAE: 1.5719393491744995\n",
      "Epoch 19/2000, Train Loss: 11.696522429646112, Val Loss: 8.729131569304892, Val MAE: 1.5711747407913208\n",
      "Epoch 20/2000, Train Loss: 11.687631187801284, Val Loss: 8.722805733808434, Val MAE: 1.5704323053359985\n",
      "Epoch 21/2000, Train Loss: 11.678720062521819, Val Loss: 8.716410404953402, Val MAE: 1.5696700811386108\n",
      "Epoch 22/2000, Train Loss: 11.669846239698144, Val Loss: 8.710073931991511, Val MAE: 1.5689626932144165\n",
      "Epoch 23/2000, Train Loss: 11.660725028091079, Val Loss: 8.703530838116857, Val MAE: 1.5682101249694824\n",
      "Epoch 24/2000, Train Loss: 11.651955418379691, Val Loss: 8.69718892921393, Val MAE: 1.567287564277649\n",
      "Epoch 25/2000, Train Loss: 11.643038367966298, Val Loss: 8.691014199456069, Val MAE: 1.5666097402572632\n",
      "Epoch 26/2000, Train Loss: 11.633834041766267, Val Loss: 8.684295334400037, Val MAE: 1.5658934116363525\n",
      "Epoch 27/2000, Train Loss: 11.624747877609455, Val Loss: 8.677875793918483, Val MAE: 1.5651776790618896\n",
      "Epoch 28/2000, Train Loss: 11.615945789351391, Val Loss: 8.671735121652953, Val MAE: 1.5645065307617188\n",
      "Epoch 29/2000, Train Loss: 11.60725109580414, Val Loss: 8.665437524392834, Val MAE: 1.5637937784194946\n",
      "Epoch 30/2000, Train Loss: 11.598489759411546, Val Loss: 8.659210999694645, Val MAE: 1.5631039142608643\n",
      "Epoch 31/2000, Train Loss: 11.589681805069766, Val Loss: 8.652936414708421, Val MAE: 1.562393307685852\n",
      "Epoch 32/2000, Train Loss: 11.58085041910693, Val Loss: 8.646755860451476, Val MAE: 1.561714768409729\n",
      "Epoch 33/2000, Train Loss: 11.572119432679672, Val Loss: 8.640418108967756, Val MAE: 1.5610579252243042\n",
      "Epoch 34/2000, Train Loss: 11.563289188303488, Val Loss: 8.634186055725001, Val MAE: 1.5603752136230469\n",
      "Epoch 35/2000, Train Loss: 11.554456280157718, Val Loss: 8.627987050644203, Val MAE: 1.5597163438796997\n",
      "Epoch 36/2000, Train Loss: 11.545697120669091, Val Loss: 8.621721581995898, Val MAE: 1.559090256690979\n",
      "Epoch 37/2000, Train Loss: 11.536820472499894, Val Loss: 8.615601561860892, Val MAE: 1.558394432067871\n",
      "Epoch 38/2000, Train Loss: 11.528048903518325, Val Loss: 8.609218225222609, Val MAE: 1.5577400922775269\n",
      "Epoch 39/2000, Train Loss: 11.519182774557995, Val Loss: 8.602812166886741, Val MAE: 1.5570673942565918\n",
      "Epoch 40/2000, Train Loss: 11.510269175567304, Val Loss: 8.59650976415907, Val MAE: 1.5564157962799072\n",
      "Epoch 41/2000, Train Loss: 11.501453929549156, Val Loss: 8.590361661843472, Val MAE: 1.5557564496994019\n",
      "Epoch 42/2000, Train Loss: 11.492508762727114, Val Loss: 8.584020632292386, Val MAE: 1.5551220178604126\n",
      "Epoch 43/2000, Train Loss: 11.483403343358551, Val Loss: 8.577650757490357, Val MAE: 1.5544534921646118\n",
      "Epoch 44/2000, Train Loss: 11.474538665937116, Val Loss: 8.571351514017122, Val MAE: 1.5538138151168823\n",
      "Epoch 45/2000, Train Loss: 11.465383444259416, Val Loss: 8.564884168036487, Val MAE: 1.5531773567199707\n",
      "Epoch 46/2000, Train Loss: 11.456197018963202, Val Loss: 8.558187651341619, Val MAE: 1.5525085926055908\n",
      "Epoch 47/2000, Train Loss: 11.44740752966789, Val Loss: 8.55214305915943, Val MAE: 1.551895260810852\n",
      "Epoch 48/2000, Train Loss: 11.438569730140138, Val Loss: 8.545775347096685, Val MAE: 1.5512609481811523\n",
      "Epoch 49/2000, Train Loss: 11.42963959112905, Val Loss: 8.539383660707491, Val MAE: 1.550620198249817\n",
      "Epoch 50/2000, Train Loss: 11.42058747820809, Val Loss: 8.533199926930678, Val MAE: 1.5499998331069946\n",
      "Epoch 51/2000, Train Loss: 11.411768706576087, Val Loss: 8.527064090100021, Val MAE: 1.5493946075439453\n",
      "Epoch 52/2000, Train Loss: 11.403188588984786, Val Loss: 8.520808590101225, Val MAE: 1.5487816333770752\n",
      "Epoch 53/2000, Train Loss: 11.393666239897625, Val Loss: 8.514157414902368, Val MAE: 1.5481456518173218\n",
      "Epoch 54/2000, Train Loss: 11.385107075990136, Val Loss: 8.508300701172026, Val MAE: 1.547562599182129\n",
      "Epoch 55/2000, Train Loss: 11.376779106413071, Val Loss: 8.502320337261066, Val MAE: 1.5469555854797363\n",
      "Epoch 56/2000, Train Loss: 11.368459209819985, Val Loss: 8.496411309597324, Val MAE: 1.546363115310669\n",
      "Epoch 57/2000, Train Loss: 11.359820787424633, Val Loss: 8.490354131394998, Val MAE: 1.545743703842163\n",
      "Epoch 58/2000, Train Loss: 11.351353830365992, Val Loss: 8.484451290685318, Val MAE: 1.5451313257217407\n",
      "Epoch 59/2000, Train Loss: 11.342703282913973, Val Loss: 8.478386104647731, Val MAE: 1.5445226430892944\n",
      "Epoch 60/2000, Train Loss: 11.334315073538667, Val Loss: 8.472358611360647, Val MAE: 1.5439023971557617\n",
      "Epoch 61/2000, Train Loss: 11.325642325661399, Val Loss: 8.466246972392188, Val MAE: 1.5432963371276855\n",
      "Epoch 62/2000, Train Loss: 11.316954127645557, Val Loss: 8.460161163900635, Val MAE: 1.542606234550476\n",
      "Epoch 63/2000, Train Loss: 11.308500080289802, Val Loss: 8.454215485457052, Val MAE: 1.5420223474502563\n",
      "Epoch 64/2000, Train Loss: 11.299921465740038, Val Loss: 8.448139860072196, Val MAE: 1.5414276123046875\n",
      "Epoch 65/2000, Train Loss: 11.291468340635623, Val Loss: 8.442355589467416, Val MAE: 1.5408397912979126\n",
      "Epoch 66/2000, Train Loss: 11.283084533269564, Val Loss: 8.43632057168499, Val MAE: 1.5402358770370483\n",
      "Epoch 67/2000, Train Loss: 11.274695155590525, Val Loss: 8.430377285479839, Val MAE: 1.5395748615264893\n",
      "Epoch 68/2000, Train Loss: 11.266256490264851, Val Loss: 8.424572115687756, Val MAE: 1.5389554500579834\n",
      "Epoch 69/2000, Train Loss: 11.257651941100164, Val Loss: 8.41842547071049, Val MAE: 1.5383155345916748\n",
      "Epoch 70/2000, Train Loss: 11.249060816786113, Val Loss: 8.412396514107378, Val MAE: 1.5377088785171509\n",
      "Epoch 71/2000, Train Loss: 11.240594276417871, Val Loss: 8.406568427435232, Val MAE: 1.5371065139770508\n",
      "Epoch 72/2000, Train Loss: 11.231981612049458, Val Loss: 8.40053308954587, Val MAE: 1.5364952087402344\n",
      "Epoch 73/2000, Train Loss: 11.223490572681719, Val Loss: 8.394637182716457, Val MAE: 1.5358879566192627\n",
      "Epoch 74/2000, Train Loss: 11.215046984976311, Val Loss: 8.388587961471309, Val MAE: 1.5352706909179688\n",
      "Epoch 75/2000, Train Loss: 11.206501871070685, Val Loss: 8.382611076245965, Val MAE: 1.5346620082855225\n",
      "Epoch 76/2000, Train Loss: 11.198033733100074, Val Loss: 8.376565586798275, Val MAE: 1.534030556678772\n",
      "Epoch 77/2000, Train Loss: 11.189467402940851, Val Loss: 8.370626863372314, Val MAE: 1.5334217548370361\n",
      "Epoch 78/2000, Train Loss: 11.180916557318481, Val Loss: 8.364580351653634, Val MAE: 1.532748818397522\n",
      "Epoch 79/2000, Train Loss: 11.172302755442532, Val Loss: 8.358530977001692, Val MAE: 1.5321362018585205\n",
      "Epoch 80/2000, Train Loss: 11.163682157585708, Val Loss: 8.352435707452333, Val MAE: 1.5315358638763428\n",
      "Epoch 81/2000, Train Loss: 11.155011072249392, Val Loss: 8.346487578036498, Val MAE: 1.5308966636657715\n",
      "Epoch 82/2000, Train Loss: 11.145897818097447, Val Loss: 8.340080654314962, Val MAE: 1.5302715301513672\n",
      "Epoch 83/2000, Train Loss: 11.137278868676526, Val Loss: 8.334072543314738, Val MAE: 1.529652714729309\n",
      "Epoch 84/2000, Train Loss: 11.128455164444885, Val Loss: 8.327859523071078, Val MAE: 1.5290324687957764\n",
      "Epoch 85/2000, Train Loss: 11.119908444431776, Val Loss: 8.321862394015826, Val MAE: 1.5284366607666016\n",
      "Epoch 86/2000, Train Loss: 11.111329318306016, Val Loss: 8.315805491674546, Val MAE: 1.5277912616729736\n",
      "Epoch 87/2000, Train Loss: 11.102766856733139, Val Loss: 8.30973737558724, Val MAE: 1.5271638631820679\n",
      "Epoch 88/2000, Train Loss: 11.094163917945231, Val Loss: 8.303670861532794, Val MAE: 1.5265451669692993\n",
      "Epoch 89/2000, Train Loss: 11.085097393899868, Val Loss: 8.297316895839648, Val MAE: 1.5259357690811157\n",
      "Epoch 90/2000, Train Loss: 11.076563165071667, Val Loss: 8.291374608748555, Val MAE: 1.5252957344055176\n",
      "Epoch 91/2000, Train Loss: 11.068155635567825, Val Loss: 8.285552870308775, Val MAE: 1.5246751308441162\n",
      "Epoch 92/2000, Train Loss: 11.059722788460057, Val Loss: 8.279653294817875, Val MAE: 1.52405846118927\n",
      "Epoch 93/2000, Train Loss: 11.051285530252223, Val Loss: 8.273642200893377, Val MAE: 1.5234163999557495\n",
      "Epoch 94/2000, Train Loss: 11.042832175944522, Val Loss: 8.267701830409878, Val MAE: 1.52280855178833\n",
      "Epoch 95/2000, Train Loss: 11.034277559742028, Val Loss: 8.261639160692956, Val MAE: 1.5221364498138428\n",
      "Epoch 96/2000, Train Loss: 11.02576321274487, Val Loss: 8.255659551181148, Val MAE: 1.5215038061141968\n",
      "Epoch 97/2000, Train Loss: 11.016883695346468, Val Loss: 8.249316744438527, Val MAE: 1.5208765268325806\n",
      "Epoch 98/2000, Train Loss: 11.008335537657526, Val Loss: 8.243377370037887, Val MAE: 1.5202362537384033\n",
      "Epoch 99/2000, Train Loss: 10.999862010960987, Val Loss: 8.237551377519349, Val MAE: 1.5195749998092651\n",
      "Epoch 100/2000, Train Loss: 10.991472701592626, Val Loss: 8.231558389292926, Val MAE: 1.5187965631484985\n",
      "Epoch 101/2000, Train Loss: 10.98299412746766, Val Loss: 8.225685810174529, Val MAE: 1.5181530714035034\n",
      "Epoch 102/2000, Train Loss: 10.974455011343858, Val Loss: 8.219580257935522, Val MAE: 1.5175294876098633\n",
      "Epoch 103/2000, Train Loss: 10.965997683225526, Val Loss: 8.213591327826954, Val MAE: 1.5169185400009155\n",
      "Epoch 104/2000, Train Loss: 10.95742589976573, Val Loss: 8.207720349690929, Val MAE: 1.5162924528121948\n",
      "Epoch 105/2000, Train Loss: 10.948985425756014, Val Loss: 8.201667679416191, Val MAE: 1.5156559944152832\n",
      "Epoch 106/2000, Train Loss: 10.940384741877605, Val Loss: 8.195859729137462, Val MAE: 1.5150423049926758\n",
      "Epoch 107/2000, Train Loss: 10.931717418987345, Val Loss: 8.189336774907419, Val MAE: 1.514399528503418\n",
      "Epoch 108/2000, Train Loss: 10.923009825852704, Val Loss: 8.183513835305714, Val MAE: 1.5137847661972046\n",
      "Epoch 109/2000, Train Loss: 10.914386555168328, Val Loss: 8.177270273048078, Val MAE: 1.5131268501281738\n",
      "Epoch 110/2000, Train Loss: 10.905641476784199, Val Loss: 8.171251085093319, Val MAE: 1.512483835220337\n",
      "Epoch 111/2000, Train Loss: 10.897165055190854, Val Loss: 8.165169182551042, Val MAE: 1.511833906173706\n",
      "Epoch 112/2000, Train Loss: 10.888561852104466, Val Loss: 8.159347479643051, Val MAE: 1.511181354522705\n",
      "Epoch 113/2000, Train Loss: 10.880121469567182, Val Loss: 8.15316551276446, Val MAE: 1.5105072259902954\n",
      "Epoch 114/2000, Train Loss: 10.871484951539474, Val Loss: 8.147362266612555, Val MAE: 1.5098551511764526\n",
      "Epoch 115/2000, Train Loss: 10.862972844730724, Val Loss: 8.141037740885448, Val MAE: 1.50922691822052\n",
      "Epoch 116/2000, Train Loss: 10.85422920322948, Val Loss: 8.135080117350958, Val MAE: 1.5085883140563965\n",
      "Epoch 117/2000, Train Loss: 10.845792566177314, Val Loss: 8.1289917294486, Val MAE: 1.5079376697540283\n",
      "Epoch 118/2000, Train Loss: 10.837177787901103, Val Loss: 8.123160775277881, Val MAE: 1.5072839260101318\n",
      "Epoch 119/2000, Train Loss: 10.828617083007316, Val Loss: 8.117118243052529, Val MAE: 1.5066863298416138\n",
      "Epoch 120/2000, Train Loss: 10.819751283530465, Val Loss: 8.1107665612707, Val MAE: 1.506036400794983\n",
      "Epoch 121/2000, Train Loss: 10.811049923077706, Val Loss: 8.104757722926978, Val MAE: 1.5053699016571045\n",
      "Epoch 122/2000, Train Loss: 10.8025330753954, Val Loss: 8.098763891083635, Val MAE: 1.5047608613967896\n",
      "Epoch 123/2000, Train Loss: 10.794083808785054, Val Loss: 8.09280345928562, Val MAE: 1.5041149854660034\n",
      "Epoch 124/2000, Train Loss: 10.785238496201009, Val Loss: 8.086690275341741, Val MAE: 1.5034817457199097\n",
      "Epoch 125/2000, Train Loss: 10.776709808746865, Val Loss: 8.080575694330037, Val MAE: 1.5028294324874878\n",
      "Epoch 126/2000, Train Loss: 10.768079812685217, Val Loss: 8.074591631470176, Val MAE: 1.502164363861084\n",
      "Epoch 127/2000, Train Loss: 10.759495283402453, Val Loss: 8.0685651098796, Val MAE: 1.5015411376953125\n",
      "Epoch 128/2000, Train Loss: 10.751009754442295, Val Loss: 8.062504815490637, Val MAE: 1.5009058713912964\n",
      "Epoch 129/2000, Train Loss: 10.74233249317238, Val Loss: 8.056482812795627, Val MAE: 1.5002562999725342\n",
      "Epoch 130/2000, Train Loss: 10.733824366630337, Val Loss: 8.050548634918655, Val MAE: 1.4996411800384521\n",
      "Epoch 131/2000, Train Loss: 10.725234734238185, Val Loss: 8.04459997986986, Val MAE: 1.498982548713684\n",
      "Epoch 132/2000, Train Loss: 10.716799642855309, Val Loss: 8.038574358707617, Val MAE: 1.4983716011047363\n",
      "Epoch 133/2000, Train Loss: 10.708045638561895, Val Loss: 8.032320506721515, Val MAE: 1.4977004528045654\n",
      "Epoch 134/2000, Train Loss: 10.699102220897597, Val Loss: 8.026162846227301, Val MAE: 1.4970611333847046\n",
      "Epoch 135/2000, Train Loss: 10.690509877664264, Val Loss: 8.020167324805676, Val MAE: 1.4963984489440918\n",
      "Epoch 136/2000, Train Loss: 10.682005252087619, Val Loss: 8.014179441384517, Val MAE: 1.4957228899002075\n",
      "Epoch 137/2000, Train Loss: 10.673632055862834, Val Loss: 8.008173297474704, Val MAE: 1.4949753284454346\n",
      "Epoch 138/2000, Train Loss: 10.665212262113512, Val Loss: 8.002302686561736, Val MAE: 1.4943139553070068\n",
      "Epoch 139/2000, Train Loss: 10.656808353667344, Val Loss: 7.996287429636697, Val MAE: 1.4936453104019165\n",
      "Epoch 140/2000, Train Loss: 10.648144033956095, Val Loss: 7.990166073284427, Val MAE: 1.4929747581481934\n",
      "Epoch 141/2000, Train Loss: 10.63969988544728, Val Loss: 7.984255731998261, Val MAE: 1.4923450946807861\n",
      "Epoch 142/2000, Train Loss: 10.631019814316502, Val Loss: 7.978147884801133, Val MAE: 1.4917129278182983\n",
      "Epoch 143/2000, Train Loss: 10.62260197234445, Val Loss: 7.972278454318628, Val MAE: 1.4910622835159302\n",
      "Epoch 144/2000, Train Loss: 10.614096606602661, Val Loss: 7.966301301191377, Val MAE: 1.4904152154922485\n",
      "Epoch 145/2000, Train Loss: 10.605664917701297, Val Loss: 7.96020852770935, Val MAE: 1.4897810220718384\n",
      "Epoch 146/2000, Train Loss: 10.59714665846884, Val Loss: 7.9541753275957925, Val MAE: 1.489111304283142\n",
      "Epoch 147/2000, Train Loss: 10.588694410699356, Val Loss: 7.948360355972774, Val MAE: 1.4884631633758545\n",
      "Epoch 148/2000, Train Loss: 10.580055070578485, Val Loss: 7.941984687307043, Val MAE: 1.487794280052185\n",
      "Epoch 149/2000, Train Loss: 10.571457402939874, Val Loss: 7.936199320681158, Val MAE: 1.487174391746521\n",
      "Epoch 150/2000, Train Loss: 10.56314831349387, Val Loss: 7.93022667277443, Val MAE: 1.4864956140518188\n",
      "Epoch 151/2000, Train Loss: 10.554678025853844, Val Loss: 7.924371476687021, Val MAE: 1.4858328104019165\n",
      "Epoch 152/2000, Train Loss: 10.54624507870409, Val Loss: 7.918375134295747, Val MAE: 1.485172986984253\n",
      "Epoch 153/2000, Train Loss: 10.537824417796779, Val Loss: 7.912453200750962, Val MAE: 1.4844352006912231\n",
      "Epoch 154/2000, Train Loss: 10.52945319994819, Val Loss: 7.90646888545575, Val MAE: 1.4837355613708496\n",
      "Epoch 155/2000, Train Loss: 10.520767829921304, Val Loss: 7.900501240596292, Val MAE: 1.4830830097198486\n",
      "Epoch 156/2000, Train Loss: 10.51234993071193, Val Loss: 7.894513757554008, Val MAE: 1.4824018478393555\n",
      "Epoch 157/2000, Train Loss: 10.503959536592866, Val Loss: 7.888551605990025, Val MAE: 1.481726050376892\n",
      "Epoch 158/2000, Train Loss: 10.495505472374738, Val Loss: 7.882607532055741, Val MAE: 1.4810600280761719\n",
      "Epoch 159/2000, Train Loss: 10.486963787104738, Val Loss: 7.876656445405468, Val MAE: 1.4803766012191772\n",
      "Epoch 160/2000, Train Loss: 10.478203420950608, Val Loss: 7.870561616374253, Val MAE: 1.4796969890594482\n",
      "Epoch 161/2000, Train Loss: 10.46978909234179, Val Loss: 7.864592144790098, Val MAE: 1.4790135622024536\n",
      "Epoch 162/2000, Train Loss: 10.461346940560775, Val Loss: 7.8586915541366915, Val MAE: 1.4783101081848145\n",
      "Epoch 163/2000, Train Loss: 10.45287071862934, Val Loss: 7.852649384912279, Val MAE: 1.477636694908142\n",
      "Epoch 164/2000, Train Loss: 10.444439881075594, Val Loss: 7.846723335872103, Val MAE: 1.4769680500030518\n",
      "Epoch 165/2000, Train Loss: 10.436042112443632, Val Loss: 7.8407586054751786, Val MAE: 1.4762462377548218\n",
      "Epoch 166/2000, Train Loss: 10.427521629281737, Val Loss: 7.83480118371649, Val MAE: 1.4755767583847046\n",
      "Epoch 167/2000, Train Loss: 10.41883956075847, Val Loss: 7.828695889775766, Val MAE: 1.4748790264129639\n",
      "Epoch 168/2000, Train Loss: 10.410249449049845, Val Loss: 7.8227186324749525, Val MAE: 1.474165678024292\n",
      "Epoch 169/2000, Train Loss: 10.4018424977441, Val Loss: 7.816646413753674, Val MAE: 1.473466157913208\n",
      "Epoch 170/2000, Train Loss: 10.393403799486002, Val Loss: 7.810669257088132, Val MAE: 1.4726077318191528\n",
      "Epoch 171/2000, Train Loss: 10.38499538675254, Val Loss: 7.804735218157522, Val MAE: 1.4719220399856567\n",
      "Epoch 172/2000, Train Loss: 10.376190878628416, Val Loss: 7.7986214408123, Val MAE: 1.4712268114089966\n",
      "Epoch 173/2000, Train Loss: 10.367781879822628, Val Loss: 7.7927067707395405, Val MAE: 1.470545768737793\n",
      "Epoch 174/2000, Train Loss: 10.359453921081192, Val Loss: 7.786847151374429, Val MAE: 1.4698747396469116\n",
      "Epoch 175/2000, Train Loss: 10.351113560403542, Val Loss: 7.780893857857051, Val MAE: 1.4691187143325806\n",
      "Epoch 176/2000, Train Loss: 10.342645632954797, Val Loss: 7.7749271166224885, Val MAE: 1.4684581756591797\n",
      "Epoch 177/2000, Train Loss: 10.334275282330537, Val Loss: 7.769005092250286, Val MAE: 1.467791199684143\n",
      "Epoch 178/2000, Train Loss: 10.325865713194558, Val Loss: 7.763187848843678, Val MAE: 1.467064619064331\n",
      "Epoch 179/2000, Train Loss: 10.317215672341584, Val Loss: 7.756957247751347, Val MAE: 1.4663655757904053\n",
      "Epoch 180/2000, Train Loss: 10.308638073210801, Val Loss: 7.750908895971357, Val MAE: 1.4656860828399658\n",
      "Epoch 181/2000, Train Loss: 10.300051682677974, Val Loss: 7.744818834414134, Val MAE: 1.4649640321731567\n",
      "Epoch 182/2000, Train Loss: 10.291576138345002, Val Loss: 7.738991798236819, Val MAE: 1.4642751216888428\n",
      "Epoch 183/2000, Train Loss: 10.283193602119264, Val Loss: 7.733012512299108, Val MAE: 1.463531494140625\n",
      "Epoch 184/2000, Train Loss: 10.274873917267797, Val Loss: 7.727113229940521, Val MAE: 1.462828516960144\n",
      "Epoch 185/2000, Train Loss: 10.266305241526547, Val Loss: 7.721136528476853, Val MAE: 1.4621139764785767\n",
      "Epoch 186/2000, Train Loss: 10.257921028494003, Val Loss: 7.715259231452823, Val MAE: 1.4614046812057495\n",
      "Epoch 187/2000, Train Loss: 10.249622018883963, Val Loss: 7.709414994900709, Val MAE: 1.460726022720337\n",
      "Epoch 188/2000, Train Loss: 10.240962431785048, Val Loss: 7.703272041418098, Val MAE: 1.4599900245666504\n",
      "Epoch 189/2000, Train Loss: 10.232637410571488, Val Loss: 7.697284585594632, Val MAE: 1.4592599868774414\n",
      "Epoch 190/2000, Train Loss: 10.224064670391936, Val Loss: 7.691219965089479, Val MAE: 1.45854651927948\n",
      "Epoch 191/2000, Train Loss: 10.215537445192583, Val Loss: 7.685278271679495, Val MAE: 1.457840919494629\n",
      "Epoch 192/2000, Train Loss: 10.207193874115859, Val Loss: 7.6792650342453275, Val MAE: 1.4570956230163574\n",
      "Epoch 193/2000, Train Loss: 10.198371117280004, Val Loss: 7.673131180828892, Val MAE: 1.4563974142074585\n",
      "Epoch 194/2000, Train Loss: 10.189659970101054, Val Loss: 7.667184105684512, Val MAE: 1.4556583166122437\n",
      "Epoch 195/2000, Train Loss: 10.181256619043234, Val Loss: 7.661214290873888, Val MAE: 1.4549999237060547\n",
      "Epoch 196/2000, Train Loss: 10.172975860263147, Val Loss: 7.655267936532848, Val MAE: 1.4542818069458008\n",
      "Epoch 197/2000, Train Loss: 10.16492722898196, Val Loss: 7.6496322801578645, Val MAE: 1.4535936117172241\n",
      "Epoch 198/2000, Train Loss: 10.15691460877293, Val Loss: 7.644120913672474, Val MAE: 1.452906608581543\n",
      "Epoch 199/2000, Train Loss: 10.149019439680707, Val Loss: 7.638548575584799, Val MAE: 1.4520800113677979\n",
      "Epoch 200/2000, Train Loss: 10.141152654282921, Val Loss: 7.632823030079722, Val MAE: 1.4513931274414062\n",
      "Epoch 201/2000, Train Loss: 10.133121020639546, Val Loss: 7.627218914976505, Val MAE: 1.4506821632385254\n",
      "Epoch 202/2000, Train Loss: 10.125129195938602, Val Loss: 7.621596080275604, Val MAE: 1.4499950408935547\n",
      "Epoch 203/2000, Train Loss: 10.117109609022231, Val Loss: 7.615831075462065, Val MAE: 1.4493045806884766\n",
      "Epoch 204/2000, Train Loss: 10.109042209718412, Val Loss: 7.610097007952103, Val MAE: 1.4486109018325806\n",
      "Epoch 205/2000, Train Loss: 10.100750167787156, Val Loss: 7.604427368242997, Val MAE: 1.4478775262832642\n",
      "Epoch 206/2000, Train Loss: 10.092714835054354, Val Loss: 7.598455628823475, Val MAE: 1.4471696615219116\n",
      "Epoch 207/2000, Train Loss: 10.084271302863733, Val Loss: 7.5927231354558264, Val MAE: 1.4464532136917114\n",
      "Epoch 208/2000, Train Loss: 10.076034516410768, Val Loss: 7.586922780895884, Val MAE: 1.4457528591156006\n",
      "Epoch 209/2000, Train Loss: 10.067988735364606, Val Loss: 7.58114320720458, Val MAE: 1.4450503587722778\n",
      "Epoch 210/2000, Train Loss: 10.059948262560157, Val Loss: 7.575281909550357, Val MAE: 1.4443422555923462\n",
      "Epoch 211/2000, Train Loss: 10.0516562985921, Val Loss: 7.569602614431825, Val MAE: 1.4435981512069702\n",
      "Epoch 212/2000, Train Loss: 10.043616974208216, Val Loss: 7.56382116464326, Val MAE: 1.4428248405456543\n",
      "Epoch 213/2000, Train Loss: 10.03526568046191, Val Loss: 7.558051938096457, Val MAE: 1.4420844316482544\n",
      "Epoch 214/2000, Train Loss: 10.027176084479613, Val Loss: 7.552215412807628, Val MAE: 1.441383719444275\n",
      "Epoch 215/2000, Train Loss: 10.01869264613013, Val Loss: 7.54633994600039, Val MAE: 1.4406530857086182\n",
      "Epoch 216/2000, Train Loss: 10.010514770466404, Val Loss: 7.540393591983112, Val MAE: 1.4399363994598389\n",
      "Epoch 217/2000, Train Loss: 10.002211459779675, Val Loss: 7.534696594159317, Val MAE: 1.4392118453979492\n",
      "Epoch 218/2000, Train Loss: 9.994058597362834, Val Loss: 7.528886948951968, Val MAE: 1.4385404586791992\n",
      "Epoch 219/2000, Train Loss: 9.98574316402546, Val Loss: 7.523107346185432, Val MAE: 1.4378262758255005\n",
      "Epoch 220/2000, Train Loss: 9.977722526233507, Val Loss: 7.517244360067581, Val MAE: 1.4371418952941895\n",
      "Epoch 221/2000, Train Loss: 9.96959502726106, Val Loss: 7.511538694625515, Val MAE: 1.4364039897918701\n",
      "Epoch 222/2000, Train Loss: 9.961347978506515, Val Loss: 7.505834736669158, Val MAE: 1.435682773590088\n",
      "Epoch 223/2000, Train Loss: 9.953187085786048, Val Loss: 7.500071384018199, Val MAE: 1.4349949359893799\n",
      "Epoch 224/2000, Train Loss: 9.9450328126853, Val Loss: 7.494404838478017, Val MAE: 1.4342776536941528\n",
      "Epoch 225/2000, Train Loss: 9.936849815990861, Val Loss: 7.488542477567644, Val MAE: 1.433559775352478\n",
      "Epoch 226/2000, Train Loss: 9.928725870951677, Val Loss: 7.482702651409447, Val MAE: 1.4328418970108032\n",
      "Epoch 227/2000, Train Loss: 9.920527937049943, Val Loss: 7.476964159444062, Val MAE: 1.4320963621139526\n",
      "Epoch 228/2000, Train Loss: 9.91227820479239, Val Loss: 7.471188245608479, Val MAE: 1.4313877820968628\n",
      "Epoch 229/2000, Train Loss: 9.904016518851634, Val Loss: 7.465214971059889, Val MAE: 1.4306594133377075\n",
      "Epoch 230/2000, Train Loss: 9.8956790220048, Val Loss: 7.4595277514660685, Val MAE: 1.4299249649047852\n",
      "Epoch 231/2000, Train Loss: 9.887529622647138, Val Loss: 7.453677120522457, Val MAE: 1.4292007684707642\n",
      "Epoch 232/2000, Train Loss: 9.879249639718148, Val Loss: 7.448010110335175, Val MAE: 1.4284244775772095\n",
      "Epoch 233/2000, Train Loss: 9.871013641357422, Val Loss: 7.442001477649113, Val MAE: 1.4276829957962036\n",
      "Epoch 234/2000, Train Loss: 9.862694878752876, Val Loss: 7.436166904291402, Val MAE: 1.4268873929977417\n",
      "Epoch 235/2000, Train Loss: 9.854139928416611, Val Loss: 7.430127952071493, Val MAE: 1.4261189699172974\n",
      "Epoch 236/2000, Train Loss: 9.845614592449959, Val Loss: 7.424109042454951, Val MAE: 1.4253848791122437\n",
      "Epoch 237/2000, Train Loss: 9.836963467239283, Val Loss: 7.418080533669865, Val MAE: 1.424634575843811\n",
      "Epoch 238/2000, Train Loss: 9.828744075353304, Val Loss: 7.412307862079827, Val MAE: 1.4238802194595337\n",
      "Epoch 239/2000, Train Loss: 9.820497851844074, Val Loss: 7.40662764144702, Val MAE: 1.4231353998184204\n",
      "Epoch 240/2000, Train Loss: 9.811697246269873, Val Loss: 7.400315183621472, Val MAE: 1.422349452972412\n",
      "Epoch 241/2000, Train Loss: 9.80344625308685, Val Loss: 7.394698127680348, Val MAE: 1.4215888977050781\n",
      "Epoch 242/2000, Train Loss: 9.795370480692673, Val Loss: 7.388797401377609, Val MAE: 1.4208306074142456\n",
      "Epoch 243/2000, Train Loss: 9.787190117298284, Val Loss: 7.383260792898615, Val MAE: 1.420103907585144\n",
      "Epoch 244/2000, Train Loss: 9.779173047442287, Val Loss: 7.377506273298517, Val MAE: 1.4193423986434937\n",
      "Epoch 245/2000, Train Loss: 9.770856456808351, Val Loss: 7.371671871032463, Val MAE: 1.418580412864685\n",
      "Epoch 246/2000, Train Loss: 9.762727695011774, Val Loss: 7.3658720341954735, Val MAE: 1.41781747341156\n",
      "Epoch 247/2000, Train Loss: 9.754575926057358, Val Loss: 7.360200449871999, Val MAE: 1.4170494079589844\n",
      "Epoch 248/2000, Train Loss: 9.746178426444935, Val Loss: 7.354397029954368, Val MAE: 1.4162918329238892\n",
      "Epoch 249/2000, Train Loss: 9.737872407491112, Val Loss: 7.348249476632895, Val MAE: 1.415507197380066\n",
      "Epoch 250/2000, Train Loss: 9.72957505041094, Val Loss: 7.3427122401121245, Val MAE: 1.4147533178329468\n",
      "Epoch 251/2000, Train Loss: 9.721494476701382, Val Loss: 7.336859723769759, Val MAE: 1.413997769355774\n",
      "Epoch 252/2000, Train Loss: 9.713213055881102, Val Loss: 7.331154031672801, Val MAE: 1.4132167100906372\n",
      "Epoch 253/2000, Train Loss: 9.70504610323356, Val Loss: 7.32540390916594, Val MAE: 1.41245698928833\n",
      "Epoch 254/2000, Train Loss: 9.696916837543451, Val Loss: 7.319478548686644, Val MAE: 1.411667823791504\n",
      "Epoch 255/2000, Train Loss: 9.688633871147236, Val Loss: 7.313838961740267, Val MAE: 1.410915732383728\n",
      "Epoch 256/2000, Train Loss: 9.680520971562192, Val Loss: 7.308049694148046, Val MAE: 1.410137414932251\n",
      "Epoch 257/2000, Train Loss: 9.672276222883765, Val Loss: 7.302237784381091, Val MAE: 1.4093751907348633\n",
      "Epoch 258/2000, Train Loss: 9.664043000789317, Val Loss: 7.296253793489364, Val MAE: 1.40858793258667\n",
      "Epoch 259/2000, Train Loss: 9.655537987661168, Val Loss: 7.2903390110250355, Val MAE: 1.407802700996399\n",
      "Epoch 260/2000, Train Loss: 9.647185712772531, Val Loss: 7.284519694005616, Val MAE: 1.4069863557815552\n",
      "Epoch 261/2000, Train Loss: 9.63900397037089, Val Loss: 7.27884385982367, Val MAE: 1.4061715602874756\n",
      "Epoch 262/2000, Train Loss: 9.6308847016363, Val Loss: 7.273051256244621, Val MAE: 1.4053999185562134\n",
      "Epoch 263/2000, Train Loss: 9.622662783594071, Val Loss: 7.267358314545502, Val MAE: 1.404636025428772\n",
      "Epoch 264/2000, Train Loss: 9.614536533847431, Val Loss: 7.261540210714535, Val MAE: 1.403823733329773\n",
      "Epoch 265/2000, Train Loss: 9.606322819334196, Val Loss: 7.2557733100714294, Val MAE: 1.4028987884521484\n",
      "Epoch 266/2000, Train Loss: 9.59814972728693, Val Loss: 7.249967034139458, Val MAE: 1.4021390676498413\n",
      "Epoch 267/2000, Train Loss: 9.589855373310105, Val Loss: 7.2441421084186635, Val MAE: 1.4013334512710571\n",
      "Epoch 268/2000, Train Loss: 9.581544644311778, Val Loss: 7.23824790917909, Val MAE: 1.4005365371704102\n",
      "Epoch 269/2000, Train Loss: 9.573374041708709, Val Loss: 7.232479542490154, Val MAE: 1.3997591733932495\n",
      "Epoch 270/2000, Train Loss: 9.564936709290768, Val Loss: 7.226670539780615, Val MAE: 1.3989728689193726\n",
      "Epoch 271/2000, Train Loss: 9.556699409898942, Val Loss: 7.22078369137699, Val MAE: 1.3981670141220093\n",
      "Epoch 272/2000, Train Loss: 9.548395489093569, Val Loss: 7.214975945176337, Val MAE: 1.3973549604415894\n",
      "Epoch 273/2000, Train Loss: 9.540048441375774, Val Loss: 7.209104516130953, Val MAE: 1.3965704441070557\n",
      "Epoch 274/2000, Train Loss: 9.531232800218435, Val Loss: 7.202998765872234, Val MAE: 1.3957387208938599\n",
      "Epoch 275/2000, Train Loss: 9.52298994061096, Val Loss: 7.19721234228176, Val MAE: 1.3947678804397583\n",
      "Epoch 276/2000, Train Loss: 9.514795857801365, Val Loss: 7.191483914339548, Val MAE: 1.3939616680145264\n",
      "Epoch 277/2000, Train Loss: 9.50669003489302, Val Loss: 7.185699287876047, Val MAE: 1.3931703567504883\n",
      "Epoch 278/2000, Train Loss: 9.498473287538564, Val Loss: 7.180088654532662, Val MAE: 1.3923776149749756\n",
      "Epoch 279/2000, Train Loss: 9.49036975793308, Val Loss: 7.174350665429035, Val MAE: 1.391564130783081\n",
      "Epoch 280/2000, Train Loss: 9.482102849994618, Val Loss: 7.168453405045457, Val MAE: 1.390738844871521\n",
      "Epoch 281/2000, Train Loss: 9.473804350048646, Val Loss: 7.162746463166403, Val MAE: 1.3899147510528564\n",
      "Epoch 282/2000, Train Loss: 9.46564142966044, Val Loss: 7.157023315370728, Val MAE: 1.3891065120697021\n",
      "Epoch 283/2000, Train Loss: 9.457319319329073, Val Loss: 7.151073583465539, Val MAE: 1.388271450996399\n",
      "Epoch 284/2000, Train Loss: 9.449007137678564, Val Loss: 7.145303928854282, Val MAE: 1.3874505758285522\n",
      "Epoch 285/2000, Train Loss: 9.440782309548988, Val Loss: 7.139630644149183, Val MAE: 1.3866357803344727\n",
      "Epoch 286/2000, Train Loss: 9.432551115144706, Val Loss: 7.133874363224132, Val MAE: 1.385807752609253\n",
      "Epoch 287/2000, Train Loss: 9.424342763917256, Val Loss: 7.12804956804543, Val MAE: 1.3848990201950073\n",
      "Epoch 288/2000, Train Loss: 9.416100987262286, Val Loss: 7.122335721579505, Val MAE: 1.3840855360031128\n",
      "Epoch 289/2000, Train Loss: 9.40765368695822, Val Loss: 7.1162538543823874, Val MAE: 1.3831802606582642\n",
      "Epoch 290/2000, Train Loss: 9.399178004491281, Val Loss: 7.110365354721105, Val MAE: 1.3823413848876953\n",
      "Epoch 291/2000, Train Loss: 9.390877801949902, Val Loss: 7.10464593346678, Val MAE: 1.3815194368362427\n",
      "Epoch 292/2000, Train Loss: 9.382637536347803, Val Loss: 7.098929537896903, Val MAE: 1.3806636333465576\n",
      "Epoch 293/2000, Train Loss: 9.373902511483456, Val Loss: 7.092660211535596, Val MAE: 1.3797988891601562\n",
      "Epoch 294/2000, Train Loss: 9.365501797183075, Val Loss: 7.086864444004064, Val MAE: 1.3789256811141968\n",
      "Epoch 295/2000, Train Loss: 9.356859607968271, Val Loss: 7.080888833146631, Val MAE: 1.3780467510223389\n",
      "Epoch 296/2000, Train Loss: 9.348634899713938, Val Loss: 7.075084289928576, Val MAE: 1.377229928970337\n",
      "Epoch 297/2000, Train Loss: 9.340388383761837, Val Loss: 7.069443788636461, Val MAE: 1.3763911724090576\n",
      "Epoch 298/2000, Train Loss: 9.332246229153638, Val Loss: 7.063663749503133, Val MAE: 1.3755409717559814\n",
      "Epoch 299/2000, Train Loss: 9.32378030389426, Val Loss: 7.057629000391136, Val MAE: 1.374505639076233\n",
      "Epoch 300/2000, Train Loss: 9.315515927090898, Val Loss: 7.051864283742368, Val MAE: 1.373660683631897\n",
      "Epoch 301/2000, Train Loss: 9.307143289651444, Val Loss: 7.046037837955734, Val MAE: 1.3728035688400269\n",
      "Epoch 302/2000, Train Loss: 9.298934713228425, Val Loss: 7.040223156562529, Val MAE: 1.3720120191574097\n",
      "Epoch 303/2000, Train Loss: 9.29072199585965, Val Loss: 7.034562205590352, Val MAE: 1.3712104558944702\n",
      "Epoch 304/2000, Train Loss: 9.282464397487356, Val Loss: 7.028709535924424, Val MAE: 1.3703347444534302\n",
      "Epoch 305/2000, Train Loss: 9.274143423172399, Val Loss: 7.022980067483333, Val MAE: 1.3694870471954346\n",
      "Epoch 306/2000, Train Loss: 9.266027938385003, Val Loss: 7.01729661222716, Val MAE: 1.368704080581665\n",
      "Epoch 307/2000, Train Loss: 9.257885005127948, Val Loss: 7.011579724271804, Val MAE: 1.367813229560852\n",
      "Epoch 308/2000, Train Loss: 9.24956424984439, Val Loss: 7.005775967205134, Val MAE: 1.366855263710022\n",
      "Epoch 309/2000, Train Loss: 9.24142952754669, Val Loss: 7.000121497376785, Val MAE: 1.3660145998001099\n",
      "Epoch 310/2000, Train Loss: 9.23323332474869, Val Loss: 6.994270780726197, Val MAE: 1.365173101425171\n",
      "Epoch 311/2000, Train Loss: 9.225046256827532, Val Loss: 6.988649486252437, Val MAE: 1.3643628358840942\n",
      "Epoch 312/2000, Train Loss: 9.216878201938872, Val Loss: 6.982920400551557, Val MAE: 1.3635449409484863\n",
      "Epoch 313/2000, Train Loss: 9.208573353777576, Val Loss: 6.977220675041606, Val MAE: 1.3626765012741089\n",
      "Epoch 314/2000, Train Loss: 9.200428221248384, Val Loss: 6.97144216014857, Val MAE: 1.361863136291504\n",
      "Epoch 315/2000, Train Loss: 9.19201678530979, Val Loss: 6.965532039412422, Val MAE: 1.3609814643859863\n",
      "Epoch 316/2000, Train Loss: 9.183647328186552, Val Loss: 6.959618512967851, Val MAE: 1.360145092010498\n",
      "Epoch 317/2000, Train Loss: 9.17523870895158, Val Loss: 6.953793448247529, Val MAE: 1.3593127727508545\n",
      "Epoch 318/2000, Train Loss: 9.166647803038359, Val Loss: 6.9479341567515975, Val MAE: 1.3584811687469482\n",
      "Epoch 319/2000, Train Loss: 9.158478193879734, Val Loss: 6.942241267582739, Val MAE: 1.3576488494873047\n",
      "Epoch 320/2000, Train Loss: 9.15040153499536, Val Loss: 6.936454059886123, Val MAE: 1.3568037748336792\n",
      "Epoch 321/2000, Train Loss: 9.142058270271953, Val Loss: 6.930828788627936, Val MAE: 1.3559808731079102\n",
      "Epoch 322/2000, Train Loss: 9.133705056344477, Val Loss: 6.924924931869527, Val MAE: 1.355126976966858\n",
      "Epoch 323/2000, Train Loss: 9.125218796438892, Val Loss: 6.918949388659112, Val MAE: 1.3542537689208984\n",
      "Epoch 324/2000, Train Loss: 9.116736232021285, Val Loss: 6.91310189829472, Val MAE: 1.3534306287765503\n",
      "Epoch 325/2000, Train Loss: 9.108577590784515, Val Loss: 6.907398594712369, Val MAE: 1.3526124954223633\n",
      "Epoch 326/2000, Train Loss: 9.100417868253173, Val Loss: 6.901583354578053, Val MAE: 1.3517576456069946\n",
      "Epoch 327/2000, Train Loss: 9.091745741493504, Val Loss: 6.895639001112064, Val MAE: 1.3509140014648438\n",
      "Epoch 328/2000, Train Loss: 9.083227797554176, Val Loss: 6.889677306285539, Val MAE: 1.350096583366394\n",
      "Epoch 329/2000, Train Loss: 9.075029714783625, Val Loss: 6.883897587258413, Val MAE: 1.349270224571228\n",
      "Epoch 330/2000, Train Loss: 9.066670634422396, Val Loss: 6.878064507350516, Val MAE: 1.3484289646148682\n",
      "Epoch 331/2000, Train Loss: 9.058547551143445, Val Loss: 6.8725263286084965, Val MAE: 1.3476276397705078\n",
      "Epoch 332/2000, Train Loss: 9.050460688779927, Val Loss: 6.866977568384348, Val MAE: 1.3467371463775635\n",
      "Epoch 333/2000, Train Loss: 9.042372875653582, Val Loss: 6.861136042979348, Val MAE: 1.3459120988845825\n",
      "Epoch 334/2000, Train Loss: 9.033368597522358, Val Loss: 6.854863054256183, Val MAE: 1.3450658321380615\n",
      "Epoch 335/2000, Train Loss: 9.025145314603519, Val Loss: 6.849252385321975, Val MAE: 1.3442578315734863\n",
      "Epoch 336/2000, Train Loss: 9.016914691012783, Val Loss: 6.8435130780592095, Val MAE: 1.3434009552001953\n",
      "Epoch 337/2000, Train Loss: 9.008772049053714, Val Loss: 6.837953894193042, Val MAE: 1.3426165580749512\n",
      "Epoch 338/2000, Train Loss: 9.000631590549464, Val Loss: 6.832202675239747, Val MAE: 1.3418217897415161\n",
      "Epoch 339/2000, Train Loss: 8.9925659828626, Val Loss: 6.826581213797817, Val MAE: 1.3410159349441528\n",
      "Epoch 340/2000, Train Loss: 8.984484346459647, Val Loss: 6.8209992018231285, Val MAE: 1.3402622938156128\n",
      "Epoch 341/2000, Train Loss: 8.97634633554694, Val Loss: 6.815143786591118, Val MAE: 1.3394402265548706\n",
      "Epoch 342/2000, Train Loss: 8.967963071780094, Val Loss: 6.809482524913596, Val MAE: 1.3386070728302002\n",
      "Epoch 343/2000, Train Loss: 8.9600298496082, Val Loss: 6.803972027096237, Val MAE: 1.3378115892410278\n",
      "Epoch 344/2000, Train Loss: 8.952213627688101, Val Loss: 6.798586847550625, Val MAE: 1.337015986442566\n",
      "Epoch 345/2000, Train Loss: 8.944374433303507, Val Loss: 6.792976008951444, Val MAE: 1.3361958265304565\n",
      "Epoch 346/2000, Train Loss: 8.936360216043697, Val Loss: 6.78765606502819, Val MAE: 1.3353804349899292\n",
      "Epoch 347/2000, Train Loss: 8.928414992596373, Val Loss: 6.782105361450258, Val MAE: 1.3345913887023926\n",
      "Epoch 348/2000, Train Loss: 8.920441191426125, Val Loss: 6.776570982725073, Val MAE: 1.33378267288208\n",
      "Epoch 349/2000, Train Loss: 8.912088090157088, Val Loss: 6.77057891107883, Val MAE: 1.3329459428787231\n",
      "Epoch 350/2000, Train Loss: 8.90390822625235, Val Loss: 6.7651344135447635, Val MAE: 1.3321592807769775\n",
      "Epoch 351/2000, Train Loss: 8.895992214029485, Val Loss: 6.759464630273016, Val MAE: 1.3312981128692627\n",
      "Epoch 352/2000, Train Loss: 8.887932292156659, Val Loss: 6.75376879769656, Val MAE: 1.3304754495620728\n",
      "Epoch 353/2000, Train Loss: 8.8797811950725, Val Loss: 6.748152866987072, Val MAE: 1.3293870687484741\n",
      "Epoch 354/2000, Train Loss: 8.871489979518802, Val Loss: 6.742527686297072, Val MAE: 1.3285781145095825\n",
      "Epoch 355/2000, Train Loss: 8.863589870240197, Val Loss: 6.736896575478778, Val MAE: 1.3277302980422974\n",
      "Epoch 356/2000, Train Loss: 8.855546669203035, Val Loss: 6.731412020115563, Val MAE: 1.32693350315094\n",
      "Epoch 357/2000, Train Loss: 8.84736495229082, Val Loss: 6.725628108401072, Val MAE: 1.3260475397109985\n",
      "Epoch 358/2000, Train Loss: 8.839277714531327, Val Loss: 6.720094321411118, Val MAE: 1.3251909017562866\n",
      "Epoch 359/2000, Train Loss: 8.83118870365086, Val Loss: 6.714458713204578, Val MAE: 1.3243454694747925\n",
      "Epoch 360/2000, Train Loss: 8.823107541335325, Val Loss: 6.708974803015076, Val MAE: 1.3235334157943726\n",
      "Epoch 361/2000, Train Loss: 8.815058719998623, Val Loss: 6.703202387681378, Val MAE: 1.3226854801177979\n",
      "Epoch 362/2000, Train Loss: 8.806888850706565, Val Loss: 6.697591580196071, Val MAE: 1.3218629360198975\n",
      "Epoch 363/2000, Train Loss: 8.798775531867143, Val Loss: 6.692041251882104, Val MAE: 1.3210093975067139\n",
      "Epoch 364/2000, Train Loss: 8.790356562936646, Val Loss: 6.686126033548911, Val MAE: 1.3201572895050049\n",
      "Epoch 365/2000, Train Loss: 8.782133375449938, Val Loss: 6.6803840410022985, Val MAE: 1.3193230628967285\n",
      "Epoch 366/2000, Train Loss: 8.773788977348659, Val Loss: 6.674641384299493, Val MAE: 1.3185100555419922\n",
      "Epoch 367/2000, Train Loss: 8.765610375268489, Val Loss: 6.669021873535794, Val MAE: 1.3176567554473877\n",
      "Epoch 368/2000, Train Loss: 8.756963812674078, Val Loss: 6.663062565918191, Val MAE: 1.316820502281189\n",
      "Epoch 369/2000, Train Loss: 8.748570832645008, Val Loss: 6.657272353559351, Val MAE: 1.3159843683242798\n",
      "Epoch 370/2000, Train Loss: 8.740554216920602, Val Loss: 6.651719086827329, Val MAE: 1.3151706457138062\n",
      "Epoch 371/2000, Train Loss: 8.732143584552905, Val Loss: 6.645803007143953, Val MAE: 1.3142889738082886\n",
      "Epoch 372/2000, Train Loss: 8.723900870519868, Val Loss: 6.640270456705815, Val MAE: 1.3134486675262451\n",
      "Epoch 373/2000, Train Loss: 8.715948402477249, Val Loss: 6.634618649747237, Val MAE: 1.3125947713851929\n",
      "Epoch 374/2000, Train Loss: 8.70773569570306, Val Loss: 6.628948050361552, Val MAE: 1.3118008375167847\n",
      "Epoch 375/2000, Train Loss: 8.699395494189321, Val Loss: 6.6231117137410775, Val MAE: 1.3108607530593872\n",
      "Epoch 376/2000, Train Loss: 8.691053047270755, Val Loss: 6.617381508189701, Val MAE: 1.3099883794784546\n",
      "Epoch 377/2000, Train Loss: 8.683029046411066, Val Loss: 6.611854170837374, Val MAE: 1.3091551065444946\n",
      "Epoch 378/2000, Train Loss: 8.675137741109571, Val Loss: 6.606287804793376, Val MAE: 1.308295726776123\n",
      "Epoch 379/2000, Train Loss: 8.667090043931028, Val Loss: 6.600743031071309, Val MAE: 1.3074229955673218\n",
      "Epoch 380/2000, Train Loss: 8.659080377265589, Val Loss: 6.595193223018331, Val MAE: 1.3066257238388062\n",
      "Epoch 381/2000, Train Loss: 8.650996963884, Val Loss: 6.589396823887295, Val MAE: 1.3057787418365479\n",
      "Epoch 382/2000, Train Loss: 8.642599133655853, Val Loss: 6.583651408799154, Val MAE: 1.3049019575119019\n",
      "Epoch 383/2000, Train Loss: 8.634464609408733, Val Loss: 6.577986492758514, Val MAE: 1.304059386253357\n",
      "Epoch 384/2000, Train Loss: 8.626543280551036, Val Loss: 6.572379649792776, Val MAE: 1.3032443523406982\n",
      "Epoch 385/2000, Train Loss: 8.618102102137323, Val Loss: 6.566514128012084, Val MAE: 1.3023704290390015\n",
      "Epoch 386/2000, Train Loss: 8.609955908000227, Val Loss: 6.560805515319799, Val MAE: 1.30153226852417\n",
      "Epoch 387/2000, Train Loss: 8.60175044921554, Val Loss: 6.555272608387878, Val MAE: 1.3007036447525024\n",
      "Epoch 388/2000, Train Loss: 8.59393239635949, Val Loss: 6.54959509301322, Val MAE: 1.2998723983764648\n",
      "Epoch 389/2000, Train Loss: 8.585988066574934, Val Loss: 6.544143449170808, Val MAE: 1.2989964485168457\n",
      "Epoch 390/2000, Train Loss: 8.578105843451892, Val Loss: 6.538622044696377, Val MAE: 1.2982068061828613\n",
      "Epoch 391/2000, Train Loss: 8.570184186323365, Val Loss: 6.533176922196463, Val MAE: 1.2973684072494507\n",
      "Epoch 392/2000, Train Loss: 8.562246287370472, Val Loss: 6.527721874804904, Val MAE: 1.29655122756958\n",
      "Epoch 393/2000, Train Loss: 8.554189166996824, Val Loss: 6.521940703408089, Val MAE: 1.2957223653793335\n",
      "Epoch 394/2000, Train Loss: 8.545585213069838, Val Loss: 6.516016294228969, Val MAE: 1.2948732376098633\n",
      "Epoch 395/2000, Train Loss: 8.536920754524552, Val Loss: 6.509914593594002, Val MAE: 1.2940309047698975\n",
      "Epoch 396/2000, Train Loss: 8.528862297466704, Val Loss: 6.504617752872892, Val MAE: 1.293228268623352\n",
      "Epoch 397/2000, Train Loss: 8.521143762983655, Val Loss: 6.499185501592688, Val MAE: 1.2924013137817383\n",
      "Epoch 398/2000, Train Loss: 8.513365319820078, Val Loss: 6.4938005585894425, Val MAE: 1.2915831804275513\n",
      "Epoch 399/2000, Train Loss: 8.5052541627004, Val Loss: 6.4881896319612125, Val MAE: 1.290747046470642\n",
      "Epoch 400/2000, Train Loss: 8.497440074159785, Val Loss: 6.482728194536304, Val MAE: 1.2899169921875\n",
      "Epoch 401/2000, Train Loss: 8.489571528648263, Val Loss: 6.477161162943528, Val MAE: 1.2888851165771484\n",
      "Epoch 402/2000, Train Loss: 8.481518499563313, Val Loss: 6.471658784862769, Val MAE: 1.2880723476409912\n",
      "Epoch 403/2000, Train Loss: 8.473669662889664, Val Loss: 6.466130145356851, Val MAE: 1.2872129678726196\n",
      "Epoch 404/2000, Train Loss: 8.465559610581007, Val Loss: 6.460472988636108, Val MAE: 1.2863961458206177\n",
      "Epoch 405/2000, Train Loss: 8.45767893339918, Val Loss: 6.454924963181838, Val MAE: 1.2855952978134155\n",
      "Epoch 406/2000, Train Loss: 8.449775770205493, Val Loss: 6.449366109966352, Val MAE: 1.2847923040390015\n",
      "Epoch 407/2000, Train Loss: 8.441596722829294, Val Loss: 6.443870665040601, Val MAE: 1.2839739322662354\n",
      "Epoch 408/2000, Train Loss: 8.43365763350525, Val Loss: 6.438277513871527, Val MAE: 1.283180594444275\n",
      "Epoch 409/2000, Train Loss: 8.425802182593209, Val Loss: 6.432623805500215, Val MAE: 1.282353162765503\n",
      "Epoch 410/2000, Train Loss: 8.417788041009993, Val Loss: 6.427188190205507, Val MAE: 1.281563401222229\n",
      "Epoch 411/2000, Train Loss: 8.40956227394425, Val Loss: 6.421467027060438, Val MAE: 1.2807540893554688\n",
      "Epoch 412/2000, Train Loss: 8.401489638254613, Val Loss: 6.415959910126256, Val MAE: 1.279929280281067\n",
      "Epoch 413/2000, Train Loss: 8.393596810213223, Val Loss: 6.410395527750576, Val MAE: 1.2790918350219727\n",
      "Epoch 414/2000, Train Loss: 8.385667214885334, Val Loss: 6.404879739759563, Val MAE: 1.2782609462738037\n",
      "Epoch 415/2000, Train Loss: 8.37761385591348, Val Loss: 6.399264013868808, Val MAE: 1.2774325609207153\n",
      "Epoch 416/2000, Train Loss: 8.369625899963633, Val Loss: 6.393703631869715, Val MAE: 1.276604413986206\n",
      "Epoch 417/2000, Train Loss: 8.361684520014915, Val Loss: 6.388079415507776, Val MAE: 1.2757970094680786\n",
      "Epoch 418/2000, Train Loss: 8.353417762240529, Val Loss: 6.382633904017889, Val MAE: 1.2749396562576294\n",
      "Epoch 419/2000, Train Loss: 8.345452216608333, Val Loss: 6.37704285855889, Val MAE: 1.2741384506225586\n",
      "Epoch 420/2000, Train Loss: 8.337447872357583, Val Loss: 6.371427519715531, Val MAE: 1.273291826248169\n",
      "Epoch 421/2000, Train Loss: 8.32936616894995, Val Loss: 6.365912678413547, Val MAE: 1.2724484205245972\n",
      "Epoch 422/2000, Train Loss: 8.32130260803968, Val Loss: 6.360166298660384, Val MAE: 1.2715650796890259\n",
      "Epoch 423/2000, Train Loss: 8.312558533846852, Val Loss: 6.353954069390453, Val MAE: 1.2707074880599976\n",
      "Epoch 424/2000, Train Loss: 8.304533920909204, Val Loss: 6.348459354124406, Val MAE: 1.2698825597763062\n",
      "Epoch 425/2000, Train Loss: 8.296214806338016, Val Loss: 6.342852634933271, Val MAE: 1.269047498703003\n",
      "Epoch 426/2000, Train Loss: 8.28828027816123, Val Loss: 6.337393666368242, Val MAE: 1.268180251121521\n",
      "Epoch 427/2000, Train Loss: 8.280428597205692, Val Loss: 6.331959858424155, Val MAE: 1.2673485279083252\n",
      "Epoch 428/2000, Train Loss: 8.272542713060874, Val Loss: 6.326412540091217, Val MAE: 1.2664754390716553\n",
      "Epoch 429/2000, Train Loss: 8.264651268761064, Val Loss: 6.320997283304858, Val MAE: 1.265670895576477\n",
      "Epoch 430/2000, Train Loss: 8.256642172100424, Val Loss: 6.315343408069874, Val MAE: 1.2648385763168335\n",
      "Epoch 431/2000, Train Loss: 8.248660064091352, Val Loss: 6.3098506766059765, Val MAE: 1.2637412548065186\n",
      "Epoch 432/2000, Train Loss: 8.240848707963865, Val Loss: 6.304374421454644, Val MAE: 1.2629036903381348\n",
      "Epoch 433/2000, Train Loss: 8.23295479810707, Val Loss: 6.298977375939107, Val MAE: 1.2621148824691772\n",
      "Epoch 434/2000, Train Loss: 8.224933413346394, Val Loss: 6.293528909789853, Val MAE: 1.2612888813018799\n",
      "Epoch 435/2000, Train Loss: 8.216703738415744, Val Loss: 6.287653471440018, Val MAE: 1.2604368925094604\n",
      "Epoch 436/2000, Train Loss: 8.208385066451502, Val Loss: 6.282040842515086, Val MAE: 1.2595996856689453\n",
      "Epoch 437/2000, Train Loss: 8.20048425511395, Val Loss: 6.276610131172361, Val MAE: 1.2587624788284302\n",
      "Epoch 438/2000, Train Loss: 8.192467086916862, Val Loss: 6.27114969540358, Val MAE: 1.257920503616333\n",
      "Epoch 439/2000, Train Loss: 8.18454958528806, Val Loss: 6.265652027894838, Val MAE: 1.257127046585083\n",
      "Epoch 440/2000, Train Loss: 8.176286575266918, Val Loss: 6.259896663111803, Val MAE: 1.2562205791473389\n",
      "Epoch 441/2000, Train Loss: 8.168287485397391, Val Loss: 6.254498951488329, Val MAE: 1.2554322481155396\n",
      "Epoch 442/2000, Train Loss: 8.16044752051096, Val Loss: 6.24893825899399, Val MAE: 1.2546511888504028\n",
      "Epoch 443/2000, Train Loss: 8.152321208607066, Val Loss: 6.24340623122398, Val MAE: 1.2538948059082031\n",
      "Epoch 444/2000, Train Loss: 8.144376071531381, Val Loss: 6.237964250894601, Val MAE: 1.2532507181167603\n",
      "Epoch 445/2000, Train Loss: 8.136209539674839, Val Loss: 6.232259255164765, Val MAE: 1.2525765895843506\n",
      "Epoch 446/2000, Train Loss: 8.127899015336228, Val Loss: 6.226406162366725, Val MAE: 1.251884937286377\n",
      "Epoch 447/2000, Train Loss: 8.119845178387706, Val Loss: 6.221045163199745, Val MAE: 1.2512445449829102\n",
      "Epoch 448/2000, Train Loss: 8.111966539285206, Val Loss: 6.215508509582407, Val MAE: 1.2505871057510376\n",
      "Epoch 449/2000, Train Loss: 8.104029007812045, Val Loss: 6.210090173355223, Val MAE: 1.2499386072158813\n",
      "Epoch 450/2000, Train Loss: 8.096027477073838, Val Loss: 6.204672332459284, Val MAE: 1.2492927312850952\n",
      "Epoch 451/2000, Train Loss: 8.088165409737181, Val Loss: 6.199120585373639, Val MAE: 1.248645305633545\n",
      "Epoch 452/2000, Train Loss: 8.080170430839466, Val Loss: 6.193704174025585, Val MAE: 1.248026728630066\n",
      "Epoch 453/2000, Train Loss: 8.071644919479864, Val Loss: 6.187734001389829, Val MAE: 1.247326374053955\n",
      "Epoch 454/2000, Train Loss: 8.063580141623923, Val Loss: 6.182446037234342, Val MAE: 1.2467259168624878\n",
      "Epoch 455/2000, Train Loss: 8.055584512863263, Val Loss: 6.176820912020002, Val MAE: 1.2460654973983765\n",
      "Epoch 456/2000, Train Loss: 8.047489148143836, Val Loss: 6.171151893413955, Val MAE: 1.2454122304916382\n",
      "Epoch 457/2000, Train Loss: 8.039479083574934, Val Loss: 6.165833733649924, Val MAE: 1.244824767112732\n",
      "Epoch 458/2000, Train Loss: 8.031372952390603, Val Loss: 6.159978261312074, Val MAE: 1.2441325187683105\n",
      "Epoch 459/2000, Train Loss: 8.023401659007313, Val Loss: 6.154756430353544, Val MAE: 1.243530511856079\n",
      "Epoch 460/2000, Train Loss: 8.01570652152337, Val Loss: 6.149235529198099, Val MAE: 1.242887258529663\n",
      "Epoch 461/2000, Train Loss: 8.007834279880152, Val Loss: 6.14391913046789, Val MAE: 1.242260217666626\n",
      "Epoch 462/2000, Train Loss: 8.000049531217185, Val Loss: 6.138614610985919, Val MAE: 1.2416452169418335\n",
      "Epoch 463/2000, Train Loss: 7.992156211200854, Val Loss: 6.133262844004587, Val MAE: 1.2410252094268799\n",
      "Epoch 464/2000, Train Loss: 7.984255561627842, Val Loss: 6.1276289525852485, Val MAE: 1.240352988243103\n",
      "Epoch 465/2000, Train Loss: 7.976411328629529, Val Loss: 6.122237796453378, Val MAE: 1.239780306816101\n",
      "Epoch 466/2000, Train Loss: 7.968601088996173, Val Loss: 6.116798164096375, Val MAE: 1.239253044128418\n",
      "Epoch 467/2000, Train Loss: 7.960651624315953, Val Loss: 6.1114526805735565, Val MAE: 1.238683819770813\n",
      "Epoch 468/2000, Train Loss: 7.952772186985657, Val Loss: 6.105897562676219, Val MAE: 1.2381117343902588\n",
      "Epoch 469/2000, Train Loss: 7.944888698884881, Val Loss: 6.100460543178066, Val MAE: 1.2375438213348389\n",
      "Epoch 470/2000, Train Loss: 7.936978373792796, Val Loss: 6.0951156056735405, Val MAE: 1.236976981163025\n",
      "Epoch 471/2000, Train Loss: 7.929082039738607, Val Loss: 6.089606646194277, Val MAE: 1.2364132404327393\n",
      "Epoch 472/2000, Train Loss: 7.920897383958081, Val Loss: 6.083891668189024, Val MAE: 1.235778570175171\n",
      "Epoch 473/2000, Train Loss: 7.912952321189924, Val Loss: 6.07855045330806, Val MAE: 1.235226035118103\n",
      "Epoch 474/2000, Train Loss: 7.905116193815358, Val Loss: 6.073197481174051, Val MAE: 1.234688401222229\n",
      "Epoch 475/2000, Train Loss: 7.896819709275762, Val Loss: 6.067298086049167, Val MAE: 1.2340641021728516\n",
      "Epoch 476/2000, Train Loss: 7.888848722223672, Val Loss: 6.0618867123957845, Val MAE: 1.2335097789764404\n",
      "Epoch 477/2000, Train Loss: 7.881034601656682, Val Loss: 6.056638903409888, Val MAE: 1.2329612970352173\n",
      "Epoch 478/2000, Train Loss: 7.8731694499705505, Val Loss: 6.051127271901992, Val MAE: 1.2323755025863647\n",
      "Epoch 479/2000, Train Loss: 7.86532588244455, Val Loss: 6.045627327088457, Val MAE: 1.2318031787872314\n",
      "Epoch 480/2000, Train Loss: 7.8573639090381455, Val Loss: 6.04026599760006, Val MAE: 1.2312583923339844\n",
      "Epoch 481/2000, Train Loss: 7.849505850870865, Val Loss: 6.034827004217608, Val MAE: 1.2306984663009644\n",
      "Epoch 482/2000, Train Loss: 7.8415996856896495, Val Loss: 6.029393469166436, Val MAE: 1.2301256656646729\n",
      "Epoch 483/2000, Train Loss: 7.833028434008724, Val Loss: 6.023404578723776, Val MAE: 1.229429841041565\n",
      "Epoch 484/2000, Train Loss: 7.825198389620114, Val Loss: 6.018286361092129, Val MAE: 1.2288942337036133\n",
      "Epoch 485/2000, Train Loss: 7.817609851363556, Val Loss: 6.013059532757497, Val MAE: 1.2283308506011963\n",
      "Epoch 486/2000, Train Loss: 7.810118307130793, Val Loss: 6.007976998504633, Val MAE: 1.2278043031692505\n",
      "Epoch 487/2000, Train Loss: 7.802786907062359, Val Loss: 6.0029076246907715, Val MAE: 1.227299690246582\n",
      "Epoch 488/2000, Train Loss: 7.795367866997156, Val Loss: 5.997818169329228, Val MAE: 1.226763367652893\n",
      "Epoch 489/2000, Train Loss: 7.788065963170745, Val Loss: 5.992694038964118, Val MAE: 1.2262259721755981\n",
      "Epoch 490/2000, Train Loss: 7.780549422209751, Val Loss: 5.987628601259354, Val MAE: 1.225691318511963\n",
      "Epoch 491/2000, Train Loss: 7.773081395163465, Val Loss: 5.982288735506572, Val MAE: 1.2251086235046387\n",
      "Epoch 492/2000, Train Loss: 7.764707034446167, Val Loss: 5.976630681451762, Val MAE: 1.2244981527328491\n",
      "Epoch 493/2000, Train Loss: 7.75722501756539, Val Loss: 5.97160150279863, Val MAE: 1.22394859790802\n",
      "Epoch 494/2000, Train Loss: 7.7499429011398995, Val Loss: 5.966497612773536, Val MAE: 1.2234230041503906\n",
      "Epoch 495/2000, Train Loss: 7.742632624252195, Val Loss: 5.961405310160517, Val MAE: 1.2229000329971313\n",
      "Epoch 496/2000, Train Loss: 7.735229869387043, Val Loss: 5.9562730351316295, Val MAE: 1.2223753929138184\n",
      "Epoch 497/2000, Train Loss: 7.727800314369001, Val Loss: 5.951313226898457, Val MAE: 1.2219233512878418\n",
      "Epoch 498/2000, Train Loss: 7.720514404320814, Val Loss: 5.946139449515475, Val MAE: 1.221374750137329\n",
      "Epoch 499/2000, Train Loss: 7.71313602310508, Val Loss: 5.941067524054491, Val MAE: 1.2208765745162964\n",
      "Epoch 500/2000, Train Loss: 7.705623978675625, Val Loss: 5.936068168638538, Val MAE: 1.2203880548477173\n",
      "Epoch 501/2000, Train Loss: 7.6982417346017495, Val Loss: 5.930778859399523, Val MAE: 1.2199456691741943\n",
      "Epoch 502/2000, Train Loss: 7.6907266438249975, Val Loss: 5.925541984154953, Val MAE: 1.2194013595581055\n",
      "Epoch 503/2000, Train Loss: 7.6829785136063675, Val Loss: 5.920152236596043, Val MAE: 1.218873381614685\n",
      "Epoch 504/2000, Train Loss: 7.675297959365223, Val Loss: 5.914979548716742, Val MAE: 1.2183352708816528\n",
      "Epoch 505/2000, Train Loss: 7.6679138516150465, Val Loss: 5.9098548385369645, Val MAE: 1.217800498008728\n",
      "Epoch 506/2000, Train Loss: 7.6605991752694385, Val Loss: 5.904619991338599, Val MAE: 1.2173088788986206\n",
      "Epoch 507/2000, Train Loss: 7.653130054959125, Val Loss: 5.89963288415172, Val MAE: 1.2167930603027344\n",
      "Epoch 508/2000, Train Loss: 7.645730063545639, Val Loss: 5.894537689412878, Val MAE: 1.2162697315216064\n",
      "Epoch 509/2000, Train Loss: 7.638361652638243, Val Loss: 5.889449468605572, Val MAE: 1.2157849073410034\n",
      "Epoch 510/2000, Train Loss: 7.630432436432249, Val Loss: 5.884029347604272, Val MAE: 1.2151833772659302\n",
      "Epoch 511/2000, Train Loss: 7.623021477598348, Val Loss: 5.8789512459738695, Val MAE: 1.2146753072738647\n",
      "Epoch 512/2000, Train Loss: 7.615552216060611, Val Loss: 5.873957181713126, Val MAE: 1.2141801118850708\n",
      "Epoch 513/2000, Train Loss: 7.6082614273277684, Val Loss: 5.868804452010233, Val MAE: 1.213662028312683\n",
      "Epoch 514/2000, Train Loss: 7.600868702096564, Val Loss: 5.863776463391186, Val MAE: 1.2131508588790894\n",
      "Epoch 515/2000, Train Loss: 7.593554349249707, Val Loss: 5.858803265083464, Val MAE: 1.2126715183258057\n",
      "Epoch 516/2000, Train Loss: 7.586157373366233, Val Loss: 5.853761134193257, Val MAE: 1.212211012840271\n",
      "Epoch 517/2000, Train Loss: 7.578629380852752, Val Loss: 5.848485275690158, Val MAE: 1.2116878032684326\n",
      "Epoch 518/2000, Train Loss: 7.57123789755347, Val Loss: 5.843542811762923, Val MAE: 1.2111964225769043\n",
      "Epoch 519/2000, Train Loss: 7.563895862898315, Val Loss: 5.838463010546553, Val MAE: 1.2106798887252808\n",
      "Epoch 520/2000, Train Loss: 7.556558944153495, Val Loss: 5.833233105273932, Val MAE: 1.2101925611495972\n",
      "Epoch 521/2000, Train Loss: 7.548878571395149, Val Loss: 5.828061551302231, Val MAE: 1.2096812725067139\n",
      "Epoch 522/2000, Train Loss: 7.541413656164218, Val Loss: 5.823020359844438, Val MAE: 1.2092504501342773\n",
      "Epoch 523/2000, Train Loss: 7.534024494001721, Val Loss: 5.817978342241923, Val MAE: 1.2087748050689697\n",
      "Epoch 524/2000, Train Loss: 7.526745699153988, Val Loss: 5.812832594083578, Val MAE: 1.2082690000534058\n",
      "Epoch 525/2000, Train Loss: 7.5191043783181435, Val Loss: 5.807536888976882, Val MAE: 1.207759976387024\n",
      "Epoch 526/2000, Train Loss: 7.511669641595683, Val Loss: 5.802635318933304, Val MAE: 1.2072985172271729\n",
      "Epoch 527/2000, Train Loss: 7.504124223458072, Val Loss: 5.797318705755806, Val MAE: 1.206796407699585\n",
      "Epoch 528/2000, Train Loss: 7.496738240870886, Val Loss: 5.792319839603935, Val MAE: 1.206383466720581\n",
      "Epoch 529/2000, Train Loss: 7.489368551988467, Val Loss: 5.78729609188543, Val MAE: 1.2059962749481201\n",
      "Epoch 530/2000, Train Loss: 7.482093725631486, Val Loss: 5.782309082306657, Val MAE: 1.205566167831421\n",
      "Epoch 531/2000, Train Loss: 7.474579847975049, Val Loss: 5.777027259491326, Val MAE: 1.2050893306732178\n",
      "Epoch 532/2000, Train Loss: 7.466946577281771, Val Loss: 5.7719356267265685, Val MAE: 1.204655647277832\n",
      "Epoch 533/2000, Train Loss: 7.459504377243476, Val Loss: 5.766700331363681, Val MAE: 1.2042330503463745\n",
      "Epoch 534/2000, Train Loss: 7.451853780013435, Val Loss: 5.761507353289272, Val MAE: 1.2037767171859741\n",
      "Epoch 535/2000, Train Loss: 7.4445598439251555, Val Loss: 5.7565992327216335, Val MAE: 1.2033437490463257\n",
      "Epoch 536/2000, Train Loss: 7.437041820955729, Val Loss: 5.751445175480743, Val MAE: 1.2029409408569336\n",
      "Epoch 537/2000, Train Loss: 7.429642597079439, Val Loss: 5.746135047345063, Val MAE: 1.2025073766708374\n",
      "Epoch 538/2000, Train Loss: 7.422162502195327, Val Loss: 5.741168115371618, Val MAE: 1.2020634412765503\n",
      "Epoch 539/2000, Train Loss: 7.414948686654079, Val Loss: 5.736313153324487, Val MAE: 1.2017313241958618\n",
      "Epoch 540/2000, Train Loss: 7.4075839751352035, Val Loss: 5.731115128646978, Val MAE: 1.2013118267059326\n",
      "Epoch 541/2000, Train Loss: 7.400268524602116, Val Loss: 5.726307162473081, Val MAE: 1.200913906097412\n",
      "Epoch 542/2000, Train Loss: 7.393191923118188, Val Loss: 5.7212427414359714, Val MAE: 1.2005419731140137\n",
      "Epoch 543/2000, Train Loss: 7.3856216337496425, Val Loss: 5.716120899285329, Val MAE: 1.2001042366027832\n",
      "Epoch 544/2000, Train Loss: 7.3780877826333535, Val Loss: 5.710851747066692, Val MAE: 1.199668526649475\n",
      "Epoch 545/2000, Train Loss: 7.37062067794266, Val Loss: 5.705847731135111, Val MAE: 1.1992933750152588\n",
      "Epoch 546/2000, Train Loss: 7.3634910886978355, Val Loss: 5.700941528392383, Val MAE: 1.1989282369613647\n",
      "Epoch 547/2000, Train Loss: 7.35637174936939, Val Loss: 5.696151836140917, Val MAE: 1.198577642440796\n",
      "Epoch 548/2000, Train Loss: 7.349378775709196, Val Loss: 5.691264750335483, Val MAE: 1.1982160806655884\n",
      "Epoch 549/2000, Train Loss: 7.34233587956008, Val Loss: 5.686437026003566, Val MAE: 1.1978281736373901\n",
      "Epoch 550/2000, Train Loss: 7.335044801154324, Val Loss: 5.6812529560171185, Val MAE: 1.1974430084228516\n",
      "Epoch 551/2000, Train Loss: 7.327811639862112, Val Loss: 5.676345426413253, Val MAE: 1.1970899105072021\n",
      "Epoch 552/2000, Train Loss: 7.320530856480592, Val Loss: 5.671505136809158, Val MAE: 1.1967785358428955\n",
      "Epoch 553/2000, Train Loss: 7.313018845411945, Val Loss: 5.666054942565934, Val MAE: 1.1963235139846802\n",
      "Epoch 554/2000, Train Loss: 7.305675809386627, Val Loss: 5.661289827343751, Val MAE: 1.1960031986236572\n",
      "Epoch 555/2000, Train Loss: 7.298381008319001, Val Loss: 5.656430415420492, Val MAE: 1.1956645250320435\n",
      "Epoch 556/2000, Train Loss: 7.290991587627532, Val Loss: 5.65115744354438, Val MAE: 1.1952656507492065\n",
      "Epoch 557/2000, Train Loss: 7.284003711620212, Val Loss: 5.646373154434163, Val MAE: 1.1949551105499268\n",
      "Epoch 558/2000, Train Loss: 7.2772813411849695, Val Loss: 5.641802366473895, Val MAE: 1.1946252584457397\n",
      "Epoch 559/2000, Train Loss: 7.270412969456043, Val Loss: 5.637265076263859, Val MAE: 1.1943124532699585\n",
      "Epoch 560/2000, Train Loss: 7.263631997470778, Val Loss: 5.632476509174728, Val MAE: 1.1939826011657715\n",
      "Epoch 561/2000, Train Loss: 7.256620117249612, Val Loss: 5.627750714671043, Val MAE: 1.1936485767364502\n",
      "Epoch 562/2000, Train Loss: 7.249683973100162, Val Loss: 5.622872074616442, Val MAE: 1.193311095237732\n",
      "Epoch 563/2000, Train Loss: 7.24294044722372, Val Loss: 5.618276698519219, Val MAE: 1.193035364151001\n",
      "Epoch 564/2000, Train Loss: 7.235810388056524, Val Loss: 5.613360473072875, Val MAE: 1.1926746368408203\n",
      "Epoch 565/2000, Train Loss: 7.228826539649083, Val Loss: 5.608694197787485, Val MAE: 1.1923547983169556\n",
      "Epoch 566/2000, Train Loss: 7.2216878810763525, Val Loss: 5.603889330252125, Val MAE: 1.192038655281067\n",
      "Epoch 567/2000, Train Loss: 7.214613962367562, Val Loss: 5.598932725422114, Val MAE: 1.1916831731796265\n",
      "Epoch 568/2000, Train Loss: 7.207562266334669, Val Loss: 5.594049355788816, Val MAE: 1.1913657188415527\n",
      "Epoch 569/2000, Train Loss: 7.200727329487069, Val Loss: 5.5893342722295305, Val MAE: 1.191119909286499\n",
      "Epoch 570/2000, Train Loss: 7.193657355360292, Val Loss: 5.584741768540859, Val MAE: 1.1908445358276367\n",
      "Epoch 571/2000, Train Loss: 7.186951062734286, Val Loss: 5.57986734748752, Val MAE: 1.1905326843261719\n",
      "Epoch 572/2000, Train Loss: 7.180042835596944, Val Loss: 5.5751289143263705, Val MAE: 1.1902499198913574\n",
      "Epoch 573/2000, Train Loss: 7.173231623389504, Val Loss: 5.570642591321357, Val MAE: 1.1899840831756592\n",
      "Epoch 574/2000, Train Loss: 7.16667681115615, Val Loss: 5.565996046810169, Val MAE: 1.189648985862732\n",
      "Epoch 575/2000, Train Loss: 7.159697325695483, Val Loss: 5.561214371176627, Val MAE: 1.1893846988677979\n",
      "Epoch 576/2000, Train Loss: 7.152875092815673, Val Loss: 5.556703552116794, Val MAE: 1.1891345977783203\n",
      "Epoch 577/2000, Train Loss: 7.145701478261961, Val Loss: 5.551600756535791, Val MAE: 1.1887962818145752\n",
      "Epoch 578/2000, Train Loss: 7.139148028928842, Val Loss: 5.547151309310451, Val MAE: 1.1885333061218262\n",
      "Epoch 579/2000, Train Loss: 7.132307484806636, Val Loss: 5.542384537209676, Val MAE: 1.1882145404815674\n",
      "Epoch 580/2000, Train Loss: 7.125219497298936, Val Loss: 5.537699762894588, Val MAE: 1.1879128217697144\n",
      "Epoch 581/2000, Train Loss: 7.118714833952451, Val Loss: 5.533198998185006, Val MAE: 1.1876697540283203\n",
      "Epoch 582/2000, Train Loss: 7.112138121875365, Val Loss: 5.528794037599087, Val MAE: 1.1874244213104248\n",
      "Epoch 583/2000, Train Loss: 7.105471547617194, Val Loss: 5.5241769149987485, Val MAE: 1.1872178316116333\n",
      "Epoch 584/2000, Train Loss: 7.098545392785661, Val Loss: 5.519314493354791, Val MAE: 1.1868808269500732\n",
      "Epoch 585/2000, Train Loss: 7.0920520948910815, Val Loss: 5.514893886329621, Val MAE: 1.1866486072540283\n",
      "Epoch 586/2000, Train Loss: 7.085561828624604, Val Loss: 5.51072528922681, Val MAE: 1.1864396333694458\n",
      "Epoch 587/2000, Train Loss: 7.079086512044133, Val Loss: 5.506292329466073, Val MAE: 1.186249017715454\n",
      "Epoch 588/2000, Train Loss: 7.072455992368701, Val Loss: 5.5015008021468725, Val MAE: 1.1860703229904175\n",
      "Epoch 589/2000, Train Loss: 7.065586052043932, Val Loss: 5.496878473512007, Val MAE: 1.185849666595459\n",
      "Epoch 590/2000, Train Loss: 7.0590988056307085, Val Loss: 5.492404936001761, Val MAE: 1.1856895685195923\n",
      "Epoch 591/2000, Train Loss: 7.052620606587598, Val Loss: 5.487960441610006, Val MAE: 1.18555748462677\n",
      "Epoch 592/2000, Train Loss: 7.0461481930675385, Val Loss: 5.483573390275064, Val MAE: 1.1854161024093628\n",
      "Epoch 593/2000, Train Loss: 7.039733926832595, Val Loss: 5.479195792475847, Val MAE: 1.185231328010559\n",
      "Epoch 594/2000, Train Loss: 7.033075703452044, Val Loss: 5.474678131653451, Val MAE: 1.1850767135620117\n",
      "Epoch 595/2000, Train Loss: 7.0265779119980705, Val Loss: 5.4702698222896835, Val MAE: 1.1849358081817627\n",
      "Epoch 596/2000, Train Loss: 7.019868772704371, Val Loss: 5.465745179437277, Val MAE: 1.184739351272583\n",
      "Epoch 597/2000, Train Loss: 7.0134723106426975, Val Loss: 5.461299355944076, Val MAE: 1.1845546960830688\n",
      "Epoch 598/2000, Train Loss: 7.0070035596715545, Val Loss: 5.456893235599021, Val MAE: 1.1844035387039185\n",
      "Epoch 599/2000, Train Loss: 7.00062961623659, Val Loss: 5.452492660879503, Val MAE: 1.184340238571167\n",
      "Epoch 600/2000, Train Loss: 6.9942237269840986, Val Loss: 5.448018591160204, Val MAE: 1.184211015701294\n",
      "Epoch 601/2000, Train Loss: 6.987792316819143, Val Loss: 5.443688034795694, Val MAE: 1.1840922832489014\n",
      "Epoch 602/2000, Train Loss: 6.9814339926317235, Val Loss: 5.439535751994552, Val MAE: 1.1839730739593506\n",
      "Epoch 603/2000, Train Loss: 6.974868340369723, Val Loss: 5.434909492700295, Val MAE: 1.1838020086288452\n",
      "Epoch 604/2000, Train Loss: 6.968405340225758, Val Loss: 5.430529116366909, Val MAE: 1.1837100982666016\n",
      "Epoch 605/2000, Train Loss: 6.962025468352691, Val Loss: 5.426147991550837, Val MAE: 1.1835952997207642\n",
      "Epoch 606/2000, Train Loss: 6.955313902323007, Val Loss: 5.421579860015882, Val MAE: 1.183476448059082\n",
      "Epoch 607/2000, Train Loss: 6.948899216050209, Val Loss: 5.417251630243295, Val MAE: 1.1833319664001465\n",
      "Epoch 608/2000, Train Loss: 6.941700558892099, Val Loss: 5.412312840960744, Val MAE: 1.1831804513931274\n",
      "Epoch 609/2000, Train Loss: 6.935390627413502, Val Loss: 5.408118704816429, Val MAE: 1.183104395866394\n",
      "Epoch 610/2000, Train Loss: 6.929267455673343, Val Loss: 5.404044937995065, Val MAE: 1.1830224990844727\n",
      "Epoch 611/2000, Train Loss: 6.923214357290695, Val Loss: 5.399751682997513, Val MAE: 1.182908058166504\n",
      "Epoch 612/2000, Train Loss: 6.91660158572721, Val Loss: 5.39524492978991, Val MAE: 1.1828184127807617\n",
      "Epoch 613/2000, Train Loss: 6.910328491086714, Val Loss: 5.391061995071396, Val MAE: 1.1827888488769531\n",
      "Epoch 614/2000, Train Loss: 6.903621269987069, Val Loss: 5.386555577864445, Val MAE: 1.182614803314209\n",
      "Epoch 615/2000, Train Loss: 6.897538586366755, Val Loss: 5.382455547161283, Val MAE: 1.1825459003448486\n",
      "Epoch 616/2000, Train Loss: 6.891585724001176, Val Loss: 5.378441804729578, Val MAE: 1.1824822425842285\n",
      "Epoch 617/2000, Train Loss: 6.885216519054596, Val Loss: 5.374108191825978, Val MAE: 1.1823937892913818\n",
      "Epoch 618/2000, Train Loss: 6.8791731558790685, Val Loss: 5.370024360816075, Val MAE: 1.1823126077651978\n",
      "Epoch 619/2000, Train Loss: 6.873003971948714, Val Loss: 5.365884317668874, Val MAE: 1.1822636127471924\n",
      "Epoch 620/2000, Train Loss: 6.867320900532737, Val Loss: 5.3619759521674455, Val MAE: 1.1822190284729004\n",
      "Epoch 621/2000, Train Loss: 6.861251354945401, Val Loss: 5.3578933570211325, Val MAE: 1.1821787357330322\n",
      "Epoch 622/2000, Train Loss: 6.855265329335405, Val Loss: 5.353948334627968, Val MAE: 1.1821019649505615\n",
      "Epoch 623/2000, Train Loss: 6.849455768482575, Val Loss: 5.349874962146181, Val MAE: 1.1820915937423706\n",
      "Epoch 624/2000, Train Loss: 6.843557519213962, Val Loss: 5.345835926562665, Val MAE: 1.1820324659347534\n",
      "Epoch 625/2000, Train Loss: 6.83759242781467, Val Loss: 5.341835395479942, Val MAE: 1.18211030960083\n",
      "Epoch 626/2000, Train Loss: 6.831628900370086, Val Loss: 5.337785134222505, Val MAE: 1.182015061378479\n",
      "Epoch 627/2000, Train Loss: 6.825155017763538, Val Loss: 5.333266430730953, Val MAE: 1.181991696357727\n",
      "Epoch 628/2000, Train Loss: 6.819189853149907, Val Loss: 5.329366754269682, Val MAE: 1.1819168329238892\n",
      "Epoch 629/2000, Train Loss: 6.813369166090822, Val Loss: 5.325467747435209, Val MAE: 1.1818954944610596\n",
      "Epoch 630/2000, Train Loss: 6.807433642558197, Val Loss: 5.321398703872747, Val MAE: 1.18184494972229\n",
      "Epoch 631/2000, Train Loss: 6.801759596878671, Val Loss: 5.317410718774291, Val MAE: 1.1818487644195557\n",
      "Epoch 632/2000, Train Loss: 6.795939536161185, Val Loss: 5.313525563243806, Val MAE: 1.1817975044250488\n",
      "Epoch 633/2000, Train Loss: 6.790199336720936, Val Loss: 5.30957345681221, Val MAE: 1.1817877292633057\n",
      "Epoch 634/2000, Train Loss: 6.78410732822126, Val Loss: 5.305411506741946, Val MAE: 1.1817742586135864\n",
      "Epoch 635/2000, Train Loss: 6.778274806416018, Val Loss: 5.301563312955142, Val MAE: 1.1817474365234375\n",
      "Epoch 636/2000, Train Loss: 6.772532980944684, Val Loss: 5.297642393178475, Val MAE: 1.1816904544830322\n",
      "Epoch 637/2000, Train Loss: 6.7667548200330385, Val Loss: 5.293692446601262, Val MAE: 1.1817076206207275\n",
      "Epoch 638/2000, Train Loss: 6.760967425446012, Val Loss: 5.289653862096134, Val MAE: 1.1816895008087158\n",
      "Epoch 639/2000, Train Loss: 6.755016911790038, Val Loss: 5.2856719697964945, Val MAE: 1.1817084550857544\n",
      "Epoch 640/2000, Train Loss: 6.749077390621604, Val Loss: 5.281601793283787, Val MAE: 1.181647777557373\n",
      "Epoch 641/2000, Train Loss: 6.743156605772927, Val Loss: 5.2776223316158015, Val MAE: 1.1816245317459106\n",
      "Epoch 642/2000, Train Loss: 6.737493755697718, Val Loss: 5.273774499019359, Val MAE: 1.1816301345825195\n",
      "Epoch 643/2000, Train Loss: 6.731870475328772, Val Loss: 5.269994282652074, Val MAE: 1.1816673278808594\n",
      "Epoch 644/2000, Train Loss: 6.726248787733723, Val Loss: 5.266062362499418, Val MAE: 1.1816602945327759\n",
      "Epoch 645/2000, Train Loss: 6.720552177895037, Val Loss: 5.262478442647033, Val MAE: 1.1816794872283936\n",
      "Epoch 646/2000, Train Loss: 6.714861972037617, Val Loss: 5.258403941600283, Val MAE: 1.1817467212677002\n",
      "Epoch 647/2000, Train Loss: 6.7088826228352705, Val Loss: 5.254418056493434, Val MAE: 1.1817961931228638\n",
      "Epoch 648/2000, Train Loss: 6.703196902793493, Val Loss: 5.250393422631415, Val MAE: 1.1817541122436523\n",
      "Epoch 649/2000, Train Loss: 6.697410008727271, Val Loss: 5.246624566085167, Val MAE: 1.1817489862442017\n",
      "Epoch 650/2000, Train Loss: 6.6918025454935925, Val Loss: 5.242797616322151, Val MAE: 1.1818121671676636\n",
      "Epoch 651/2000, Train Loss: 6.686162296434918, Val Loss: 5.2390106351488805, Val MAE: 1.1818203926086426\n",
      "Epoch 652/2000, Train Loss: 6.680603174697431, Val Loss: 5.235080664635702, Val MAE: 1.1818983554840088\n",
      "Epoch 653/2000, Train Loss: 6.674851298493964, Val Loss: 5.231372938105675, Val MAE: 1.181922197341919\n",
      "Epoch 654/2000, Train Loss: 6.669041810688667, Val Loss: 5.227275860802395, Val MAE: 1.1819125413894653\n",
      "Epoch 655/2000, Train Loss: 6.663346124957019, Val Loss: 5.223570955606309, Val MAE: 1.181903600692749\n",
      "Epoch 656/2000, Train Loss: 6.657636415584864, Val Loss: 5.219558588368452, Val MAE: 1.181918740272522\n",
      "Epoch 657/2000, Train Loss: 6.652020698328587, Val Loss: 5.2158283281279365, Val MAE: 1.1819038391113281\n",
      "Epoch 658/2000, Train Loss: 6.646473075835643, Val Loss: 5.21213535128792, Val MAE: 1.181937575340271\n",
      "Epoch 659/2000, Train Loss: 6.640983904394722, Val Loss: 5.208316966178497, Val MAE: 1.1820987462997437\n",
      "Epoch 660/2000, Train Loss: 6.635263319901889, Val Loss: 5.204517824443307, Val MAE: 1.182056188583374\n",
      "Epoch 661/2000, Train Loss: 6.629730509935273, Val Loss: 5.200693697986875, Val MAE: 1.1820640563964844\n",
      "Epoch 662/2000, Train Loss: 6.624234406563126, Val Loss: 5.1971594098896725, Val MAE: 1.1822015047073364\n",
      "Epoch 663/2000, Train Loss: 6.61870820357324, Val Loss: 5.193150790862915, Val MAE: 1.1822444200515747\n",
      "Epoch 664/2000, Train Loss: 6.613081801374376, Val Loss: 5.189442475800087, Val MAE: 1.1823034286499023\n",
      "Epoch 665/2000, Train Loss: 6.607244105845326, Val Loss: 5.185506681857381, Val MAE: 1.1823655366897583\n",
      "Epoch 666/2000, Train Loss: 6.601792768149395, Val Loss: 5.1817303149748275, Val MAE: 1.1824239492416382\n",
      "Epoch 667/2000, Train Loss: 6.596369469424601, Val Loss: 5.17823703113914, Val MAE: 1.1824909448623657\n",
      "Epoch 668/2000, Train Loss: 6.591124755233081, Val Loss: 5.1745737503158065, Val MAE: 1.182557225227356\n",
      "Epoch 669/2000, Train Loss: 6.585735217767946, Val Loss: 5.170936995392709, Val MAE: 1.1826075315475464\n",
      "Epoch 670/2000, Train Loss: 6.580455038651036, Val Loss: 5.167288348357391, Val MAE: 1.1826947927474976\n",
      "Epoch 671/2000, Train Loss: 6.575158411482781, Val Loss: 5.163998516522876, Val MAE: 1.182782769203186\n",
      "Epoch 672/2000, Train Loss: 6.569982444895293, Val Loss: 5.160350494729135, Val MAE: 1.1828585863113403\n",
      "Epoch 673/2000, Train Loss: 6.564705106779791, Val Loss: 5.156714097146444, Val MAE: 1.1829097270965576\n",
      "Epoch 674/2000, Train Loss: 6.559377571135719, Val Loss: 5.153162551785665, Val MAE: 1.1829875707626343\n",
      "Epoch 675/2000, Train Loss: 6.554196082106116, Val Loss: 5.149570042490373, Val MAE: 1.183076024055481\n",
      "Epoch 676/2000, Train Loss: 6.548518326421606, Val Loss: 5.145673737824549, Val MAE: 1.183113694190979\n",
      "Epoch 677/2000, Train Loss: 6.542949394998994, Val Loss: 5.141979167006147, Val MAE: 1.1832119226455688\n",
      "Epoch 678/2000, Train Loss: 6.5376928532761465, Val Loss: 5.138521741735419, Val MAE: 1.1832696199417114\n",
      "Epoch 679/2000, Train Loss: 6.532540472080297, Val Loss: 5.135092671810994, Val MAE: 1.183322787284851\n",
      "Epoch 680/2000, Train Loss: 6.527483259240516, Val Loss: 5.131493395159212, Val MAE: 1.1833888292312622\n",
      "Epoch 681/2000, Train Loss: 6.522265875193933, Val Loss: 5.128194975615369, Val MAE: 1.1834818124771118\n",
      "Epoch 682/2000, Train Loss: 6.517293954629507, Val Loss: 5.124791217671604, Val MAE: 1.1835856437683105\n",
      "Epoch 683/2000, Train Loss: 6.512198257099539, Val Loss: 5.121390940354565, Val MAE: 1.1836835145950317\n",
      "Epoch 684/2000, Train Loss: 6.5071095035713435, Val Loss: 5.117819564431671, Val MAE: 1.1837471723556519\n",
      "Epoch 685/2000, Train Loss: 6.501681085741666, Val Loss: 5.1141521173463325, Val MAE: 1.183793306350708\n",
      "Epoch 686/2000, Train Loss: 6.496499578211977, Val Loss: 5.110867796964415, Val MAE: 1.1838804483413696\n",
      "Epoch 687/2000, Train Loss: 6.491482958541311, Val Loss: 5.107419240988028, Val MAE: 1.1840916872024536\n",
      "Epoch 688/2000, Train Loss: 6.486060624910592, Val Loss: 5.1036382526041955, Val MAE: 1.184164047241211\n",
      "Epoch 689/2000, Train Loss: 6.481008579850884, Val Loss: 5.100180473128759, Val MAE: 1.1842859983444214\n",
      "Epoch 690/2000, Train Loss: 6.47592479338316, Val Loss: 5.096984612666012, Val MAE: 1.1844035387039185\n",
      "Epoch 691/2000, Train Loss: 6.470446958180057, Val Loss: 5.0931587714643225, Val MAE: 1.184522032737732\n",
      "Epoch 692/2000, Train Loss: 6.4654462301150835, Val Loss: 5.08985043678462, Val MAE: 1.1846383810043335\n",
      "Epoch 693/2000, Train Loss: 6.46059104060221, Val Loss: 5.086718749469073, Val MAE: 1.1847755908966064\n",
      "Epoch 694/2000, Train Loss: 6.456016458479973, Val Loss: 5.08360792034137, Val MAE: 1.1849263906478882\n",
      "Epoch 695/2000, Train Loss: 6.451271309366947, Val Loss: 5.080424126657093, Val MAE: 1.1850441694259644\n",
      "Epoch 696/2000, Train Loss: 6.446557619516691, Val Loss: 5.077137643879089, Val MAE: 1.1851482391357422\n",
      "Epoch 697/2000, Train Loss: 6.441628814552338, Val Loss: 5.073793328644371, Val MAE: 1.1853222846984863\n",
      "Epoch 698/2000, Train Loss: 6.436600868600285, Val Loss: 5.070489597672553, Val MAE: 1.185390830039978\n",
      "Epoch 699/2000, Train Loss: 6.4318710441019995, Val Loss: 5.067178819850674, Val MAE: 1.1855576038360596\n",
      "Epoch 700/2000, Train Loss: 6.427052280472609, Val Loss: 5.064021179819201, Val MAE: 1.1858052015304565\n",
      "Epoch 701/2000, Train Loss: 6.422408872414153, Val Loss: 5.06086417589718, Val MAE: 1.185957908630371\n",
      "Epoch 702/2000, Train Loss: 6.417614521132719, Val Loss: 5.057702870458717, Val MAE: 1.1860570907592773\n",
      "Epoch 703/2000, Train Loss: 6.412717066755774, Val Loss: 5.054519756959649, Val MAE: 1.1862014532089233\n",
      "Epoch 704/2000, Train Loss: 6.408337431470769, Val Loss: 5.051476553375796, Val MAE: 1.1863124370574951\n",
      "Epoch 705/2000, Train Loss: 6.40371392507566, Val Loss: 5.048430305376179, Val MAE: 1.186436653137207\n",
      "Epoch 706/2000, Train Loss: 6.3988359689389025, Val Loss: 5.045027357844386, Val MAE: 1.186601996421814\n",
      "Epoch 707/2000, Train Loss: 6.394039438731634, Val Loss: 5.04182328467118, Val MAE: 1.1868906021118164\n",
      "Epoch 708/2000, Train Loss: 6.3896013477278855, Val Loss: 5.038823122935971, Val MAE: 1.1870195865631104\n",
      "Epoch 709/2000, Train Loss: 6.384979256457843, Val Loss: 5.035678232877743, Val MAE: 1.1871379613876343\n",
      "Epoch 710/2000, Train Loss: 6.380157229696556, Val Loss: 5.032467421387711, Val MAE: 1.1872364282608032\n",
      "Epoch 711/2000, Train Loss: 6.375687474311611, Val Loss: 5.029474380063847, Val MAE: 1.1874324083328247\n",
      "Epoch 712/2000, Train Loss: 6.371257665522871, Val Loss: 5.026247437588695, Val MAE: 1.187553882598877\n",
      "Epoch 713/2000, Train Loss: 6.366350563200067, Val Loss: 5.023229538707986, Val MAE: 1.187710165977478\n",
      "Epoch 714/2000, Train Loss: 6.361844542877321, Val Loss: 5.020144234122489, Val MAE: 1.1878660917282104\n",
      "Epoch 715/2000, Train Loss: 6.3570586150989765, Val Loss: 5.017057686571882, Val MAE: 1.188048243522644\n",
      "Epoch 716/2000, Train Loss: 6.352634875065921, Val Loss: 5.013910870679368, Val MAE: 1.1882023811340332\n",
      "Epoch 717/2000, Train Loss: 6.348230513497641, Val Loss: 5.011053560471793, Val MAE: 1.1883913278579712\n",
      "Epoch 718/2000, Train Loss: 6.34361079331169, Val Loss: 5.008139158448014, Val MAE: 1.1885755062103271\n",
      "Epoch 719/2000, Train Loss: 6.339181399248348, Val Loss: 5.005071447622471, Val MAE: 1.1887617111206055\n",
      "Epoch 720/2000, Train Loss: 6.334525232560923, Val Loss: 5.002106240907992, Val MAE: 1.1889547109603882\n",
      "Epoch 721/2000, Train Loss: 6.330472187885921, Val Loss: 4.999279204864553, Val MAE: 1.189150094985962\n",
      "Epoch 722/2000, Train Loss: 6.325848982627712, Val Loss: 4.996370165554557, Val MAE: 1.189344882965088\n",
      "Epoch 723/2000, Train Loss: 6.321586091288718, Val Loss: 4.993392895942363, Val MAE: 1.1895099878311157\n",
      "Epoch 724/2000, Train Loss: 6.317075013828791, Val Loss: 4.990313015089143, Val MAE: 1.1897609233856201\n",
      "Epoch 725/2000, Train Loss: 6.312785146260076, Val Loss: 4.987462207396317, Val MAE: 1.1899495124816895\n",
      "Epoch 726/2000, Train Loss: 6.308622349877532, Val Loss: 4.9845680580276435, Val MAE: 1.190117359161377\n",
      "Epoch 727/2000, Train Loss: 6.304257137319288, Val Loss: 4.981546264584726, Val MAE: 1.1902740001678467\n",
      "Epoch 728/2000, Train Loss: 6.299676990897232, Val Loss: 4.97870923531513, Val MAE: 1.1904499530792236\n",
      "Epoch 729/2000, Train Loss: 6.2949470072489335, Val Loss: 4.975409991828006, Val MAE: 1.1906418800354004\n",
      "Epoch 730/2000, Train Loss: 6.290847073740034, Val Loss: 4.972682558586747, Val MAE: 1.1908303499221802\n",
      "Epoch 731/2000, Train Loss: 6.2866905078473865, Val Loss: 4.970015649767372, Val MAE: 1.1909958124160767\n",
      "Epoch 732/2000, Train Loss: 6.282874056571536, Val Loss: 4.967575029849889, Val MAE: 1.1912016868591309\n",
      "Epoch 733/2000, Train Loss: 6.279096116851434, Val Loss: 4.964877478992493, Val MAE: 1.191408395767212\n",
      "Epoch 734/2000, Train Loss: 6.274987946695356, Val Loss: 4.96222922383683, Val MAE: 1.1915922164916992\n",
      "Epoch 735/2000, Train Loss: 6.270864402584661, Val Loss: 4.959333950486474, Val MAE: 1.1917580366134644\n",
      "Epoch 736/2000, Train Loss: 6.266728769406535, Val Loss: 4.956527169394915, Val MAE: 1.1919206380844116\n",
      "Epoch 737/2000, Train Loss: 6.262663638054961, Val Loss: 4.954003069015939, Val MAE: 1.192148208618164\n",
      "Epoch 738/2000, Train Loss: 6.258850141005083, Val Loss: 4.951368460100232, Val MAE: 1.1923279762268066\n",
      "Epoch 739/2000, Train Loss: 6.254780225748932, Val Loss: 4.948608303301799, Val MAE: 1.1925047636032104\n",
      "Epoch 740/2000, Train Loss: 6.2508428694435185, Val Loss: 4.946034411003622, Val MAE: 1.1926584243774414\n",
      "Epoch 741/2000, Train Loss: 6.246926376583456, Val Loss: 4.943555674784061, Val MAE: 1.1929006576538086\n",
      "Epoch 742/2000, Train Loss: 6.242934739735266, Val Loss: 4.940853114748799, Val MAE: 1.1930527687072754\n",
      "Epoch 743/2000, Train Loss: 6.239040453204306, Val Loss: 4.938180252155802, Val MAE: 1.1932567358016968\n",
      "Epoch 744/2000, Train Loss: 6.235278953552964, Val Loss: 4.935707076806135, Val MAE: 1.193455457687378\n",
      "Epoch 745/2000, Train Loss: 6.231523609711325, Val Loss: 4.933155028862277, Val MAE: 1.193624496459961\n",
      "Epoch 746/2000, Train Loss: 6.227660962151745, Val Loss: 4.930610347272256, Val MAE: 1.1938527822494507\n",
      "Epoch 747/2000, Train Loss: 6.223919971574356, Val Loss: 4.928000445517264, Val MAE: 1.1940126419067383\n",
      "Epoch 748/2000, Train Loss: 6.220083003775236, Val Loss: 4.925590489455682, Val MAE: 1.1942065954208374\n",
      "Epoch 749/2000, Train Loss: 6.216120273481392, Val Loss: 4.922981414480472, Val MAE: 1.1943870782852173\n",
      "Epoch 750/2000, Train Loss: 6.212530459607149, Val Loss: 4.920657916469719, Val MAE: 1.1945996284484863\n",
      "Epoch 751/2000, Train Loss: 6.2087461703344475, Val Loss: 4.917984187157136, Val MAE: 1.1947492361068726\n",
      "Epoch 752/2000, Train Loss: 6.20499820592769, Val Loss: 4.915648642754344, Val MAE: 1.1949143409729004\n",
      "Epoch 753/2000, Train Loss: 6.201587948053469, Val Loss: 4.9132172587818985, Val MAE: 1.1951539516448975\n",
      "Epoch 754/2000, Train Loss: 6.1980056734490265, Val Loss: 4.910864653869525, Val MAE: 1.1953266859054565\n",
      "Epoch 755/2000, Train Loss: 6.19439690019253, Val Loss: 4.908523715935706, Val MAE: 1.1955251693725586\n",
      "Epoch 756/2000, Train Loss: 6.190742956572989, Val Loss: 4.906063933799586, Val MAE: 1.1959131956100464\n",
      "Epoch 757/2000, Train Loss: 6.187062365714374, Val Loss: 4.903569873838912, Val MAE: 1.1960912942886353\n",
      "Epoch 758/2000, Train Loss: 6.183181703108359, Val Loss: 4.901055990655239, Val MAE: 1.1962813138961792\n",
      "Epoch 759/2000, Train Loss: 6.179607316305712, Val Loss: 4.898614496737719, Val MAE: 1.196455717086792\n",
      "Epoch 760/2000, Train Loss: 6.1760852217997755, Val Loss: 4.896195433834406, Val MAE: 1.1966739892959595\n",
      "Epoch 761/2000, Train Loss: 6.1723821292576995, Val Loss: 4.893870891589583, Val MAE: 1.1968556642532349\n",
      "Epoch 762/2000, Train Loss: 6.168842129015049, Val Loss: 4.89167990825601, Val MAE: 1.1969798803329468\n",
      "Epoch 763/2000, Train Loss: 6.165390150517912, Val Loss: 4.8892092802187825, Val MAE: 1.1971806287765503\n",
      "Epoch 764/2000, Train Loss: 6.162015777133694, Val Loss: 4.886985214509246, Val MAE: 1.1973546743392944\n",
      "Epoch 765/2000, Train Loss: 6.1586227327625656, Val Loss: 4.884721715026719, Val MAE: 1.1975135803222656\n",
      "Epoch 766/2000, Train Loss: 6.15513946901466, Val Loss: 4.882539927836244, Val MAE: 1.1977187395095825\n",
      "Epoch 767/2000, Train Loss: 6.151715713941076, Val Loss: 4.880266584279969, Val MAE: 1.1978636980056763\n",
      "Epoch 768/2000, Train Loss: 6.148352188567455, Val Loss: 4.877809303579485, Val MAE: 1.1980280876159668\n",
      "Epoch 769/2000, Train Loss: 6.144839739126218, Val Loss: 4.875627290733218, Val MAE: 1.198213815689087\n",
      "Epoch 770/2000, Train Loss: 6.14139807402198, Val Loss: 4.873436444898055, Val MAE: 1.1984025239944458\n",
      "Epoch 771/2000, Train Loss: 6.137740339482332, Val Loss: 4.870832182047522, Val MAE: 1.198567509651184\n",
      "Epoch 772/2000, Train Loss: 6.13426361446303, Val Loss: 4.86864726158871, Val MAE: 1.1987650394439697\n",
      "Epoch 773/2000, Train Loss: 6.13075103138648, Val Loss: 4.8663990130076025, Val MAE: 1.1989545822143555\n",
      "Epoch 774/2000, Train Loss: 6.1270227294764, Val Loss: 4.863887149406465, Val MAE: 1.1991475820541382\n",
      "Epoch 775/2000, Train Loss: 6.12381892903042, Val Loss: 4.861626532269041, Val MAE: 1.199333667755127\n",
      "Epoch 776/2000, Train Loss: 6.120400163502997, Val Loss: 4.859589638666609, Val MAE: 1.1995776891708374\n",
      "Epoch 777/2000, Train Loss: 6.1168387221675395, Val Loss: 4.857088989277524, Val MAE: 1.199777364730835\n",
      "Epoch 778/2000, Train Loss: 6.1135289974819, Val Loss: 4.854986845578734, Val MAE: 1.1999409198760986\n",
      "Epoch 779/2000, Train Loss: 6.11036684263998, Val Loss: 4.8528642994623015, Val MAE: 1.2001307010650635\n",
      "Epoch 780/2000, Train Loss: 6.107215146358495, Val Loss: 4.850742925814991, Val MAE: 1.2003740072250366\n",
      "Epoch 781/2000, Train Loss: 6.103784149666816, Val Loss: 4.8485899658771014, Val MAE: 1.2005928754806519\n",
      "Epoch 782/2000, Train Loss: 6.100456039230447, Val Loss: 4.846324069821459, Val MAE: 1.2007752656936646\n",
      "Epoch 783/2000, Train Loss: 6.097224511155603, Val Loss: 4.844386013421252, Val MAE: 1.2009791135787964\n",
      "Epoch 784/2000, Train Loss: 6.094123562123106, Val Loss: 4.842250036589038, Val MAE: 1.201194167137146\n",
      "Epoch 785/2000, Train Loss: 6.090948595777862, Val Loss: 4.840081286882087, Val MAE: 1.2014366388320923\n",
      "Epoch 786/2000, Train Loss: 6.087838145091705, Val Loss: 4.838126225746054, Val MAE: 1.2016371488571167\n",
      "Epoch 787/2000, Train Loss: 6.084963085531703, Val Loss: 4.836292167348186, Val MAE: 1.2018455266952515\n",
      "Epoch 788/2000, Train Loss: 6.081763617856128, Val Loss: 4.833948757335191, Val MAE: 1.202074408531189\n",
      "Epoch 789/2000, Train Loss: 6.078587623917102, Val Loss: 4.832044767089716, Val MAE: 1.202278733253479\n",
      "Epoch 790/2000, Train Loss: 6.075702436581719, Val Loss: 4.830078174719426, Val MAE: 1.2025045156478882\n",
      "Epoch 791/2000, Train Loss: 6.072828057339589, Val Loss: 4.828328781880027, Val MAE: 1.2027080059051514\n",
      "Epoch 792/2000, Train Loss: 6.069978552078133, Val Loss: 4.826299426980375, Val MAE: 1.202908992767334\n",
      "Epoch 793/2000, Train Loss: 6.066936398336975, Val Loss: 4.824357658448651, Val MAE: 1.2031346559524536\n",
      "Epoch 794/2000, Train Loss: 6.064001020655703, Val Loss: 4.8223962521400505, Val MAE: 1.203371286392212\n",
      "Epoch 795/2000, Train Loss: 6.061071129311053, Val Loss: 4.820527234990297, Val MAE: 1.203566312789917\n",
      "Epoch 796/2000, Train Loss: 6.058364517329667, Val Loss: 4.818745763060146, Val MAE: 1.2037650346755981\n",
      "Epoch 797/2000, Train Loss: 6.05560975870575, Val Loss: 4.81690436175137, Val MAE: 1.2040035724639893\n",
      "Epoch 798/2000, Train Loss: 6.052504935209592, Val Loss: 4.8147190622868035, Val MAE: 1.2042027711868286\n",
      "Epoch 799/2000, Train Loss: 6.049552054385803, Val Loss: 4.812964514202959, Val MAE: 1.2044286727905273\n",
      "Epoch 800/2000, Train Loss: 6.046843949295934, Val Loss: 4.8111543834268105, Val MAE: 1.204639196395874\n",
      "Epoch 801/2000, Train Loss: 6.043747897387521, Val Loss: 4.809137173167129, Val MAE: 1.2048553228378296\n",
      "Epoch 802/2000, Train Loss: 6.040718491235937, Val Loss: 4.807154756978037, Val MAE: 1.2050665616989136\n",
      "Epoch 803/2000, Train Loss: 6.037887731369893, Val Loss: 4.805287220505044, Val MAE: 1.2053061723709106\n",
      "Epoch 804/2000, Train Loss: 6.035104440542866, Val Loss: 4.803534630527647, Val MAE: 1.2055081129074097\n",
      "Epoch 805/2000, Train Loss: 6.0324664295447565, Val Loss: 4.801811258582853, Val MAE: 1.2057154178619385\n",
      "Epoch 806/2000, Train Loss: 6.029823765178872, Val Loss: 4.8001114547663315, Val MAE: 1.2059181928634644\n",
      "Epoch 807/2000, Train Loss: 6.027205355617053, Val Loss: 4.798309959635491, Val MAE: 1.2061465978622437\n",
      "Epoch 808/2000, Train Loss: 6.024617396830865, Val Loss: 4.796599804674546, Val MAE: 1.20634925365448\n",
      "Epoch 809/2000, Train Loss: 6.0220722809251965, Val Loss: 4.795016407649817, Val MAE: 1.2065749168395996\n",
      "Epoch 810/2000, Train Loss: 6.0192297316956225, Val Loss: 4.793089800099219, Val MAE: 1.2069545984268188\n",
      "Epoch 811/2000, Train Loss: 6.016349037030012, Val Loss: 4.791199556350943, Val MAE: 1.2071772813796997\n",
      "Epoch 812/2000, Train Loss: 6.013657927513123, Val Loss: 4.789545340296321, Val MAE: 1.2073698043823242\n",
      "Epoch 813/2000, Train Loss: 6.011075119474041, Val Loss: 4.78785586793005, Val MAE: 1.2076231241226196\n",
      "Epoch 814/2000, Train Loss: 6.008163884807797, Val Loss: 4.7859567666030305, Val MAE: 1.2078741788864136\n",
      "Epoch 815/2000, Train Loss: 6.005545984084239, Val Loss: 4.784264947544402, Val MAE: 1.2080585956573486\n",
      "Epoch 816/2000, Train Loss: 6.002960968858675, Val Loss: 4.782721421679878, Val MAE: 1.2082561254501343\n",
      "Epoch 817/2000, Train Loss: 6.000527032841821, Val Loss: 4.78101528426559, Val MAE: 1.2084778547286987\n",
      "Epoch 818/2000, Train Loss: 5.998158267265744, Val Loss: 4.779528760857235, Val MAE: 1.2087231874465942\n",
      "Epoch 819/2000, Train Loss: 5.995402027346103, Val Loss: 4.777769550516849, Val MAE: 1.2089207172393799\n",
      "Epoch 820/2000, Train Loss: 5.992843057897089, Val Loss: 4.77622835573953, Val MAE: 1.2091323137283325\n",
      "Epoch 821/2000, Train Loss: 5.990495444314613, Val Loss: 4.774489655351545, Val MAE: 1.2093533277511597\n",
      "Epoch 822/2000, Train Loss: 5.987803919182704, Val Loss: 4.772911471955654, Val MAE: 1.209586262702942\n",
      "Epoch 823/2000, Train Loss: 5.985420722650867, Val Loss: 4.771404935859555, Val MAE: 1.209794044494629\n",
      "Epoch 824/2000, Train Loss: 5.982916606815085, Val Loss: 4.7697056316601945, Val MAE: 1.2100435495376587\n",
      "Epoch 825/2000, Train Loss: 5.980492253018243, Val Loss: 4.768012486673951, Val MAE: 1.2102867364883423\n",
      "Epoch 826/2000, Train Loss: 5.978015121967207, Val Loss: 4.766552409189424, Val MAE: 1.2105134725570679\n",
      "Epoch 827/2000, Train Loss: 5.975951448037432, Val Loss: 4.765221264964248, Val MAE: 1.210700511932373\n",
      "Epoch 828/2000, Train Loss: 5.973670147308824, Val Loss: 4.763748179170794, Val MAE: 1.2109311819076538\n",
      "Epoch 829/2000, Train Loss: 5.971420258394818, Val Loss: 4.762133183453496, Val MAE: 1.2111130952835083\n",
      "Epoch 830/2000, Train Loss: 5.96895352729787, Val Loss: 4.760646571148568, Val MAE: 1.2113711833953857\n",
      "Epoch 831/2000, Train Loss: 5.966308553676592, Val Loss: 4.758901299788492, Val MAE: 1.21160089969635\n",
      "Epoch 832/2000, Train Loss: 5.963998886899393, Val Loss: 4.75740610787601, Val MAE: 1.2118353843688965\n",
      "Epoch 833/2000, Train Loss: 5.96171288849379, Val Loss: 4.755915858643496, Val MAE: 1.2120405435562134\n",
      "Epoch 834/2000, Train Loss: 5.959478839620281, Val Loss: 4.754365378541975, Val MAE: 1.2122427225112915\n",
      "Epoch 835/2000, Train Loss: 5.9571034458580865, Val Loss: 4.7529626640454525, Val MAE: 1.2124383449554443\n",
      "Epoch 836/2000, Train Loss: 5.954878971243487, Val Loss: 4.751511017743527, Val MAE: 1.2126795053482056\n",
      "Epoch 837/2000, Train Loss: 5.95259981020638, Val Loss: 4.7500535035227225, Val MAE: 1.212907075881958\n",
      "Epoch 838/2000, Train Loss: 5.95036455151831, Val Loss: 4.748523907926608, Val MAE: 1.2131108045578003\n",
      "Epoch 839/2000, Train Loss: 5.9480740561414125, Val Loss: 4.747100462142642, Val MAE: 1.2134203910827637\n",
      "Epoch 840/2000, Train Loss: 5.945570874595294, Val Loss: 4.7454803879981435, Val MAE: 1.2136156558990479\n",
      "Epoch 841/2000, Train Loss: 5.943247173340382, Val Loss: 4.744094766963889, Val MAE: 1.2138423919677734\n",
      "Epoch 842/2000, Train Loss: 5.94108191306426, Val Loss: 4.742526025113862, Val MAE: 1.2140672206878662\n",
      "Epoch 843/2000, Train Loss: 5.93882024642257, Val Loss: 4.741244539870756, Val MAE: 1.214267373085022\n",
      "Epoch 844/2000, Train Loss: 5.93664743568814, Val Loss: 4.739826921749068, Val MAE: 1.214463710784912\n",
      "Epoch 845/2000, Train Loss: 5.934464867637471, Val Loss: 4.7384049853412655, Val MAE: 1.214699149131775\n",
      "Epoch 846/2000, Train Loss: 5.932345640626982, Val Loss: 4.7370076040642, Val MAE: 1.2149288654327393\n",
      "Epoch 847/2000, Train Loss: 5.9302008789300595, Val Loss: 4.735688943345481, Val MAE: 1.2151494026184082\n",
      "Epoch 848/2000, Train Loss: 5.927992101602023, Val Loss: 4.734146001465677, Val MAE: 1.2153838872909546\n",
      "Epoch 849/2000, Train Loss: 5.925740364286923, Val Loss: 4.7327496584886175, Val MAE: 1.215600609779358\n",
      "Epoch 850/2000, Train Loss: 5.923566034579273, Val Loss: 4.731437920784856, Val MAE: 1.215851902961731\n",
      "Epoch 851/2000, Train Loss: 5.921508841651265, Val Loss: 4.729950996775796, Val MAE: 1.2160775661468506\n",
      "Epoch 852/2000, Train Loss: 5.91928091398573, Val Loss: 4.72857706901126, Val MAE: 1.2163053750991821\n",
      "Epoch 853/2000, Train Loss: 5.917302121300872, Val Loss: 4.7272985408918595, Val MAE: 1.2165184020996094\n",
      "Epoch 854/2000, Train Loss: 5.915223778183781, Val Loss: 4.7258505667171145, Val MAE: 1.21677827835083\n",
      "Epoch 855/2000, Train Loss: 5.913052949685416, Val Loss: 4.724574054707223, Val MAE: 1.2169995307922363\n",
      "Epoch 856/2000, Train Loss: 5.9109452194565835, Val Loss: 4.723199833413278, Val MAE: 1.2172071933746338\n",
      "Epoch 857/2000, Train Loss: 5.908678679809157, Val Loss: 4.721679176448837, Val MAE: 1.217488408088684\n",
      "Epoch 858/2000, Train Loss: 5.906343195011164, Val Loss: 4.720226028004265, Val MAE: 1.2177281379699707\n",
      "Epoch 859/2000, Train Loss: 5.904288956397586, Val Loss: 4.718953751514512, Val MAE: 1.2179081439971924\n",
      "Epoch 860/2000, Train Loss: 5.902266532101819, Val Loss: 4.717548698409805, Val MAE: 1.2181870937347412\n",
      "Epoch 861/2000, Train Loss: 5.900194600285151, Val Loss: 4.716381814236951, Val MAE: 1.2183917760849\n",
      "Epoch 862/2000, Train Loss: 5.897710990711662, Val Loss: 4.714645323309842, Val MAE: 1.2186452150344849\n",
      "Epoch 863/2000, Train Loss: 5.895818688329234, Val Loss: 4.713584691761281, Val MAE: 1.2188855409622192\n",
      "Epoch 864/2000, Train Loss: 5.893939167705934, Val Loss: 4.712341053830826, Val MAE: 1.2190961837768555\n",
      "Epoch 865/2000, Train Loss: 5.8921874472050035, Val Loss: 4.711180314896848, Val MAE: 1.21931791305542\n",
      "Epoch 866/2000, Train Loss: 5.890056134564259, Val Loss: 4.709807520635485, Val MAE: 1.2195535898208618\n",
      "Epoch 867/2000, Train Loss: 5.888316267992659, Val Loss: 4.708649543133073, Val MAE: 1.2197831869125366\n",
      "Epoch 868/2000, Train Loss: 5.886492771888847, Val Loss: 4.707513835087536, Val MAE: 1.2200168371200562\n",
      "Epoch 869/2000, Train Loss: 5.88440154980125, Val Loss: 4.7061955548940215, Val MAE: 1.220285177230835\n",
      "Epoch 870/2000, Train Loss: 5.88269910285075, Val Loss: 4.705097835546169, Val MAE: 1.2205058336257935\n",
      "Epoch 871/2000, Train Loss: 5.880956318032305, Val Loss: 4.704000689095165, Val MAE: 1.2207063436508179\n",
      "Epoch 872/2000, Train Loss: 5.87925109119133, Val Loss: 4.702817060012283, Val MAE: 1.2209285497665405\n",
      "Epoch 873/2000, Train Loss: 5.877410553415967, Val Loss: 4.701734308880849, Val MAE: 1.2211779356002808\n",
      "Epoch 874/2000, Train Loss: 5.875652679837381, Val Loss: 4.700620956061862, Val MAE: 1.2213799953460693\n",
      "Epoch 875/2000, Train Loss: 5.8738837030538065, Val Loss: 4.699533958574683, Val MAE: 1.2216119766235352\n",
      "Epoch 876/2000, Train Loss: 5.872164512101151, Val Loss: 4.69835406999419, Val MAE: 1.2218471765518188\n",
      "Epoch 877/2000, Train Loss: 5.870536773790336, Val Loss: 4.6974129531209865, Val MAE: 1.22207510471344\n",
      "Epoch 878/2000, Train Loss: 5.868844542424909, Val Loss: 4.696279506091996, Val MAE: 1.2222959995269775\n",
      "Epoch 879/2000, Train Loss: 5.867089972890701, Val Loss: 4.695141655523477, Val MAE: 1.222527265548706\n",
      "Epoch 880/2000, Train Loss: 5.865284597220625, Val Loss: 4.693985776644288, Val MAE: 1.2229230403900146\n",
      "Epoch 881/2000, Train Loss: 5.863530195875439, Val Loss: 4.692861661543762, Val MAE: 1.2231460809707642\n",
      "Epoch 882/2000, Train Loss: 5.861940992129221, Val Loss: 4.691911434640331, Val MAE: 1.2233246564865112\n",
      "Epoch 883/2000, Train Loss: 5.860295667395178, Val Loss: 4.690828168304183, Val MAE: 1.2235649824142456\n",
      "Epoch 884/2000, Train Loss: 5.858596713018062, Val Loss: 4.689654832990385, Val MAE: 1.22378408908844\n",
      "Epoch 885/2000, Train Loss: 5.856841142983094, Val Loss: 4.688637358460604, Val MAE: 1.2240031957626343\n",
      "Epoch 886/2000, Train Loss: 5.854845620964729, Val Loss: 4.687255486423575, Val MAE: 1.2242259979248047\n",
      "Epoch 887/2000, Train Loss: 5.853130960804151, Val Loss: 4.686184385448225, Val MAE: 1.2244452238082886\n",
      "Epoch 888/2000, Train Loss: 5.8514363547354895, Val Loss: 4.6850683481615825, Val MAE: 1.2246737480163574\n",
      "Epoch 889/2000, Train Loss: 5.849710260693545, Val Loss: 4.684048242022203, Val MAE: 1.2249435186386108\n",
      "Epoch 890/2000, Train Loss: 5.848105237999959, Val Loss: 4.683027988983186, Val MAE: 1.225163221359253\n",
      "Epoch 891/2000, Train Loss: 5.846503726005797, Val Loss: 4.682061782813682, Val MAE: 1.225345253944397\n",
      "Epoch 892/2000, Train Loss: 5.84490964513176, Val Loss: 4.6809813442955335, Val MAE: 1.225584864616394\n",
      "Epoch 893/2000, Train Loss: 5.84324590105619, Val Loss: 4.679969388340401, Val MAE: 1.2257901430130005\n",
      "Epoch 894/2000, Train Loss: 5.841662221880101, Val Loss: 4.678901008944812, Val MAE: 1.2260324954986572\n",
      "Epoch 895/2000, Train Loss: 5.83999319348277, Val Loss: 4.677893359442865, Val MAE: 1.2262589931488037\n",
      "Epoch 896/2000, Train Loss: 5.838128544097031, Val Loss: 4.676732038917739, Val MAE: 1.2265160083770752\n",
      "Epoch 897/2000, Train Loss: 5.836244478316288, Val Loss: 4.6753790218588405, Val MAE: 1.226817011833191\n",
      "Epoch 898/2000, Train Loss: 5.834348784776068, Val Loss: 4.674377912593873, Val MAE: 1.2270208597183228\n",
      "Epoch 899/2000, Train Loss: 5.832810949276389, Val Loss: 4.67347143473705, Val MAE: 1.2272558212280273\n",
      "Epoch 900/2000, Train Loss: 5.831210234757195, Val Loss: 4.6724006592816725, Val MAE: 1.227465271949768\n",
      "Epoch 901/2000, Train Loss: 5.829680300508518, Val Loss: 4.671452405215717, Val MAE: 1.2277878522872925\n",
      "Epoch 902/2000, Train Loss: 5.828193150834766, Val Loss: 4.670461539114554, Val MAE: 1.2279913425445557\n",
      "Epoch 903/2000, Train Loss: 5.826616476154715, Val Loss: 4.669519455472785, Val MAE: 1.2281917333602905\n",
      "Epoch 904/2000, Train Loss: 5.825038648525119, Val Loss: 4.6685323885048, Val MAE: 1.228434681892395\n",
      "Epoch 905/2000, Train Loss: 5.823422205658424, Val Loss: 4.667476129924922, Val MAE: 1.2286744117736816\n",
      "Epoch 906/2000, Train Loss: 5.821960372711295, Val Loss: 4.666619671763867, Val MAE: 1.2288641929626465\n",
      "Epoch 907/2000, Train Loss: 5.820533085710482, Val Loss: 4.665641404043033, Val MAE: 1.2291136980056763\n",
      "Epoch 908/2000, Train Loss: 5.819203759630791, Val Loss: 4.6648488731776165, Val MAE: 1.2292916774749756\n",
      "Epoch 909/2000, Train Loss: 5.817751526023834, Val Loss: 4.663852626026616, Val MAE: 1.2295247316360474\n",
      "Epoch 910/2000, Train Loss: 5.816052545524194, Val Loss: 4.662858378224251, Val MAE: 1.2297686338424683\n",
      "Epoch 911/2000, Train Loss: 5.814709827081481, Val Loss: 4.6620357575144356, Val MAE: 1.2299844026565552\n",
      "Epoch 912/2000, Train Loss: 5.813378035456063, Val Loss: 4.661202766429486, Val MAE: 1.2301710844039917\n",
      "Epoch 913/2000, Train Loss: 5.812090860778311, Val Loss: 4.660443512549786, Val MAE: 1.2303870916366577\n",
      "Epoch 914/2000, Train Loss: 5.810870646783423, Val Loss: 4.659595008423244, Val MAE: 1.2305974960327148\n",
      "Epoch 915/2000, Train Loss: 5.8094015403207155, Val Loss: 4.658655696843318, Val MAE: 1.2308439016342163\n",
      "Epoch 916/2000, Train Loss: 5.8080034340091125, Val Loss: 4.657802894641096, Val MAE: 1.2310621738433838\n",
      "Epoch 917/2000, Train Loss: 5.80663171647119, Val Loss: 4.65689823242623, Val MAE: 1.2313135862350464\n",
      "Epoch 918/2000, Train Loss: 5.805243475951527, Val Loss: 4.656068290822853, Val MAE: 1.2315163612365723\n",
      "Epoch 919/2000, Train Loss: 5.803863067331559, Val Loss: 4.65522179340753, Val MAE: 1.2317638397216797\n",
      "Epoch 920/2000, Train Loss: 5.80256443309978, Val Loss: 4.654327696520747, Val MAE: 1.2319989204406738\n",
      "Epoch 921/2000, Train Loss: 5.801115554063663, Val Loss: 4.653513394020439, Val MAE: 1.2322231531143188\n",
      "Epoch 922/2000, Train Loss: 5.799781476141154, Val Loss: 4.652667722217445, Val MAE: 1.2324271202087402\n",
      "Epoch 923/2000, Train Loss: 5.798444771196496, Val Loss: 4.65171899611321, Val MAE: 1.2326905727386475\n",
      "Epoch 924/2000, Train Loss: 5.7970220700937505, Val Loss: 4.6509725554252235, Val MAE: 1.2328884601593018\n",
      "Epoch 925/2000, Train Loss: 5.795719358405374, Val Loss: 4.650086224519127, Val MAE: 1.2331149578094482\n",
      "Epoch 926/2000, Train Loss: 5.794360872686637, Val Loss: 4.64928594648134, Val MAE: 1.2333203554153442\n",
      "Epoch 927/2000, Train Loss: 5.793148403979706, Val Loss: 4.648375585904037, Val MAE: 1.2335718870162964\n",
      "Epoch 928/2000, Train Loss: 5.791768871783563, Val Loss: 4.64767971153217, Val MAE: 1.2337530851364136\n",
      "Epoch 929/2000, Train Loss: 5.790458762014899, Val Loss: 4.6467562607658195, Val MAE: 1.2340539693832397\n",
      "Epoch 930/2000, Train Loss: 5.7888977893172, Val Loss: 4.645852971599092, Val MAE: 1.2342928647994995\n",
      "Epoch 931/2000, Train Loss: 5.787577812856864, Val Loss: 4.644949474127039, Val MAE: 1.234524130821228\n",
      "Epoch 932/2000, Train Loss: 5.7862308235785935, Val Loss: 4.644191517504885, Val MAE: 1.2347335815429688\n",
      "Epoch 933/2000, Train Loss: 5.785059388327631, Val Loss: 4.6432762514829165, Val MAE: 1.2349586486816406\n",
      "Epoch 934/2000, Train Loss: 5.7836741754226635, Val Loss: 4.642501964015285, Val MAE: 1.2351739406585693\n",
      "Epoch 935/2000, Train Loss: 5.782407791029, Val Loss: 4.641776342508127, Val MAE: 1.2353532314300537\n",
      "Epoch 936/2000, Train Loss: 5.781157557553077, Val Loss: 4.640979320027932, Val MAE: 1.2355711460113525\n",
      "Epoch 937/2000, Train Loss: 5.779917539313455, Val Loss: 4.640164853397786, Val MAE: 1.2358001470565796\n",
      "Epoch 938/2000, Train Loss: 5.778592976009829, Val Loss: 4.639344914533257, Val MAE: 1.236018180847168\n",
      "Epoch 939/2000, Train Loss: 5.777208182996131, Val Loss: 4.638461779767838, Val MAE: 1.2362728118896484\n",
      "Epoch 940/2000, Train Loss: 5.775950807082281, Val Loss: 4.63764156839979, Val MAE: 1.2364767789840698\n",
      "Epoch 941/2000, Train Loss: 5.774532361625816, Val Loss: 4.636779723281231, Val MAE: 1.2367039918899536\n",
      "Epoch 942/2000, Train Loss: 5.773277516280942, Val Loss: 4.636043432390126, Val MAE: 1.2369425296783447\n",
      "Epoch 943/2000, Train Loss: 5.77209064368477, Val Loss: 4.635188543538409, Val MAE: 1.23721182346344\n",
      "Epoch 944/2000, Train Loss: 5.770918358941253, Val Loss: 4.634578968091743, Val MAE: 1.2373639345169067\n",
      "Epoch 945/2000, Train Loss: 5.769844881897057, Val Loss: 4.6338566742720095, Val MAE: 1.2375799417495728\n",
      "Epoch 946/2000, Train Loss: 5.768565283378073, Val Loss: 4.6331128710543545, Val MAE: 1.2378020286560059\n",
      "Epoch 947/2000, Train Loss: 5.767264082734669, Val Loss: 4.6322177444856, Val MAE: 1.2380467653274536\n",
      "Epoch 948/2000, Train Loss: 5.766027501027328, Val Loss: 4.6314353363720455, Val MAE: 1.238240361213684\n",
      "Epoch 949/2000, Train Loss: 5.764645234215373, Val Loss: 4.630632712240295, Val MAE: 1.2384964227676392\n",
      "Epoch 950/2000, Train Loss: 5.763595836214327, Val Loss: 4.630033465105249, Val MAE: 1.2386436462402344\n",
      "Epoch 951/2000, Train Loss: 5.7625107681088075, Val Loss: 4.62928972129864, Val MAE: 1.2388944625854492\n",
      "Epoch 952/2000, Train Loss: 5.761373780900296, Val Loss: 4.628578678659332, Val MAE: 1.2390987873077393\n",
      "Epoch 953/2000, Train Loss: 5.760239515731584, Val Loss: 4.627859008435424, Val MAE: 1.2392882108688354\n",
      "Epoch 954/2000, Train Loss: 5.759002765774565, Val Loss: 4.627085951199447, Val MAE: 1.2395360469818115\n",
      "Epoch 955/2000, Train Loss: 5.757596005092967, Val Loss: 4.626257885769596, Val MAE: 1.2398216724395752\n",
      "Epoch 956/2000, Train Loss: 5.756413279912527, Val Loss: 4.625569262167835, Val MAE: 1.2400394678115845\n",
      "Epoch 957/2000, Train Loss: 5.755346330567648, Val Loss: 4.624859496804438, Val MAE: 1.2404797077178955\n",
      "Epoch 958/2000, Train Loss: 5.754262114739806, Val Loss: 4.624123901087703, Val MAE: 1.240681529045105\n",
      "Epoch 959/2000, Train Loss: 5.75317781926556, Val Loss: 4.6234913439379905, Val MAE: 1.2408509254455566\n",
      "Epoch 960/2000, Train Loss: 5.752114268947327, Val Loss: 4.622753059910977, Val MAE: 1.2410557270050049\n",
      "Epoch 961/2000, Train Loss: 5.751062427802843, Val Loss: 4.622138481311441, Val MAE: 1.2412426471710205\n",
      "Epoch 962/2000, Train Loss: 5.749937913877185, Val Loss: 4.621394387837939, Val MAE: 1.2414636611938477\n",
      "Epoch 963/2000, Train Loss: 5.748864895938371, Val Loss: 4.620746754479455, Val MAE: 1.2416608333587646\n",
      "Epoch 964/2000, Train Loss: 5.7477814175865864, Val Loss: 4.620052561844428, Val MAE: 1.2419047355651855\n",
      "Epoch 965/2000, Train Loss: 5.746756855797735, Val Loss: 4.619410071832927, Val MAE: 1.2420886754989624\n",
      "Epoch 966/2000, Train Loss: 5.745768213854393, Val Loss: 4.618815167588512, Val MAE: 1.2422384023666382\n",
      "Epoch 967/2000, Train Loss: 5.7447938847897655, Val Loss: 4.6181204381127525, Val MAE: 1.242449164390564\n",
      "Epoch 968/2000, Train Loss: 5.74359630017947, Val Loss: 4.617467915125954, Val MAE: 1.2426546812057495\n",
      "Epoch 969/2000, Train Loss: 5.742669444556475, Val Loss: 4.616866301827309, Val MAE: 1.2428416013717651\n",
      "Epoch 970/2000, Train Loss: 5.741549965277254, Val Loss: 4.616089364340691, Val MAE: 1.2431349754333496\n",
      "Epoch 971/2000, Train Loss: 5.740475111245462, Val Loss: 4.615438675287906, Val MAE: 1.2433269023895264\n",
      "Epoch 972/2000, Train Loss: 5.739422849916538, Val Loss: 4.614804997289275, Val MAE: 1.2435388565063477\n",
      "Epoch 973/2000, Train Loss: 5.738367005636767, Val Loss: 4.614154238019168, Val MAE: 1.2436918020248413\n",
      "Epoch 974/2000, Train Loss: 5.737294233799628, Val Loss: 4.613438209392658, Val MAE: 1.243924617767334\n",
      "Epoch 975/2000, Train Loss: 5.736282586409449, Val Loss: 4.612779950373989, Val MAE: 1.2441349029541016\n",
      "Epoch 976/2000, Train Loss: 5.735233032997582, Val Loss: 4.612165154833494, Val MAE: 1.2443130016326904\n",
      "Epoch 977/2000, Train Loss: 5.7342458075390095, Val Loss: 4.611524599602842, Val MAE: 1.2445249557495117\n",
      "Epoch 978/2000, Train Loss: 5.733285741412979, Val Loss: 4.610947508359049, Val MAE: 1.244737982749939\n",
      "Epoch 979/2000, Train Loss: 5.73227261557791, Val Loss: 4.610301172569042, Val MAE: 1.2449522018432617\n",
      "Epoch 980/2000, Train Loss: 5.731236545194643, Val Loss: 4.609695957812268, Val MAE: 1.2451410293579102\n",
      "Epoch 981/2000, Train Loss: 5.730269460778385, Val Loss: 4.609019373615427, Val MAE: 1.2453538179397583\n",
      "Epoch 982/2000, Train Loss: 5.729239454748187, Val Loss: 4.608406316213251, Val MAE: 1.245530128479004\n",
      "Epoch 983/2000, Train Loss: 5.728252092565901, Val Loss: 4.60779748852328, Val MAE: 1.2458126544952393\n",
      "Epoch 984/2000, Train Loss: 5.727155761022419, Val Loss: 4.607192533668571, Val MAE: 1.2461720705032349\n",
      "Epoch 985/2000, Train Loss: 5.726217105236597, Val Loss: 4.606555857538708, Val MAE: 1.2463651895523071\n",
      "Epoch 986/2000, Train Loss: 5.725312752833683, Val Loss: 4.606031121020242, Val MAE: 1.246503233909607\n",
      "Epoch 987/2000, Train Loss: 5.724431291525852, Val Loss: 4.605506222806578, Val MAE: 1.2466909885406494\n",
      "Epoch 988/2000, Train Loss: 5.723472983413345, Val Loss: 4.604802458997317, Val MAE: 1.2469063997268677\n",
      "Epoch 989/2000, Train Loss: 5.722374542112105, Val Loss: 4.6041693370055965, Val MAE: 1.2472165822982788\n",
      "Epoch 990/2000, Train Loss: 5.7213042904919815, Val Loss: 4.603479644421517, Val MAE: 1.2474660873413086\n",
      "Epoch 991/2000, Train Loss: 5.720167327283034, Val Loss: 4.602861276069494, Val MAE: 1.247649908065796\n",
      "Epoch 992/2000, Train Loss: 5.719269568755151, Val Loss: 4.602233965342909, Val MAE: 1.2478874921798706\n",
      "Epoch 993/2000, Train Loss: 5.718104671105601, Val Loss: 4.601572605181397, Val MAE: 1.248084306716919\n",
      "Epoch 994/2000, Train Loss: 5.7170852260156115, Val Loss: 4.601019409615693, Val MAE: 1.2482396364212036\n",
      "Epoch 995/2000, Train Loss: 5.71618979595086, Val Loss: 4.60039686862174, Val MAE: 1.248462438583374\n",
      "Epoch 996/2000, Train Loss: 5.715133747219878, Val Loss: 4.599818300285677, Val MAE: 1.2486618757247925\n",
      "Epoch 997/2000, Train Loss: 5.714140688224596, Val Loss: 4.599239990267697, Val MAE: 1.2488430738449097\n",
      "Epoch 998/2000, Train Loss: 5.713335628263662, Val Loss: 4.598682602547754, Val MAE: 1.2490099668502808\n",
      "Epoch 999/2000, Train Loss: 5.712467886537839, Val Loss: 4.598148624681112, Val MAE: 1.249191403388977\n",
      "Epoch 1000/2000, Train Loss: 5.711661981679044, Val Loss: 4.597475996801234, Val MAE: 1.2493220567703247\n",
      "Epoch 1001/2000, Train Loss: 5.710749028333977, Val Loss: 4.597001002118812, Val MAE: 1.249505877494812\n",
      "Epoch 1002/2000, Train Loss: 5.709858555968451, Val Loss: 4.59644807594733, Val MAE: 1.2496955394744873\n",
      "Epoch 1003/2000, Train Loss: 5.709085195973576, Val Loss: 4.595949134477011, Val MAE: 1.24986732006073\n",
      "Epoch 1004/2000, Train Loss: 5.708203850154799, Val Loss: 4.595364843743054, Val MAE: 1.2500736713409424\n",
      "Epoch 1005/2000, Train Loss: 5.707184378780536, Val Loss: 4.5947514147035715, Val MAE: 1.2502800226211548\n",
      "Epoch 1006/2000, Train Loss: 5.706093794616947, Val Loss: 4.59411173728038, Val MAE: 1.25053071975708\n",
      "Epoch 1007/2000, Train Loss: 5.705183179581505, Val Loss: 4.593520010749655, Val MAE: 1.2507445812225342\n",
      "Epoch 1008/2000, Train Loss: 5.704441716260146, Val Loss: 4.59308539531597, Val MAE: 1.2508728504180908\n",
      "Epoch 1009/2000, Train Loss: 5.703550364625017, Val Loss: 4.592546640043183, Val MAE: 1.2510727643966675\n",
      "Epoch 1010/2000, Train Loss: 5.702684947561215, Val Loss: 4.592045294945165, Val MAE: 1.2512767314910889\n",
      "Epoch 1011/2000, Train Loss: 5.701953331294142, Val Loss: 4.591621085848865, Val MAE: 1.2514398097991943\n",
      "Epoch 1012/2000, Train Loss: 5.701201931289125, Val Loss: 4.591124928818913, Val MAE: 1.2516111135482788\n",
      "Epoch 1013/2000, Train Loss: 5.70042755187134, Val Loss: 4.590657993787386, Val MAE: 1.2517915964126587\n",
      "Epoch 1014/2000, Train Loss: 5.699679478861422, Val Loss: 4.590161036295215, Val MAE: 1.2519696950912476\n",
      "Epoch 1015/2000, Train Loss: 5.698871711539446, Val Loss: 4.58971267953867, Val MAE: 1.2521342039108276\n",
      "Epoch 1016/2000, Train Loss: 5.698165477939668, Val Loss: 4.5891880040445665, Val MAE: 1.2523165941238403\n",
      "Epoch 1017/2000, Train Loss: 5.697373467918329, Val Loss: 4.588715007244133, Val MAE: 1.2524702548980713\n",
      "Epoch 1018/2000, Train Loss: 5.696568719324943, Val Loss: 4.588246008260982, Val MAE: 1.2526826858520508\n",
      "Epoch 1019/2000, Train Loss: 5.695782504864593, Val Loss: 4.587743827360352, Val MAE: 1.252838373184204\n",
      "Epoch 1020/2000, Train Loss: 5.694997058471151, Val Loss: 4.587282589688076, Val MAE: 1.252985954284668\n",
      "Epoch 1021/2000, Train Loss: 5.694266715833824, Val Loss: 4.58681250457454, Val MAE: 1.2531616687774658\n",
      "Epoch 1022/2000, Train Loss: 5.693487308403854, Val Loss: 4.586325025945667, Val MAE: 1.2533468008041382\n",
      "Epoch 1023/2000, Train Loss: 5.692801280482956, Val Loss: 4.585860424518116, Val MAE: 1.2534544467926025\n",
      "Epoch 1024/2000, Train Loss: 5.692058518670469, Val Loss: 4.585407035510371, Val MAE: 1.2536202669143677\n",
      "Epoch 1025/2000, Train Loss: 5.691325758011403, Val Loss: 4.584914345968896, Val MAE: 1.2538118362426758\n",
      "Epoch 1026/2000, Train Loss: 5.690572816287161, Val Loss: 4.5844741590262394, Val MAE: 1.253961205482483\n",
      "Epoch 1027/2000, Train Loss: 5.689645434978697, Val Loss: 4.583861533042014, Val MAE: 1.2542479038238525\n",
      "Epoch 1028/2000, Train Loss: 5.6888355827072745, Val Loss: 4.583439360218724, Val MAE: 1.2543872594833374\n",
      "Epoch 1029/2000, Train Loss: 5.687997266195279, Val Loss: 4.582873966926195, Val MAE: 1.2546229362487793\n",
      "Epoch 1030/2000, Train Loss: 5.687156883395673, Val Loss: 4.582416267763442, Val MAE: 1.2547857761383057\n",
      "Epoch 1031/2000, Train Loss: 5.686449153775923, Val Loss: 4.5819965328170555, Val MAE: 1.2549619674682617\n",
      "Epoch 1032/2000, Train Loss: 5.685628567168962, Val Loss: 4.581476204707397, Val MAE: 1.2551449537277222\n",
      "Epoch 1033/2000, Train Loss: 5.684918233465146, Val Loss: 4.581016876613061, Val MAE: 1.2553592920303345\n",
      "Epoch 1034/2000, Train Loss: 5.68412314923518, Val Loss: 4.580590290042359, Val MAE: 1.2555207014083862\n",
      "Epoch 1035/2000, Train Loss: 5.683464631296725, Val Loss: 4.5801802254333275, Val MAE: 1.255670189857483\n",
      "Epoch 1036/2000, Train Loss: 5.682690245163813, Val Loss: 4.579686214722048, Val MAE: 1.255875825881958\n",
      "Epoch 1037/2000, Train Loss: 5.6820070999124805, Val Loss: 4.579216533136649, Val MAE: 1.256033182144165\n",
      "Epoch 1038/2000, Train Loss: 5.681365776235892, Val Loss: 4.578797924201789, Val MAE: 1.2561945915222168\n",
      "Epoch 1039/2000, Train Loss: 5.680658238551995, Val Loss: 4.578346083905753, Val MAE: 1.2563835382461548\n",
      "Epoch 1040/2000, Train Loss: 5.6799481429432594, Val Loss: 4.577904594871472, Val MAE: 1.2565101385116577\n",
      "Epoch 1041/2000, Train Loss: 5.6793166203285335, Val Loss: 4.577476869863788, Val MAE: 1.256703495979309\n",
      "Epoch 1042/2000, Train Loss: 5.678453709086052, Val Loss: 4.576939950578326, Val MAE: 1.2568762302398682\n",
      "Epoch 1043/2000, Train Loss: 5.677696205574509, Val Loss: 4.576495237296491, Val MAE: 1.257046103477478\n",
      "Epoch 1044/2000, Train Loss: 5.6769841898086115, Val Loss: 4.576060685750067, Val MAE: 1.2572070360183716\n",
      "Epoch 1045/2000, Train Loss: 5.6762389833034295, Val Loss: 4.575564632664515, Val MAE: 1.2574118375778198\n",
      "Epoch 1046/2000, Train Loss: 5.67547220157477, Val Loss: 4.575144539104672, Val MAE: 1.2575733661651611\n",
      "Epoch 1047/2000, Train Loss: 5.674741692750069, Val Loss: 4.574689839651265, Val MAE: 1.2577803134918213\n",
      "Epoch 1048/2000, Train Loss: 5.674043148149467, Val Loss: 4.574248987753091, Val MAE: 1.2579408884048462\n",
      "Epoch 1049/2000, Train Loss: 5.673372716399029, Val Loss: 4.573788373958408, Val MAE: 1.2581225633621216\n",
      "Epoch 1050/2000, Train Loss: 5.672420289493804, Val Loss: 4.573155856402371, Val MAE: 1.2583216428756714\n",
      "Epoch 1051/2000, Train Loss: 5.671509260582147, Val Loss: 4.57262981525791, Val MAE: 1.2585780620574951\n",
      "Epoch 1052/2000, Train Loss: 5.670807909164855, Val Loss: 4.572183682540739, Val MAE: 1.258748173713684\n",
      "Epoch 1053/2000, Train Loss: 5.670098116108326, Val Loss: 4.571808876155869, Val MAE: 1.2588696479797363\n",
      "Epoch 1054/2000, Train Loss: 5.669421373261202, Val Loss: 4.571358796379228, Val MAE: 1.2590538263320923\n",
      "Epoch 1055/2000, Train Loss: 5.66872489727336, Val Loss: 4.570915093759852, Val MAE: 1.2592686414718628\n",
      "Epoch 1056/2000, Train Loss: 5.668008117856941, Val Loss: 4.570445351419956, Val MAE: 1.259473204612732\n",
      "Epoch 1057/2000, Train Loss: 5.667200344711987, Val Loss: 4.570005913102252, Val MAE: 1.2596666812896729\n",
      "Epoch 1058/2000, Train Loss: 5.666491012560302, Val Loss: 4.569523513105911, Val MAE: 1.2599403858184814\n",
      "Epoch 1059/2000, Train Loss: 5.665915197077406, Val Loss: 4.569089973582996, Val MAE: 1.2601003646850586\n",
      "Epoch 1060/2000, Train Loss: 5.665181642759647, Val Loss: 4.5686877550571925, Val MAE: 1.2602697610855103\n",
      "Epoch 1061/2000, Train Loss: 5.664514076111274, Val Loss: 4.568287459344376, Val MAE: 1.2604093551635742\n",
      "Epoch 1062/2000, Train Loss: 5.6638617295787, Val Loss: 4.56782982328276, Val MAE: 1.2605750560760498\n",
      "Epoch 1063/2000, Train Loss: 5.6631814299032515, Val Loss: 4.5674355498214405, Val MAE: 1.2607289552688599\n",
      "Epoch 1064/2000, Train Loss: 5.6625256117842735, Val Loss: 4.56701620276284, Val MAE: 1.260877013206482\n",
      "Epoch 1065/2000, Train Loss: 5.661778214262493, Val Loss: 4.56666929742952, Val MAE: 1.2610920667648315\n",
      "Epoch 1066/2000, Train Loss: 5.6611113844280165, Val Loss: 4.56623261297547, Val MAE: 1.2612816095352173\n",
      "Epoch 1067/2000, Train Loss: 5.660427186026502, Val Loss: 4.565778496227865, Val MAE: 1.261489748954773\n",
      "Epoch 1068/2000, Train Loss: 5.659854720696665, Val Loss: 4.5654495589611095, Val MAE: 1.2615824937820435\n",
      "Epoch 1069/2000, Train Loss: 5.659159277155085, Val Loss: 4.564879412514957, Val MAE: 1.2617378234863281\n",
      "Epoch 1070/2000, Train Loss: 5.6584829443021905, Val Loss: 4.564529438410688, Val MAE: 1.2618308067321777\n",
      "Epoch 1071/2000, Train Loss: 5.657818205482762, Val Loss: 4.564107566790318, Val MAE: 1.2620278596878052\n",
      "Epoch 1072/2000, Train Loss: 5.657097371333005, Val Loss: 4.5637028071472026, Val MAE: 1.2622355222702026\n",
      "Epoch 1073/2000, Train Loss: 5.65633637352949, Val Loss: 4.563248195505048, Val MAE: 1.2624101638793945\n",
      "Epoch 1074/2000, Train Loss: 5.655665610031082, Val Loss: 4.56283759157958, Val MAE: 1.2625610828399658\n",
      "Epoch 1075/2000, Train Loss: 5.655036056199423, Val Loss: 4.562472884814571, Val MAE: 1.2626808881759644\n",
      "Epoch 1076/2000, Train Loss: 5.654440232824276, Val Loss: 4.562079980065973, Val MAE: 1.2628443241119385\n",
      "Epoch 1077/2000, Train Loss: 5.65384909320557, Val Loss: 4.561722861957832, Val MAE: 1.263038992881775\n",
      "Epoch 1078/2000, Train Loss: 5.653203690229957, Val Loss: 4.561332217656721, Val MAE: 1.2631758451461792\n",
      "Epoch 1079/2000, Train Loss: 5.652589590934433, Val Loss: 4.560957794454623, Val MAE: 1.2633428573608398\n",
      "Epoch 1080/2000, Train Loss: 5.651687976938333, Val Loss: 4.560408957865764, Val MAE: 1.263575792312622\n",
      "Epoch 1081/2000, Train Loss: 5.651065699442756, Val Loss: 4.560045390335594, Val MAE: 1.2637193202972412\n",
      "Epoch 1082/2000, Train Loss: 5.650439521270012, Val Loss: 4.559682245317876, Val MAE: 1.263831615447998\n",
      "Epoch 1083/2000, Train Loss: 5.649855594783819, Val Loss: 4.559304704816323, Val MAE: 1.2639853954315186\n",
      "Epoch 1084/2000, Train Loss: 5.64925132710057, Val Loss: 4.558906542022867, Val MAE: 1.2641382217407227\n",
      "Epoch 1085/2000, Train Loss: 5.648483281543167, Val Loss: 4.5585152696084785, Val MAE: 1.264379620552063\n",
      "Epoch 1086/2000, Train Loss: 5.647805380530079, Val Loss: 4.558095899752275, Val MAE: 1.2645286321640015\n",
      "Epoch 1087/2000, Train Loss: 5.647188845228308, Val Loss: 4.557726480383573, Val MAE: 1.2646853923797607\n",
      "Epoch 1088/2000, Train Loss: 5.6465873770021515, Val Loss: 4.557350860336634, Val MAE: 1.2648119926452637\n",
      "Epoch 1089/2000, Train Loss: 5.645905097918724, Val Loss: 4.556888984214133, Val MAE: 1.2649612426757812\n",
      "Epoch 1090/2000, Train Loss: 5.645249961997955, Val Loss: 4.556527196621801, Val MAE: 1.2651052474975586\n",
      "Epoch 1091/2000, Train Loss: 5.64463791077302, Val Loss: 4.556216139680758, Val MAE: 1.2653019428253174\n",
      "Epoch 1092/2000, Train Loss: 5.644010398804818, Val Loss: 4.555848453281902, Val MAE: 1.2654340267181396\n",
      "Epoch 1093/2000, Train Loss: 5.643459700179391, Val Loss: 4.555472991116873, Val MAE: 1.265560269355774\n",
      "Epoch 1094/2000, Train Loss: 5.642895091322517, Val Loss: 4.555135829089664, Val MAE: 1.2656757831573486\n",
      "Epoch 1095/2000, Train Loss: 5.642327195662737, Val Loss: 4.554768492105439, Val MAE: 1.2657968997955322\n",
      "Epoch 1096/2000, Train Loss: 5.641780972319024, Val Loss: 4.554402419643139, Val MAE: 1.2659430503845215\n",
      "Epoch 1097/2000, Train Loss: 5.641086158022952, Val Loss: 4.553939339647612, Val MAE: 1.2661032676696777\n",
      "Epoch 1098/2000, Train Loss: 5.6404913587841925, Val Loss: 4.553602338142283, Val MAE: 1.2662345170974731\n",
      "Epoch 1099/2000, Train Loss: 5.639850196877199, Val Loss: 4.553254136454871, Val MAE: 1.266481637954712\n",
      "Epoch 1100/2000, Train Loss: 5.639249749825395, Val Loss: 4.552932000277549, Val MAE: 1.2666223049163818\n",
      "Epoch 1101/2000, Train Loss: 5.6386825815146455, Val Loss: 4.552553948853898, Val MAE: 1.2667030096054077\n",
      "Epoch 1102/2000, Train Loss: 5.638121575804546, Val Loss: 4.552120594797641, Val MAE: 1.26688814163208\n",
      "Epoch 1103/2000, Train Loss: 5.637520491399548, Val Loss: 4.551795755127284, Val MAE: 1.2669482231140137\n",
      "Epoch 1104/2000, Train Loss: 5.636960604782829, Val Loss: 4.551418151501126, Val MAE: 1.267053484916687\n",
      "Epoch 1105/2000, Train Loss: 5.636356613205764, Val Loss: 4.551037535822298, Val MAE: 1.2671897411346436\n",
      "Epoch 1106/2000, Train Loss: 5.635729797497857, Val Loss: 4.550645566479427, Val MAE: 1.2673742771148682\n",
      "Epoch 1107/2000, Train Loss: 5.635116789606242, Val Loss: 4.550236353501091, Val MAE: 1.2675542831420898\n",
      "Epoch 1108/2000, Train Loss: 5.634510377894586, Val Loss: 4.549876876902862, Val MAE: 1.267656922340393\n",
      "Epoch 1109/2000, Train Loss: 5.633927046363033, Val Loss: 4.549498963168287, Val MAE: 1.267786979675293\n",
      "Epoch 1110/2000, Train Loss: 5.633230513439411, Val Loss: 4.549043949394245, Val MAE: 1.2679554224014282\n",
      "Epoch 1111/2000, Train Loss: 5.632638933862404, Val Loss: 4.548682159302741, Val MAE: 1.2680556774139404\n",
      "Epoch 1112/2000, Train Loss: 5.631973121350623, Val Loss: 4.548315509355913, Val MAE: 1.26826810836792\n",
      "Epoch 1113/2000, Train Loss: 5.631360854999019, Val Loss: 4.547921440850093, Val MAE: 1.2684191465377808\n",
      "Epoch 1114/2000, Train Loss: 5.630781517928944, Val Loss: 4.547530649625879, Val MAE: 1.2685739994049072\n",
      "Epoch 1115/2000, Train Loss: 5.6302107878261864, Val Loss: 4.547149806874474, Val MAE: 1.2686680555343628\n",
      "Epoch 1116/2000, Train Loss: 5.629660817305463, Val Loss: 4.546803149393225, Val MAE: 1.2688075304031372\n",
      "Epoch 1117/2000, Train Loss: 5.629050432928058, Val Loss: 4.546394267173733, Val MAE: 1.2688578367233276\n",
      "Epoch 1118/2000, Train Loss: 5.628498505057927, Val Loss: 4.54607199171162, Val MAE: 1.2689754962921143\n",
      "Epoch 1119/2000, Train Loss: 5.627937490289861, Val Loss: 4.545673165769558, Val MAE: 1.269127607345581\n",
      "Epoch 1120/2000, Train Loss: 5.627274688742701, Val Loss: 4.545248370447497, Val MAE: 1.2692575454711914\n",
      "Epoch 1121/2000, Train Loss: 5.626699301410401, Val Loss: 4.544922045342566, Val MAE: 1.269376277923584\n",
      "Epoch 1122/2000, Train Loss: 5.626070371961658, Val Loss: 4.544534446956135, Val MAE: 1.2696048021316528\n",
      "Epoch 1123/2000, Train Loss: 5.625532121153667, Val Loss: 4.544192649083813, Val MAE: 1.2696762084960938\n",
      "Epoch 1124/2000, Train Loss: 5.62494385533611, Val Loss: 4.543818346510722, Val MAE: 1.2698544263839722\n",
      "Epoch 1125/2000, Train Loss: 5.624360489432815, Val Loss: 4.543473894354396, Val MAE: 1.2699933052062988\n",
      "Epoch 1126/2000, Train Loss: 5.623761900406194, Val Loss: 4.543083955863799, Val MAE: 1.2700947523117065\n",
      "Epoch 1127/2000, Train Loss: 5.623254980846451, Val Loss: 4.542703253007311, Val MAE: 1.2701358795166016\n",
      "Epoch 1128/2000, Train Loss: 5.622698910039024, Val Loss: 4.542370415531744, Val MAE: 1.2702332735061646\n",
      "Epoch 1129/2000, Train Loss: 5.622146890315466, Val Loss: 4.5419539914708436, Val MAE: 1.2704559564590454\n",
      "Epoch 1130/2000, Train Loss: 5.621544821776722, Val Loss: 4.541612597009329, Val MAE: 1.2705445289611816\n",
      "Epoch 1131/2000, Train Loss: 5.6210095527173705, Val Loss: 4.541236209270991, Val MAE: 1.2707421779632568\n",
      "Epoch 1132/2000, Train Loss: 5.620384635128273, Val Loss: 4.5408945355359025, Val MAE: 1.2707626819610596\n",
      "Epoch 1133/2000, Train Loss: 5.61980488940851, Val Loss: 4.5405116329864255, Val MAE: 1.2709506750106812\n",
      "Epoch 1134/2000, Train Loss: 5.619222423600051, Val Loss: 4.540160801537394, Val MAE: 1.2710566520690918\n",
      "Epoch 1135/2000, Train Loss: 5.618742207982647, Val Loss: 4.539821813906741, Val MAE: 1.2711347341537476\n",
      "Epoch 1136/2000, Train Loss: 5.61819620468885, Val Loss: 4.5394430676313835, Val MAE: 1.2712244987487793\n",
      "Epoch 1137/2000, Train Loss: 5.617673663384555, Val Loss: 4.539076871160916, Val MAE: 1.2713382244110107\n",
      "Epoch 1138/2000, Train Loss: 5.617085220031895, Val Loss: 4.538729373594438, Val MAE: 1.2714850902557373\n",
      "Epoch 1139/2000, Train Loss: 5.616552940379004, Val Loss: 4.538356853786885, Val MAE: 1.2715791463851929\n",
      "Epoch 1140/2000, Train Loss: 5.615957470729523, Val Loss: 4.538020574448146, Val MAE: 1.2716978788375854\n",
      "Epoch 1141/2000, Train Loss: 5.615472571011961, Val Loss: 4.537625370708507, Val MAE: 1.2717775106430054\n",
      "Epoch 1142/2000, Train Loss: 5.61472170580598, Val Loss: 4.5372196051548785, Val MAE: 1.2719402313232422\n",
      "Epoch 1143/2000, Train Loss: 5.614168361471296, Val Loss: 4.5368644000038385, Val MAE: 1.2720398902893066\n",
      "Epoch 1144/2000, Train Loss: 5.6135906879986, Val Loss: 4.536510054767132, Val MAE: 1.2721481323242188\n",
      "Epoch 1145/2000, Train Loss: 5.613039543379599, Val Loss: 4.53616251943149, Val MAE: 1.2722835540771484\n",
      "Epoch 1146/2000, Train Loss: 5.612472299319594, Val Loss: 4.5358074180605845, Val MAE: 1.2723875045776367\n",
      "Epoch 1147/2000, Train Loss: 5.6119303612728455, Val Loss: 4.53540257094648, Val MAE: 1.2724958658218384\n",
      "Epoch 1148/2000, Train Loss: 5.611315782562283, Val Loss: 4.535090335276653, Val MAE: 1.272606611251831\n",
      "Epoch 1149/2000, Train Loss: 5.610775998036606, Val Loss: 4.534730157291326, Val MAE: 1.2727138996124268\n",
      "Epoch 1150/2000, Train Loss: 5.61026323181318, Val Loss: 4.5343627125846115, Val MAE: 1.2728149890899658\n",
      "Epoch 1151/2000, Train Loss: 5.609726953700569, Val Loss: 4.53403468025247, Val MAE: 1.2729604244232178\n",
      "Epoch 1152/2000, Train Loss: 5.609083538842128, Val Loss: 4.533576369367716, Val MAE: 1.2731550931930542\n",
      "Epoch 1153/2000, Train Loss: 5.608477519863011, Val Loss: 4.5332125317862655, Val MAE: 1.2732080221176147\n",
      "Epoch 1154/2000, Train Loss: 5.607904634823874, Val Loss: 4.532865033879524, Val MAE: 1.2733415365219116\n",
      "Epoch 1155/2000, Train Loss: 5.607338840958222, Val Loss: 4.532561153525442, Val MAE: 1.2734228372573853\n",
      "Epoch 1156/2000, Train Loss: 5.60682508602152, Val Loss: 4.5321879806715675, Val MAE: 1.2735322713851929\n",
      "Epoch 1157/2000, Train Loss: 5.606248156833519, Val Loss: 4.531859019926683, Val MAE: 1.2736170291900635\n",
      "Epoch 1158/2000, Train Loss: 5.605598076834284, Val Loss: 4.531385370244191, Val MAE: 1.2737985849380493\n",
      "Epoch 1159/2000, Train Loss: 5.605048010760117, Val Loss: 4.531065263147429, Val MAE: 1.2738962173461914\n",
      "Epoch 1160/2000, Train Loss: 5.604492353778357, Val Loss: 4.530737568062591, Val MAE: 1.2739920616149902\n",
      "Epoch 1161/2000, Train Loss: 5.603979765479244, Val Loss: 4.530386086019475, Val MAE: 1.2741197347640991\n",
      "Epoch 1162/2000, Train Loss: 5.603448308792011, Val Loss: 4.530004850581405, Val MAE: 1.274154782295227\n",
      "Epoch 1163/2000, Train Loss: 5.602840468550311, Val Loss: 4.529653513853944, Val MAE: 1.2743773460388184\n",
      "Epoch 1164/2000, Train Loss: 5.602357914408459, Val Loss: 4.5293428646415235, Val MAE: 1.2744715213775635\n",
      "Epoch 1165/2000, Train Loss: 5.601839506682741, Val Loss: 4.529015373749527, Val MAE: 1.2745472192764282\n",
      "Epoch 1166/2000, Train Loss: 5.601284187312365, Val Loss: 4.52866805319007, Val MAE: 1.2746901512145996\n",
      "Epoch 1167/2000, Train Loss: 5.600757399601723, Val Loss: 4.52834492308183, Val MAE: 1.274819254875183\n",
      "Epoch 1168/2000, Train Loss: 5.600068329147277, Val Loss: 4.5279278947847095, Val MAE: 1.2750053405761719\n",
      "Epoch 1169/2000, Train Loss: 5.599428606137451, Val Loss: 4.527770422293445, Val MAE: 1.2752127647399902\n",
      "Epoch 1170/2000, Train Loss: 5.5988971692089144, Val Loss: 4.527437202749759, Val MAE: 1.2753310203552246\n",
      "Epoch 1171/2000, Train Loss: 5.598418222838858, Val Loss: 4.5271240775744745, Val MAE: 1.2753925323486328\n",
      "Epoch 1172/2000, Train Loss: 5.597945539404612, Val Loss: 4.526818580995863, Val MAE: 1.27544367313385\n",
      "Epoch 1173/2000, Train Loss: 5.59747668437589, Val Loss: 4.5265178200297465, Val MAE: 1.2755610942840576\n",
      "Epoch 1174/2000, Train Loss: 5.596950780746894, Val Loss: 4.526210695704607, Val MAE: 1.2756582498550415\n",
      "Epoch 1175/2000, Train Loss: 5.596484521047555, Val Loss: 4.525841389971925, Val MAE: 1.2757869958877563\n",
      "Epoch 1176/2000, Train Loss: 5.595923149116651, Val Loss: 4.525514221473003, Val MAE: 1.2758287191390991\n",
      "Epoch 1177/2000, Train Loss: 5.595446392555129, Val Loss: 4.525194702819577, Val MAE: 1.2759456634521484\n",
      "Epoch 1178/2000, Train Loss: 5.594942688133209, Val Loss: 4.524869464607689, Val MAE: 1.2760435342788696\n",
      "Epoch 1179/2000, Train Loss: 5.594421236291831, Val Loss: 4.524563288371863, Val MAE: 1.2762162685394287\n",
      "Epoch 1180/2000, Train Loss: 5.593926564081391, Val Loss: 4.524235899500021, Val MAE: 1.2762633562088013\n",
      "Epoch 1181/2000, Train Loss: 5.593433867558695, Val Loss: 4.523912339745544, Val MAE: 1.2763630151748657\n",
      "Epoch 1182/2000, Train Loss: 5.5929477366857645, Val Loss: 4.523583369391171, Val MAE: 1.2764289379119873\n",
      "Epoch 1183/2000, Train Loss: 5.592437586273235, Val Loss: 4.523231884121426, Val MAE: 1.2765778303146362\n",
      "Epoch 1184/2000, Train Loss: 5.591847957717191, Val Loss: 4.522958367312048, Val MAE: 1.2767114639282227\n",
      "Epoch 1185/2000, Train Loss: 5.591320048345397, Val Loss: 4.5226033849861675, Val MAE: 1.27683687210083\n",
      "Epoch 1186/2000, Train Loss: 5.590801484872722, Val Loss: 4.522322735988249, Val MAE: 1.2769358158111572\n",
      "Epoch 1187/2000, Train Loss: 5.590295692635359, Val Loss: 4.521967311837073, Val MAE: 1.2769790887832642\n",
      "Epoch 1188/2000, Train Loss: 5.589801566329061, Val Loss: 4.521657934104364, Val MAE: 1.277071237564087\n",
      "Epoch 1189/2000, Train Loss: 5.589362085593442, Val Loss: 4.5213365246226465, Val MAE: 1.277168869972229\n",
      "Epoch 1190/2000, Train Loss: 5.588790222780029, Val Loss: 4.5209975332139045, Val MAE: 1.2772825956344604\n",
      "Epoch 1191/2000, Train Loss: 5.588308502569936, Val Loss: 4.520682705023627, Val MAE: 1.27733314037323\n",
      "Epoch 1192/2000, Train Loss: 5.587704819974615, Val Loss: 4.520306546927437, Val MAE: 1.2774286270141602\n",
      "Epoch 1193/2000, Train Loss: 5.587232691704661, Val Loss: 4.5199865955771426, Val MAE: 1.2775784730911255\n",
      "Epoch 1194/2000, Train Loss: 5.58666312063727, Val Loss: 4.519681159236769, Val MAE: 1.2776747941970825\n",
      "Epoch 1195/2000, Train Loss: 5.586102629290184, Val Loss: 4.519343038431303, Val MAE: 1.2778364419937134\n",
      "Epoch 1196/2000, Train Loss: 5.585484220667804, Val Loss: 4.519002396642692, Val MAE: 1.2779264450073242\n",
      "Epoch 1197/2000, Train Loss: 5.5849754926792805, Val Loss: 4.518695503414616, Val MAE: 1.2779817581176758\n",
      "Epoch 1198/2000, Train Loss: 5.584497098091944, Val Loss: 4.518370815700902, Val MAE: 1.2780818939208984\n",
      "Epoch 1199/2000, Train Loss: 5.5840002894603735, Val Loss: 4.51803564879838, Val MAE: 1.2781569957733154\n",
      "Epoch 1200/2000, Train Loss: 5.583492566319625, Val Loss: 4.517729579745315, Val MAE: 1.278250813484192\n",
      "Epoch 1201/2000, Train Loss: 5.5829360708291045, Val Loss: 4.517415739056163, Val MAE: 1.2783743143081665\n",
      "Epoch 1202/2000, Train Loss: 5.582289286352077, Val Loss: 4.517032724384248, Val MAE: 1.2784719467163086\n",
      "Epoch 1203/2000, Train Loss: 5.581784401043461, Val Loss: 4.516692138246195, Val MAE: 1.278576135635376\n",
      "Epoch 1204/2000, Train Loss: 5.581305696326972, Val Loss: 4.516407133146065, Val MAE: 1.2786550521850586\n",
      "Epoch 1205/2000, Train Loss: 5.580862232674737, Val Loss: 4.516066549143453, Val MAE: 1.2786861658096313\n",
      "Epoch 1206/2000, Train Loss: 5.580335197487551, Val Loss: 4.515773272326612, Val MAE: 1.278794288635254\n",
      "Epoch 1207/2000, Train Loss: 5.579866741201366, Val Loss: 4.515443677167723, Val MAE: 1.2788857221603394\n",
      "Epoch 1208/2000, Train Loss: 5.579338701641543, Val Loss: 4.515126032822245, Val MAE: 1.2789820432662964\n",
      "Epoch 1209/2000, Train Loss: 5.5788708930099675, Val Loss: 4.514794320067552, Val MAE: 1.2790101766586304\n",
      "Epoch 1210/2000, Train Loss: 5.578344404616543, Val Loss: 4.514447047813671, Val MAE: 1.2790831327438354\n",
      "Epoch 1211/2000, Train Loss: 5.577782554514049, Val Loss: 4.514121430314432, Val MAE: 1.2792547941207886\n",
      "Epoch 1212/2000, Train Loss: 5.577289894445618, Val Loss: 4.513823013770299, Val MAE: 1.2793549299240112\n",
      "Epoch 1213/2000, Train Loss: 5.576715061751926, Val Loss: 4.513570344823552, Val MAE: 1.2795836925506592\n",
      "Epoch 1214/2000, Train Loss: 5.576223914313106, Val Loss: 4.513233891094294, Val MAE: 1.2796975374221802\n",
      "Epoch 1215/2000, Train Loss: 5.575730213801929, Val Loss: 4.512949716059242, Val MAE: 1.279800295829773\n",
      "Epoch 1216/2000, Train Loss: 5.575170700068067, Val Loss: 4.5125949121249, Val MAE: 1.2800648212432861\n",
      "Epoch 1217/2000, Train Loss: 5.574673474279802, Val Loss: 4.51228054524876, Val MAE: 1.2801471948623657\n",
      "Epoch 1218/2000, Train Loss: 5.574188630972335, Val Loss: 4.511978171308209, Val MAE: 1.280199646949768\n",
      "Epoch 1219/2000, Train Loss: 5.573804095124616, Val Loss: 4.511669584592497, Val MAE: 1.2801848649978638\n",
      "Epoch 1220/2000, Train Loss: 5.573325346422406, Val Loss: 4.511332508980289, Val MAE: 1.2801992893218994\n",
      "Epoch 1221/2000, Train Loss: 5.5728508452385705, Val Loss: 4.511040530242319, Val MAE: 1.2802656888961792\n",
      "Epoch 1222/2000, Train Loss: 5.572382119164538, Val Loss: 4.510712041723447, Val MAE: 1.280312418937683\n",
      "Epoch 1223/2000, Train Loss: 5.571911257998752, Val Loss: 4.510416612850399, Val MAE: 1.2803572416305542\n",
      "Epoch 1224/2000, Train Loss: 5.5715141127560806, Val Loss: 4.510118114349879, Val MAE: 1.28036630153656\n",
      "Epoch 1225/2000, Train Loss: 5.571033882899678, Val Loss: 4.50985008798481, Val MAE: 1.2804749011993408\n",
      "Epoch 1226/2000, Train Loss: 5.570578509100095, Val Loss: 4.509515564742051, Val MAE: 1.2805423736572266\n",
      "Epoch 1227/2000, Train Loss: 5.5700988781359175, Val Loss: 4.509208116709717, Val MAE: 1.2805975675582886\n",
      "Epoch 1228/2000, Train Loss: 5.569631617945926, Val Loss: 4.508870612166998, Val MAE: 1.2806832790374756\n",
      "Epoch 1229/2000, Train Loss: 5.569084076218439, Val Loss: 4.5085564291852664, Val MAE: 1.280753254890442\n",
      "Epoch 1230/2000, Train Loss: 5.568640798208995, Val Loss: 4.50825594744579, Val MAE: 1.280802845954895\n",
      "Epoch 1231/2000, Train Loss: 5.568126773656304, Val Loss: 4.50795507598815, Val MAE: 1.280956745147705\n",
      "Epoch 1232/2000, Train Loss: 5.567634237225052, Val Loss: 4.507634371777219, Val MAE: 1.2810096740722656\n",
      "Epoch 1233/2000, Train Loss: 5.567172718824493, Val Loss: 4.507302822243041, Val MAE: 1.2810404300689697\n",
      "Epoch 1234/2000, Train Loss: 5.5667572797881375, Val Loss: 4.50698688225014, Val MAE: 1.281103491783142\n",
      "Epoch 1235/2000, Train Loss: 5.566239991699177, Val Loss: 4.506671281991981, Val MAE: 1.2811777591705322\n",
      "Epoch 1236/2000, Train Loss: 5.565813134774424, Val Loss: 4.506390862917806, Val MAE: 1.2811983823776245\n",
      "Epoch 1237/2000, Train Loss: 5.565328185238055, Val Loss: 4.50615698303294, Val MAE: 1.2812861204147339\n",
      "Epoch 1238/2000, Train Loss: 5.564894331267844, Val Loss: 4.505883364015677, Val MAE: 1.2813626527786255\n",
      "Epoch 1239/2000, Train Loss: 5.564471169113304, Val Loss: 4.50560263401176, Val MAE: 1.2814829349517822\n",
      "Epoch 1240/2000, Train Loss: 5.563931724938641, Val Loss: 4.505220959904626, Val MAE: 1.2814342975616455\n",
      "Epoch 1241/2000, Train Loss: 5.563496220872069, Val Loss: 4.5049208502365845, Val MAE: 1.2815355062484741\n",
      "Epoch 1242/2000, Train Loss: 5.563012548023519, Val Loss: 4.50461277987544, Val MAE: 1.2815905809402466\n",
      "Epoch 1243/2000, Train Loss: 5.562575160405691, Val Loss: 4.50430959600398, Val MAE: 1.281652808189392\n",
      "Epoch 1244/2000, Train Loss: 5.562115653884297, Val Loss: 4.5039739790159885, Val MAE: 1.281596064567566\n",
      "Epoch 1245/2000, Train Loss: 5.561648090383899, Val Loss: 4.503658747391438, Val MAE: 1.2816754579544067\n",
      "Epoch 1246/2000, Train Loss: 5.561188369140211, Val Loss: 4.503349081280194, Val MAE: 1.281717300415039\n",
      "Epoch 1247/2000, Train Loss: 5.56071042624306, Val Loss: 4.50315681416688, Val MAE: 1.2818561792373657\n",
      "Epoch 1248/2000, Train Loss: 5.560266468709327, Val Loss: 4.502871383210336, Val MAE: 1.2819137573242188\n",
      "Epoch 1249/2000, Train Loss: 5.559816029332224, Val Loss: 4.502565552259055, Val MAE: 1.2819831371307373\n",
      "Epoch 1250/2000, Train Loss: 5.559360837515853, Val Loss: 4.502241550761414, Val MAE: 1.282035231590271\n",
      "Epoch 1251/2000, Train Loss: 5.558888211502635, Val Loss: 4.501957435225408, Val MAE: 1.2820957899093628\n",
      "Epoch 1252/2000, Train Loss: 5.55838534482622, Val Loss: 4.501602540558248, Val MAE: 1.282274603843689\n",
      "Epoch 1253/2000, Train Loss: 5.557969354709744, Val Loss: 4.501332997161103, Val MAE: 1.282296895980835\n",
      "Epoch 1254/2000, Train Loss: 5.557438134823596, Val Loss: 4.501139301448826, Val MAE: 1.2824962139129639\n",
      "Epoch 1255/2000, Train Loss: 5.557008621003605, Val Loss: 4.500822077738488, Val MAE: 1.2825266122817993\n",
      "Epoch 1256/2000, Train Loss: 5.556628630960327, Val Loss: 4.500560892038927, Val MAE: 1.2825734615325928\n",
      "Epoch 1257/2000, Train Loss: 5.556084056257231, Val Loss: 4.500213381648064, Val MAE: 1.2826608419418335\n",
      "Epoch 1258/2000, Train Loss: 5.555612824826907, Val Loss: 4.499938929667623, Val MAE: 1.2827116250991821\n",
      "Epoch 1259/2000, Train Loss: 5.555136971549716, Val Loss: 4.499613977581498, Val MAE: 1.2827544212341309\n",
      "Epoch 1260/2000, Train Loss: 5.554688266045041, Val Loss: 4.4993185660618495, Val MAE: 1.282796025276184\n",
      "Epoch 1261/2000, Train Loss: 5.5542665320465, Val Loss: 4.4990306362275065, Val MAE: 1.2828353643417358\n",
      "Epoch 1262/2000, Train Loss: 5.553823396785935, Val Loss: 4.498736307682015, Val MAE: 1.2828679084777832\n",
      "Epoch 1263/2000, Train Loss: 5.553339072036857, Val Loss: 4.498477623476757, Val MAE: 1.2829991579055786\n",
      "Epoch 1264/2000, Train Loss: 5.552894193572946, Val Loss: 4.4981658075620805, Val MAE: 1.2830264568328857\n",
      "Epoch 1265/2000, Train Loss: 5.552445174557545, Val Loss: 4.497855013749731, Val MAE: 1.283068299293518\n",
      "Epoch 1266/2000, Train Loss: 5.551969873371409, Val Loss: 4.497554283325128, Val MAE: 1.2832419872283936\n",
      "Epoch 1267/2000, Train Loss: 5.551489041812384, Val Loss: 4.497247741987386, Val MAE: 1.283284068107605\n",
      "Epoch 1268/2000, Train Loss: 5.55104477525647, Val Loss: 4.496980212828306, Val MAE: 1.2833757400512695\n",
      "Epoch 1269/2000, Train Loss: 5.5505799056716985, Val Loss: 4.4966889690461125, Val MAE: 1.2834162712097168\n",
      "Epoch 1270/2000, Train Loss: 5.550100379652537, Val Loss: 4.49639242509688, Val MAE: 1.2835420370101929\n",
      "Epoch 1271/2000, Train Loss: 5.549618008017054, Val Loss: 4.496073537243633, Val MAE: 1.2836041450500488\n",
      "Epoch 1272/2000, Train Loss: 5.549210571126666, Val Loss: 4.495750609791185, Val MAE: 1.2835893630981445\n",
      "Epoch 1273/2000, Train Loss: 5.548759940268631, Val Loss: 4.495485742836017, Val MAE: 1.2836971282958984\n",
      "Epoch 1274/2000, Train Loss: 5.548267817395149, Val Loss: 4.495177227047485, Val MAE: 1.283759593963623\n",
      "Epoch 1275/2000, Train Loss: 5.547815601610587, Val Loss: 4.4949203758375855, Val MAE: 1.2838172912597656\n",
      "Epoch 1276/2000, Train Loss: 5.547333448842228, Val Loss: 4.494586046133923, Val MAE: 1.283866286277771\n",
      "Epoch 1277/2000, Train Loss: 5.546952953655651, Val Loss: 4.49429622778273, Val MAE: 1.2839277982711792\n",
      "Epoch 1278/2000, Train Loss: 5.546442482170613, Val Loss: 4.49398224352617, Val MAE: 1.2839770317077637\n",
      "Epoch 1279/2000, Train Loss: 5.545904891866194, Val Loss: 4.493684710573962, Val MAE: 1.2841095924377441\n",
      "Epoch 1280/2000, Train Loss: 5.545468512027849, Val Loss: 4.493399539284819, Val MAE: 1.284180760383606\n",
      "Epoch 1281/2000, Train Loss: 5.545040909438476, Val Loss: 4.493092764960968, Val MAE: 1.2842618227005005\n",
      "Epoch 1282/2000, Train Loss: 5.544666920458769, Val Loss: 4.492824466526509, Val MAE: 1.2842856645584106\n",
      "Epoch 1283/2000, Train Loss: 5.5440542576089, Val Loss: 4.492473600335478, Val MAE: 1.2843551635742188\n",
      "Epoch 1284/2000, Train Loss: 5.543610268726116, Val Loss: 4.492209559960628, Val MAE: 1.2844468355178833\n",
      "Epoch 1285/2000, Train Loss: 5.543042342083749, Val Loss: 4.491853589453097, Val MAE: 1.2844300270080566\n",
      "Epoch 1286/2000, Train Loss: 5.5425672139896625, Val Loss: 4.491606651481212, Val MAE: 1.2845981121063232\n",
      "Epoch 1287/2000, Train Loss: 5.542124564159192, Val Loss: 4.4912989879687, Val MAE: 1.284635066986084\n",
      "Epoch 1288/2000, Train Loss: 5.541650310138511, Val Loss: 4.491063921470341, Val MAE: 1.284721851348877\n",
      "Epoch 1289/2000, Train Loss: 5.541148403768139, Val Loss: 4.490686764210228, Val MAE: 1.2847739458084106\n",
      "Epoch 1290/2000, Train Loss: 5.540671776884123, Val Loss: 4.490413868556342, Val MAE: 1.2847951650619507\n",
      "Epoch 1291/2000, Train Loss: 5.540307871845716, Val Loss: 4.490244814530602, Val MAE: 1.285020112991333\n",
      "Epoch 1292/2000, Train Loss: 5.539737465585427, Val Loss: 4.489957962965402, Val MAE: 1.2850658893585205\n",
      "Epoch 1293/2000, Train Loss: 5.539311209883101, Val Loss: 4.489672635284465, Val MAE: 1.2851430177688599\n",
      "Epoch 1294/2000, Train Loss: 5.538898216982548, Val Loss: 4.48937715743705, Val MAE: 1.2852702140808105\n",
      "Epoch 1295/2000, Train Loss: 5.538418596441096, Val Loss: 4.489084316191711, Val MAE: 1.2853446006774902\n",
      "Epoch 1296/2000, Train Loss: 5.537978486116253, Val Loss: 4.488922979031491, Val MAE: 1.2854663133621216\n",
      "Epoch 1297/2000, Train Loss: 5.537599428376155, Val Loss: 4.4886338979005815, Val MAE: 1.285513997077942\n",
      "Epoch 1298/2000, Train Loss: 5.537166299781126, Val Loss: 4.488343741950088, Val MAE: 1.2855852842330933\n",
      "Epoch 1299/2000, Train Loss: 5.5367400574029055, Val Loss: 4.488101084530354, Val MAE: 1.285666823387146\n",
      "Epoch 1300/2000, Train Loss: 5.5363542249115385, Val Loss: 4.487803551437348, Val MAE: 1.2856643199920654\n",
      "Epoch 1301/2000, Train Loss: 5.535941536746808, Val Loss: 4.487544938393935, Val MAE: 1.2857335805892944\n",
      "Epoch 1302/2000, Train Loss: 5.535498355428108, Val Loss: 4.487247398014613, Val MAE: 1.2857238054275513\n",
      "Epoch 1303/2000, Train Loss: 5.535084420337444, Val Loss: 4.486941973794633, Val MAE: 1.2857935428619385\n",
      "Epoch 1304/2000, Train Loss: 5.534611491691145, Val Loss: 4.486672472801265, Val MAE: 1.2858576774597168\n",
      "Epoch 1305/2000, Train Loss: 5.534192764128242, Val Loss: 4.486440506272428, Val MAE: 1.2859063148498535\n",
      "Epoch 1306/2000, Train Loss: 5.533783650139456, Val Loss: 4.486179495385782, Val MAE: 1.285953402519226\n",
      "Epoch 1307/2000, Train Loss: 5.533412915233679, Val Loss: 4.485922751147447, Val MAE: 1.285962700843811\n",
      "Epoch 1308/2000, Train Loss: 5.5329698904176094, Val Loss: 4.485664376217549, Val MAE: 1.2861146926879883\n",
      "Epoch 1309/2000, Train Loss: 5.53252239497012, Val Loss: 4.485401728010084, Val MAE: 1.2861475944519043\n",
      "Epoch 1310/2000, Train Loss: 5.532089409457578, Val Loss: 4.485127363535832, Val MAE: 1.2861824035644531\n",
      "Epoch 1311/2000, Train Loss: 5.531677643084845, Val Loss: 4.4848234651008925, Val MAE: 1.2862181663513184\n",
      "Epoch 1312/2000, Train Loss: 5.531235255573951, Val Loss: 4.484583384690322, Val MAE: 1.286281943321228\n",
      "Epoch 1313/2000, Train Loss: 5.530851920230094, Val Loss: 4.48426711667241, Val MAE: 1.2862790822982788\n",
      "Epoch 1314/2000, Train Loss: 5.530417824050303, Val Loss: 4.484016013016381, Val MAE: 1.2863620519638062\n",
      "Epoch 1315/2000, Train Loss: 5.53001033531924, Val Loss: 4.483734955280784, Val MAE: 1.2863917350769043\n",
      "Epoch 1316/2000, Train Loss: 5.529563021304003, Val Loss: 4.4834410313546185, Val MAE: 1.2864515781402588\n",
      "Epoch 1317/2000, Train Loss: 5.529083989530276, Val Loss: 4.4831294557945, Val MAE: 1.2865025997161865\n",
      "Epoch 1318/2000, Train Loss: 5.528715434965641, Val Loss: 4.482817955221248, Val MAE: 1.2864975929260254\n",
      "Epoch 1319/2000, Train Loss: 5.528303366341455, Val Loss: 4.482554261691457, Val MAE: 1.2865962982177734\n",
      "Epoch 1320/2000, Train Loss: 5.52778702448634, Val Loss: 4.482246835670603, Val MAE: 1.2866181135177612\n",
      "Epoch 1321/2000, Train Loss: 5.527375201923391, Val Loss: 4.481988212947301, Val MAE: 1.2866655588150024\n",
      "Epoch 1322/2000, Train Loss: 5.526970145498558, Val Loss: 4.481682076768612, Val MAE: 1.2866992950439453\n",
      "Epoch 1323/2000, Train Loss: 5.526499283038778, Val Loss: 4.481403904038621, Val MAE: 1.2867650985717773\n",
      "Epoch 1324/2000, Train Loss: 5.5260336667582335, Val Loss: 4.481180955551741, Val MAE: 1.2868938446044922\n",
      "Epoch 1325/2000, Train Loss: 5.525672054507111, Val Loss: 4.480913563143081, Val MAE: 1.2869102954864502\n",
      "Epoch 1326/2000, Train Loss: 5.525224797276014, Val Loss: 4.480616714600034, Val MAE: 1.286974549293518\n",
      "Epoch 1327/2000, Train Loss: 5.524812304989777, Val Loss: 4.480345588934234, Val MAE: 1.2870465517044067\n",
      "Epoch 1328/2000, Train Loss: 5.524312081803622, Val Loss: 4.480016610753818, Val MAE: 1.2871153354644775\n",
      "Epoch 1329/2000, Train Loss: 5.523866839454164, Val Loss: 4.4797823384169515, Val MAE: 1.287178635597229\n",
      "Epoch 1330/2000, Train Loss: 5.523440669559882, Val Loss: 4.47945583073173, Val MAE: 1.2872527837753296\n",
      "Epoch 1331/2000, Train Loss: 5.523017247302238, Val Loss: 4.4792185693979265, Val MAE: 1.2873190641403198\n",
      "Epoch 1332/2000, Train Loss: 5.522613794541585, Val Loss: 4.478959694563404, Val MAE: 1.2873541116714478\n",
      "Epoch 1333/2000, Train Loss: 5.522117265205694, Val Loss: 4.478725129283789, Val MAE: 1.2875170707702637\n",
      "Epoch 1334/2000, Train Loss: 5.52180391734054, Val Loss: 4.47845505436105, Val MAE: 1.2875386476516724\n",
      "Epoch 1335/2000, Train Loss: 5.521354207432901, Val Loss: 4.478208579873945, Val MAE: 1.287609577178955\n",
      "Epoch 1336/2000, Train Loss: 5.520969433247477, Val Loss: 4.477893636487131, Val MAE: 1.2876211404800415\n",
      "Epoch 1337/2000, Train Loss: 5.5205168523490835, Val Loss: 4.4776531462711615, Val MAE: 1.2876771688461304\n",
      "Epoch 1338/2000, Train Loss: 5.520019301889386, Val Loss: 4.477307304305824, Val MAE: 1.287795901298523\n",
      "Epoch 1339/2000, Train Loss: 5.519622500586542, Val Loss: 4.477036035330746, Val MAE: 1.2878743410110474\n",
      "Epoch 1340/2000, Train Loss: 5.519197112022617, Val Loss: 4.476730312289685, Val MAE: 1.2878715991973877\n",
      "Epoch 1341/2000, Train Loss: 5.518837078293758, Val Loss: 4.4763980819719045, Val MAE: 1.2878046035766602\n",
      "Epoch 1342/2000, Train Loss: 5.518431982159453, Val Loss: 4.476153145073436, Val MAE: 1.2878432273864746\n",
      "Epoch 1343/2000, Train Loss: 5.518029408991903, Val Loss: 4.475914658465254, Val MAE: 1.287865400314331\n",
      "Epoch 1344/2000, Train Loss: 5.517639691988196, Val Loss: 4.475634896790418, Val MAE: 1.28789222240448\n",
      "Epoch 1345/2000, Train Loss: 5.5172087230708255, Val Loss: 4.475325359885148, Val MAE: 1.2879453897476196\n",
      "Epoch 1346/2000, Train Loss: 5.516738799727918, Val Loss: 4.475119393383424, Val MAE: 1.2881064414978027\n",
      "Epoch 1347/2000, Train Loss: 5.5162987222988535, Val Loss: 4.474890183440343, Val MAE: 1.28816556930542\n",
      "Epoch 1348/2000, Train Loss: 5.5159379889230715, Val Loss: 4.474647751110276, Val MAE: 1.2882224321365356\n",
      "Epoch 1349/2000, Train Loss: 5.515475540504042, Val Loss: 4.474357378119089, Val MAE: 1.2882543802261353\n",
      "Epoch 1350/2000, Train Loss: 5.515094632209561, Val Loss: 4.474063018715288, Val MAE: 1.2882380485534668\n",
      "Epoch 1351/2000, Train Loss: 5.514677716077911, Val Loss: 4.473825829704915, Val MAE: 1.28822660446167\n",
      "Epoch 1352/2000, Train Loss: 5.514287578728014, Val Loss: 4.473589423216703, Val MAE: 1.2883827686309814\n",
      "Epoch 1353/2000, Train Loss: 5.513877442867332, Val Loss: 4.473332938235107, Val MAE: 1.2884100675582886\n",
      "Epoch 1354/2000, Train Loss: 5.513490801103403, Val Loss: 4.473067915216675, Val MAE: 1.2884479761123657\n",
      "Epoch 1355/2000, Train Loss: 5.513081713803264, Val Loss: 4.472819655124597, Val MAE: 1.2885088920593262\n",
      "Epoch 1356/2000, Train Loss: 5.51267780420172, Val Loss: 4.4725869467760635, Val MAE: 1.2885698080062866\n",
      "Epoch 1357/2000, Train Loss: 5.51228833036798, Val Loss: 4.472299734567564, Val MAE: 1.288555383682251\n",
      "Epoch 1358/2000, Train Loss: 5.511774368493172, Val Loss: 4.472046400888229, Val MAE: 1.2887189388275146\n",
      "Epoch 1359/2000, Train Loss: 5.511346734200921, Val Loss: 4.47173750278518, Val MAE: 1.2888603210449219\n",
      "Epoch 1360/2000, Train Loss: 5.51085234076553, Val Loss: 4.471475137077917, Val MAE: 1.2889291048049927\n",
      "Epoch 1361/2000, Train Loss: 5.510459421124193, Val Loss: 4.471183387946895, Val MAE: 1.2889400720596313\n",
      "Epoch 1362/2000, Train Loss: 5.510080721436556, Val Loss: 4.470940433137529, Val MAE: 1.289040207862854\n",
      "Epoch 1363/2000, Train Loss: 5.509706662823743, Val Loss: 4.4707029966153495, Val MAE: 1.2890392541885376\n",
      "Epoch 1364/2000, Train Loss: 5.509263216761753, Val Loss: 4.47045844339245, Val MAE: 1.2891522645950317\n",
      "Epoch 1365/2000, Train Loss: 5.508900403895566, Val Loss: 4.470201528353954, Val MAE: 1.2891873121261597\n",
      "Epoch 1366/2000, Train Loss: 5.508504735566221, Val Loss: 4.469958386024621, Val MAE: 1.2892265319824219\n",
      "Epoch 1367/2000, Train Loss: 5.508112825887983, Val Loss: 4.4696662370144855, Val MAE: 1.2892265319824219\n",
      "Epoch 1368/2000, Train Loss: 5.50772618996725, Val Loss: 4.46930895913539, Val MAE: 1.289209008216858\n",
      "Epoch 1369/2000, Train Loss: 5.507291644690318, Val Loss: 4.469113621010086, Val MAE: 1.2892744541168213\n",
      "Epoch 1370/2000, Train Loss: 5.506948662049856, Val Loss: 4.468853094000516, Val MAE: 1.289262294769287\n",
      "Epoch 1371/2000, Train Loss: 5.506576375042568, Val Loss: 4.468552974159793, Val MAE: 1.289300799369812\n",
      "Epoch 1372/2000, Train Loss: 5.506096224610162, Val Loss: 4.468307099368159, Val MAE: 1.2894271612167358\n",
      "Epoch 1373/2000, Train Loss: 5.505792409949905, Val Loss: 4.468041801393971, Val MAE: 1.2894129753112793\n",
      "Epoch 1374/2000, Train Loss: 5.5053908789335795, Val Loss: 4.467794140962165, Val MAE: 1.2894245386123657\n",
      "Epoch 1375/2000, Train Loss: 5.5049458281156, Val Loss: 4.46760351230779, Val MAE: 1.289541482925415\n",
      "Epoch 1376/2000, Train Loss: 5.504534781929434, Val Loss: 4.467423916977691, Val MAE: 1.2896003723144531\n",
      "Epoch 1377/2000, Train Loss: 5.5041998723468755, Val Loss: 4.4672300347427685, Val MAE: 1.2896512746810913\n",
      "Epoch 1378/2000, Train Loss: 5.503836522432325, Val Loss: 4.466969267108778, Val MAE: 1.2896373271942139\n",
      "Epoch 1379/2000, Train Loss: 5.503526767849761, Val Loss: 4.4667689232845005, Val MAE: 1.2896676063537598\n",
      "Epoch 1380/2000, Train Loss: 5.503094694184157, Val Loss: 4.466479414079602, Val MAE: 1.2897580862045288\n",
      "Epoch 1381/2000, Train Loss: 5.502701577532081, Val Loss: 4.46623032103608, Val MAE: 1.289778232574463\n",
      "Epoch 1382/2000, Train Loss: 5.502321804879964, Val Loss: 4.4660520917318, Val MAE: 1.2899000644683838\n",
      "Epoch 1383/2000, Train Loss: 5.501935724578444, Val Loss: 4.465781525856866, Val MAE: 1.2900077104568481\n",
      "Epoch 1384/2000, Train Loss: 5.5015671295259185, Val Loss: 4.465573542254178, Val MAE: 1.2900294065475464\n",
      "Epoch 1385/2000, Train Loss: 5.501232070871092, Val Loss: 4.4652500976141045, Val MAE: 1.2899913787841797\n",
      "Epoch 1386/2000, Train Loss: 5.500793639484545, Val Loss: 4.465026572663484, Val MAE: 1.2900596857070923\n",
      "Epoch 1387/2000, Train Loss: 5.500451397216466, Val Loss: 4.464767284081208, Val MAE: 1.2901241779327393\n",
      "Epoch 1388/2000, Train Loss: 5.500118824164288, Val Loss: 4.464532635108692, Val MAE: 1.2902405261993408\n",
      "Epoch 1389/2000, Train Loss: 5.499805917571649, Val Loss: 4.464272420354716, Val MAE: 1.2902166843414307\n",
      "Epoch 1390/2000, Train Loss: 5.4993693540668875, Val Loss: 4.464033983553958, Val MAE: 1.2902144193649292\n",
      "Epoch 1391/2000, Train Loss: 5.499028943448733, Val Loss: 4.463737640289341, Val MAE: 1.2902085781097412\n",
      "Epoch 1392/2000, Train Loss: 5.498602412934219, Val Loss: 4.463485888421066, Val MAE: 1.290216088294983\n",
      "Epoch 1393/2000, Train Loss: 5.498269746320138, Val Loss: 4.46320663319798, Val MAE: 1.2902482748031616\n",
      "Epoch 1394/2000, Train Loss: 5.4979465992026935, Val Loss: 4.462961913222872, Val MAE: 1.2902597188949585\n",
      "Epoch 1395/2000, Train Loss: 5.497540446764093, Val Loss: 4.462762773494552, Val MAE: 1.2902978658676147\n",
      "Epoch 1396/2000, Train Loss: 5.497172620920831, Val Loss: 4.462468785587258, Val MAE: 1.2902741432189941\n",
      "Epoch 1397/2000, Train Loss: 5.496891428042947, Val Loss: 4.462182047622879, Val MAE: 1.2903088331222534\n",
      "Epoch 1398/2000, Train Loss: 5.496416141737753, Val Loss: 4.461997224507838, Val MAE: 1.2904202938079834\n",
      "Epoch 1399/2000, Train Loss: 5.496029541968006, Val Loss: 4.461754961147553, Val MAE: 1.2905142307281494\n",
      "Epoch 1400/2000, Train Loss: 5.495656123058807, Val Loss: 4.461495726009992, Val MAE: 1.290506362915039\n",
      "Epoch 1401/2000, Train Loss: 5.495295367295901, Val Loss: 4.461265865144298, Val MAE: 1.2905079126358032\n",
      "Epoch 1402/2000, Train Loss: 5.494984238207583, Val Loss: 4.46099419740476, Val MAE: 1.2905306816101074\n",
      "Epoch 1403/2000, Train Loss: 5.494583953874567, Val Loss: 4.460784105698424, Val MAE: 1.2905925512313843\n",
      "Epoch 1404/2000, Train Loss: 5.494201641959445, Val Loss: 4.460523253573677, Val MAE: 1.2905935049057007\n",
      "Epoch 1405/2000, Train Loss: 5.493847431612467, Val Loss: 4.460302103774285, Val MAE: 1.2906259298324585\n",
      "Epoch 1406/2000, Train Loss: 5.493423055235696, Val Loss: 4.460012946694385, Val MAE: 1.2907363176345825\n",
      "Epoch 1407/2000, Train Loss: 5.493000350134343, Val Loss: 4.459780990924892, Val MAE: 1.2907824516296387\n",
      "Epoch 1408/2000, Train Loss: 5.492658774708391, Val Loss: 4.459576295633016, Val MAE: 1.2907912731170654\n",
      "Epoch 1409/2000, Train Loss: 5.492333153088349, Val Loss: 4.459322113408817, Val MAE: 1.2908204793930054\n",
      "Epoch 1410/2000, Train Loss: 5.491946310633882, Val Loss: 4.459076157505588, Val MAE: 1.2908574342727661\n",
      "Epoch 1411/2000, Train Loss: 5.49160553933484, Val Loss: 4.4588584390564225, Val MAE: 1.2909075021743774\n",
      "Epoch 1412/2000, Train Loss: 5.491176051394748, Val Loss: 4.458615541270398, Val MAE: 1.290920615196228\n",
      "Epoch 1413/2000, Train Loss: 5.490707644775732, Val Loss: 4.458303160695579, Val MAE: 1.2909795045852661\n",
      "Epoch 1414/2000, Train Loss: 5.4903435294227005, Val Loss: 4.45803260830209, Val MAE: 1.2909871339797974\n",
      "Epoch 1415/2000, Train Loss: 5.4900271222905195, Val Loss: 4.457831948843059, Val MAE: 1.2910393476486206\n",
      "Epoch 1416/2000, Train Loss: 5.489569933799423, Val Loss: 4.45760828637936, Val MAE: 1.2911311388015747\n",
      "Epoch 1417/2000, Train Loss: 5.489207194003515, Val Loss: 4.457333029384219, Val MAE: 1.291123390197754\n",
      "Epoch 1418/2000, Train Loss: 5.488909364070142, Val Loss: 4.457121207465337, Val MAE: 1.291240930557251\n",
      "Epoch 1419/2000, Train Loss: 5.488512706562492, Val Loss: 4.456901263557081, Val MAE: 1.2912497520446777\n",
      "Epoch 1420/2000, Train Loss: 5.488236379429313, Val Loss: 4.456622141083394, Val MAE: 1.2912516593933105\n",
      "Epoch 1421/2000, Train Loss: 5.48776748287144, Val Loss: 4.456415064632893, Val MAE: 1.2913776636123657\n",
      "Epoch 1422/2000, Train Loss: 5.48729918195564, Val Loss: 4.456193884913846, Val MAE: 1.2914135456085205\n",
      "Epoch 1423/2000, Train Loss: 5.486964565988635, Val Loss: 4.4559568471913265, Val MAE: 1.291462779045105\n",
      "Epoch 1424/2000, Train Loss: 5.486581773900274, Val Loss: 4.455747798026547, Val MAE: 1.2914904356002808\n",
      "Epoch 1425/2000, Train Loss: 5.486188766914275, Val Loss: 4.455653461696595, Val MAE: 1.2916815280914307\n",
      "Epoch 1426/2000, Train Loss: 5.485760677984968, Val Loss: 4.4554278087193575, Val MAE: 1.2917803525924683\n",
      "Epoch 1427/2000, Train Loss: 5.4854044459729865, Val Loss: 4.45519484043356, Val MAE: 1.2918306589126587\n",
      "Epoch 1428/2000, Train Loss: 5.485055677602864, Val Loss: 4.454963565412469, Val MAE: 1.2918660640716553\n",
      "Epoch 1429/2000, Train Loss: 5.484674395268007, Val Loss: 4.454725675801123, Val MAE: 1.2919777631759644\n",
      "Epoch 1430/2000, Train Loss: 5.484325194132377, Val Loss: 4.45450171307316, Val MAE: 1.2920138835906982\n",
      "Epoch 1431/2000, Train Loss: 5.483980428493169, Val Loss: 4.454262740992186, Val MAE: 1.2920050621032715\n",
      "Epoch 1432/2000, Train Loss: 5.483641418705479, Val Loss: 4.454042086204676, Val MAE: 1.2920483350753784\n",
      "Epoch 1433/2000, Train Loss: 5.483269089011518, Val Loss: 4.4537864130313, Val MAE: 1.2920674085617065\n",
      "Epoch 1434/2000, Train Loss: 5.4829092404250375, Val Loss: 4.453560944241802, Val MAE: 1.2920863628387451\n",
      "Epoch 1435/2000, Train Loss: 5.482575034108543, Val Loss: 4.453352451441795, Val MAE: 1.2920851707458496\n",
      "Epoch 1436/2000, Train Loss: 5.482250802653615, Val Loss: 4.453133617611382, Val MAE: 1.2921249866485596\n",
      "Epoch 1437/2000, Train Loss: 5.48189434859007, Val Loss: 4.452916955572413, Val MAE: 1.2921372652053833\n",
      "Epoch 1438/2000, Train Loss: 5.481547638713262, Val Loss: 4.452665335632215, Val MAE: 1.2921353578567505\n",
      "Epoch 1439/2000, Train Loss: 5.481226812062464, Val Loss: 4.452426447009477, Val MAE: 1.2921314239501953\n",
      "Epoch 1440/2000, Train Loss: 5.480884743988352, Val Loss: 4.4522162170508714, Val MAE: 1.2921693325042725\n",
      "Epoch 1441/2000, Train Loss: 5.480522266984633, Val Loss: 4.4519816018465, Val MAE: 1.29222571849823\n",
      "Epoch 1442/2000, Train Loss: 5.480139119654207, Val Loss: 4.451763774055665, Val MAE: 1.292250156402588\n",
      "Epoch 1443/2000, Train Loss: 5.47977603395726, Val Loss: 4.4514776016548865, Val MAE: 1.2922481298446655\n",
      "Epoch 1444/2000, Train Loss: 5.479392818227066, Val Loss: 4.451267790571442, Val MAE: 1.2923228740692139\n",
      "Epoch 1445/2000, Train Loss: 5.479052739680379, Val Loss: 4.451020153617765, Val MAE: 1.2923310995101929\n",
      "Epoch 1446/2000, Train Loss: 5.478724001220641, Val Loss: 4.450760946827611, Val MAE: 1.2923035621643066\n",
      "Epoch 1447/2000, Train Loss: 5.478363118954559, Val Loss: 4.450538165808663, Val MAE: 1.292333960533142\n",
      "Epoch 1448/2000, Train Loss: 5.47802916375397, Val Loss: 4.450288822871493, Val MAE: 1.2923321723937988\n",
      "Epoch 1449/2000, Train Loss: 5.4777342823499575, Val Loss: 4.4500634411071225, Val MAE: 1.292324185371399\n",
      "Epoch 1450/2000, Train Loss: 5.477302721591624, Val Loss: 4.449830359314371, Val MAE: 1.2924178838729858\n",
      "Epoch 1451/2000, Train Loss: 5.476979443305545, Val Loss: 4.449642044763396, Val MAE: 1.2925070524215698\n",
      "Epoch 1452/2000, Train Loss: 5.476586283628135, Val Loss: 4.449507050286597, Val MAE: 1.2926441431045532\n",
      "Epoch 1453/2000, Train Loss: 5.476064113771171, Val Loss: 4.4490736346545185, Val MAE: 1.29263174533844\n",
      "Epoch 1454/2000, Train Loss: 5.475708007165503, Val Loss: 4.448866106675366, Val MAE: 1.2926884889602661\n",
      "Epoch 1455/2000, Train Loss: 5.475377892168034, Val Loss: 4.4486426677525515, Val MAE: 1.2927159070968628\n",
      "Epoch 1456/2000, Train Loss: 5.475047203384875, Val Loss: 4.44846538471894, Val MAE: 1.2927665710449219\n",
      "Epoch 1457/2000, Train Loss: 5.47459549027188, Val Loss: 4.448221166795633, Val MAE: 1.2928389310836792\n",
      "Epoch 1458/2000, Train Loss: 5.474295318531052, Val Loss: 4.4480058499091255, Val MAE: 1.2928539514541626\n",
      "Epoch 1459/2000, Train Loss: 5.474064418709909, Val Loss: 4.447850468684369, Val MAE: 1.292942762374878\n",
      "Epoch 1460/2000, Train Loss: 5.473660778966979, Val Loss: 4.447615164764754, Val MAE: 1.2929037809371948\n",
      "Epoch 1461/2000, Train Loss: 5.473337157408289, Val Loss: 4.447357346340428, Val MAE: 1.2929151058197021\n",
      "Epoch 1462/2000, Train Loss: 5.4729885181546045, Val Loss: 4.447174193434359, Val MAE: 1.292934775352478\n",
      "Epoch 1463/2000, Train Loss: 5.472671772764043, Val Loss: 4.447022055124673, Val MAE: 1.2929753065109253\n",
      "Epoch 1464/2000, Train Loss: 5.472281358944027, Val Loss: 4.446830629926967, Val MAE: 1.2931865453720093\n",
      "Epoch 1465/2000, Train Loss: 5.471987883339218, Val Loss: 4.446634278487502, Val MAE: 1.293209433555603\n",
      "Epoch 1466/2000, Train Loss: 5.471644840214599, Val Loss: 4.446407805394939, Val MAE: 1.2932175397872925\n",
      "Epoch 1467/2000, Train Loss: 5.471256708354769, Val Loss: 4.446159338035922, Val MAE: 1.2932254076004028\n",
      "Epoch 1468/2000, Train Loss: 5.470940060094707, Val Loss: 4.446006690869181, Val MAE: 1.2933253049850464\n",
      "Epoch 1469/2000, Train Loss: 5.470585886222213, Val Loss: 4.445807496123896, Val MAE: 1.2933461666107178\n",
      "Epoch 1470/2000, Train Loss: 5.470310217328117, Val Loss: 4.445539928303929, Val MAE: 1.2933380603790283\n",
      "Epoch 1471/2000, Train Loss: 5.46993885871675, Val Loss: 4.445317042390193, Val MAE: 1.293337106704712\n",
      "Epoch 1472/2000, Train Loss: 5.4695956031874005, Val Loss: 4.445046181542667, Val MAE: 1.2933390140533447\n",
      "Epoch 1473/2000, Train Loss: 5.469240466807558, Val Loss: 4.444891985934081, Val MAE: 1.293373465538025\n",
      "Epoch 1474/2000, Train Loss: 5.468933772523173, Val Loss: 4.444699711593118, Val MAE: 1.2935242652893066\n",
      "Epoch 1475/2000, Train Loss: 5.468498405963789, Val Loss: 4.444460257723575, Val MAE: 1.2935508489608765\n",
      "Epoch 1476/2000, Train Loss: 5.468164228094967, Val Loss: 4.444351477909276, Val MAE: 1.293655276298523\n",
      "Epoch 1477/2000, Train Loss: 5.467827959979405, Val Loss: 4.4441463327079305, Val MAE: 1.2936513423919678\n",
      "Epoch 1478/2000, Train Loss: 5.467485908332964, Val Loss: 4.443917336121319, Val MAE: 1.2936763763427734\n",
      "Epoch 1479/2000, Train Loss: 5.46720038793142, Val Loss: 4.443679709615201, Val MAE: 1.2936538457870483\n",
      "Epoch 1480/2000, Train Loss: 5.466825891543923, Val Loss: 4.443476522966163, Val MAE: 1.2937442064285278\n",
      "Epoch 1481/2000, Train Loss: 5.466452097795711, Val Loss: 4.443318053939211, Val MAE: 1.2938333749771118\n",
      "Epoch 1482/2000, Train Loss: 5.466094087703176, Val Loss: 4.443124757941783, Val MAE: 1.293893575668335\n",
      "Epoch 1483/2000, Train Loss: 5.465780232945808, Val Loss: 4.442887104367177, Val MAE: 1.293927788734436\n",
      "Epoch 1484/2000, Train Loss: 5.465429854724592, Val Loss: 4.442630948337514, Val MAE: 1.2939457893371582\n",
      "Epoch 1485/2000, Train Loss: 5.465064623745682, Val Loss: 4.442446318350909, Val MAE: 1.2940045595169067\n",
      "Epoch 1486/2000, Train Loss: 5.4647423502871595, Val Loss: 4.442255051832968, Val MAE: 1.2940353155136108\n",
      "Epoch 1487/2000, Train Loss: 5.464423637882663, Val Loss: 4.441960846978849, Val MAE: 1.2940165996551514\n",
      "Epoch 1488/2000, Train Loss: 5.464050737536908, Val Loss: 4.441804024823538, Val MAE: 1.2940988540649414\n",
      "Epoch 1489/2000, Train Loss: 5.463705678807832, Val Loss: 4.4415496357665285, Val MAE: 1.294111967086792\n",
      "Epoch 1490/2000, Train Loss: 5.4633352636805705, Val Loss: 4.441334610297455, Val MAE: 1.2941951751708984\n",
      "Epoch 1491/2000, Train Loss: 5.463045692185049, Val Loss: 4.441119787918301, Val MAE: 1.2942527532577515\n",
      "Epoch 1492/2000, Train Loss: 5.462700085529964, Val Loss: 4.440931951682868, Val MAE: 1.2941988706588745\n",
      "Epoch 1493/2000, Train Loss: 5.462414319299697, Val Loss: 4.440668808369655, Val MAE: 1.2942049503326416\n",
      "Epoch 1494/2000, Train Loss: 5.46206780625166, Val Loss: 4.440487739971773, Val MAE: 1.2942490577697754\n",
      "Epoch 1495/2000, Train Loss: 5.461749626791137, Val Loss: 4.440276857999366, Val MAE: 1.2942848205566406\n",
      "Epoch 1496/2000, Train Loss: 5.461344123048407, Val Loss: 4.440041837694608, Val MAE: 1.2943588495254517\n",
      "Epoch 1497/2000, Train Loss: 5.461014112288302, Val Loss: 4.439857909235898, Val MAE: 1.2944804430007935\n",
      "Epoch 1498/2000, Train Loss: 5.4607214321012085, Val Loss: 4.439604323081613, Val MAE: 1.294467568397522\n",
      "Epoch 1499/2000, Train Loss: 5.460351040761215, Val Loss: 4.439411542500098, Val MAE: 1.2944988012313843\n",
      "Epoch 1500/2000, Train Loss: 5.460006336526114, Val Loss: 4.439178591391702, Val MAE: 1.2947404384613037\n",
      "Epoch 1501/2000, Train Loss: 5.4596793738925475, Val Loss: 4.43896831528643, Val MAE: 1.2947192192077637\n",
      "Epoch 1502/2000, Train Loss: 5.459362733901922, Val Loss: 4.438800601778537, Val MAE: 1.2948205471038818\n",
      "Epoch 1503/2000, Train Loss: 5.4590350400803045, Val Loss: 4.438578357928851, Val MAE: 1.2948155403137207\n",
      "Epoch 1504/2000, Train Loss: 5.458707871560002, Val Loss: 4.438332328000876, Val MAE: 1.294786810874939\n",
      "Epoch 1505/2000, Train Loss: 5.458491251297204, Val Loss: 4.438072113915691, Val MAE: 1.294825553894043\n",
      "Epoch 1506/2000, Train Loss: 5.458172053139116, Val Loss: 4.437784498261185, Val MAE: 1.294821858406067\n",
      "Epoch 1507/2000, Train Loss: 5.457817195713763, Val Loss: 4.437561328732592, Val MAE: 1.2948169708251953\n",
      "Epoch 1508/2000, Train Loss: 5.457506919974711, Val Loss: 4.437413722546551, Val MAE: 1.2948641777038574\n",
      "Epoch 1509/2000, Train Loss: 5.457169125167454, Val Loss: 4.437169460142691, Val MAE: 1.294915795326233\n",
      "Epoch 1510/2000, Train Loss: 5.456828906576135, Val Loss: 4.437066736078169, Val MAE: 1.2950493097305298\n",
      "Epoch 1511/2000, Train Loss: 5.456492709984126, Val Loss: 4.436892028082544, Val MAE: 1.2950726747512817\n",
      "Epoch 1512/2000, Train Loss: 5.456143779702879, Val Loss: 4.43658288218609, Val MAE: 1.2950278520584106\n",
      "Epoch 1513/2000, Train Loss: 5.455834262396539, Val Loss: 4.436414892795518, Val MAE: 1.2950892448425293\n",
      "Epoch 1514/2000, Train Loss: 5.4554985336241595, Val Loss: 4.436217881105547, Val MAE: 1.295059084892273\n",
      "Epoch 1515/2000, Train Loss: 5.455212697083462, Val Loss: 4.436002569956573, Val MAE: 1.2951548099517822\n",
      "Epoch 1516/2000, Train Loss: 5.454841407903014, Val Loss: 4.435816786502759, Val MAE: 1.2951973676681519\n",
      "Epoch 1517/2000, Train Loss: 5.45453143835553, Val Loss: 4.435574019929086, Val MAE: 1.295180082321167\n",
      "Epoch 1518/2000, Train Loss: 5.454218375796056, Val Loss: 4.435347591786403, Val MAE: 1.2951698303222656\n",
      "Epoch 1519/2000, Train Loss: 5.453867795024183, Val Loss: 4.435144328073723, Val MAE: 1.2951970100402832\n",
      "Epoch 1520/2000, Train Loss: 5.453533785435368, Val Loss: 4.434917739197964, Val MAE: 1.2951948642730713\n",
      "Epoch 1521/2000, Train Loss: 5.45321663684405, Val Loss: 4.434687684335577, Val MAE: 1.2951991558074951\n",
      "Epoch 1522/2000, Train Loss: 5.452895883546594, Val Loss: 4.434526141339869, Val MAE: 1.295217514038086\n",
      "Epoch 1523/2000, Train Loss: 5.452569997941461, Val Loss: 4.434277460295854, Val MAE: 1.2952412366867065\n",
      "Epoch 1524/2000, Train Loss: 5.452245426307378, Val Loss: 4.434040094642189, Val MAE: 1.2952451705932617\n",
      "Epoch 1525/2000, Train Loss: 5.451942208017067, Val Loss: 4.433936415213769, Val MAE: 1.2953503131866455\n",
      "Epoch 1526/2000, Train Loss: 5.451609651301577, Val Loss: 4.4336808078871, Val MAE: 1.295322299003601\n",
      "Epoch 1527/2000, Train Loss: 5.451286972863056, Val Loss: 4.433470766351918, Val MAE: 1.2953391075134277\n",
      "Epoch 1528/2000, Train Loss: 5.450951882909726, Val Loss: 4.433251349287709, Val MAE: 1.2953228950500488\n",
      "Epoch 1529/2000, Train Loss: 5.450584468880373, Val Loss: 4.43306699227395, Val MAE: 1.295604944229126\n",
      "Epoch 1530/2000, Train Loss: 5.450299642788021, Val Loss: 4.432832362098018, Val MAE: 1.2956788539886475\n",
      "Epoch 1531/2000, Train Loss: 5.449919235431356, Val Loss: 4.432615859067346, Val MAE: 1.2956409454345703\n",
      "Epoch 1532/2000, Train Loss: 5.4495879815868635, Val Loss: 4.432399239185758, Val MAE: 1.2956527471542358\n",
      "Epoch 1533/2000, Train Loss: 5.449273107050232, Val Loss: 4.4321751937387495, Val MAE: 1.295654535293579\n",
      "Epoch 1534/2000, Train Loss: 5.4489220042726885, Val Loss: 4.431971023953336, Val MAE: 1.2956594228744507\n",
      "Epoch 1535/2000, Train Loss: 5.448623996253253, Val Loss: 4.43173403148576, Val MAE: 1.2956537008285522\n",
      "Epoch 1536/2000, Train Loss: 5.4482140327567485, Val Loss: 4.431512715204025, Val MAE: 1.2957589626312256\n",
      "Epoch 1537/2000, Train Loss: 5.447912581740112, Val Loss: 4.431331039730488, Val MAE: 1.2958272695541382\n",
      "Epoch 1538/2000, Train Loss: 5.447565759134099, Val Loss: 4.431071654654394, Val MAE: 1.2958413362503052\n",
      "Epoch 1539/2000, Train Loss: 5.447249813432448, Val Loss: 4.4309225548088085, Val MAE: 1.2958654165267944\n",
      "Epoch 1540/2000, Train Loss: 5.446907731821935, Val Loss: 4.430708696734248, Val MAE: 1.2958948612213135\n",
      "Epoch 1541/2000, Train Loss: 5.4465646807766515, Val Loss: 4.4305102259624665, Val MAE: 1.2959206104278564\n",
      "Epoch 1542/2000, Train Loss: 5.446257704148622, Val Loss: 4.43028133451704, Val MAE: 1.2959195375442505\n",
      "Epoch 1543/2000, Train Loss: 5.445948815572213, Val Loss: 4.430088304065344, Val MAE: 1.295965552330017\n",
      "Epoch 1544/2000, Train Loss: 5.445606934975737, Val Loss: 4.429979596304612, Val MAE: 1.296040654182434\n",
      "Epoch 1545/2000, Train Loss: 5.4452127278093725, Val Loss: 4.429671460953284, Val MAE: 1.2959775924682617\n",
      "Epoch 1546/2000, Train Loss: 5.444841176231002, Val Loss: 4.429409870376268, Val MAE: 1.2959537506103516\n",
      "Epoch 1547/2000, Train Loss: 5.444557011289222, Val Loss: 4.429192855461376, Val MAE: 1.2959537506103516\n",
      "Epoch 1548/2000, Train Loss: 5.444228030933292, Val Loss: 4.428966554471358, Val MAE: 1.2959160804748535\n",
      "Epoch 1549/2000, Train Loss: 5.443929969860061, Val Loss: 4.428730315298546, Val MAE: 1.295953631401062\n",
      "Epoch 1550/2000, Train Loss: 5.4434435919756154, Val Loss: 4.42854321941616, Val MAE: 1.2961312532424927\n",
      "Epoch 1551/2000, Train Loss: 5.443110769768745, Val Loss: 4.428346084891341, Val MAE: 1.2960962057113647\n",
      "Epoch 1552/2000, Train Loss: 5.442801737397141, Val Loss: 4.428131420957291, Val MAE: 1.2961360216140747\n",
      "Epoch 1553/2000, Train Loss: 5.442485283964201, Val Loss: 4.427930959730636, Val MAE: 1.2961398363113403\n",
      "Epoch 1554/2000, Train Loss: 5.4421903825032665, Val Loss: 4.4276734577623875, Val MAE: 1.296086072921753\n",
      "Epoch 1555/2000, Train Loss: 5.441872354763658, Val Loss: 4.427395571287223, Val MAE: 1.2962391376495361\n",
      "Epoch 1556/2000, Train Loss: 5.441477776867726, Val Loss: 4.427287516701878, Val MAE: 1.2963534593582153\n",
      "Epoch 1557/2000, Train Loss: 5.441090018325454, Val Loss: 4.427073635641984, Val MAE: 1.2964454889297485\n",
      "Epoch 1558/2000, Train Loss: 5.440713641763219, Val Loss: 4.426740030029158, Val MAE: 1.2963522672653198\n",
      "Epoch 1559/2000, Train Loss: 5.440459349747429, Val Loss: 4.426514177024364, Val MAE: 1.2963656187057495\n",
      "Epoch 1560/2000, Train Loss: 5.44004173741614, Val Loss: 4.426343876111695, Val MAE: 1.2964348793029785\n",
      "Epoch 1561/2000, Train Loss: 5.439751692348775, Val Loss: 4.4261600704409005, Val MAE: 1.2964428663253784\n",
      "Epoch 1562/2000, Train Loss: 5.439374802333368, Val Loss: 4.425898325419801, Val MAE: 1.2964208126068115\n",
      "Epoch 1563/2000, Train Loss: 5.4391169780953765, Val Loss: 4.425734382715282, Val MAE: 1.2964328527450562\n",
      "Epoch 1564/2000, Train Loss: 5.438784686001864, Val Loss: 4.425559520744902, Val MAE: 1.2964931726455688\n",
      "Epoch 1565/2000, Train Loss: 5.438551678424613, Val Loss: 4.425373189644081, Val MAE: 1.2965368032455444\n",
      "Epoch 1566/2000, Train Loss: 5.438205173494373, Val Loss: 4.425201208877751, Val MAE: 1.29657781124115\n",
      "Epoch 1567/2000, Train Loss: 5.437946330902534, Val Loss: 4.425042410605536, Val MAE: 1.2966026067733765\n",
      "Epoch 1568/2000, Train Loss: 5.437617105900675, Val Loss: 4.42481154948473, Val MAE: 1.2965915203094482\n",
      "Epoch 1569/2000, Train Loss: 5.437291449413532, Val Loss: 4.424609675299465, Val MAE: 1.2965840101242065\n",
      "Epoch 1570/2000, Train Loss: 5.436978783180464, Val Loss: 4.424407759367481, Val MAE: 1.2966539859771729\n",
      "Epoch 1571/2000, Train Loss: 5.4366386185831095, Val Loss: 4.4242313535781355, Val MAE: 1.2966375350952148\n",
      "Epoch 1572/2000, Train Loss: 5.436332280470849, Val Loss: 4.423989781795994, Val MAE: 1.2966197729110718\n",
      "Epoch 1573/2000, Train Loss: 5.4360387521812354, Val Loss: 4.423765919128741, Val MAE: 1.296634554862976\n",
      "Epoch 1574/2000, Train Loss: 5.435749225419525, Val Loss: 4.423623073194909, Val MAE: 1.2966703176498413\n",
      "Epoch 1575/2000, Train Loss: 5.43542327651984, Val Loss: 4.423396255650858, Val MAE: 1.2966686487197876\n",
      "Epoch 1576/2000, Train Loss: 5.435139703459462, Val Loss: 4.4231885137872435, Val MAE: 1.2966606616973877\n",
      "Epoch 1577/2000, Train Loss: 5.434820172906408, Val Loss: 4.422983503459006, Val MAE: 1.2966866493225098\n",
      "Epoch 1578/2000, Train Loss: 5.434557419780798, Val Loss: 4.422764407626287, Val MAE: 1.2967407703399658\n",
      "Epoch 1579/2000, Train Loss: 5.43421913066745, Val Loss: 4.422607654661644, Val MAE: 1.2967522144317627\n",
      "Epoch 1580/2000, Train Loss: 5.433967028712927, Val Loss: 4.422445043616407, Val MAE: 1.2968053817749023\n",
      "Epoch 1581/2000, Train Loss: 5.433610396922201, Val Loss: 4.422196510199487, Val MAE: 1.2967973947525024\n",
      "Epoch 1582/2000, Train Loss: 5.433289868213752, Val Loss: 4.422009910490569, Val MAE: 1.296868085861206\n",
      "Epoch 1583/2000, Train Loss: 5.432949186827143, Val Loss: 4.421846691747819, Val MAE: 1.296900749206543\n",
      "Epoch 1584/2000, Train Loss: 5.432669768339905, Val Loss: 4.421612695234967, Val MAE: 1.2969655990600586\n",
      "Epoch 1585/2000, Train Loss: 5.432307106507197, Val Loss: 4.42146896996836, Val MAE: 1.296988844871521\n",
      "Epoch 1586/2000, Train Loss: 5.431976986060796, Val Loss: 4.421245612579537, Val MAE: 1.2969926595687866\n",
      "Epoch 1587/2000, Train Loss: 5.431635823308047, Val Loss: 4.421038584997804, Val MAE: 1.297007441520691\n",
      "Epoch 1588/2000, Train Loss: 5.431343615135959, Val Loss: 4.420852502629043, Val MAE: 1.2970120906829834\n",
      "Epoch 1589/2000, Train Loss: 5.43101828910279, Val Loss: 4.420661455357638, Val MAE: 1.297050952911377\n",
      "Epoch 1590/2000, Train Loss: 5.43070724149572, Val Loss: 4.42046087810139, Val MAE: 1.2970718145370483\n",
      "Epoch 1591/2000, Train Loss: 5.430383212530791, Val Loss: 4.4202859545551885, Val MAE: 1.2970941066741943\n",
      "Epoch 1592/2000, Train Loss: 5.430099896207108, Val Loss: 4.420073010754867, Val MAE: 1.2970786094665527\n",
      "Epoch 1593/2000, Train Loss: 5.429749230900969, Val Loss: 4.4198661462059174, Val MAE: 1.297122836112976\n",
      "Epoch 1594/2000, Train Loss: 5.429429014793406, Val Loss: 4.4197119877094355, Val MAE: 1.297156572341919\n",
      "Epoch 1595/2000, Train Loss: 5.429105248832961, Val Loss: 4.419471309680169, Val MAE: 1.2972073554992676\n",
      "Epoch 1596/2000, Train Loss: 5.428777275286018, Val Loss: 4.419279235484093, Val MAE: 1.29728364944458\n",
      "Epoch 1597/2000, Train Loss: 5.428452659332606, Val Loss: 4.419086882378172, Val MAE: 1.297302484512329\n",
      "Epoch 1598/2000, Train Loss: 5.428141475289939, Val Loss: 4.418932243524574, Val MAE: 1.2973471879959106\n",
      "Epoch 1599/2000, Train Loss: 5.42782958456425, Val Loss: 4.418753043638439, Val MAE: 1.2973507642745972\n",
      "Epoch 1600/2000, Train Loss: 5.427455444025378, Val Loss: 4.4185222922347664, Val MAE: 1.2973923683166504\n",
      "Epoch 1601/2000, Train Loss: 5.427138861678187, Val Loss: 4.418309217996485, Val MAE: 1.2975406646728516\n",
      "Epoch 1602/2000, Train Loss: 5.426824487048218, Val Loss: 4.418126262571868, Val MAE: 1.2976192235946655\n",
      "Epoch 1603/2000, Train Loss: 5.4264655896087195, Val Loss: 4.417927151167486, Val MAE: 1.297574758529663\n",
      "Epoch 1604/2000, Train Loss: 5.4261832145046505, Val Loss: 4.417775031595718, Val MAE: 1.2976518869400024\n",
      "Epoch 1605/2000, Train Loss: 5.4258486422301955, Val Loss: 4.417562923891338, Val MAE: 1.297680139541626\n",
      "Epoch 1606/2000, Train Loss: 5.425552839178243, Val Loss: 4.417368436516739, Val MAE: 1.2976582050323486\n",
      "Epoch 1607/2000, Train Loss: 5.425257549134653, Val Loss: 4.417224612433141, Val MAE: 1.2977725267410278\n",
      "Epoch 1608/2000, Train Loss: 5.424912479871649, Val Loss: 4.41703466643968, Val MAE: 1.2977648973464966\n",
      "Epoch 1609/2000, Train Loss: 5.424641275341184, Val Loss: 4.416819425079766, Val MAE: 1.297739028930664\n",
      "Epoch 1610/2000, Train Loss: 5.424298185182879, Val Loss: 4.416628043717287, Val MAE: 1.2977334260940552\n",
      "Epoch 1611/2000, Train Loss: 5.423998724269899, Val Loss: 4.416421698843401, Val MAE: 1.2977391481399536\n",
      "Epoch 1612/2000, Train Loss: 5.423726554722443, Val Loss: 4.4161940241892506, Val MAE: 1.2977325916290283\n",
      "Epoch 1613/2000, Train Loss: 5.423386286165732, Val Loss: 4.416046288586038, Val MAE: 1.2977962493896484\n",
      "Epoch 1614/2000, Train Loss: 5.423079951030671, Val Loss: 4.415790774737756, Val MAE: 1.2977813482284546\n",
      "Epoch 1615/2000, Train Loss: 5.42274480002706, Val Loss: 4.415633019804955, Val MAE: 1.2978076934814453\n",
      "Epoch 1616/2000, Train Loss: 5.422424957580773, Val Loss: 4.415432606907341, Val MAE: 1.2978273630142212\n",
      "Epoch 1617/2000, Train Loss: 5.422111154087848, Val Loss: 4.415202305331005, Val MAE: 1.2978999614715576\n",
      "Epoch 1618/2000, Train Loss: 5.421716622128739, Val Loss: 4.4150286590255154, Val MAE: 1.297938585281372\n",
      "Epoch 1619/2000, Train Loss: 5.42139285972678, Val Loss: 4.4147425545951515, Val MAE: 1.2979178428649902\n",
      "Epoch 1620/2000, Train Loss: 5.421080491274312, Val Loss: 4.414573228077626, Val MAE: 1.2979203462600708\n",
      "Epoch 1621/2000, Train Loss: 5.420822186175648, Val Loss: 4.414351228278453, Val MAE: 1.297925591468811\n",
      "Epoch 1622/2000, Train Loss: 5.420460065920285, Val Loss: 4.41419216983431, Val MAE: 1.2979453802108765\n",
      "Epoch 1623/2000, Train Loss: 5.420188100763383, Val Loss: 4.414008376283909, Val MAE: 1.2980037927627563\n",
      "Epoch 1624/2000, Train Loss: 5.419855857315995, Val Loss: 4.413800311417091, Val MAE: 1.297977328300476\n",
      "Epoch 1625/2000, Train Loss: 5.419519947212457, Val Loss: 4.41357700092586, Val MAE: 1.2980414628982544\n",
      "Epoch 1626/2000, Train Loss: 5.419211675861555, Val Loss: 4.413472274180473, Val MAE: 1.2981189489364624\n",
      "Epoch 1627/2000, Train Loss: 5.418886409200516, Val Loss: 4.413236498058311, Val MAE: 1.2980941534042358\n",
      "Epoch 1628/2000, Train Loss: 5.418566861178529, Val Loss: 4.41304183121272, Val MAE: 1.2981202602386475\n",
      "Epoch 1629/2000, Train Loss: 5.418282023723284, Val Loss: 4.412823025424649, Val MAE: 1.2981135845184326\n",
      "Epoch 1630/2000, Train Loss: 5.4179521336484315, Val Loss: 4.4126374990217325, Val MAE: 1.2980786561965942\n",
      "Epoch 1631/2000, Train Loss: 5.417611062931076, Val Loss: 4.412317171974445, Val MAE: 1.2980424165725708\n",
      "Epoch 1632/2000, Train Loss: 5.417383023162387, Val Loss: 4.412127645208141, Val MAE: 1.298020362854004\n",
      "Epoch 1633/2000, Train Loss: 5.417056273249467, Val Loss: 4.412020689061308, Val MAE: 1.298130989074707\n",
      "Epoch 1634/2000, Train Loss: 5.416727949191628, Val Loss: 4.411802330730469, Val MAE: 1.2981723546981812\n",
      "Epoch 1635/2000, Train Loss: 5.416413718033355, Val Loss: 4.411523695559953, Val MAE: 1.2981164455413818\n",
      "Epoch 1636/2000, Train Loss: 5.416089412638743, Val Loss: 4.411342665248029, Val MAE: 1.2981531620025635\n",
      "Epoch 1637/2000, Train Loss: 5.4157818586564295, Val Loss: 4.411161307415624, Val MAE: 1.2981421947479248\n",
      "Epoch 1638/2000, Train Loss: 5.415505077614066, Val Loss: 4.410917734700864, Val MAE: 1.298134684562683\n",
      "Epoch 1639/2000, Train Loss: 5.415176662531766, Val Loss: 4.4107671060665385, Val MAE: 1.2981668710708618\n",
      "Epoch 1640/2000, Train Loss: 5.41487336668263, Val Loss: 4.410538814011521, Val MAE: 1.2981456518173218\n",
      "Epoch 1641/2000, Train Loss: 5.414575864049117, Val Loss: 4.410365652122835, Val MAE: 1.2981524467468262\n",
      "Epoch 1642/2000, Train Loss: 5.414196907584347, Val Loss: 4.4101402556802345, Val MAE: 1.2982349395751953\n",
      "Epoch 1643/2000, Train Loss: 5.413875822458047, Val Loss: 4.409925733659211, Val MAE: 1.298231601715088\n",
      "Epoch 1644/2000, Train Loss: 5.4136147777293395, Val Loss: 4.409710695025489, Val MAE: 1.2982195615768433\n",
      "Epoch 1645/2000, Train Loss: 5.413297703987546, Val Loss: 4.409472421468712, Val MAE: 1.2981514930725098\n",
      "Epoch 1646/2000, Train Loss: 5.41303568277786, Val Loss: 4.4092802339886115, Val MAE: 1.2981599569320679\n",
      "Epoch 1647/2000, Train Loss: 5.4127431219921345, Val Loss: 4.409102314405554, Val MAE: 1.2981798648834229\n",
      "Epoch 1648/2000, Train Loss: 5.412419372206627, Val Loss: 4.4089711962018425, Val MAE: 1.2982569932937622\n",
      "Epoch 1649/2000, Train Loss: 5.4121135261359665, Val Loss: 4.408705609285925, Val MAE: 1.2982392311096191\n",
      "Epoch 1650/2000, Train Loss: 5.411839913238179, Val Loss: 4.408527951587843, Val MAE: 1.2982145547866821\n",
      "Epoch 1651/2000, Train Loss: 5.411639989959013, Val Loss: 4.408329775366258, Val MAE: 1.2982052564620972\n",
      "Epoch 1652/2000, Train Loss: 5.411364556329383, Val Loss: 4.4081857318718605, Val MAE: 1.2982255220413208\n",
      "Epoch 1653/2000, Train Loss: 5.411017232866429, Val Loss: 4.408004737134994, Val MAE: 1.2982730865478516\n",
      "Epoch 1654/2000, Train Loss: 5.410690519994117, Val Loss: 4.407881658025614, Val MAE: 1.2983355522155762\n",
      "Epoch 1655/2000, Train Loss: 5.41037356352062, Val Loss: 4.407703661965573, Val MAE: 1.2983139753341675\n",
      "Epoch 1656/2000, Train Loss: 5.410049581806728, Val Loss: 4.407489215154348, Val MAE: 1.2983797788619995\n",
      "Epoch 1657/2000, Train Loss: 5.409759727290236, Val Loss: 4.407350458239946, Val MAE: 1.298401117324829\n",
      "Epoch 1658/2000, Train Loss: 5.409458776786822, Val Loss: 4.407152677872988, Val MAE: 1.2984169721603394\n",
      "Epoch 1659/2000, Train Loss: 5.409175637295644, Val Loss: 4.406997212415605, Val MAE: 1.2984081506729126\n",
      "Epoch 1660/2000, Train Loss: 5.408918202449056, Val Loss: 4.406960115916147, Val MAE: 1.2985727787017822\n",
      "Epoch 1661/2000, Train Loss: 5.40856473546177, Val Loss: 4.406784026406881, Val MAE: 1.2985434532165527\n",
      "Epoch 1662/2000, Train Loss: 5.408306212729321, Val Loss: 4.406641862453438, Val MAE: 1.2986420392990112\n",
      "Epoch 1663/2000, Train Loss: 5.408023204146461, Val Loss: 4.406484269837695, Val MAE: 1.2986383438110352\n",
      "Epoch 1664/2000, Train Loss: 5.407741572996008, Val Loss: 4.4062753551588285, Val MAE: 1.2986103296279907\n",
      "Epoch 1665/2000, Train Loss: 5.407461929515389, Val Loss: 4.406112647948302, Val MAE: 1.2987028360366821\n",
      "Epoch 1666/2000, Train Loss: 5.407161783169212, Val Loss: 4.405912156227067, Val MAE: 1.2986730337142944\n",
      "Epoch 1667/2000, Train Loss: 5.406861157792152, Val Loss: 4.405706449944203, Val MAE: 1.2986140251159668\n",
      "Epoch 1668/2000, Train Loss: 5.406605469484989, Val Loss: 4.405511647110849, Val MAE: 1.2986611127853394\n",
      "Epoch 1669/2000, Train Loss: 5.406298807550479, Val Loss: 4.405307214583938, Val MAE: 1.2986732721328735\n",
      "Epoch 1670/2000, Train Loss: 5.406013204090631, Val Loss: 4.405210120541843, Val MAE: 1.2987951040267944\n",
      "Epoch 1671/2000, Train Loss: 5.405567888650674, Val Loss: 4.40496236351062, Val MAE: 1.2988035678863525\n",
      "Epoch 1672/2000, Train Loss: 5.4052748890388935, Val Loss: 4.404774240973428, Val MAE: 1.2988146543502808\n",
      "Epoch 1673/2000, Train Loss: 5.40493954991065, Val Loss: 4.4045746823230125, Val MAE: 1.2988179922103882\n",
      "Epoch 1674/2000, Train Loss: 5.404709332975151, Val Loss: 4.404370682938831, Val MAE: 1.2987931966781616\n",
      "Epoch 1675/2000, Train Loss: 5.404420490342532, Val Loss: 4.404248518596484, Val MAE: 1.2988485097885132\n",
      "Epoch 1676/2000, Train Loss: 5.404196925208558, Val Loss: 4.404152108224358, Val MAE: 1.2989975214004517\n",
      "Epoch 1677/2000, Train Loss: 5.403838846382649, Val Loss: 4.403931815183069, Val MAE: 1.2989614009857178\n",
      "Epoch 1678/2000, Train Loss: 5.403575492843359, Val Loss: 4.403712230897325, Val MAE: 1.298923134803772\n",
      "Epoch 1679/2000, Train Loss: 5.4032803626688075, Val Loss: 4.403492839388021, Val MAE: 1.2989649772644043\n",
      "Epoch 1680/2000, Train Loss: 5.40302164001413, Val Loss: 4.403368859943442, Val MAE: 1.2989801168441772\n",
      "Epoch 1681/2000, Train Loss: 5.402715066718279, Val Loss: 4.40318833608327, Val MAE: 1.298996090888977\n",
      "Epoch 1682/2000, Train Loss: 5.402441603014071, Val Loss: 4.403038221597671, Val MAE: 1.2990425825119019\n",
      "Epoch 1683/2000, Train Loss: 5.4021733114483235, Val Loss: 4.402895684575471, Val MAE: 1.2990515232086182\n",
      "Epoch 1684/2000, Train Loss: 5.401871995007167, Val Loss: 4.402710622830654, Val MAE: 1.2990577220916748\n",
      "Epoch 1685/2000, Train Loss: 5.401601015727871, Val Loss: 4.402547897621402, Val MAE: 1.299059510231018\n",
      "Epoch 1686/2000, Train Loss: 5.401347996745375, Val Loss: 4.402373564947308, Val MAE: 1.299045443534851\n",
      "Epoch 1687/2000, Train Loss: 5.401047930465139, Val Loss: 4.402308173325118, Val MAE: 1.299127459526062\n",
      "Epoch 1688/2000, Train Loss: 5.400772967364442, Val Loss: 4.402008202249609, Val MAE: 1.2990320920944214\n",
      "Epoch 1689/2000, Train Loss: 5.40050892711818, Val Loss: 4.401878094626224, Val MAE: 1.2990797758102417\n",
      "Epoch 1690/2000, Train Loss: 5.400236621478843, Val Loss: 4.401705219168362, Val MAE: 1.2990914583206177\n",
      "Epoch 1691/2000, Train Loss: 5.399984057886549, Val Loss: 4.401470199227333, Val MAE: 1.2990753650665283\n",
      "Epoch 1692/2000, Train Loss: 5.399710367945512, Val Loss: 4.401333807256278, Val MAE: 1.2991243600845337\n",
      "Epoch 1693/2000, Train Loss: 5.399366126475373, Val Loss: 4.401122573417003, Val MAE: 1.2991383075714111\n",
      "Epoch 1694/2000, Train Loss: 5.399084916472273, Val Loss: 4.400992818352744, Val MAE: 1.299176812171936\n",
      "Epoch 1695/2000, Train Loss: 5.398808057466711, Val Loss: 4.400811647148583, Val MAE: 1.299242615699768\n",
      "Epoch 1696/2000, Train Loss: 5.398580055203658, Val Loss: 4.400552510135756, Val MAE: 1.2992504835128784\n",
      "Epoch 1697/2000, Train Loss: 5.3982464736643445, Val Loss: 4.400443679540176, Val MAE: 1.2992993593215942\n",
      "Epoch 1698/2000, Train Loss: 5.397968286251602, Val Loss: 4.400287013448129, Val MAE: 1.2994036674499512\n",
      "Epoch 1699/2000, Train Loss: 5.397692680540855, Val Loss: 4.400107790165999, Val MAE: 1.2993826866149902\n",
      "Epoch 1700/2000, Train Loss: 5.397442818981983, Val Loss: 4.399936683393839, Val MAE: 1.2993991374969482\n",
      "Epoch 1701/2000, Train Loss: 5.397119789547318, Val Loss: 4.399751820831787, Val MAE: 1.2995119094848633\n",
      "Epoch 1702/2000, Train Loss: 5.396829000806873, Val Loss: 4.399607486236753, Val MAE: 1.299526333808899\n",
      "Epoch 1703/2000, Train Loss: 5.3966085876728656, Val Loss: 4.399418652339245, Val MAE: 1.2994372844696045\n",
      "Epoch 1704/2000, Train Loss: 5.396339424702577, Val Loss: 4.399243092607326, Val MAE: 1.2995047569274902\n",
      "Epoch 1705/2000, Train Loss: 5.396054259148834, Val Loss: 4.39907687515255, Val MAE: 1.2994890213012695\n",
      "Epoch 1706/2000, Train Loss: 5.395770513198593, Val Loss: 4.398928193783197, Val MAE: 1.2994893789291382\n",
      "Epoch 1707/2000, Train Loss: 5.395537493105983, Val Loss: 4.398758976032415, Val MAE: 1.2994853258132935\n",
      "Epoch 1708/2000, Train Loss: 5.395236135823109, Val Loss: 4.3986114513217, Val MAE: 1.2995049953460693\n",
      "Epoch 1709/2000, Train Loss: 5.3949775902839985, Val Loss: 4.398482712962496, Val MAE: 1.2995461225509644\n",
      "Epoch 1710/2000, Train Loss: 5.394689140338426, Val Loss: 4.3983437559501395, Val MAE: 1.2996565103530884\n",
      "Epoch 1711/2000, Train Loss: 5.394401301845928, Val Loss: 4.398199461177578, Val MAE: 1.299597978591919\n",
      "Epoch 1712/2000, Train Loss: 5.394119723195784, Val Loss: 4.39801347729728, Val MAE: 1.2996094226837158\n",
      "Epoch 1713/2000, Train Loss: 5.393863323262136, Val Loss: 4.397840846640857, Val MAE: 1.2996176481246948\n",
      "Epoch 1714/2000, Train Loss: 5.393577382797464, Val Loss: 4.397700510039105, Val MAE: 1.2997106313705444\n",
      "Epoch 1715/2000, Train Loss: 5.393305030897806, Val Loss: 4.397534527201352, Val MAE: 1.299750804901123\n",
      "Epoch 1716/2000, Train Loss: 5.393012827092951, Val Loss: 4.397303294431506, Val MAE: 1.2996816635131836\n",
      "Epoch 1717/2000, Train Loss: 5.392667341345525, Val Loss: 4.397126098575555, Val MAE: 1.299784779548645\n",
      "Epoch 1718/2000, Train Loss: 5.39234081408062, Val Loss: 4.396939671109981, Val MAE: 1.2997742891311646\n",
      "Epoch 1719/2000, Train Loss: 5.392088311498169, Val Loss: 4.396799917108431, Val MAE: 1.299843192100525\n",
      "Epoch 1720/2000, Train Loss: 5.391844085954746, Val Loss: 4.396633547521013, Val MAE: 1.2998178005218506\n",
      "Epoch 1721/2000, Train Loss: 5.391601754051698, Val Loss: 4.396462969259014, Val MAE: 1.2998294830322266\n",
      "Epoch 1722/2000, Train Loss: 5.391286817625711, Val Loss: 4.396267978764895, Val MAE: 1.299816608428955\n",
      "Epoch 1723/2000, Train Loss: 5.391036598701813, Val Loss: 4.395967030595607, Val MAE: 1.2997702360153198\n",
      "Epoch 1724/2000, Train Loss: 5.390695041624306, Val Loss: 4.3958417221786465, Val MAE: 1.2998006343841553\n",
      "Epoch 1725/2000, Train Loss: 5.390434470103505, Val Loss: 4.395672407136189, Val MAE: 1.2998223304748535\n",
      "Epoch 1726/2000, Train Loss: 5.390138384728128, Val Loss: 4.395534354143256, Val MAE: 1.2998595237731934\n",
      "Epoch 1727/2000, Train Loss: 5.389880963225863, Val Loss: 4.395373464217336, Val MAE: 1.299837350845337\n",
      "Epoch 1728/2000, Train Loss: 5.389614442633806, Val Loss: 4.395215813099869, Val MAE: 1.299921989440918\n",
      "Epoch 1729/2000, Train Loss: 5.389299587365568, Val Loss: 4.3949914122191, Val MAE: 1.2998707294464111\n",
      "Epoch 1730/2000, Train Loss: 5.389083336942717, Val Loss: 4.394815538483342, Val MAE: 1.2998820543289185\n",
      "Epoch 1731/2000, Train Loss: 5.388745000113302, Val Loss: 4.394715067885054, Val MAE: 1.300036072731018\n",
      "Epoch 1732/2000, Train Loss: 5.388404642387194, Val Loss: 4.394497433821047, Val MAE: 1.3000391721725464\n",
      "Epoch 1733/2000, Train Loss: 5.388130445169788, Val Loss: 4.3943271961972465, Val MAE: 1.300032377243042\n",
      "Epoch 1734/2000, Train Loss: 5.387850994332674, Val Loss: 4.394173924120392, Val MAE: 1.3000247478485107\n",
      "Epoch 1735/2000, Train Loss: 5.3875926531639, Val Loss: 4.394007446000895, Val MAE: 1.3001033067703247\n",
      "Epoch 1736/2000, Train Loss: 5.387310149064705, Val Loss: 4.39383218748363, Val MAE: 1.3000857830047607\n",
      "Epoch 1737/2000, Train Loss: 5.387031523472904, Val Loss: 4.393687049258412, Val MAE: 1.3001147508621216\n",
      "Epoch 1738/2000, Train Loss: 5.386738505848814, Val Loss: 4.393521879836331, Val MAE: 1.3000714778900146\n",
      "Epoch 1739/2000, Train Loss: 5.386556959216922, Val Loss: 4.393317719284943, Val MAE: 1.3000638484954834\n",
      "Epoch 1740/2000, Train Loss: 5.3861812213220945, Val Loss: 4.3932082541580275, Val MAE: 1.300143837928772\n",
      "Epoch 1741/2000, Train Loss: 5.385920401667643, Val Loss: 4.393006457259336, Val MAE: 1.3001549243927002\n",
      "Epoch 1742/2000, Train Loss: 5.385617374241595, Val Loss: 4.392821064028214, Val MAE: 1.300208330154419\n",
      "Epoch 1743/2000, Train Loss: 5.385282076197693, Val Loss: 4.392726087382459, Val MAE: 1.30023992061615\n",
      "Epoch 1744/2000, Train Loss: 5.384992607251275, Val Loss: 4.392532074521846, Val MAE: 1.3002527952194214\n",
      "Epoch 1745/2000, Train Loss: 5.384680515875487, Val Loss: 4.392351770166337, Val MAE: 1.300426721572876\n",
      "Epoch 1746/2000, Train Loss: 5.384403152090562, Val Loss: 4.392210134183328, Val MAE: 1.3004533052444458\n",
      "Epoch 1747/2000, Train Loss: 5.384127823143106, Val Loss: 4.392072873181245, Val MAE: 1.300453543663025\n",
      "Epoch 1748/2000, Train Loss: 5.383853653554516, Val Loss: 4.391910912014368, Val MAE: 1.3004759550094604\n",
      "Epoch 1749/2000, Train Loss: 5.383637308215108, Val Loss: 4.391784865602734, Val MAE: 1.3004710674285889\n",
      "Epoch 1750/2000, Train Loss: 5.3833494859278765, Val Loss: 4.3915737651933835, Val MAE: 1.300460934638977\n",
      "Epoch 1751/2000, Train Loss: 5.383078920630943, Val Loss: 4.391482415490263, Val MAE: 1.3004950284957886\n",
      "Epoch 1752/2000, Train Loss: 5.382794946957799, Val Loss: 4.3913187666671485, Val MAE: 1.3005540370941162\n",
      "Epoch 1753/2000, Train Loss: 5.382565438029643, Val Loss: 4.391202667519802, Val MAE: 1.3006495237350464\n",
      "Epoch 1754/2000, Train Loss: 5.382280170189639, Val Loss: 4.391080199569229, Val MAE: 1.3006902933120728\n",
      "Epoch 1755/2000, Train Loss: 5.382014465461431, Val Loss: 4.39094903506632, Val MAE: 1.3007376194000244\n",
      "Epoch 1756/2000, Train Loss: 5.381745234960778, Val Loss: 4.39078090460751, Val MAE: 1.3007595539093018\n",
      "Epoch 1757/2000, Train Loss: 5.38145994541603, Val Loss: 4.390619711002966, Val MAE: 1.300769329071045\n",
      "Epoch 1758/2000, Train Loss: 5.381202141983053, Val Loss: 4.390445319803681, Val MAE: 1.30072820186615\n",
      "Epoch 1759/2000, Train Loss: 5.38092053179178, Val Loss: 4.3902775867952135, Val MAE: 1.300795316696167\n",
      "Epoch 1760/2000, Train Loss: 5.380666298005766, Val Loss: 4.390099075600856, Val MAE: 1.3007618188858032\n",
      "Epoch 1761/2000, Train Loss: 5.380383498997022, Val Loss: 4.390005149921095, Val MAE: 1.3007744550704956\n",
      "Epoch 1762/2000, Train Loss: 5.380131041300022, Val Loss: 4.389855915965057, Val MAE: 1.3008335828781128\n",
      "Epoch 1763/2000, Train Loss: 5.379862601592065, Val Loss: 4.389696093736671, Val MAE: 1.300817847251892\n",
      "Epoch 1764/2000, Train Loss: 5.379527574640116, Val Loss: 4.389503292751124, Val MAE: 1.3008402585983276\n",
      "Epoch 1765/2000, Train Loss: 5.379223426937895, Val Loss: 4.389350187004082, Val MAE: 1.3008973598480225\n",
      "Epoch 1766/2000, Train Loss: 5.3789640856221865, Val Loss: 4.389205437851703, Val MAE: 1.3008942604064941\n",
      "Epoch 1767/2000, Train Loss: 5.378705763913883, Val Loss: 4.389040588346992, Val MAE: 1.3009278774261475\n",
      "Epoch 1768/2000, Train Loss: 5.378453808367818, Val Loss: 4.388855818312938, Val MAE: 1.3009275197982788\n",
      "Epoch 1769/2000, Train Loss: 5.378232378957431, Val Loss: 4.388691385336748, Val MAE: 1.3008921146392822\n",
      "Epoch 1770/2000, Train Loss: 5.378023641889099, Val Loss: 4.388592671808295, Val MAE: 1.3009432554244995\n",
      "Epoch 1771/2000, Train Loss: 5.377603477829994, Val Loss: 4.388408950205863, Val MAE: 1.3009744882583618\n",
      "Epoch 1772/2000, Train Loss: 5.377340547750569, Val Loss: 4.388236621256889, Val MAE: 1.3009840250015259\n",
      "Epoch 1773/2000, Train Loss: 5.3771296678113485, Val Loss: 4.388050693273544, Val MAE: 1.300971508026123\n",
      "Epoch 1774/2000, Train Loss: 5.376821830615987, Val Loss: 4.387924587515395, Val MAE: 1.3010209798812866\n",
      "Epoch 1775/2000, Train Loss: 5.376563390042112, Val Loss: 4.387769275762904, Val MAE: 1.3010063171386719\n",
      "Epoch 1776/2000, Train Loss: 5.376276435489732, Val Loss: 4.387630230164904, Val MAE: 1.301056981086731\n",
      "Epoch 1777/2000, Train Loss: 5.376021293379882, Val Loss: 4.387528020753635, Val MAE: 1.3011420965194702\n",
      "Epoch 1778/2000, Train Loss: 5.375712635326661, Val Loss: 4.387375911225484, Val MAE: 1.3011823892593384\n",
      "Epoch 1779/2000, Train Loss: 5.375452670507547, Val Loss: 4.387227304620067, Val MAE: 1.3011656999588013\n",
      "Epoch 1780/2000, Train Loss: 5.375121814263239, Val Loss: 4.38701554227063, Val MAE: 1.3012369871139526\n",
      "Epoch 1781/2000, Train Loss: 5.374840470473187, Val Loss: 4.386844030867412, Val MAE: 1.3012341260910034\n",
      "Epoch 1782/2000, Train Loss: 5.37467471036044, Val Loss: 4.386652278782814, Val MAE: 1.3011964559555054\n",
      "Epoch 1783/2000, Train Loss: 5.374322456129532, Val Loss: 4.386520774509963, Val MAE: 1.301274061203003\n",
      "Epoch 1784/2000, Train Loss: 5.374043618646049, Val Loss: 4.386331562944284, Val MAE: 1.3012195825576782\n",
      "Epoch 1785/2000, Train Loss: 5.373825694359788, Val Loss: 4.386159250116724, Val MAE: 1.3011866807937622\n",
      "Epoch 1786/2000, Train Loss: 5.373589178276839, Val Loss: 4.38611608767134, Val MAE: 1.3012880086898804\n",
      "Epoch 1787/2000, Train Loss: 5.373337470045569, Val Loss: 4.3859732207820175, Val MAE: 1.3013064861297607\n",
      "Epoch 1788/2000, Train Loss: 5.373104467644297, Val Loss: 4.385982902594439, Val MAE: 1.3014179468154907\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1789/2000, Train Loss: 5.372818547479172, Val Loss: 4.38584496714468, Val MAE: 1.3014404773712158\n",
      "Epoch 1790/2000, Train Loss: 5.372598583371539, Val Loss: 4.385730594161927, Val MAE: 1.3014756441116333\n",
      "Epoch 1791/2000, Train Loss: 5.372417833701564, Val Loss: 4.385507527017218, Val MAE: 1.3014295101165771\n",
      "Epoch 1792/2000, Train Loss: 5.372145946507861, Val Loss: 4.385413159207096, Val MAE: 1.30150306224823\n",
      "Epoch 1793/2000, Train Loss: 5.371887225945818, Val Loss: 4.385206847608559, Val MAE: 1.3014408349990845\n",
      "Epoch 1794/2000, Train Loss: 5.371614011042477, Val Loss: 4.385101378996541, Val MAE: 1.3014657497406006\n",
      "Epoch 1795/2000, Train Loss: 5.37133162251321, Val Loss: 4.384955443031206, Val MAE: 1.301513910293579\n",
      "Epoch 1796/2000, Train Loss: 5.371093259252396, Val Loss: 4.384766263497157, Val MAE: 1.301487684249878\n",
      "Epoch 1797/2000, Train Loss: 5.3708480248781205, Val Loss: 4.384650654163886, Val MAE: 1.3014789819717407\n",
      "Epoch 1798/2000, Train Loss: 5.370618861436521, Val Loss: 4.3845118814800665, Val MAE: 1.3015655279159546\n",
      "Epoch 1799/2000, Train Loss: 5.370344757872003, Val Loss: 4.384499777208163, Val MAE: 1.3015751838684082\n",
      "Epoch 1800/2000, Train Loss: 5.37012915768164, Val Loss: 4.3843433411102595, Val MAE: 1.3015540838241577\n",
      "Epoch 1801/2000, Train Loss: 5.369888964974734, Val Loss: 4.384193383450583, Val MAE: 1.3015754222869873\n",
      "Epoch 1802/2000, Train Loss: 5.369627986361085, Val Loss: 4.384078044520589, Val MAE: 1.3015903234481812\n",
      "Epoch 1803/2000, Train Loss: 5.36940066583121, Val Loss: 4.383952772828538, Val MAE: 1.3016146421432495\n",
      "Epoch 1804/2000, Train Loss: 5.369164661604158, Val Loss: 4.38374358816879, Val MAE: 1.3015732765197754\n",
      "Epoch 1805/2000, Train Loss: 5.368909168761006, Val Loss: 4.383609259504033, Val MAE: 1.301592469215393\n",
      "Epoch 1806/2000, Train Loss: 5.368677981672973, Val Loss: 4.383487396775268, Val MAE: 1.301601767539978\n",
      "Epoch 1807/2000, Train Loss: 5.368435749690601, Val Loss: 4.383412669305726, Val MAE: 1.3016563653945923\n",
      "Epoch 1808/2000, Train Loss: 5.368201929969736, Val Loss: 4.383235909737001, Val MAE: 1.3016276359558105\n",
      "Epoch 1809/2000, Train Loss: 5.367977499153106, Val Loss: 4.383219641095072, Val MAE: 1.3016927242279053\n",
      "Epoch 1810/2000, Train Loss: 5.367749935898075, Val Loss: 4.383064126499056, Val MAE: 1.3016926050186157\n",
      "Epoch 1811/2000, Train Loss: 5.367513889535311, Val Loss: 4.382936363192055, Val MAE: 1.301694393157959\n",
      "Epoch 1812/2000, Train Loss: 5.367281510192633, Val Loss: 4.382952279509522, Val MAE: 1.3018196821212769\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1813/2000, Train Loss: 5.367096431866753, Val Loss: 4.382812191675028, Val MAE: 1.301888108253479\n",
      "Epoch 1814/2000, Train Loss: 5.366870591566068, Val Loss: 4.382688510371006, Val MAE: 1.3019005060195923\n",
      "Epoch 1815/2000, Train Loss: 5.366659958941642, Val Loss: 4.382544273443109, Val MAE: 1.3019216060638428\n",
      "Epoch 1816/2000, Train Loss: 5.366348962123876, Val Loss: 4.382567949722132, Val MAE: 1.3020379543304443\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1817/2000, Train Loss: 5.366201557815479, Val Loss: 4.38239765345581, Val MAE: 1.3020414113998413\n",
      "Epoch 1818/2000, Train Loss: 5.365932869620045, Val Loss: 4.382244787019069, Val MAE: 1.3020336627960205\n",
      "Epoch 1819/2000, Train Loss: 5.365692125165802, Val Loss: 4.382132980274403, Val MAE: 1.3020161390304565\n",
      "Epoch 1820/2000, Train Loss: 5.365478684966487, Val Loss: 4.381940299741865, Val MAE: 1.302024245262146\n",
      "Epoch 1821/2000, Train Loss: 5.365226659595239, Val Loss: 4.381807919937795, Val MAE: 1.3020164966583252\n",
      "Epoch 1822/2000, Train Loss: 5.364987696203804, Val Loss: 4.381680227170779, Val MAE: 1.3020286560058594\n",
      "Epoch 1823/2000, Train Loss: 5.3647679706765, Val Loss: 4.381541100073987, Val MAE: 1.30201256275177\n",
      "Epoch 1824/2000, Train Loss: 5.364557614967005, Val Loss: 4.381376371726277, Val MAE: 1.302001714706421\n",
      "Epoch 1825/2000, Train Loss: 5.364276117029799, Val Loss: 4.381257843149928, Val MAE: 1.3020492792129517\n",
      "Epoch 1826/2000, Train Loss: 5.364031805273959, Val Loss: 4.381112533221095, Val MAE: 1.302034616470337\n",
      "Epoch 1827/2000, Train Loss: 5.363795678683505, Val Loss: 4.380922239690315, Val MAE: 1.3020176887512207\n",
      "Epoch 1828/2000, Train Loss: 5.363529216159474, Val Loss: 4.380737605104296, Val MAE: 1.301994800567627\n",
      "Epoch 1829/2000, Train Loss: 5.363296753354118, Val Loss: 4.380591218044438, Val MAE: 1.3020073175430298\n",
      "Epoch 1830/2000, Train Loss: 5.3630603805760595, Val Loss: 4.380440059750099, Val MAE: 1.3019827604293823\n",
      "Epoch 1831/2000, Train Loss: 5.362799315755047, Val Loss: 4.380301909254292, Val MAE: 1.3019976615905762\n",
      "Epoch 1832/2000, Train Loss: 5.362588531450145, Val Loss: 4.3802061221027, Val MAE: 1.3020483255386353\n",
      "Epoch 1833/2000, Train Loss: 5.362317702825228, Val Loss: 4.380056021438809, Val MAE: 1.302079200744629\n",
      "Epoch 1834/2000, Train Loss: 5.362070318940759, Val Loss: 4.379921110596244, Val MAE: 1.302065134048462\n",
      "Epoch 1835/2000, Train Loss: 5.361811763274621, Val Loss: 4.379784649985981, Val MAE: 1.3020731210708618\n",
      "Epoch 1836/2000, Train Loss: 5.361592208751789, Val Loss: 4.379601581382939, Val MAE: 1.3020765781402588\n",
      "Epoch 1837/2000, Train Loss: 5.361422422297774, Val Loss: 4.379446722719613, Val MAE: 1.3020776510238647\n",
      "Epoch 1838/2000, Train Loss: 5.361045012642279, Val Loss: 4.379359108326942, Val MAE: 1.3021613359451294\n",
      "Epoch 1839/2000, Train Loss: 5.3607942834799776, Val Loss: 4.379193287876647, Val MAE: 1.3021687269210815\n",
      "Epoch 1840/2000, Train Loss: 5.360522297538282, Val Loss: 4.379016729790394, Val MAE: 1.302204966545105\n",
      "Epoch 1841/2000, Train Loss: 5.360273278390374, Val Loss: 4.378860330112337, Val MAE: 1.3021944761276245\n",
      "Epoch 1842/2000, Train Loss: 5.360012612872968, Val Loss: 4.378747575841551, Val MAE: 1.3023518323898315\n",
      "Epoch 1843/2000, Train Loss: 5.359755168614588, Val Loss: 4.378596636840678, Val MAE: 1.3023293018341064\n",
      "Epoch 1844/2000, Train Loss: 5.359505950353201, Val Loss: 4.378439662869521, Val MAE: 1.3024110794067383\n",
      "Epoch 1845/2000, Train Loss: 5.359203106917067, Val Loss: 4.378239328068072, Val MAE: 1.3023992776870728\n",
      "Epoch 1846/2000, Train Loss: 5.358964773559667, Val Loss: 4.378093090019827, Val MAE: 1.302403450012207\n",
      "Epoch 1847/2000, Train Loss: 5.358697442219086, Val Loss: 4.377939445531275, Val MAE: 1.302370309829712\n",
      "Epoch 1848/2000, Train Loss: 5.358440741573131, Val Loss: 4.377800168202618, Val MAE: 1.3024097681045532\n",
      "Epoch 1849/2000, Train Loss: 5.358206517822545, Val Loss: 4.377675285724204, Val MAE: 1.302451252937317\n",
      "Epoch 1850/2000, Train Loss: 5.357991399887944, Val Loss: 4.377490350181662, Val MAE: 1.3024324178695679\n",
      "Epoch 1851/2000, Train Loss: 5.357787693210017, Val Loss: 4.377266817604463, Val MAE: 1.3023854494094849\n",
      "Epoch 1852/2000, Train Loss: 5.3574258747587695, Val Loss: 4.3771183322968445, Val MAE: 1.3024629354476929\n",
      "Epoch 1853/2000, Train Loss: 5.357165828326988, Val Loss: 4.376983589731802, Val MAE: 1.3024723529815674\n",
      "Epoch 1854/2000, Train Loss: 5.356891343883129, Val Loss: 4.376910849490503, Val MAE: 1.3025673627853394\n",
      "Epoch 1855/2000, Train Loss: 5.356649269404211, Val Loss: 4.3767350599287065, Val MAE: 1.3025225400924683\n",
      "Epoch 1856/2000, Train Loss: 5.356439308363191, Val Loss: 4.376639917633665, Val MAE: 1.3027679920196533\n",
      "Epoch 1857/2000, Train Loss: 5.356135742443145, Val Loss: 4.3766661297617935, Val MAE: 1.3029170036315918\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1858/2000, Train Loss: 5.355988689444282, Val Loss: 4.376504133958516, Val MAE: 1.3028806447982788\n",
      "Epoch 1859/2000, Train Loss: 5.355730512895933, Val Loss: 4.3764144016532445, Val MAE: 1.3029484748840332\n",
      "Epoch 1860/2000, Train Loss: 5.355474151135138, Val Loss: 4.37622553856354, Val MAE: 1.3028879165649414\n",
      "Epoch 1861/2000, Train Loss: 5.355215713240237, Val Loss: 4.376214841665245, Val MAE: 1.3029892444610596\n",
      "Epoch 1862/2000, Train Loss: 5.355058957310836, Val Loss: 4.376149699443907, Val MAE: 1.3030340671539307\n",
      "Epoch 1863/2000, Train Loss: 5.354764311090415, Val Loss: 4.375989931540226, Val MAE: 1.3030550479888916\n",
      "Epoch 1864/2000, Train Loss: 5.354493036360721, Val Loss: 4.37586996869778, Val MAE: 1.303085446357727\n",
      "Epoch 1865/2000, Train Loss: 5.354287853396246, Val Loss: 4.375769913431228, Val MAE: 1.3032716512680054\n",
      "Epoch 1866/2000, Train Loss: 5.353989332509979, Val Loss: 4.3756410461476465, Val MAE: 1.3033212423324585\n",
      "Epoch 1867/2000, Train Loss: 5.353747540344377, Val Loss: 4.375519264941141, Val MAE: 1.303315281867981\n",
      "Epoch 1868/2000, Train Loss: 5.353511384800784, Val Loss: 4.375353980463321, Val MAE: 1.3032945394515991\n",
      "Epoch 1869/2000, Train Loss: 5.353264270959425, Val Loss: 4.3752166693604835, Val MAE: 1.3033350706100464\n",
      "Epoch 1870/2000, Train Loss: 5.35303748737682, Val Loss: 4.375089784001741, Val MAE: 1.3032948970794678\n",
      "Epoch 1871/2000, Train Loss: 5.352814422048416, Val Loss: 4.3749917896713795, Val MAE: 1.303322196006775\n",
      "Epoch 1872/2000, Train Loss: 5.352511992003248, Val Loss: 4.374839075648879, Val MAE: 1.303403377532959\n",
      "Epoch 1873/2000, Train Loss: 5.352257067259285, Val Loss: 4.3747158004073645, Val MAE: 1.303421974182129\n",
      "Epoch 1874/2000, Train Loss: 5.352024360460051, Val Loss: 4.374505829083638, Val MAE: 1.3034026622772217\n",
      "Epoch 1875/2000, Train Loss: 5.351731016888224, Val Loss: 4.374326511654328, Val MAE: 1.3034124374389648\n",
      "Epoch 1876/2000, Train Loss: 5.351492510519649, Val Loss: 4.374180153035742, Val MAE: 1.303402066230774\n",
      "Epoch 1877/2000, Train Loss: 5.35123349012481, Val Loss: 4.374039335771808, Val MAE: 1.303420066833496\n",
      "Epoch 1878/2000, Train Loss: 5.351003494806173, Val Loss: 4.373791890468184, Val MAE: 1.3033407926559448\n",
      "Epoch 1879/2000, Train Loss: 5.350747586994999, Val Loss: 4.373648655672712, Val MAE: 1.303323745727539\n",
      "Epoch 1880/2000, Train Loss: 5.3505043467681155, Val Loss: 4.373503899363082, Val MAE: 1.3033783435821533\n",
      "Epoch 1881/2000, Train Loss: 5.350275034496872, Val Loss: 4.373385315617239, Val MAE: 1.3033876419067383\n",
      "Epoch 1882/2000, Train Loss: 5.349990135132053, Val Loss: 4.373267511941316, Val MAE: 1.303430438041687\n",
      "Epoch 1883/2000, Train Loss: 5.349754994404041, Val Loss: 4.373062393773259, Val MAE: 1.3033956289291382\n",
      "Epoch 1884/2000, Train Loss: 5.349552092752754, Val Loss: 4.372920927921618, Val MAE: 1.3034018278121948\n",
      "Epoch 1885/2000, Train Loss: 5.3492682002777965, Val Loss: 4.3728480895438535, Val MAE: 1.3034701347351074\n",
      "Epoch 1886/2000, Train Loss: 5.349040477214384, Val Loss: 4.372767753112973, Val MAE: 1.303486943244934\n",
      "Epoch 1887/2000, Train Loss: 5.348794691274739, Val Loss: 4.372676919482824, Val MAE: 1.3034709692001343\n",
      "Epoch 1888/2000, Train Loss: 5.348529142796427, Val Loss: 4.372460438886026, Val MAE: 1.3034175634384155\n",
      "Epoch 1889/2000, Train Loss: 5.348318047649663, Val Loss: 4.37230755543615, Val MAE: 1.3033607006072998\n",
      "Epoch 1890/2000, Train Loss: 5.348085625039027, Val Loss: 4.372161714040388, Val MAE: 1.3033874034881592\n",
      "Epoch 1891/2000, Train Loss: 5.347878845345376, Val Loss: 4.372003115913061, Val MAE: 1.3033348321914673\n",
      "Epoch 1892/2000, Train Loss: 5.34762961883477, Val Loss: 4.371866074251377, Val MAE: 1.3033473491668701\n",
      "Epoch 1893/2000, Train Loss: 5.347399171527723, Val Loss: 4.371697730271835, Val MAE: 1.3033320903778076\n",
      "Epoch 1894/2000, Train Loss: 5.347201828070218, Val Loss: 4.37158164628378, Val MAE: 1.3033242225646973\n",
      "Epoch 1895/2000, Train Loss: 5.346919532708591, Val Loss: 4.371447302436265, Val MAE: 1.3033593893051147\n",
      "Epoch 1896/2000, Train Loss: 5.34666265981977, Val Loss: 4.371295940969873, Val MAE: 1.3033732175827026\n",
      "Epoch 1897/2000, Train Loss: 5.346390909870411, Val Loss: 4.37113419083629, Val MAE: 1.3034183979034424\n",
      "Epoch 1898/2000, Train Loss: 5.346141988580877, Val Loss: 4.3709510953172925, Val MAE: 1.3033833503723145\n",
      "Epoch 1899/2000, Train Loss: 5.345911972922257, Val Loss: 4.370739982893148, Val MAE: 1.303356647491455\n",
      "Epoch 1900/2000, Train Loss: 5.345687271862858, Val Loss: 4.370606720846469, Val MAE: 1.3033835887908936\n",
      "Epoch 1901/2000, Train Loss: 5.345463536002419, Val Loss: 4.370459379150173, Val MAE: 1.3033486604690552\n",
      "Epoch 1902/2000, Train Loss: 5.345194084390371, Val Loss: 4.370345852271778, Val MAE: 1.3033889532089233\n",
      "Epoch 1903/2000, Train Loss: 5.344939202515047, Val Loss: 4.370245496072169, Val MAE: 1.3034197092056274\n",
      "Epoch 1904/2000, Train Loss: 5.344707852464518, Val Loss: 4.370085360708199, Val MAE: 1.3033735752105713\n",
      "Epoch 1905/2000, Train Loss: 5.344468764404752, Val Loss: 4.369985989815607, Val MAE: 1.3034087419509888\n",
      "Epoch 1906/2000, Train Loss: 5.344254233620384, Val Loss: 4.369857810519812, Val MAE: 1.3034483194351196\n",
      "Epoch 1907/2000, Train Loss: 5.3439598509705695, Val Loss: 4.369862884376931, Val MAE: 1.303654670715332\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1908/2000, Train Loss: 5.343753402236358, Val Loss: 4.369780626001321, Val MAE: 1.3036930561065674\n",
      "Epoch 1909/2000, Train Loss: 5.343596101454187, Val Loss: 4.369777991757618, Val MAE: 1.3037731647491455\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1910/2000, Train Loss: 5.343305615265948, Val Loss: 4.369621580933023, Val MAE: 1.3037463426589966\n",
      "Epoch 1911/2000, Train Loss: 5.343076001554525, Val Loss: 4.369503576783683, Val MAE: 1.3037480115890503\n",
      "Epoch 1912/2000, Train Loss: 5.342852955069923, Val Loss: 4.369376151885573, Val MAE: 1.3037688732147217\n",
      "Epoch 1913/2000, Train Loss: 5.34262367724877, Val Loss: 4.369233240433565, Val MAE: 1.303740382194519\n",
      "Epoch 1914/2000, Train Loss: 5.342423966409717, Val Loss: 4.369104243756279, Val MAE: 1.3037607669830322\n",
      "Epoch 1915/2000, Train Loss: 5.342162677682077, Val Loss: 4.368928007323911, Val MAE: 1.30375337600708\n",
      "Epoch 1916/2000, Train Loss: 5.3419251484162045, Val Loss: 4.3687520977315, Val MAE: 1.3037031888961792\n",
      "Epoch 1917/2000, Train Loss: 5.341693510679248, Val Loss: 4.368615627288818, Val MAE: 1.3037225008010864\n",
      "Epoch 1918/2000, Train Loss: 5.341441695289016, Val Loss: 4.368471622115045, Val MAE: 1.3037036657333374\n",
      "Epoch 1919/2000, Train Loss: 5.341315386657637, Val Loss: 4.36828073896761, Val MAE: 1.3036506175994873\n",
      "Epoch 1920/2000, Train Loss: 5.340968076053759, Val Loss: 4.368215487650999, Val MAE: 1.3037359714508057\n",
      "Epoch 1921/2000, Train Loss: 5.34070632420733, Val Loss: 4.368020579054599, Val MAE: 1.3037517070770264\n",
      "Epoch 1922/2000, Train Loss: 5.340477268521142, Val Loss: 4.3678948215142945, Val MAE: 1.3037559986114502\n",
      "Epoch 1923/2000, Train Loss: 5.340225023635773, Val Loss: 4.367766088787026, Val MAE: 1.3037391901016235\n",
      "Epoch 1924/2000, Train Loss: 5.339978128792958, Val Loss: 4.367624997834521, Val MAE: 1.303772211074829\n",
      "Epoch 1925/2000, Train Loss: 5.33972655115409, Val Loss: 4.367627032699548, Val MAE: 1.3038979768753052\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1926/2000, Train Loss: 5.3395028871305925, Val Loss: 4.367475682450092, Val MAE: 1.3038612604141235\n",
      "Epoch 1927/2000, Train Loss: 5.339239645845369, Val Loss: 4.367315064969025, Val MAE: 1.3040030002593994\n",
      "Epoch 1928/2000, Train Loss: 5.338993785015764, Val Loss: 4.367220232852801, Val MAE: 1.3040636777877808\n",
      "Epoch 1929/2000, Train Loss: 5.3387648730135675, Val Loss: 4.36710962139246, Val MAE: 1.3041040897369385\n",
      "Epoch 1930/2000, Train Loss: 5.338517562343437, Val Loss: 4.36694771584094, Val MAE: 1.3040852546691895\n",
      "Epoch 1931/2000, Train Loss: 5.338345098689583, Val Loss: 4.366727350945548, Val MAE: 1.3040567636489868\n",
      "Epoch 1932/2000, Train Loss: 5.338066373460166, Val Loss: 4.366575228065018, Val MAE: 1.3040344715118408\n",
      "Epoch 1933/2000, Train Loss: 5.337808068300847, Val Loss: 4.3664238865450615, Val MAE: 1.3039926290512085\n",
      "Epoch 1934/2000, Train Loss: 5.337573442077378, Val Loss: 4.36627824280675, Val MAE: 1.3040112257003784\n",
      "Epoch 1935/2000, Train Loss: 5.337354194681227, Val Loss: 4.366159183350135, Val MAE: 1.30403470993042\n",
      "Epoch 1936/2000, Train Loss: 5.337104467293463, Val Loss: 4.3660203216113445, Val MAE: 1.3040066957473755\n",
      "Epoch 1937/2000, Train Loss: 5.336854325220737, Val Loss: 4.365869713064254, Val MAE: 1.3040059804916382\n",
      "Epoch 1938/2000, Train Loss: 5.336593879146686, Val Loss: 4.365748562587528, Val MAE: 1.304027795791626\n",
      "Epoch 1939/2000, Train Loss: 5.3363373418675994, Val Loss: 4.365584696418657, Val MAE: 1.3040311336517334\n",
      "Epoch 1940/2000, Train Loss: 5.336102711135367, Val Loss: 4.365474903536594, Val MAE: 1.3041037321090698\n",
      "Epoch 1941/2000, Train Loss: 5.335854591799235, Val Loss: 4.365353754702515, Val MAE: 1.3041239976882935\n",
      "Epoch 1942/2000, Train Loss: 5.335588195511574, Val Loss: 4.365179378146261, Val MAE: 1.3040844202041626\n",
      "Epoch 1943/2000, Train Loss: 5.335382831953921, Val Loss: 4.365058520412821, Val MAE: 1.3041064739227295\n",
      "Epoch 1944/2000, Train Loss: 5.33511708646487, Val Loss: 4.364919214807157, Val MAE: 1.304133415222168\n",
      "Epoch 1945/2000, Train Loss: 5.334882860995697, Val Loss: 4.364749861983802, Val MAE: 1.3041110038757324\n",
      "Epoch 1946/2000, Train Loss: 5.334628477177028, Val Loss: 4.3646273592090985, Val MAE: 1.3041023015975952\n",
      "Epoch 1947/2000, Train Loss: 5.334418041896949, Val Loss: 4.364466460577146, Val MAE: 1.3041325807571411\n",
      "Epoch 1948/2000, Train Loss: 5.3342671009384635, Val Loss: 4.364439248076574, Val MAE: 1.3042724132537842\n",
      "Epoch 1949/2000, Train Loss: 5.333920356214774, Val Loss: 4.364328941092716, Val MAE: 1.3043121099472046\n",
      "Epoch 1950/2000, Train Loss: 5.333690317223806, Val Loss: 4.364185687946522, Val MAE: 1.3043237924575806\n",
      "Epoch 1951/2000, Train Loss: 5.333449209723091, Val Loss: 4.364003798177862, Val MAE: 1.3042315244674683\n",
      "Epoch 1952/2000, Train Loss: 5.33316124098191, Val Loss: 4.363876869899081, Val MAE: 1.3043469190597534\n",
      "Epoch 1953/2000, Train Loss: 5.3329244659584285, Val Loss: 4.363704015725241, Val MAE: 1.3043075799942017\n",
      "Epoch 1954/2000, Train Loss: 5.3327641085983135, Val Loss: 4.3635673541253, Val MAE: 1.3043279647827148\n",
      "Epoch 1955/2000, Train Loss: 5.332523113028166, Val Loss: 4.363432312786109, Val MAE: 1.3042820692062378\n",
      "Epoch 1956/2000, Train Loss: 5.332244737581774, Val Loss: 4.363343354027102, Val MAE: 1.3043900728225708\n",
      "Epoch 1957/2000, Train Loss: 5.332020618537064, Val Loss: 4.363198637751144, Val MAE: 1.3044371604919434\n",
      "Epoch 1958/2000, Train Loss: 5.331748931346464, Val Loss: 4.363245374363238, Val MAE: 1.3045135736465454\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1959/2000, Train Loss: 5.331495691995608, Val Loss: 4.3630967428834415, Val MAE: 1.3045597076416016\n",
      "Epoch 1960/2000, Train Loss: 5.331281096511489, Val Loss: 4.36294977596895, Val MAE: 1.3045729398727417\n",
      "Epoch 1961/2000, Train Loss: 5.331012200255811, Val Loss: 4.362870165142487, Val MAE: 1.3046348094940186\n",
      "Epoch 1962/2000, Train Loss: 5.330742175434305, Val Loss: 4.362728602562363, Val MAE: 1.3046425580978394\n",
      "Epoch 1963/2000, Train Loss: 5.330560310239546, Val Loss: 4.362643899950456, Val MAE: 1.3046549558639526\n",
      "Epoch 1964/2000, Train Loss: 5.330255443723101, Val Loss: 4.362511089068698, Val MAE: 1.3047223091125488\n",
      "Epoch 1965/2000, Train Loss: 5.330071693521342, Val Loss: 4.362316489430863, Val MAE: 1.3046820163726807\n",
      "Epoch 1966/2000, Train Loss: 5.32978635500697, Val Loss: 4.362263505665336, Val MAE: 1.3047339916229248\n",
      "Epoch 1967/2000, Train Loss: 5.329567325557993, Val Loss: 4.36213944000522, Val MAE: 1.3049532175064087\n",
      "Epoch 1968/2000, Train Loss: 5.329389467976957, Val Loss: 4.36202788550084, Val MAE: 1.3049473762512207\n",
      "Epoch 1969/2000, Train Loss: 5.32912708202243, Val Loss: 4.361960303431421, Val MAE: 1.3050122261047363\n",
      "Epoch 1970/2000, Train Loss: 5.328927282724161, Val Loss: 4.36184529136485, Val MAE: 1.3050107955932617\n",
      "Epoch 1971/2000, Train Loss: 5.3287091306947785, Val Loss: 4.3616475001329515, Val MAE: 1.304967999458313\n",
      "Epoch 1972/2000, Train Loss: 5.32850720386654, Val Loss: 4.361517608071876, Val MAE: 1.3049288988113403\n",
      "Epoch 1973/2000, Train Loss: 5.328258372064846, Val Loss: 4.361408669624741, Val MAE: 1.3049358129501343\n",
      "Epoch 1974/2000, Train Loss: 5.328037953128081, Val Loss: 4.361468368580961, Val MAE: 1.3050436973571777\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1975/2000, Train Loss: 5.327865888034796, Val Loss: 4.361294022225958, Val MAE: 1.3050111532211304\n",
      "Epoch 1976/2000, Train Loss: 5.32761859052702, Val Loss: 4.361192819968922, Val MAE: 1.3050652742385864\n",
      "Epoch 1977/2000, Train Loss: 5.327348113868744, Val Loss: 4.3610880426769185, Val MAE: 1.3050713539123535\n",
      "Epoch 1978/2000, Train Loss: 5.327098461856188, Val Loss: 4.360912943543411, Val MAE: 1.3050668239593506\n",
      "Epoch 1979/2000, Train Loss: 5.326919165696428, Val Loss: 4.360729453859367, Val MAE: 1.305032730102539\n",
      "Epoch 1980/2000, Train Loss: 5.326671595570191, Val Loss: 4.36064794995184, Val MAE: 1.3050367832183838\n",
      "Epoch 1981/2000, Train Loss: 5.326471287327512, Val Loss: 4.3605303746743465, Val MAE: 1.3050869703292847\n",
      "Epoch 1982/2000, Train Loss: 5.326176462212282, Val Loss: 4.360533427675878, Val MAE: 1.305161714553833\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1983/2000, Train Loss: 5.325961969600765, Val Loss: 4.360400344701264, Val MAE: 1.305131196975708\n",
      "Epoch 1984/2000, Train Loss: 5.325782382180927, Val Loss: 4.360217899529952, Val MAE: 1.3051049709320068\n",
      "Epoch 1985/2000, Train Loss: 5.325566714783698, Val Loss: 4.360086854400597, Val MAE: 1.3051016330718994\n",
      "Epoch 1986/2000, Train Loss: 5.325307193895855, Val Loss: 4.359949528560864, Val MAE: 1.305027961730957\n",
      "Epoch 1987/2000, Train Loss: 5.325085478406424, Val Loss: 4.359787620615771, Val MAE: 1.3050683736801147\n",
      "Epoch 1988/2000, Train Loss: 5.324837311131821, Val Loss: 4.3596856653690335, Val MAE: 1.3050509691238403\n",
      "Epoch 1989/2000, Train Loss: 5.324643408872055, Val Loss: 4.359549269479091, Val MAE: 1.3050569295883179\n",
      "Epoch 1990/2000, Train Loss: 5.32440181923689, Val Loss: 4.359470126502157, Val MAE: 1.3051490783691406\n",
      "Epoch 1991/2000, Train Loss: 5.324178093324037, Val Loss: 4.35948552637119, Val MAE: 1.305241584777832\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1992/2000, Train Loss: 5.3239546376893365, Val Loss: 4.359365893865195, Val MAE: 1.3052425384521484\n",
      "Epoch 1993/2000, Train Loss: 5.323721983054276, Val Loss: 4.359255437353465, Val MAE: 1.3051847219467163\n",
      "Epoch 1994/2000, Train Loss: 5.323539453989534, Val Loss: 4.359071819378635, Val MAE: 1.305174708366394\n",
      "Epoch 1995/2000, Train Loss: 5.323260543789599, Val Loss: 4.35899011187666, Val MAE: 1.3052128553390503\n",
      "Epoch 1996/2000, Train Loss: 5.323060830218497, Val Loss: 4.358885026330085, Val MAE: 1.3052464723587036\n",
      "Epoch 1997/2000, Train Loss: 5.322831640492624, Val Loss: 4.358829032999324, Val MAE: 1.3053035736083984\n",
      "Epoch 1998/2000, Train Loss: 5.322658982355365, Val Loss: 4.35878287079766, Val MAE: 1.3053734302520752\n",
      "Epoch 1999/2000, Train Loss: 5.322401136847332, Val Loss: 4.358629293845395, Val MAE: 1.305354118347168\n",
      "Epoch 2000/2000, Train Loss: 5.322174046941738, Val Loss: 4.358511954804105, Val MAE: 1.3053399324417114\n",
      "Test Loss (MSE): 6.280025005340576\n",
      "Test Mean Absolute Error (MAE): 1.6264150510648734\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(CNNModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (conv2d): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=145, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'test_mae': 1.6264150510648734})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 229\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_cnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=6,\n",
    "                kernel_size=3,\n",
    "                num_filters=6,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = NUM_FEATURES_DICT['GK']['large'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Total Number of Iterations:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 400, 'position': 'GK', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 400\n",
      "position GK\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  0\n",
      "Epoch 1/750, Train Loss: 9.532903823956058, Val Loss: 6.050033757160968, Val MAE: 1.3708890676498413\n",
      "Epoch 2/750, Train Loss: 7.091832490271435, Val Loss: 4.836160622149941, Val MAE: 1.268945336341858\n",
      "Epoch 3/750, Train Loss: 5.895921195378297, Val Loss: 4.378261085077534, Val MAE: 1.2924792766571045\n",
      "Epoch 4/750, Train Loss: 5.4557939927969406, Val Loss: 4.260883070047446, Val MAE: 1.320525050163269\n",
      "Epoch 5/750, Train Loss: 5.312854018916592, Val Loss: 4.232095880583516, Val MAE: 1.3313206434249878\n",
      "Epoch 6/750, Train Loss: 5.250508269590794, Val Loss: 4.219717365385979, Val MAE: 1.3156825304031372\n",
      "Epoch 7/750, Train Loss: 5.216082090923874, Val Loss: 4.216196618845144, Val MAE: 1.3221603631973267\n",
      "Epoch 8/750, Train Loss: 5.195659722854842, Val Loss: 4.201277194201477, Val MAE: 1.2924753427505493\n",
      "Epoch 9/750, Train Loss: 5.166544487227255, Val Loss: 4.19694513044958, Val MAE: 1.3072617053985596\n",
      "Epoch 10/750, Train Loss: 5.138951808820748, Val Loss: 4.198118625029804, Val MAE: 1.318352222442627\n",
      "Epoch 11/750, Train Loss: 5.1138222776808275, Val Loss: 4.1930904426091296, Val MAE: 1.3031290769577026\n",
      "Epoch 12/750, Train Loss: 5.096082768406784, Val Loss: 4.192381575820953, Val MAE: 1.2950536012649536\n",
      "Epoch 13/750, Train Loss: 5.07732138640198, Val Loss: 4.191984372028685, Val MAE: 1.3018518686294556\n",
      "Epoch 14/750, Train Loss: 5.05607247126151, Val Loss: 4.1818428251452335, Val MAE: 1.291283130645752\n",
      "Epoch 15/750, Train Loss: 5.039868597097928, Val Loss: 4.18044326135258, Val MAE: 1.310742735862732\n",
      "Epoch 16/750, Train Loss: 5.0222014946966675, Val Loss: 4.182488619964423, Val MAE: 1.285316824913025\n",
      "Epoch 17/750, Train Loss: 5.010648222435442, Val Loss: 4.1814046351693746, Val MAE: 1.285658359527588\n",
      "Epoch 18/750, Train Loss: 5.000459140482557, Val Loss: 4.185459991374354, Val MAE: 1.3257662057876587\n",
      "Epoch 19/750, Train Loss: 5.0335651338985, Val Loss: 4.184692544801028, Val MAE: 1.311974287033081\n",
      "Epoch 20/750, Train Loss: 4.97998731185049, Val Loss: 4.185274612927062, Val MAE: 1.3197100162506104\n",
      "Epoch 21/750, Train Loss: 4.968016558295189, Val Loss: 4.176760395575227, Val MAE: 1.2683422565460205\n",
      "Epoch 22/750, Train Loss: 4.962576158334636, Val Loss: 4.188585371177966, Val MAE: 1.301282525062561\n",
      "Epoch 23/750, Train Loss: 4.954153522545803, Val Loss: 4.179144661726914, Val MAE: 1.2899590730667114\n",
      "Epoch 24/750, Train Loss: 4.94627882052956, Val Loss: 4.175795972769655, Val MAE: 1.3158546686172485\n",
      "Epoch 25/750, Train Loss: 4.93451917555633, Val Loss: 4.173413555582208, Val MAE: 1.3030327558517456\n",
      "Epoch 26/750, Train Loss: 4.927468977272106, Val Loss: 4.172604715542531, Val MAE: 1.2918576002120972\n",
      "Epoch 27/750, Train Loss: 4.919934269531126, Val Loss: 4.177299315183181, Val MAE: 1.2838627099990845\n",
      "Epoch 28/750, Train Loss: 4.914275211750895, Val Loss: 4.176313566551434, Val MAE: 1.3156694173812866\n",
      "Epoch 29/750, Train Loss: 4.910745604876101, Val Loss: 4.178795665666813, Val MAE: 1.3194324970245361\n",
      "Epoch 30/750, Train Loss: 4.897334581476054, Val Loss: 4.180557784402934, Val MAE: 1.2726573944091797\n",
      "Epoch 31/750, Train Loss: 4.889927460024768, Val Loss: 4.178805146947151, Val MAE: 1.278296709060669\n",
      "Epoch 32/750, Train Loss: 4.89685624554814, Val Loss: 4.177537451367678, Val MAE: 1.289605736732483\n",
      "Epoch 33/750, Train Loss: 4.883840364872843, Val Loss: 4.1777132575084845, Val MAE: 1.2790672779083252\n",
      "Epoch 34/750, Train Loss: 4.881336671526428, Val Loss: 4.184354928088939, Val MAE: 1.3170260190963745\n",
      "Epoch 35/750, Train Loss: 4.870411378881178, Val Loss: 4.20206383897329, Val MAE: 1.3064382076263428\n",
      "Epoch 36/750, Train Loss: 4.877712289111504, Val Loss: 4.197198032006973, Val MAE: 1.2691116333007812\n",
      "Epoch 37/750, Train Loss: 4.861374750227909, Val Loss: 4.191786127942284, Val MAE: 1.3041391372680664\n",
      "Epoch 38/750, Train Loss: 4.8547999296777125, Val Loss: 4.1986984306198405, Val MAE: 1.2770776748657227\n",
      "Epoch 39/750, Train Loss: 4.847570676978278, Val Loss: 4.191683188397584, Val MAE: 1.2842919826507568\n",
      "Epoch 40/750, Train Loss: 4.852620502663435, Val Loss: 4.18725376135017, Val MAE: 1.2988111972808838\n",
      "Epoch 41/750, Train Loss: 4.840778167082788, Val Loss: 4.17786308304062, Val MAE: 1.296138882637024\n",
      "Epoch 42/750, Train Loss: 4.832840664254115, Val Loss: 4.180550693972843, Val MAE: 1.2966471910476685\n",
      "Epoch 43/750, Train Loss: 4.8288704705206, Val Loss: 4.173359176317068, Val MAE: 1.2744181156158447\n",
      "Epoch 44/750, Train Loss: 4.825379253161649, Val Loss: 4.16983852754897, Val MAE: 1.2925279140472412\n",
      "Epoch 45/750, Train Loss: 4.8186616486739595, Val Loss: 4.174774011547171, Val MAE: 1.3074531555175781\n",
      "Epoch 46/750, Train Loss: 4.812805350435193, Val Loss: 4.176222410070615, Val MAE: 1.2670632600784302\n",
      "Epoch 47/750, Train Loss: 4.811956197306453, Val Loss: 4.17739857066804, Val MAE: 1.3037431240081787\n",
      "Epoch 48/750, Train Loss: 4.8113146836931335, Val Loss: 4.173121260787089, Val MAE: 1.2924222946166992\n",
      "Epoch 49/750, Train Loss: 4.803231951622659, Val Loss: 4.179383679813757, Val MAE: 1.272545337677002\n",
      "Epoch 50/750, Train Loss: 4.795247553485058, Val Loss: 4.179658811521811, Val MAE: 1.263248324394226\n",
      "Epoch 51/750, Train Loss: 4.795218774389218, Val Loss: 4.183585603194913, Val MAE: 1.3066989183425903\n",
      "Epoch 52/750, Train Loss: 4.788349981734395, Val Loss: 4.184496341047325, Val MAE: 1.30089271068573\n",
      "Epoch 53/750, Train Loss: 4.783206506047191, Val Loss: 4.17742913977837, Val MAE: 1.2778496742248535\n",
      "Epoch 54/750, Train Loss: 4.780779156950144, Val Loss: 4.181262457394224, Val MAE: 1.300736904144287\n",
      "Epoch 55/750, Train Loss: 4.776300093535652, Val Loss: 4.179742782193375, Val MAE: 1.2524853944778442\n",
      "Epoch 56/750, Train Loss: 4.777346226706434, Val Loss: 4.173739242940907, Val MAE: 1.2721706628799438\n",
      "Epoch 57/750, Train Loss: 4.78197033447561, Val Loss: 4.17080244125109, Val MAE: 1.2766375541687012\n",
      "Epoch 58/750, Train Loss: 4.764435985518602, Val Loss: 4.183388601912288, Val MAE: 1.2838459014892578\n",
      "Epoch 59/750, Train Loss: 4.759457006059799, Val Loss: 4.1847414932382385, Val MAE: 1.284332513809204\n",
      "Epoch 60/750, Train Loss: 4.76427007918442, Val Loss: 4.176238384314879, Val MAE: 1.2562048435211182\n",
      "Epoch 61/750, Train Loss: 4.762664339112136, Val Loss: 4.187998493895756, Val MAE: 1.3103309869766235\n",
      "Epoch 62/750, Train Loss: 4.749779438551925, Val Loss: 4.186140946128706, Val MAE: 1.2887955904006958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 1/6 [00:11<00:55, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/750, Train Loss: 4.742922413017565, Val Loss: 4.183592261033734, Val MAE: 1.291318416595459\n",
      "Epoch 64/750, Train Loss: 4.74161289836529, Val Loss: 4.1840561280569695, Val MAE: 1.2986146211624146\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.219419002532959\n",
      "Test Mean Absolute Error (MAE): 1.640297709928669\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 401, 'position': 'GK', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 401\n",
      "position GK\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  1\n",
      "Epoch 1/750, Train Loss: 8.670582514927254, Val Loss: 8.214560693073738, Val MAE: 1.5819294452667236\n",
      "Epoch 2/750, Train Loss: 6.16797861845597, Val Loss: 6.363329891096707, Val MAE: 1.4279330968856812\n",
      "Epoch 3/750, Train Loss: 5.14169823012737, Val Loss: 5.8034652135330225, Val MAE: 1.4937920570373535\n",
      "Epoch 4/750, Train Loss: 4.8678507790062, Val Loss: 5.7038653698099315, Val MAE: 1.5532119274139404\n",
      "Epoch 5/750, Train Loss: 4.798993104733295, Val Loss: 5.686524876547818, Val MAE: 1.5703115463256836\n",
      "Epoch 6/750, Train Loss: 4.761781551082682, Val Loss: 5.681839633002952, Val MAE: 1.5846267938613892\n",
      "Epoch 7/750, Train Loss: 4.739864013950277, Val Loss: 5.681876350662217, Val MAE: 1.588753342628479\n",
      "Epoch 8/750, Train Loss: 4.704218280981786, Val Loss: 5.685972552053893, Val MAE: 1.5879311561584473\n",
      "Epoch 9/750, Train Loss: 4.687339838247121, Val Loss: 5.688393642030769, Val MAE: 1.5855937004089355\n",
      "Epoch 10/750, Train Loss: 4.660725380322948, Val Loss: 5.6943596095296005, Val MAE: 1.5836588144302368\n",
      "Epoch 11/750, Train Loss: 4.641423634860827, Val Loss: 5.698644323939055, Val MAE: 1.5894984006881714\n",
      "Epoch 12/750, Train Loss: 4.628629295722298, Val Loss: 5.705161850206816, Val MAE: 1.5969815254211426\n",
      "Epoch 13/750, Train Loss: 4.615530704119191, Val Loss: 5.713165877901729, Val MAE: 1.5866708755493164\n",
      "Epoch 14/750, Train Loss: 4.592593464051714, Val Loss: 5.723688825964928, Val MAE: 1.6066269874572754\n",
      "Epoch 15/750, Train Loss: 4.5820743963585135, Val Loss: 5.731807342276501, Val MAE: 1.5997213125228882\n",
      "Epoch 16/750, Train Loss: 4.577094637829324, Val Loss: 5.736533349033576, Val MAE: 1.5906039476394653\n",
      "Epoch 17/750, Train Loss: 4.5623769375108045, Val Loss: 5.735746370338315, Val MAE: 1.6010817289352417\n",
      "Epoch 18/750, Train Loss: 4.549348158865982, Val Loss: 5.742584473673423, Val MAE: 1.584398627281189\n",
      "Epoch 19/750, Train Loss: 4.540024236122274, Val Loss: 5.741568204895336, Val MAE: 1.5938057899475098\n",
      "Epoch 20/750, Train Loss: 4.533394674336688, Val Loss: 5.745054573644346, Val MAE: 1.5947887897491455\n",
      "Epoch 21/750, Train Loss: 4.526879288394999, Val Loss: 5.75209110436128, Val MAE: 1.608553171157837\n",
      "Epoch 22/750, Train Loss: 4.520293164697493, Val Loss: 5.748448703082363, Val MAE: 1.6042704582214355\n",
      "Epoch 23/750, Train Loss: 4.515310441485103, Val Loss: 5.751250163581803, Val MAE: 1.5847489833831787\n",
      "Epoch 24/750, Train Loss: 4.501397707447502, Val Loss: 5.756766552713947, Val MAE: 1.5977215766906738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 2/6 [00:16<00:32,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/750, Train Loss: 4.495806095763023, Val Loss: 5.764225619671932, Val MAE: 1.6107170581817627\n",
      "Epoch 26/750, Train Loss: 4.499744471555911, Val Loss: 5.762889217753806, Val MAE: 1.5904847383499146\n",
      "Early stopping\n",
      "Test Loss (MSE): 5.073462963104248\n",
      "Test Mean Absolute Error (MAE): 1.552388937172422\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 402, 'position': 'GK', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 402\n",
      "position GK\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  2\n",
      "Epoch 1/750, Train Loss: 8.664784403094218, Val Loss: 8.777210263198432, Val MAE: 1.7322840690612793\n",
      "Epoch 2/750, Train Loss: 5.941652948327461, Val Loss: 6.649761963166652, Val MAE: 1.576332688331604\n",
      "Epoch 3/750, Train Loss: 5.000201251308262, Val Loss: 6.156170678035223, Val MAE: 1.6077051162719727\n",
      "Epoch 4/750, Train Loss: 4.811476080128995, Val Loss: 6.030959824922288, Val MAE: 1.65025794506073\n",
      "Epoch 5/750, Train Loss: 4.763057318995098, Val Loss: 5.999100305582093, Val MAE: 1.6681578159332275\n",
      "Epoch 6/750, Train Loss: 4.730747506867096, Val Loss: 5.990969581990442, Val MAE: 1.6594806909561157\n",
      "Epoch 7/750, Train Loss: 4.710208366599593, Val Loss: 5.9817841304883945, Val MAE: 1.6665725708007812\n",
      "Epoch 8/750, Train Loss: 4.682414769292128, Val Loss: 5.975355734873439, Val MAE: 1.6697627305984497\n",
      "Epoch 9/750, Train Loss: 4.668595173544113, Val Loss: 5.969590674260936, Val MAE: 1.6556122303009033\n",
      "Epoch 10/750, Train Loss: 4.6464549257173, Val Loss: 5.964721032403485, Val MAE: 1.6605095863342285\n",
      "Epoch 11/750, Train Loss: 4.631989001894097, Val Loss: 5.955433888304251, Val MAE: 1.663459062576294\n",
      "Epoch 12/750, Train Loss: 4.618704090425399, Val Loss: 5.945841463533394, Val MAE: 1.658382773399353\n",
      "Epoch 13/750, Train Loss: 4.609740252675002, Val Loss: 5.947765419348621, Val MAE: 1.6499717235565186\n",
      "Epoch 14/750, Train Loss: 4.593790647281142, Val Loss: 5.942730635879009, Val MAE: 1.6583248376846313\n",
      "Epoch 15/750, Train Loss: 4.584330229762553, Val Loss: 5.939012553688413, Val MAE: 1.6627119779586792\n",
      "Epoch 16/750, Train Loss: 4.579936671874338, Val Loss: 5.935332641932796, Val MAE: 1.6663029193878174\n",
      "Epoch 17/750, Train Loss: 4.562228296585563, Val Loss: 5.94017886149383, Val MAE: 1.6623905897140503\n",
      "Epoch 18/750, Train Loss: 4.555108897831158, Val Loss: 5.939038936377607, Val MAE: 1.6414254903793335\n",
      "Epoch 19/750, Train Loss: 4.544122342549478, Val Loss: 5.931862484701463, Val MAE: 1.6523470878601074\n",
      "Epoch 20/750, Train Loss: 4.539132569702127, Val Loss: 5.934596597199502, Val MAE: 1.6685339212417603\n",
      "Epoch 21/750, Train Loss: 4.52823432397809, Val Loss: 5.9394558562901185, Val MAE: 1.6469943523406982\n",
      "Epoch 22/750, Train Loss: 4.5229246287682905, Val Loss: 5.9316129367016845, Val MAE: 1.661949634552002\n",
      "Epoch 23/750, Train Loss: 4.5231825604649005, Val Loss: 5.931763299812284, Val MAE: 1.6619408130645752\n",
      "Epoch 24/750, Train Loss: 4.507760863774635, Val Loss: 5.934569898459052, Val MAE: 1.6535626649856567\n",
      "Epoch 25/750, Train Loss: 4.498638587702063, Val Loss: 5.931107572122179, Val MAE: 1.6682100296020508\n",
      "Epoch 26/750, Train Loss: 4.497300135830718, Val Loss: 5.933432260228997, Val MAE: 1.6774795055389404\n",
      "Epoch 27/750, Train Loss: 4.489526061931634, Val Loss: 5.93900991486744, Val MAE: 1.6485915184020996\n",
      "Epoch 28/750, Train Loss: 4.482711744608789, Val Loss: 5.931875376556441, Val MAE: 1.671144723892212\n",
      "Epoch 29/750, Train Loss: 4.47895224766768, Val Loss: 5.933586785831258, Val MAE: 1.65972101688385\n",
      "Epoch 30/750, Train Loss: 4.480631898382145, Val Loss: 5.933117284098514, Val MAE: 1.6591426134109497\n",
      "Epoch 31/750, Train Loss: 4.4687365010704, Val Loss: 5.9282827667148, Val MAE: 1.6883020401000977\n",
      "Epoch 32/750, Train Loss: 4.464999786534453, Val Loss: 5.934984826834605, Val MAE: 1.6689907312393188\n",
      "Epoch 33/750, Train Loss: 4.456184442432049, Val Loss: 5.93779315065199, Val MAE: 1.6791069507598877\n",
      "Epoch 34/750, Train Loss: 4.457890725285962, Val Loss: 5.929751949613933, Val MAE: 1.669769287109375\n",
      "Epoch 35/750, Train Loss: 4.450010529925891, Val Loss: 5.941283936783477, Val MAE: 1.6466060876846313\n",
      "Epoch 36/750, Train Loss: 4.451663304409671, Val Loss: 5.9389615203813255, Val MAE: 1.6758222579956055\n",
      "Epoch 37/750, Train Loss: 4.443811839072333, Val Loss: 5.9354770373332, Val MAE: 1.668016791343689\n",
      "Epoch 38/750, Train Loss: 4.4336515293828755, Val Loss: 5.942440311746556, Val MAE: 1.669905424118042\n",
      "Epoch 39/750, Train Loss: 4.431221425408163, Val Loss: 5.943798483366904, Val MAE: 1.64597749710083\n",
      "Epoch 40/750, Train Loss: 4.4279101410472705, Val Loss: 5.944559418861842, Val MAE: 1.6513826847076416\n",
      "Epoch 41/750, Train Loss: 4.421330453466918, Val Loss: 5.9364791930843195, Val MAE: 1.6640515327453613\n",
      "Epoch 42/750, Train Loss: 4.424189727735152, Val Loss: 5.9454317410326905, Val MAE: 1.6799594163894653\n",
      "Epoch 43/750, Train Loss: 4.415784860675029, Val Loss: 5.943605636894789, Val MAE: 1.6641771793365479\n",
      "Epoch 44/750, Train Loss: 4.414923700108405, Val Loss: 5.947711915597211, Val MAE: 1.6689478158950806\n",
      "Epoch 45/750, Train Loss: 4.412349050740748, Val Loss: 5.946281987920338, Val MAE: 1.6734020709991455\n",
      "Epoch 46/750, Train Loss: 4.405274344958818, Val Loss: 5.947935127832437, Val MAE: 1.6791505813598633\n",
      "Epoch 47/750, Train Loss: 4.397229922576915, Val Loss: 5.955195166095121, Val MAE: 1.6478351354599\n",
      "Epoch 48/750, Train Loss: 4.394897318620462, Val Loss: 5.942986804393542, Val MAE: 1.6736884117126465\n",
      "Epoch 49/750, Train Loss: 4.3911507254801405, Val Loss: 5.954807884613788, Val MAE: 1.6425641775131226\n",
      "Epoch 50/750, Train Loss: 4.383659484755167, Val Loss: 5.948862438781561, Val MAE: 1.6817399263381958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 3/6 [00:26<00:25,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/750, Train Loss: 4.380264687988623, Val Loss: 5.955125397435146, Val MAE: 1.6526963710784912\n",
      "Early stopping\n",
      "Test Loss (MSE): 5.00268030166626\n",
      "Test Mean Absolute Error (MAE): 1.3848136586395545\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 403, 'position': 'GK', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 403\n",
      "position GK\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  3\n",
      "Epoch 1/750, Train Loss: 6.601691612618507, Val Loss: 10.852313934496747, Val MAE: 2.071204662322998\n",
      "Epoch 2/750, Train Loss: 4.731387855000579, Val Loss: 8.153987675178342, Val MAE: 1.805308222770691\n",
      "Epoch 3/750, Train Loss: 4.077954717454194, Val Loss: 7.4943951459434945, Val MAE: 1.8184692859649658\n",
      "Epoch 4/750, Train Loss: 3.951732886319905, Val Loss: 7.280326487378376, Val MAE: 1.8496254682540894\n",
      "Epoch 5/750, Train Loss: 3.902385708913638, Val Loss: 7.26536681555151, Val MAE: 1.8491814136505127\n",
      "Epoch 6/750, Train Loss: 3.87481029839874, Val Loss: 7.254186772912498, Val MAE: 1.856732726097107\n",
      "Epoch 7/750, Train Loss: 3.847860097196061, Val Loss: 7.247455606227968, Val MAE: 1.8619942665100098\n",
      "Epoch 8/750, Train Loss: 3.826203540570474, Val Loss: 7.273560493748363, Val MAE: 1.856148362159729\n",
      "Epoch 9/750, Train Loss: 3.8097408892791393, Val Loss: 7.278198086343161, Val MAE: 1.8627257347106934\n",
      "Epoch 10/750, Train Loss: 3.789766767810535, Val Loss: 7.259402165374135, Val MAE: 1.8668107986450195\n",
      "Epoch 11/750, Train Loss: 3.7794223669378053, Val Loss: 7.296756003930317, Val MAE: 1.8665499687194824\n",
      "Epoch 12/750, Train Loss: 3.7682201738302417, Val Loss: 7.28389353093093, Val MAE: 1.862953543663025\n",
      "Epoch 13/750, Train Loss: 3.7519011469934718, Val Loss: 7.29553967801536, Val MAE: 1.86845862865448\n",
      "Epoch 14/750, Train Loss: 3.7379437250898064, Val Loss: 7.297755179366445, Val MAE: 1.8669973611831665\n",
      "Epoch 15/750, Train Loss: 3.7307632892807097, Val Loss: 7.323292151893058, Val MAE: 1.8638330698013306\n",
      "Epoch 16/750, Train Loss: 3.7192799755603594, Val Loss: 7.273144743694522, Val MAE: 1.8805537223815918\n",
      "Epoch 17/750, Train Loss: 3.7153446550314135, Val Loss: 7.28619007133856, Val MAE: 1.8777697086334229\n",
      "Epoch 18/750, Train Loss: 3.701853847228034, Val Loss: 7.321605164442605, Val MAE: 1.8692920207977295\n",
      "Epoch 19/750, Train Loss: 3.6958663215526957, Val Loss: 7.311453944493115, Val MAE: 1.87068510055542\n",
      "Epoch 20/750, Train Loss: 3.686402312592964, Val Loss: 7.311052558092567, Val MAE: 1.883728265762329\n",
      "Epoch 21/750, Train Loss: 3.6773248022002294, Val Loss: 7.3283488924910385, Val MAE: 1.8742488622665405\n",
      "Epoch 22/750, Train Loss: 3.672193241257199, Val Loss: 7.337722268918665, Val MAE: 1.8702402114868164\n",
      "Epoch 23/750, Train Loss: 3.6683192432271263, Val Loss: 7.343003231141625, Val MAE: 1.8722673654556274\n",
      "Epoch 24/750, Train Loss: 3.6678077940306912, Val Loss: 7.340715207510847, Val MAE: 1.8689881563186646\n",
      "Epoch 25/750, Train Loss: 3.6524292103127936, Val Loss: 7.341584566162854, Val MAE: 1.8731029033660889\n",
      "Epoch 26/750, Train Loss: 3.6474308636836232, Val Loss: 7.3373851543519555, Val MAE: 1.8693017959594727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 4/6 [00:32<00:15,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/750, Train Loss: 3.6405523939628823, Val Loss: 7.33852248424437, Val MAE: 1.878535509109497\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.357857704162598\n",
      "Test Mean Absolute Error (MAE): 1.7097733468113552\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 404, 'position': 'GK', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 404\n",
      "position GK\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  4\n",
      "Epoch 1/750, Train Loss: 9.333599421708495, Val Loss: 7.265775648856556, Val MAE: 1.5713489055633545\n",
      "Epoch 2/750, Train Loss: 6.486102101541591, Val Loss: 5.498603504811951, Val MAE: 1.4496772289276123\n",
      "Epoch 3/750, Train Loss: 5.505146211537632, Val Loss: 5.136376850552612, Val MAE: 1.476633071899414\n",
      "Epoch 4/750, Train Loss: 5.327093444485605, Val Loss: 5.088814339631206, Val MAE: 1.5011605024337769\n",
      "Epoch 5/750, Train Loss: 5.291638423434243, Val Loss: 5.076962441580262, Val MAE: 1.5312834978103638\n",
      "Epoch 6/750, Train Loss: 5.265625704260706, Val Loss: 5.06260965441703, Val MAE: 1.4991706609725952\n",
      "Epoch 7/750, Train Loss: 5.239146170437584, Val Loss: 5.057047856658744, Val MAE: 1.5052250623703003\n",
      "Epoch 8/750, Train Loss: 5.224284467022561, Val Loss: 5.047735533052748, Val MAE: 1.4949061870574951\n",
      "Epoch 9/750, Train Loss: 5.202123962559746, Val Loss: 5.042213591994072, Val MAE: 1.5066182613372803\n",
      "Epoch 10/750, Train Loss: 5.201998325475542, Val Loss: 5.0403993323188, Val MAE: 1.4968478679656982\n",
      "Epoch 11/750, Train Loss: 5.17352681087224, Val Loss: 5.038067396010859, Val MAE: 1.4926241636276245\n",
      "Epoch 12/750, Train Loss: 5.164846820325362, Val Loss: 5.032891580434384, Val MAE: 1.4880878925323486\n",
      "Epoch 13/750, Train Loss: 5.1471499966847585, Val Loss: 5.035341546454085, Val MAE: 1.482275128364563\n",
      "Epoch 14/750, Train Loss: 5.144268427493668, Val Loss: 5.037607333232507, Val MAE: 1.4876923561096191\n",
      "Epoch 15/750, Train Loss: 5.136482337642146, Val Loss: 5.038518461645866, Val MAE: 1.4998804330825806\n",
      "Epoch 16/750, Train Loss: 5.1168934535054325, Val Loss: 5.035824213662365, Val MAE: 1.49115788936615\n",
      "Epoch 17/750, Train Loss: 5.1099259122565455, Val Loss: 5.04091963396529, Val MAE: 1.5022883415222168\n",
      "Epoch 18/750, Train Loss: 5.09924674133322, Val Loss: 5.036344071244894, Val MAE: 1.4823521375656128\n",
      "Epoch 19/750, Train Loss: 5.090953591461816, Val Loss: 5.035311662350757, Val MAE: 1.5018973350524902\n",
      "Epoch 20/750, Train Loss: 5.084494947891129, Val Loss: 5.036893426061987, Val MAE: 1.4955896139144897\n",
      "Epoch 21/750, Train Loss: 5.089580867269003, Val Loss: 5.030760930930052, Val MAE: 1.4816601276397705\n",
      "Epoch 22/750, Train Loss: 5.072506890868595, Val Loss: 5.031772715143356, Val MAE: 1.4954389333724976\n",
      "Epoch 23/750, Train Loss: 5.0647261675121715, Val Loss: 5.034287229297299, Val MAE: 1.486890435218811\n",
      "Epoch 24/750, Train Loss: 5.060832974649501, Val Loss: 5.031756269127084, Val MAE: 1.5064494609832764\n",
      "Epoch 25/750, Train Loss: 5.062832759587344, Val Loss: 5.046572750296847, Val MAE: 1.522338628768921\n",
      "Epoch 26/750, Train Loss: 5.054271537371383, Val Loss: 5.078609864205543, Val MAE: 1.508136510848999\n",
      "Epoch 27/750, Train Loss: 5.045441303438353, Val Loss: 5.068916012414584, Val MAE: 1.5052564144134521\n",
      "Epoch 28/750, Train Loss: 5.045500756054083, Val Loss: 5.065311728527819, Val MAE: 1.5258091688156128\n",
      "Epoch 29/750, Train Loss: 5.039573223680134, Val Loss: 5.052968378016488, Val MAE: 1.4886823892593384\n",
      "Epoch 30/750, Train Loss: 5.038479080808642, Val Loss: 5.048989935937445, Val MAE: 1.4887351989746094\n",
      "Epoch 31/750, Train Loss: 5.025910158759182, Val Loss: 5.043084141083377, Val MAE: 1.4925099611282349\n",
      "Epoch 32/750, Train Loss: 5.015468597081432, Val Loss: 5.047764542092989, Val MAE: 1.4822765588760376\n",
      "Epoch 33/750, Train Loss: 5.026976246112924, Val Loss: 5.049717298084563, Val MAE: 1.4894845485687256\n",
      "Epoch 34/750, Train Loss: 5.006797519041, Val Loss: 5.055597123045188, Val MAE: 1.5002766847610474\n",
      "Epoch 35/750, Train Loss: 5.000511611218922, Val Loss: 5.059491982382358, Val MAE: 1.4946242570877075\n",
      "Epoch 36/750, Train Loss: 4.995907096360163, Val Loss: 5.066524937936051, Val MAE: 1.5053465366363525\n",
      "Epoch 37/750, Train Loss: 4.996508123722553, Val Loss: 5.066254161520506, Val MAE: 1.4953659772872925\n",
      "Epoch 38/750, Train Loss: 4.997831450421337, Val Loss: 5.065193260366351, Val MAE: 1.4909613132476807\n",
      "Epoch 39/750, Train Loss: 4.984715778189459, Val Loss: 5.061322427514116, Val MAE: 1.4799318313598633\n",
      "Epoch 40/750, Train Loss: 4.979117622983935, Val Loss: 5.063116849685202, Val MAE: 1.5006389617919922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 5/6 [00:39<00:07,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/750, Train Loss: 4.980516727564239, Val Loss: 5.060458962078933, Val MAE: 1.499809741973877\n",
      "Early stopping\n",
      "Test Loss (MSE): 4.549375057220459\n",
      "Test Mean Absolute Error (MAE): 1.283268077690903\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 405, 'position': 'GK', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 405\n",
      "position GK\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  5\n",
      "Epoch 1/750, Train Loss: 9.707776188767905, Val Loss: 7.389597045050727, Val MAE: 1.515391230583191\n",
      "Epoch 2/750, Train Loss: 6.992338761326351, Val Loss: 5.795730248676593, Val MAE: 1.4100865125656128\n",
      "Epoch 3/750, Train Loss: 5.559889472727132, Val Loss: 5.34479600909357, Val MAE: 1.4387606382369995\n",
      "Epoch 4/750, Train Loss: 5.170362577058865, Val Loss: 5.309078242297463, Val MAE: 1.4730409383773804\n",
      "Epoch 5/750, Train Loss: 5.073840613678665, Val Loss: 5.31552710033172, Val MAE: 1.4858449697494507\n",
      "Epoch 6/750, Train Loss: 5.048711711642651, Val Loss: 5.322184292923117, Val MAE: 1.4975463151931763\n",
      "Epoch 7/750, Train Loss: 5.024316102328185, Val Loss: 5.319296534632294, Val MAE: 1.4821079969406128\n",
      "Epoch 8/750, Train Loss: 5.004303978388697, Val Loss: 5.321855566684443, Val MAE: 1.5046831369400024\n",
      "Epoch 9/750, Train Loss: 4.993195409469538, Val Loss: 5.308537064210537, Val MAE: 1.5046353340148926\n",
      "Epoch 10/750, Train Loss: 4.9766754889570715, Val Loss: 5.305363011472102, Val MAE: 1.4980710744857788\n",
      "Epoch 11/750, Train Loss: 4.971955071477329, Val Loss: 5.3028580157409815, Val MAE: 1.4774500131607056\n",
      "Epoch 12/750, Train Loss: 4.956423704516929, Val Loss: 5.278591216823118, Val MAE: 1.496146321296692\n",
      "Epoch 13/750, Train Loss: 4.949404073504016, Val Loss: 5.283372827911974, Val MAE: 1.493441104888916\n",
      "Epoch 14/750, Train Loss: 4.944050770399892, Val Loss: 5.261779324362909, Val MAE: 1.483579158782959\n",
      "Epoch 15/750, Train Loss: 4.935693901220407, Val Loss: 5.268311926270128, Val MAE: 1.4910372495651245\n",
      "Epoch 16/750, Train Loss: 4.922546407178199, Val Loss: 5.262033689003409, Val MAE: 1.468945860862732\n",
      "Epoch 17/750, Train Loss: 4.912825736331279, Val Loss: 5.267677051174062, Val MAE: 1.4758341312408447\n",
      "Epoch 18/750, Train Loss: 4.912148636022653, Val Loss: 5.248643973250531, Val MAE: 1.4816581010818481\n",
      "Epoch 19/750, Train Loss: 4.897082586024459, Val Loss: 5.245444477257408, Val MAE: 1.4744622707366943\n",
      "Epoch 20/750, Train Loss: 4.887747168788448, Val Loss: 5.238947220624706, Val MAE: 1.472603440284729\n",
      "Epoch 21/750, Train Loss: 4.887894545294421, Val Loss: 5.234621322584077, Val MAE: 1.4641193151474\n",
      "Epoch 22/750, Train Loss: 4.873879855396838, Val Loss: 5.232240556364701, Val MAE: 1.50063955783844\n",
      "Epoch 23/750, Train Loss: 4.864992913902837, Val Loss: 5.228731734092246, Val MAE: 1.4697328805923462\n",
      "Epoch 24/750, Train Loss: 4.867787366424877, Val Loss: 5.236320628060235, Val MAE: 1.4919605255126953\n",
      "Epoch 25/750, Train Loss: 4.856981814526357, Val Loss: 5.20856497806376, Val MAE: 1.470438003540039\n",
      "Epoch 26/750, Train Loss: 4.846132787948661, Val Loss: 5.211960035870333, Val MAE: 1.4662929773330688\n",
      "Epoch 27/750, Train Loss: 4.842351144242864, Val Loss: 5.212877419819481, Val MAE: 1.4370859861373901\n",
      "Epoch 28/750, Train Loss: 4.835447086677419, Val Loss: 5.214818207870627, Val MAE: 1.4677830934524536\n",
      "Epoch 29/750, Train Loss: 4.826805176949419, Val Loss: 5.2074668631307395, Val MAE: 1.4575684070587158\n",
      "Epoch 30/750, Train Loss: 4.822703494167658, Val Loss: 5.21567866164194, Val MAE: 1.445953607559204\n",
      "Epoch 31/750, Train Loss: 4.820402695810919, Val Loss: 5.206931892135334, Val MAE: 1.4786345958709717\n",
      "Epoch 32/750, Train Loss: 4.812340592513035, Val Loss: 5.205455107681441, Val MAE: 1.4505665302276611\n",
      "Epoch 33/750, Train Loss: 4.816519030402689, Val Loss: 5.199844218195884, Val MAE: 1.434173345565796\n",
      "Epoch 34/750, Train Loss: 4.801555611765508, Val Loss: 5.214376554056326, Val MAE: 1.4841411113739014\n",
      "Epoch 35/750, Train Loss: 4.793428331262925, Val Loss: 5.206980686232518, Val MAE: 1.4644010066986084\n",
      "Epoch 36/750, Train Loss: 4.788894919293149, Val Loss: 5.183407310402263, Val MAE: 1.4727476835250854\n",
      "Epoch 37/750, Train Loss: 4.77556151155782, Val Loss: 5.199759525125948, Val MAE: 1.4447989463806152\n",
      "Epoch 38/750, Train Loss: 4.775359630914708, Val Loss: 5.191042919487274, Val MAE: 1.4508577585220337\n",
      "Epoch 39/750, Train Loss: 4.776785661769986, Val Loss: 5.20088577121263, Val MAE: 1.4441391229629517\n",
      "Epoch 40/750, Train Loss: 4.765785512297211, Val Loss: 5.189072302249675, Val MAE: 1.4536997079849243\n",
      "Epoch 41/750, Train Loss: 4.762956494773548, Val Loss: 5.201788488128376, Val MAE: 1.449307918548584\n",
      "Epoch 42/750, Train Loss: 4.758645746369675, Val Loss: 5.188380914488122, Val MAE: 1.467950701713562\n",
      "Epoch 43/750, Train Loss: 4.7583612649911, Val Loss: 5.200554064555161, Val MAE: 1.4575165510177612\n",
      "Epoch 44/750, Train Loss: 4.745484054377335, Val Loss: 5.1934487618191145, Val MAE: 1.4856278896331787\n",
      "Epoch 45/750, Train Loss: 4.7415081525756415, Val Loss: 5.1656555032506235, Val MAE: 1.4545488357543945\n",
      "Epoch 46/750, Train Loss: 4.730583476277784, Val Loss: 5.198162673224865, Val MAE: 1.469044804573059\n",
      "Epoch 47/750, Train Loss: 4.738484612963191, Val Loss: 5.1829880194298354, Val MAE: 1.4249225854873657\n",
      "Epoch 48/750, Train Loss: 4.720534406813783, Val Loss: 5.18557324879606, Val MAE: 1.442313551902771\n",
      "Epoch 49/750, Train Loss: 4.72106241123899, Val Loss: 5.191481151864376, Val MAE: 1.431023359298706\n",
      "Epoch 50/750, Train Loss: 4.7155647428802965, Val Loss: 5.166542689005534, Val MAE: 1.4465105533599854\n",
      "Epoch 51/750, Train Loss: 4.705036514440622, Val Loss: 5.182461854251152, Val MAE: 1.4806889295578003\n",
      "Epoch 52/750, Train Loss: 4.70909946344303, Val Loss: 5.182316664425606, Val MAE: 1.4522181749343872\n",
      "Epoch 53/750, Train Loss: 4.6969403006213755, Val Loss: 5.1757052780503585, Val MAE: 1.447466492652893\n",
      "Epoch 54/750, Train Loss: 4.693278731441827, Val Loss: 5.177702547798694, Val MAE: 1.471522331237793\n",
      "Epoch 55/750, Train Loss: 4.689760816798491, Val Loss: 5.164372103128448, Val MAE: 1.4302995204925537\n",
      "Epoch 56/750, Train Loss: 4.685680825982539, Val Loss: 5.171512582865493, Val MAE: 1.4672996997833252\n",
      "Epoch 57/750, Train Loss: 4.681808682874, Val Loss: 5.168487356675436, Val MAE: 1.4841662645339966\n",
      "Epoch 58/750, Train Loss: 4.681065643039954, Val Loss: 5.175591328147805, Val MAE: 1.4233558177947998\n",
      "Epoch 59/750, Train Loss: 4.673371298502886, Val Loss: 5.17077000674694, Val MAE: 1.4648836851119995\n",
      "Epoch 60/750, Train Loss: 4.667870959361119, Val Loss: 5.1783659577556245, Val MAE: 1.4233876466751099\n",
      "Epoch 61/750, Train Loss: 4.659353560936286, Val Loss: 5.184189830027836, Val MAE: 1.4576586484909058\n",
      "Epoch 62/750, Train Loss: 4.658373246803416, Val Loss: 5.17254450026439, Val MAE: 1.4722052812576294\n",
      "Epoch 63/750, Train Loss: 4.659779649457304, Val Loss: 5.17317623189171, Val MAE: 1.4522491693496704\n",
      "Epoch 64/750, Train Loss: 4.6490598926082205, Val Loss: 5.168706871161065, Val MAE: 1.450980305671692\n",
      "Epoch 65/750, Train Loss: 4.649342414045829, Val Loss: 5.184088359976038, Val MAE: 1.4231411218643188\n",
      "Epoch 66/750, Train Loss: 4.669153605530418, Val Loss: 5.182784413694403, Val MAE: 1.4639986753463745\n",
      "Epoch 67/750, Train Loss: 4.657799443819119, Val Loss: 5.178286508775093, Val MAE: 1.4139692783355713\n",
      "Epoch 68/750, Train Loss: 4.631270649276391, Val Loss: 5.171807543585931, Val MAE: 1.457740306854248\n",
      "Epoch 69/750, Train Loss: 4.6279244097046375, Val Loss: 5.173252363533295, Val MAE: 1.4693989753723145\n",
      "Epoch 70/750, Train Loss: 4.62408636862019, Val Loss: 5.168333055081315, Val MAE: 1.4292800426483154\n",
      "Epoch 71/750, Train Loss: 4.6231450028073, Val Loss: 5.166127878362211, Val MAE: 1.4418468475341797\n",
      "Epoch 72/750, Train Loss: 4.615904403145338, Val Loss: 5.18365367178999, Val MAE: 1.4642239809036255\n",
      "Epoch 73/750, Train Loss: 4.6108244535419765, Val Loss: 5.192261081346324, Val MAE: 1.4595685005187988\n",
      "Epoch 74/750, Train Loss: 4.6139384631054625, Val Loss: 5.176580333560472, Val MAE: 1.44924795627594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:52<00:00,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/750, Train Loss: 4.602231385056123, Val Loss: 5.174055634156826, Val MAE: 1.475074291229248\n",
      "Early stopping\n",
      "Test Loss (MSE): 5.178800582885742\n",
      "Test Mean Absolute Error (MAE): 1.589289438086406\n",
      "Logging experiment results to gridsearch\\results\\cnn_2d_gk_12-11.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Dean\\Code\\csci-567\\final_project\\ml-premier-predictor\\final_project\\cnn2d\\..\\..\\final_project\\cnn2d\\experiment.py:212: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  log_df = pd.concat([log_df, experiment_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from final_project.cnn2d.experiment import gridsearch_cnn\n",
    "\n",
    "gridsearch_cnn(experiment_name=\"cnn_2d_gk\", verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized)  {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
