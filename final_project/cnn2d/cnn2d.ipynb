{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.cnn2d.model import build_train_cnn, full_cnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from final_project.cnn2d.config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2178, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 9).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2178, 7)\n",
      "Shape of a given window (prior to preprocessing): (10, 9)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.89197166e+00 4.73813459e+01 0.00000000e+00 1.18063754e-03\n",
      " 1.49940968e-01 9.55312869e+00 2.59740260e-02 1.18063754e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[2.67958644e+00 4.48476343e+01 1.00000000e+00 3.43401171e-02\n",
      " 3.57013549e-01 1.04005585e+01 1.59057776e-01 3.43401171e-02]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building CNN Architecture ======\n",
      "====== Done Building CNN Architecture ======\n",
      "Epoch 1/2000, Train Loss: 10.42613003744685, Val Loss: 11.712574833570278, Val MAE: 2.0174736976623535\n",
      "Epoch 2/2000, Train Loss: 10.414046128333291, Val Loss: 11.700141782323792, Val MAE: 2.01576566696167\n",
      "Epoch 3/2000, Train Loss: 10.402071974459112, Val Loss: 11.687955767936902, Val MAE: 2.01406192779541\n",
      "Epoch 4/2000, Train Loss: 10.390134902595507, Val Loss: 11.675762934839888, Val MAE: 2.01240611076355\n",
      "Epoch 5/2000, Train Loss: 10.378216145297504, Val Loss: 11.663640287334788, Val MAE: 2.010798931121826\n",
      "Epoch 6/2000, Train Loss: 10.366416496820928, Val Loss: 11.651336421297021, Val MAE: 2.0091066360473633\n",
      "Epoch 7/2000, Train Loss: 10.35450474756077, Val Loss: 11.639295589944272, Val MAE: 2.007422924041748\n",
      "Epoch 8/2000, Train Loss: 10.342600986289051, Val Loss: 11.627201086241906, Val MAE: 2.0058515071868896\n",
      "Epoch 9/2000, Train Loss: 10.330609648502048, Val Loss: 11.615271635659754, Val MAE: 2.004176616668701\n",
      "Epoch 10/2000, Train Loss: 10.318830325499153, Val Loss: 11.602950564190133, Val MAE: 2.002528667449951\n",
      "Epoch 11/2000, Train Loss: 10.30694301240448, Val Loss: 11.59099439541771, Val MAE: 2.000896692276001\n",
      "Epoch 12/2000, Train Loss: 10.295171530552004, Val Loss: 11.578505431340165, Val MAE: 1.9992713928222656\n",
      "Epoch 13/2000, Train Loss: 10.283206761945975, Val Loss: 11.56656267708295, Val MAE: 1.9976246356964111\n",
      "Epoch 14/2000, Train Loss: 10.271476830616956, Val Loss: 11.554433336711092, Val MAE: 1.996061086654663\n",
      "Epoch 15/2000, Train Loss: 10.259658929401317, Val Loss: 11.542178588370755, Val MAE: 1.994402527809143\n",
      "Epoch 16/2000, Train Loss: 10.24778819586508, Val Loss: 11.530103133965845, Val MAE: 1.9928189516067505\n",
      "Epoch 17/2000, Train Loss: 10.236023936124829, Val Loss: 11.517937170009906, Val MAE: 1.9911787509918213\n",
      "Epoch 18/2000, Train Loss: 10.224220110492922, Val Loss: 11.505860505038745, Val MAE: 1.9895751476287842\n",
      "Epoch 19/2000, Train Loss: 10.212536553702826, Val Loss: 11.493653015016694, Val MAE: 1.9879359006881714\n",
      "Epoch 20/2000, Train Loss: 10.200771636282798, Val Loss: 11.481703859894242, Val MAE: 1.9864132404327393\n",
      "Epoch 21/2000, Train Loss: 10.189004132967908, Val Loss: 11.469579954567838, Val MAE: 1.984816074371338\n",
      "Epoch 22/2000, Train Loss: 10.17724769892144, Val Loss: 11.457713683072019, Val MAE: 1.9832960367202759\n",
      "Epoch 23/2000, Train Loss: 10.165589192698029, Val Loss: 11.445827830429764, Val MAE: 1.9817053079605103\n",
      "Epoch 24/2000, Train Loss: 10.153892158495935, Val Loss: 11.433940763138745, Val MAE: 1.9801255464553833\n",
      "Epoch 25/2000, Train Loss: 10.142164131424222, Val Loss: 11.421694109905255, Val MAE: 1.978508472442627\n",
      "Epoch 26/2000, Train Loss: 10.130537138186165, Val Loss: 11.409523596094079, Val MAE: 1.9769295454025269\n",
      "Epoch 27/2000, Train Loss: 10.118818669496903, Val Loss: 11.397831924055538, Val MAE: 1.9754208326339722\n",
      "Epoch 28/2000, Train Loss: 10.107329917301815, Val Loss: 11.385724551057162, Val MAE: 1.9737623929977417\n",
      "Epoch 29/2000, Train Loss: 10.09547530194349, Val Loss: 11.373936386038995, Val MAE: 1.97231924533844\n",
      "Epoch 30/2000, Train Loss: 10.083915594524465, Val Loss: 11.36162936371075, Val MAE: 1.970641851425171\n",
      "Epoch 31/2000, Train Loss: 10.072252080351646, Val Loss: 11.349556601312878, Val MAE: 1.9691208600997925\n",
      "Epoch 32/2000, Train Loss: 10.060509766713148, Val Loss: 11.337884565126407, Val MAE: 1.9675631523132324\n",
      "Epoch 33/2000, Train Loss: 10.048966360633145, Val Loss: 11.325798166737165, Val MAE: 1.9660180807113647\n",
      "Epoch 34/2000, Train Loss: 10.037322044372559, Val Loss: 11.31394844624686, Val MAE: 1.9645280838012695\n",
      "Epoch 35/2000, Train Loss: 10.025777542417208, Val Loss: 11.30177160628038, Val MAE: 1.9629206657409668\n",
      "Epoch 36/2000, Train Loss: 10.014202828152253, Val Loss: 11.289886447052433, Val MAE: 1.9613317251205444\n",
      "Epoch 37/2000, Train Loss: 10.0026371834537, Val Loss: 11.27797549279177, Val MAE: 1.9598033428192139\n",
      "Epoch 38/2000, Train Loss: 9.991015897964155, Val Loss: 11.26644597855741, Val MAE: 1.9583121538162231\n",
      "Epoch 39/2000, Train Loss: 9.97949017659192, Val Loss: 11.25431836406662, Val MAE: 1.9566909074783325\n",
      "Epoch 40/2000, Train Loss: 9.967910682362907, Val Loss: 11.242221017304349, Val MAE: 1.9551247358322144\n",
      "Epoch 41/2000, Train Loss: 9.956258062798549, Val Loss: 11.23019014931705, Val MAE: 1.9535434246063232\n",
      "Epoch 42/2000, Train Loss: 9.944565850293424, Val Loss: 11.218394040040774, Val MAE: 1.9520106315612793\n",
      "Epoch 43/2000, Train Loss: 9.932854921350216, Val Loss: 11.206518805904748, Val MAE: 1.9505013227462769\n",
      "Epoch 44/2000, Train Loss: 9.921273401529321, Val Loss: 11.194222777061267, Val MAE: 1.948920726776123\n",
      "Epoch 45/2000, Train Loss: 9.909463730198445, Val Loss: 11.182144672903297, Val MAE: 1.947352409362793\n",
      "Epoch 46/2000, Train Loss: 9.897918609204902, Val Loss: 11.169839190293665, Val MAE: 1.9457372426986694\n",
      "Epoch 47/2000, Train Loss: 9.886125417737173, Val Loss: 11.158227063919583, Val MAE: 1.9442707300186157\n",
      "Epoch 48/2000, Train Loss: 9.874638024180209, Val Loss: 11.146102820357232, Val MAE: 1.9426913261413574\n",
      "Epoch 49/2000, Train Loss: 9.862927225072728, Val Loss: 11.134029641135099, Val MAE: 1.9411226511001587\n",
      "Epoch 50/2000, Train Loss: 9.851235287695689, Val Loss: 11.122402861408174, Val MAE: 1.9396220445632935\n",
      "Epoch 51/2000, Train Loss: 9.839737298616134, Val Loss: 11.109930574996014, Val MAE: 1.9380087852478027\n",
      "Epoch 52/2000, Train Loss: 9.82790608382882, Val Loss: 11.098132820978556, Val MAE: 1.9365240335464478\n",
      "Epoch 53/2000, Train Loss: 9.816415716801895, Val Loss: 11.085887485477206, Val MAE: 1.9349156618118286\n",
      "Epoch 54/2000, Train Loss: 9.804699703012526, Val Loss: 11.073993205411794, Val MAE: 1.9333857297897339\n",
      "Epoch 55/2000, Train Loss: 9.79304712595391, Val Loss: 11.061971623399486, Val MAE: 1.931870937347412\n",
      "Epoch 56/2000, Train Loss: 9.78137102853537, Val Loss: 11.049713216517887, Val MAE: 1.930250644683838\n",
      "Epoch 57/2000, Train Loss: 9.769568488219182, Val Loss: 11.037382653416836, Val MAE: 1.9286714792251587\n",
      "Epoch 58/2000, Train Loss: 9.757817065889677, Val Loss: 11.025560065695684, Val MAE: 1.9271173477172852\n",
      "Epoch 59/2000, Train Loss: 9.746119364733827, Val Loss: 11.013697241981552, Val MAE: 1.9256541728973389\n",
      "Epoch 60/2000, Train Loss: 9.73445031090339, Val Loss: 11.001239939095223, Val MAE: 1.9240167140960693\n",
      "Epoch 61/2000, Train Loss: 9.722678631206, Val Loss: 10.98897682844776, Val MAE: 1.9224579334259033\n",
      "Epoch 62/2000, Train Loss: 9.710921988494972, Val Loss: 10.976953065048342, Val MAE: 1.9209866523742676\n",
      "Epoch 63/2000, Train Loss: 9.699236533243807, Val Loss: 10.964782489897454, Val MAE: 1.9194118976593018\n",
      "Epoch 64/2000, Train Loss: 9.687516056351376, Val Loss: 10.952713777655608, Val MAE: 1.91787588596344\n",
      "Epoch 65/2000, Train Loss: 9.675874836827406, Val Loss: 10.940554825920765, Val MAE: 1.9162434339523315\n",
      "Epoch 66/2000, Train Loss: 9.664109599455056, Val Loss: 10.928563527456701, Val MAE: 1.9148329496383667\n",
      "Epoch 67/2000, Train Loss: 9.65239076908056, Val Loss: 10.916525483641722, Val MAE: 1.9132707118988037\n",
      "Epoch 68/2000, Train Loss: 9.640616276275782, Val Loss: 10.904065799733548, Val MAE: 1.911699891090393\n",
      "Epoch 69/2000, Train Loss: 9.6287550377498, Val Loss: 10.891806813135538, Val MAE: 1.910158395767212\n",
      "Epoch 70/2000, Train Loss: 9.616901369883138, Val Loss: 10.879399414544237, Val MAE: 1.9085646867752075\n",
      "Epoch 71/2000, Train Loss: 9.605090561330416, Val Loss: 10.866911692774458, Val MAE: 1.9069318771362305\n",
      "Epoch 72/2000, Train Loss: 9.59330050871863, Val Loss: 10.854724893218851, Val MAE: 1.9053900241851807\n",
      "Epoch 73/2000, Train Loss: 9.58138306832584, Val Loss: 10.842640534859814, Val MAE: 1.9038496017456055\n",
      "Epoch 74/2000, Train Loss: 9.569688039622005, Val Loss: 10.8301790027994, Val MAE: 1.9022763967514038\n",
      "Epoch 75/2000, Train Loss: 9.55769367156191, Val Loss: 10.818222855256028, Val MAE: 1.900755763053894\n",
      "Epoch 76/2000, Train Loss: 9.546003954530342, Val Loss: 10.80570813411311, Val MAE: 1.8991293907165527\n",
      "Epoch 77/2000, Train Loss: 9.534064540213858, Val Loss: 10.793518772578402, Val MAE: 1.8976093530654907\n",
      "Epoch 78/2000, Train Loss: 9.522276598591858, Val Loss: 10.781290548815303, Val MAE: 1.8959933519363403\n",
      "Epoch 79/2000, Train Loss: 9.510509619256858, Val Loss: 10.769187627079551, Val MAE: 1.8945049047470093\n",
      "Epoch 80/2000, Train Loss: 9.498704370841795, Val Loss: 10.756924060313668, Val MAE: 1.8929088115692139\n",
      "Epoch 81/2000, Train Loss: 9.48676632057326, Val Loss: 10.744985789366782, Val MAE: 1.8914400339126587\n",
      "Epoch 82/2000, Train Loss: 9.475005389419135, Val Loss: 10.732223766307309, Val MAE: 1.8897491693496704\n",
      "Epoch 83/2000, Train Loss: 9.462986199550535, Val Loss: 10.720241632363567, Val MAE: 1.8881975412368774\n",
      "Epoch 84/2000, Train Loss: 9.451204819269659, Val Loss: 10.707575935207002, Val MAE: 1.886614441871643\n",
      "Epoch 85/2000, Train Loss: 9.439205684383646, Val Loss: 10.69513027369976, Val MAE: 1.8849612474441528\n",
      "Epoch 86/2000, Train Loss: 9.427205309875587, Val Loss: 10.682966401621904, Val MAE: 1.8834408521652222\n",
      "Epoch 87/2000, Train Loss: 9.415334481281052, Val Loss: 10.670318147091017, Val MAE: 1.8817347288131714\n",
      "Epoch 88/2000, Train Loss: 9.403308257871162, Val Loss: 10.658213912113888, Val MAE: 1.8801984786987305\n",
      "Epoch 89/2000, Train Loss: 9.391453389024038, Val Loss: 10.64577355082721, Val MAE: 1.8786219358444214\n",
      "Epoch 90/2000, Train Loss: 9.379375116172062, Val Loss: 10.633567291273646, Val MAE: 1.877081036567688\n",
      "Epoch 91/2000, Train Loss: 9.367497414785227, Val Loss: 10.620853270161641, Val MAE: 1.875421404838562\n",
      "Epoch 92/2000, Train Loss: 9.355344896764771, Val Loss: 10.608548290925484, Val MAE: 1.8738255500793457\n",
      "Epoch 93/2000, Train Loss: 9.343408815671404, Val Loss: 10.596133381026249, Val MAE: 1.8722325563430786\n",
      "Epoch 94/2000, Train Loss: 9.331413638842937, Val Loss: 10.583663415010662, Val MAE: 1.8706332445144653\n",
      "Epoch 95/2000, Train Loss: 9.31941654693752, Val Loss: 10.571441708668454, Val MAE: 1.869049310684204\n",
      "Epoch 96/2000, Train Loss: 9.307408185986684, Val Loss: 10.558830425551493, Val MAE: 1.867425799369812\n",
      "Epoch 97/2000, Train Loss: 9.29534562325748, Val Loss: 10.546351482913103, Val MAE: 1.8658702373504639\n",
      "Epoch 98/2000, Train Loss: 9.283480424741871, Val Loss: 10.53365623461057, Val MAE: 1.8641997575759888\n",
      "Epoch 99/2000, Train Loss: 9.271346175689944, Val Loss: 10.521725060698921, Val MAE: 1.8626805543899536\n",
      "Epoch 100/2000, Train Loss: 9.259432875356566, Val Loss: 10.509281085166213, Val MAE: 1.8610894680023193\n",
      "Epoch 101/2000, Train Loss: 9.247524545876674, Val Loss: 10.496820276220367, Val MAE: 1.8594893217086792\n",
      "Epoch 102/2000, Train Loss: 9.235528055620735, Val Loss: 10.484505008650968, Val MAE: 1.8578888177871704\n",
      "Epoch 103/2000, Train Loss: 9.223454640788816, Val Loss: 10.472269280214016, Val MAE: 1.856414794921875\n",
      "Epoch 104/2000, Train Loss: 9.211638456805213, Val Loss: 10.459171495413127, Val MAE: 1.8546597957611084\n",
      "Epoch 105/2000, Train Loss: 9.199315638364425, Val Loss: 10.447308114334328, Val MAE: 1.853179931640625\n",
      "Epoch 106/2000, Train Loss: 9.18743279223697, Val Loss: 10.434675331393333, Val MAE: 1.8515095710754395\n",
      "Epoch 107/2000, Train Loss: 9.17549312172678, Val Loss: 10.421977890680914, Val MAE: 1.8499090671539307\n",
      "Epoch 108/2000, Train Loss: 9.1634619834937, Val Loss: 10.409714364638067, Val MAE: 1.8482840061187744\n",
      "Epoch 109/2000, Train Loss: 9.151531958309526, Val Loss: 10.39719380519978, Val MAE: 1.8466792106628418\n",
      "Epoch 110/2000, Train Loss: 9.139615389864101, Val Loss: 10.384580828759768, Val MAE: 1.8450813293457031\n",
      "Epoch 111/2000, Train Loss: 9.127617380027832, Val Loss: 10.372372732893245, Val MAE: 1.8435438871383667\n",
      "Epoch 112/2000, Train Loss: 9.115696233134216, Val Loss: 10.360072654097864, Val MAE: 1.8419536352157593\n",
      "Epoch 113/2000, Train Loss: 9.103772040510874, Val Loss: 10.347418706098647, Val MAE: 1.840272307395935\n",
      "Epoch 114/2000, Train Loss: 9.09178554760579, Val Loss: 10.334963607869737, Val MAE: 1.8386694192886353\n",
      "Epoch 115/2000, Train Loss: 9.07988254139064, Val Loss: 10.322330363196869, Val MAE: 1.8370660543441772\n",
      "Epoch 116/2000, Train Loss: 9.06783650721298, Val Loss: 10.310046120559516, Val MAE: 1.8354605436325073\n",
      "Epoch 117/2000, Train Loss: 9.05589452400393, Val Loss: 10.297538566875131, Val MAE: 1.8338431119918823\n",
      "Epoch 118/2000, Train Loss: 9.043962802454176, Val Loss: 10.28497476100105, Val MAE: 1.8322070837020874\n",
      "Epoch 119/2000, Train Loss: 9.03197922853442, Val Loss: 10.27221844784201, Val MAE: 1.8306251764297485\n",
      "Epoch 120/2000, Train Loss: 9.019742490406747, Val Loss: 10.260165857840073, Val MAE: 1.8290668725967407\n",
      "Epoch 121/2000, Train Loss: 9.007922133904803, Val Loss: 10.247544334461427, Val MAE: 1.8274258375167847\n",
      "Epoch 122/2000, Train Loss: 8.995983810053845, Val Loss: 10.234788807083483, Val MAE: 1.825751781463623\n",
      "Epoch 123/2000, Train Loss: 8.983925587547462, Val Loss: 10.222434293019445, Val MAE: 1.8242267370224\n",
      "Epoch 124/2000, Train Loss: 8.972013671742072, Val Loss: 10.209790508734853, Val MAE: 1.8225996494293213\n",
      "Epoch 125/2000, Train Loss: 8.959959105116042, Val Loss: 10.197508378286068, Val MAE: 1.8210071325302124\n",
      "Epoch 126/2000, Train Loss: 8.948034160527456, Val Loss: 10.184821004532788, Val MAE: 1.8193409442901611\n",
      "Epoch 127/2000, Train Loss: 8.93594452395818, Val Loss: 10.17239650328682, Val MAE: 1.8177565336227417\n",
      "Epoch 128/2000, Train Loss: 8.924030616179085, Val Loss: 10.159728851934819, Val MAE: 1.8161190748214722\n",
      "Epoch 129/2000, Train Loss: 8.911822836828, Val Loss: 10.14726170234076, Val MAE: 1.8144644498825073\n",
      "Epoch 130/2000, Train Loss: 8.899814849927708, Val Loss: 10.13461660522304, Val MAE: 1.81278657913208\n",
      "Epoch 131/2000, Train Loss: 8.887802092337338, Val Loss: 10.121807089917464, Val MAE: 1.8111563920974731\n",
      "Epoch 132/2000, Train Loss: 8.875589795784974, Val Loss: 10.109641048700025, Val MAE: 1.80959951877594\n",
      "Epoch 133/2000, Train Loss: 8.863657893392602, Val Loss: 10.096872751129, Val MAE: 1.807924509048462\n",
      "Epoch 134/2000, Train Loss: 8.851741095222955, Val Loss: 10.084070977078726, Val MAE: 1.806243896484375\n",
      "Epoch 135/2000, Train Loss: 8.839559652430506, Val Loss: 10.071411038710647, Val MAE: 1.8045934438705444\n",
      "Epoch 136/2000, Train Loss: 8.827437311169394, Val Loss: 10.059130797035074, Val MAE: 1.8030425310134888\n",
      "Epoch 137/2000, Train Loss: 8.81536888419326, Val Loss: 10.046614707536893, Val MAE: 1.8014004230499268\n",
      "Epoch 138/2000, Train Loss: 8.803254074949885, Val Loss: 10.033757217942853, Val MAE: 1.799688458442688\n",
      "Epoch 139/2000, Train Loss: 8.791120356060505, Val Loss: 10.02109201568855, Val MAE: 1.7980668544769287\n",
      "Epoch 140/2000, Train Loss: 8.778939772656903, Val Loss: 10.008468041885388, Val MAE: 1.7964022159576416\n",
      "Epoch 141/2000, Train Loss: 8.766923850320726, Val Loss: 9.995698386879816, Val MAE: 1.7946865558624268\n",
      "Epoch 142/2000, Train Loss: 8.754741656722281, Val Loss: 9.983020886064391, Val MAE: 1.7930632829666138\n",
      "Epoch 143/2000, Train Loss: 8.742746128255389, Val Loss: 9.970413907341761, Val MAE: 1.7913671731948853\n",
      "Epoch 144/2000, Train Loss: 8.730654575208405, Val Loss: 9.957801110430124, Val MAE: 1.7897610664367676\n",
      "Epoch 145/2000, Train Loss: 8.718515759349062, Val Loss: 9.945584231860018, Val MAE: 1.78814697265625\n",
      "Epoch 146/2000, Train Loss: 8.706521590679547, Val Loss: 9.932904922084449, Val MAE: 1.7864621877670288\n",
      "Epoch 147/2000, Train Loss: 8.69429746593803, Val Loss: 9.920361437209666, Val MAE: 1.7848469018936157\n",
      "Epoch 148/2000, Train Loss: 8.682143889910977, Val Loss: 9.908021206725133, Val MAE: 1.7832058668136597\n",
      "Epoch 149/2000, Train Loss: 8.670144386384244, Val Loss: 9.8951049133727, Val MAE: 1.7815676927566528\n",
      "Epoch 150/2000, Train Loss: 8.657912507242669, Val Loss: 9.882397001430597, Val MAE: 1.7798703908920288\n",
      "Epoch 151/2000, Train Loss: 8.645797530101609, Val Loss: 9.869907509280393, Val MAE: 1.7782282829284668\n",
      "Epoch 152/2000, Train Loss: 8.633638253667947, Val Loss: 9.857206015758319, Val MAE: 1.7765369415283203\n",
      "Epoch 153/2000, Train Loss: 8.621432574873419, Val Loss: 9.844546103824491, Val MAE: 1.7748985290527344\n",
      "Epoch 154/2000, Train Loss: 8.609241189601162, Val Loss: 9.831766832772999, Val MAE: 1.773168921470642\n",
      "Epoch 155/2000, Train Loss: 8.59706763819319, Val Loss: 9.819118531803563, Val MAE: 1.771470546722412\n",
      "Epoch 156/2000, Train Loss: 8.584883592503576, Val Loss: 9.806616517268631, Val MAE: 1.7698544263839722\n",
      "Epoch 157/2000, Train Loss: 8.572670922673503, Val Loss: 9.79357544383774, Val MAE: 1.7681918144226074\n",
      "Epoch 158/2000, Train Loss: 8.560416810516406, Val Loss: 9.781169384922066, Val MAE: 1.766553282737732\n",
      "Epoch 159/2000, Train Loss: 8.548252943462453, Val Loss: 9.768317981125557, Val MAE: 1.764894723892212\n",
      "Epoch 160/2000, Train Loss: 8.535865122147472, Val Loss: 9.756125248866539, Val MAE: 1.7633451223373413\n",
      "Epoch 161/2000, Train Loss: 8.523816577812454, Val Loss: 9.743244041522889, Val MAE: 1.7616699934005737\n",
      "Epoch 162/2000, Train Loss: 8.511594862180552, Val Loss: 9.730581466874032, Val MAE: 1.7600998878479004\n",
      "Epoch 163/2000, Train Loss: 8.49937876233019, Val Loss: 9.718179437395644, Val MAE: 1.7585704326629639\n",
      "Epoch 164/2000, Train Loss: 8.487191084331593, Val Loss: 9.705313658061092, Val MAE: 1.7568892240524292\n",
      "Epoch 165/2000, Train Loss: 8.474868394953699, Val Loss: 9.692821442876777, Val MAE: 1.7553633451461792\n",
      "Epoch 166/2000, Train Loss: 8.462707170211322, Val Loss: 9.680280239818847, Val MAE: 1.7537542581558228\n",
      "Epoch 167/2000, Train Loss: 8.450359185279092, Val Loss: 9.667380062888746, Val MAE: 1.7522034645080566\n",
      "Epoch 168/2000, Train Loss: 8.438179901778408, Val Loss: 9.6545853869964, Val MAE: 1.7505459785461426\n",
      "Epoch 169/2000, Train Loss: 8.425900513001354, Val Loss: 9.641954060694943, Val MAE: 1.7489687204360962\n",
      "Epoch 170/2000, Train Loss: 8.413827077689396, Val Loss: 9.629229442101636, Val MAE: 1.7473543882369995\n",
      "Epoch 171/2000, Train Loss: 8.401542193692546, Val Loss: 9.616986801363018, Val MAE: 1.7458304166793823\n",
      "Epoch 172/2000, Train Loss: 8.389338555946535, Val Loss: 9.604325000962167, Val MAE: 1.7442125082015991\n",
      "Epoch 173/2000, Train Loss: 8.377040989781507, Val Loss: 9.591920888383095, Val MAE: 1.7426042556762695\n",
      "Epoch 174/2000, Train Loss: 8.364812663926877, Val Loss: 9.57876586301686, Val MAE: 1.740892767906189\n",
      "Epoch 175/2000, Train Loss: 8.352413791118604, Val Loss: 9.566240166148095, Val MAE: 1.7393913269042969\n",
      "Epoch 176/2000, Train Loss: 8.340267926406241, Val Loss: 9.553327386509883, Val MAE: 1.737760305404663\n",
      "Epoch 177/2000, Train Loss: 8.327913526970912, Val Loss: 9.54100548486187, Val MAE: 1.73617684841156\n",
      "Epoch 178/2000, Train Loss: 8.315897130116072, Val Loss: 9.528091587228317, Val MAE: 1.7345043420791626\n",
      "Epoch 179/2000, Train Loss: 8.303542622484303, Val Loss: 9.515843829880023, Val MAE: 1.732972264289856\n",
      "Epoch 180/2000, Train Loss: 8.291416413586955, Val Loss: 9.503095873006403, Val MAE: 1.7313259840011597\n",
      "Epoch 181/2000, Train Loss: 8.279195975639059, Val Loss: 9.49044726993123, Val MAE: 1.7297197580337524\n",
      "Epoch 182/2000, Train Loss: 8.267101789216362, Val Loss: 9.477460971842074, Val MAE: 1.7280853986740112\n",
      "Epoch 183/2000, Train Loss: 8.254683814520195, Val Loss: 9.465454188519962, Val MAE: 1.726639747619629\n",
      "Epoch 184/2000, Train Loss: 8.242527365491302, Val Loss: 9.4526859587186, Val MAE: 1.7250795364379883\n",
      "Epoch 185/2000, Train Loss: 8.230374513221134, Val Loss: 9.439915528240268, Val MAE: 1.7235056161880493\n",
      "Epoch 186/2000, Train Loss: 8.218081550041706, Val Loss: 9.42741996233594, Val MAE: 1.7219454050064087\n",
      "Epoch 187/2000, Train Loss: 8.205856307785554, Val Loss: 9.414933845604935, Val MAE: 1.720416784286499\n",
      "Epoch 188/2000, Train Loss: 8.193614548570332, Val Loss: 9.40196966165549, Val MAE: 1.7188327312469482\n",
      "Epoch 189/2000, Train Loss: 8.181255326665202, Val Loss: 9.389277346738398, Val MAE: 1.7172529697418213\n",
      "Epoch 190/2000, Train Loss: 8.169007522360433, Val Loss: 9.376608197615571, Val MAE: 1.7156879901885986\n",
      "Epoch 191/2000, Train Loss: 8.156629891604233, Val Loss: 9.364132492713733, Val MAE: 1.714127779006958\n",
      "Epoch 192/2000, Train Loss: 8.14460622684689, Val Loss: 9.351121419913149, Val MAE: 1.7125037908554077\n",
      "Epoch 193/2000, Train Loss: 8.13236034979117, Val Loss: 9.338704485599308, Val MAE: 1.7109826803207397\n",
      "Epoch 194/2000, Train Loss: 8.12019761068508, Val Loss: 9.326276769580906, Val MAE: 1.7094764709472656\n",
      "Epoch 195/2000, Train Loss: 8.108109836640196, Val Loss: 9.313428690988724, Val MAE: 1.7078653573989868\n",
      "Epoch 196/2000, Train Loss: 8.095910234621316, Val Loss: 9.300803731565606, Val MAE: 1.7063530683517456\n",
      "Epoch 197/2000, Train Loss: 8.083670722027845, Val Loss: 9.288321530369863, Val MAE: 1.7048228979110718\n",
      "Epoch 198/2000, Train Loss: 8.071429876299693, Val Loss: 9.275798275862655, Val MAE: 1.7032718658447266\n",
      "Epoch 199/2000, Train Loss: 8.059334290471996, Val Loss: 9.262838477141237, Val MAE: 1.7016425132751465\n",
      "Epoch 200/2000, Train Loss: 8.04693313703738, Val Loss: 9.250524673151643, Val MAE: 1.700137972831726\n",
      "Epoch 201/2000, Train Loss: 8.034851231876416, Val Loss: 9.23797240975785, Val MAE: 1.6986514329910278\n",
      "Epoch 202/2000, Train Loss: 8.022718722468644, Val Loss: 9.225125689416716, Val MAE: 1.6970477104187012\n",
      "Epoch 203/2000, Train Loss: 8.010415513859382, Val Loss: 9.212607778914988, Val MAE: 1.6955019235610962\n",
      "Epoch 204/2000, Train Loss: 7.998303350018914, Val Loss: 9.199819766495326, Val MAE: 1.6939525604248047\n",
      "Epoch 205/2000, Train Loss: 7.985929533187149, Val Loss: 9.187431439553222, Val MAE: 1.6924582719802856\n",
      "Epoch 206/2000, Train Loss: 7.973850399014243, Val Loss: 9.1745471556301, Val MAE: 1.6908713579177856\n",
      "Epoch 207/2000, Train Loss: 7.9616114726136145, Val Loss: 9.162047819936113, Val MAE: 1.6893657445907593\n",
      "Epoch 208/2000, Train Loss: 7.949473682446843, Val Loss: 9.14929675606832, Val MAE: 1.6878021955490112\n",
      "Epoch 209/2000, Train Loss: 7.937266641150031, Val Loss: 9.136841616810184, Val MAE: 1.686303973197937\n",
      "Epoch 210/2000, Train Loss: 7.925138867654908, Val Loss: 9.124187058986049, Val MAE: 1.6847667694091797\n",
      "Epoch 211/2000, Train Loss: 7.913047972915625, Val Loss: 9.111526676849143, Val MAE: 1.6832764148712158\n",
      "Epoch 212/2000, Train Loss: 7.9008829466141215, Val Loss: 9.09876551468895, Val MAE: 1.6817106008529663\n",
      "Epoch 213/2000, Train Loss: 7.8885821878813465, Val Loss: 9.08666307285224, Val MAE: 1.6802796125411987\n",
      "Epoch 214/2000, Train Loss: 7.876524500947331, Val Loss: 9.073621497170565, Val MAE: 1.6786272525787354\n",
      "Epoch 215/2000, Train Loss: 7.864294522778528, Val Loss: 9.061027958989143, Val MAE: 1.6771267652511597\n",
      "Epoch 216/2000, Train Loss: 7.852078751574846, Val Loss: 9.048786965543277, Val MAE: 1.6756484508514404\n",
      "Epoch 217/2000, Train Loss: 7.840064353262779, Val Loss: 9.035709690557768, Val MAE: 1.6740365028381348\n",
      "Epoch 218/2000, Train Loss: 7.827684699233193, Val Loss: 9.02360760430767, Val MAE: 1.6726137399673462\n",
      "Epoch 219/2000, Train Loss: 7.815713530035034, Val Loss: 9.010734301315596, Val MAE: 1.6710598468780518\n",
      "Epoch 220/2000, Train Loss: 7.803422610995449, Val Loss: 8.998169722826514, Val MAE: 1.6695820093154907\n",
      "Epoch 221/2000, Train Loss: 7.791475002730878, Val Loss: 8.985291427333062, Val MAE: 1.6680251359939575\n",
      "Epoch 222/2000, Train Loss: 7.779159607724973, Val Loss: 8.97325953269658, Val MAE: 1.6666502952575684\n",
      "Epoch 223/2000, Train Loss: 7.767215806815767, Val Loss: 8.960485359985535, Val MAE: 1.6651476621627808\n",
      "Epoch 224/2000, Train Loss: 7.755097523693907, Val Loss: 8.94782706156169, Val MAE: 1.663662314414978\n",
      "Epoch 225/2000, Train Loss: 7.742979839516614, Val Loss: 8.93525520518218, Val MAE: 1.6622366905212402\n",
      "Epoch 226/2000, Train Loss: 7.731041834845149, Val Loss: 8.922450421403532, Val MAE: 1.660724401473999\n",
      "Epoch 227/2000, Train Loss: 7.718754841790593, Val Loss: 8.910466819593351, Val MAE: 1.6594032049179077\n",
      "Epoch 228/2000, Train Loss: 7.706846729862825, Val Loss: 8.89764716547646, Val MAE: 1.6579445600509644\n",
      "Epoch 229/2000, Train Loss: 7.694627951957418, Val Loss: 8.885599882022975, Val MAE: 1.6565711498260498\n",
      "Epoch 230/2000, Train Loss: 7.682714129189812, Val Loss: 8.873097529150035, Val MAE: 1.6551740169525146\n",
      "Epoch 231/2000, Train Loss: 7.670721747307183, Val Loss: 8.86039106343707, Val MAE: 1.653718113899231\n",
      "Epoch 232/2000, Train Loss: 7.658607582025821, Val Loss: 8.848278428593726, Val MAE: 1.652397632598877\n",
      "Epoch 233/2000, Train Loss: 7.646682231507386, Val Loss: 8.835758276998181, Val MAE: 1.6509525775909424\n",
      "Epoch 234/2000, Train Loss: 7.6346530933627434, Val Loss: 8.823270588296733, Val MAE: 1.6496164798736572\n",
      "Epoch 235/2000, Train Loss: 7.622560657210636, Val Loss: 8.811400444744384, Val MAE: 1.6482774019241333\n",
      "Epoch 236/2000, Train Loss: 7.610622733686499, Val Loss: 8.79863890148189, Val MAE: 1.6468095779418945\n",
      "Epoch 237/2000, Train Loss: 7.598442990350955, Val Loss: 8.785933039368015, Val MAE: 1.6453900337219238\n",
      "Epoch 238/2000, Train Loss: 7.586412201051387, Val Loss: 8.773394135375545, Val MAE: 1.6439697742462158\n",
      "Epoch 239/2000, Train Loss: 7.574332727218949, Val Loss: 8.761191357896752, Val MAE: 1.6425554752349854\n",
      "Epoch 240/2000, Train Loss: 7.562352401897239, Val Loss: 8.748988858436885, Val MAE: 1.64118230342865\n",
      "Epoch 241/2000, Train Loss: 7.550555251212715, Val Loss: 8.73641391567988, Val MAE: 1.6397422552108765\n",
      "Epoch 242/2000, Train Loss: 7.538482347608965, Val Loss: 8.724205790113096, Val MAE: 1.638433814048767\n",
      "Epoch 243/2000, Train Loss: 7.526515844768604, Val Loss: 8.712015085840878, Val MAE: 1.6370818614959717\n",
      "Epoch 244/2000, Train Loss: 7.514563254444487, Val Loss: 8.699510362254431, Val MAE: 1.6356265544891357\n",
      "Epoch 245/2000, Train Loss: 7.5025766487059755, Val Loss: 8.6872230466098, Val MAE: 1.6342589855194092\n",
      "Epoch 246/2000, Train Loss: 7.490813219759801, Val Loss: 8.67441395088418, Val MAE: 1.6328290700912476\n",
      "Epoch 247/2000, Train Loss: 7.478686246338694, Val Loss: 8.66248400754308, Val MAE: 1.6315382719039917\n",
      "Epoch 248/2000, Train Loss: 7.46695542451435, Val Loss: 8.65047315130495, Val MAE: 1.6301804780960083\n",
      "Epoch 249/2000, Train Loss: 7.455068476582656, Val Loss: 8.637983977794647, Val MAE: 1.6287184953689575\n",
      "Epoch 250/2000, Train Loss: 7.4432108467942895, Val Loss: 8.625810891798098, Val MAE: 1.6273880004882812\n",
      "Epoch 251/2000, Train Loss: 7.4313696959030295, Val Loss: 8.613712216893287, Val MAE: 1.6260344982147217\n",
      "Epoch 252/2000, Train Loss: 7.419681416145213, Val Loss: 8.601588627451086, Val MAE: 1.6246734857559204\n",
      "Epoch 253/2000, Train Loss: 7.407993596415466, Val Loss: 8.589093806196566, Val MAE: 1.623250961303711\n",
      "Epoch 254/2000, Train Loss: 7.396122161148047, Val Loss: 8.577017543862944, Val MAE: 1.6219898462295532\n",
      "Epoch 255/2000, Train Loss: 7.384246526313176, Val Loss: 8.565218735435238, Val MAE: 1.6206157207489014\n",
      "Epoch 256/2000, Train Loss: 7.3726280203128365, Val Loss: 8.552891241973393, Val MAE: 1.6193547248840332\n",
      "Epoch 257/2000, Train Loss: 7.360865589865216, Val Loss: 8.540782198105774, Val MAE: 1.6179834604263306\n",
      "Epoch 258/2000, Train Loss: 7.349121013375504, Val Loss: 8.52858872282995, Val MAE: 1.616621971130371\n",
      "Epoch 259/2000, Train Loss: 7.337504396948668, Val Loss: 8.516240888262448, Val MAE: 1.6151963472366333\n",
      "Epoch 260/2000, Train Loss: 7.325653340480316, Val Loss: 8.504383918561347, Val MAE: 1.6139087677001953\n",
      "Epoch 261/2000, Train Loss: 7.314122293911258, Val Loss: 8.492104910007894, Val MAE: 1.6124958992004395\n",
      "Epoch 262/2000, Train Loss: 7.302366658586351, Val Loss: 8.480511828644635, Val MAE: 1.6112451553344727\n",
      "Epoch 263/2000, Train Loss: 7.290804031795582, Val Loss: 8.46814298935949, Val MAE: 1.6098668575286865\n",
      "Epoch 264/2000, Train Loss: 7.279338195992444, Val Loss: 8.45576775625144, Val MAE: 1.608475685119629\n",
      "Epoch 265/2000, Train Loss: 7.267579061672019, Val Loss: 8.443952394879027, Val MAE: 1.6072335243225098\n",
      "Epoch 266/2000, Train Loss: 7.2559608563032105, Val Loss: 8.432032110960517, Val MAE: 1.605962872505188\n",
      "Epoch 267/2000, Train Loss: 7.244378513416556, Val Loss: 8.420197033514714, Val MAE: 1.604682207107544\n",
      "Epoch 268/2000, Train Loss: 7.232971682154378, Val Loss: 8.40815609611877, Val MAE: 1.6033440828323364\n",
      "Epoch 269/2000, Train Loss: 7.221353929664946, Val Loss: 8.396445983076749, Val MAE: 1.6020723581314087\n",
      "Epoch 270/2000, Train Loss: 7.209778184442505, Val Loss: 8.385002269524417, Val MAE: 1.6008254289627075\n",
      "Epoch 271/2000, Train Loss: 7.198416946386402, Val Loss: 8.372299698321786, Val MAE: 1.5994116067886353\n",
      "Epoch 272/2000, Train Loss: 7.1867886849315274, Val Loss: 8.360259564363794, Val MAE: 1.598065972328186\n",
      "Epoch 273/2000, Train Loss: 7.175334588828311, Val Loss: 8.34862674160363, Val MAE: 1.5967981815338135\n",
      "Epoch 274/2000, Train Loss: 7.164014836377804, Val Loss: 8.33693031701323, Val MAE: 1.5956127643585205\n",
      "Epoch 275/2000, Train Loss: 7.1525244210488985, Val Loss: 8.325259026803382, Val MAE: 1.594303011894226\n",
      "Epoch 276/2000, Train Loss: 7.141286647880096, Val Loss: 8.313074376076868, Val MAE: 1.592980146408081\n",
      "Epoch 277/2000, Train Loss: 7.1298819909799045, Val Loss: 8.301538562121456, Val MAE: 1.5917826890945435\n",
      "Epoch 278/2000, Train Loss: 7.118617209275306, Val Loss: 8.290055545225535, Val MAE: 1.5905095338821411\n",
      "Epoch 279/2000, Train Loss: 7.107436801471432, Val Loss: 8.278012738856551, Val MAE: 1.5892070531845093\n",
      "Epoch 280/2000, Train Loss: 7.096097456191695, Val Loss: 8.266253765519352, Val MAE: 1.5879048109054565\n",
      "Epoch 281/2000, Train Loss: 7.084832819006625, Val Loss: 8.254612280081396, Val MAE: 1.5866705179214478\n",
      "Epoch 282/2000, Train Loss: 7.073682243279158, Val Loss: 8.242701062192655, Val MAE: 1.585375189781189\n",
      "Epoch 283/2000, Train Loss: 7.06237522786788, Val Loss: 8.231218074690807, Val MAE: 1.5841796398162842\n",
      "Epoch 284/2000, Train Loss: 7.0511657554094755, Val Loss: 8.219737309809418, Val MAE: 1.582929253578186\n",
      "Epoch 285/2000, Train Loss: 7.040042082726279, Val Loss: 8.2080969259347, Val MAE: 1.5816059112548828\n",
      "Epoch 286/2000, Train Loss: 7.028826213153486, Val Loss: 8.196486531871638, Val MAE: 1.5804007053375244\n",
      "Epoch 287/2000, Train Loss: 7.017728706619534, Val Loss: 8.184808604215107, Val MAE: 1.5792330503463745\n",
      "Epoch 288/2000, Train Loss: 7.006557980851184, Val Loss: 8.173147234504354, Val MAE: 1.5779846906661987\n",
      "Epoch 289/2000, Train Loss: 6.995382402472597, Val Loss: 8.161521544705515, Val MAE: 1.576798439025879\n",
      "Epoch 290/2000, Train Loss: 6.984226406876426, Val Loss: 8.150102263750279, Val MAE: 1.575568675994873\n",
      "Epoch 291/2000, Train Loss: 6.973161108489941, Val Loss: 8.138565895696209, Val MAE: 1.5743757486343384\n",
      "Epoch 292/2000, Train Loss: 6.962078080571452, Val Loss: 8.12678989421015, Val MAE: 1.573101282119751\n",
      "Epoch 293/2000, Train Loss: 6.950959626047886, Val Loss: 8.11559898365442, Val MAE: 1.5719093084335327\n",
      "Epoch 294/2000, Train Loss: 6.939966114452244, Val Loss: 8.104184753376327, Val MAE: 1.5707485675811768\n",
      "Epoch 295/2000, Train Loss: 6.929030116218802, Val Loss: 8.092599288547692, Val MAE: 1.5695263147354126\n",
      "Epoch 296/2000, Train Loss: 6.9181336120036665, Val Loss: 8.080969214439392, Val MAE: 1.5682592391967773\n",
      "Epoch 297/2000, Train Loss: 6.907101753271959, Val Loss: 8.069776435727126, Val MAE: 1.5671526193618774\n",
      "Epoch 298/2000, Train Loss: 6.896190199705152, Val Loss: 8.058633913732555, Val MAE: 1.566001534461975\n",
      "Epoch 299/2000, Train Loss: 6.885421346420214, Val Loss: 8.046872038139055, Val MAE: 1.5647685527801514\n",
      "Epoch 300/2000, Train Loss: 6.874335128638887, Val Loss: 8.035871094322367, Val MAE: 1.563660979270935\n",
      "Epoch 301/2000, Train Loss: 6.863561015074991, Val Loss: 8.024876782134788, Val MAE: 1.5625696182250977\n",
      "Epoch 302/2000, Train Loss: 6.852845258419093, Val Loss: 8.013322646181061, Val MAE: 1.5613635778427124\n",
      "Epoch 303/2000, Train Loss: 6.841943641922269, Val Loss: 8.001860901815434, Val MAE: 1.5602139234542847\n",
      "Epoch 304/2000, Train Loss: 6.831144383892634, Val Loss: 7.990372039275627, Val MAE: 1.559140682220459\n",
      "Epoch 305/2000, Train Loss: 6.820346054613494, Val Loss: 7.979253740955706, Val MAE: 1.558028221130371\n",
      "Epoch 306/2000, Train Loss: 6.809690051952097, Val Loss: 7.968467614524169, Val MAE: 1.5569524765014648\n",
      "Epoch 307/2000, Train Loss: 6.799061663919755, Val Loss: 7.957357754548118, Val MAE: 1.555832862854004\n",
      "Epoch 308/2000, Train Loss: 6.788489766793274, Val Loss: 7.945968883494809, Val MAE: 1.554727554321289\n",
      "Epoch 309/2000, Train Loss: 6.7776672932858215, Val Loss: 7.935043016627227, Val MAE: 1.5536530017852783\n",
      "Epoch 310/2000, Train Loss: 6.7672086132983145, Val Loss: 7.923908176997753, Val MAE: 1.5526084899902344\n",
      "Epoch 311/2000, Train Loss: 6.75672405233646, Val Loss: 7.912459892463194, Val MAE: 1.5514615774154663\n",
      "Epoch 312/2000, Train Loss: 6.746011876209821, Val Loss: 7.901728071476499, Val MAE: 1.5504409074783325\n",
      "Epoch 313/2000, Train Loss: 6.735443173969584, Val Loss: 7.890849421591791, Val MAE: 1.5493744611740112\n",
      "Epoch 314/2000, Train Loss: 6.72491638954107, Val Loss: 7.879932039506631, Val MAE: 1.5482858419418335\n",
      "Epoch 315/2000, Train Loss: 6.714458862811665, Val Loss: 7.869071431253871, Val MAE: 1.5472332239151\n",
      "Epoch 316/2000, Train Loss: 6.703994984371736, Val Loss: 7.858019257551187, Val MAE: 1.5461472272872925\n",
      "Epoch 317/2000, Train Loss: 6.69343468976755, Val Loss: 7.847310952740173, Val MAE: 1.5451310873031616\n",
      "Epoch 318/2000, Train Loss: 6.683285259543593, Val Loss: 7.835940925123757, Val MAE: 1.5440517663955688\n",
      "Epoch 319/2000, Train Loss: 6.6726594705442555, Val Loss: 7.825644146599998, Val MAE: 1.543036937713623\n",
      "Epoch 320/2000, Train Loss: 6.662349609733594, Val Loss: 7.8147987353883375, Val MAE: 1.542043924331665\n",
      "Epoch 321/2000, Train Loss: 6.652075601358274, Val Loss: 7.803618832606158, Val MAE: 1.5409650802612305\n",
      "Epoch 322/2000, Train Loss: 6.6417370615361, Val Loss: 7.79273898889349, Val MAE: 1.5399470329284668\n",
      "Epoch 323/2000, Train Loss: 6.631327089266414, Val Loss: 7.782411608079525, Val MAE: 1.539076805114746\n",
      "Epoch 324/2000, Train Loss: 6.621208404992349, Val Loss: 7.771455512369332, Val MAE: 1.5380555391311646\n",
      "Epoch 325/2000, Train Loss: 6.610932416622218, Val Loss: 7.76066966467116, Val MAE: 1.5371088981628418\n",
      "Epoch 326/2000, Train Loss: 6.600692064294552, Val Loss: 7.7503180057831, Val MAE: 1.5361729860305786\n",
      "Epoch 327/2000, Train Loss: 6.590671739470051, Val Loss: 7.739435031499765, Val MAE: 1.535218596458435\n",
      "Epoch 328/2000, Train Loss: 6.580445350858729, Val Loss: 7.729347561830527, Val MAE: 1.5343774557113647\n",
      "Epoch 329/2000, Train Loss: 6.570532346660546, Val Loss: 7.718482771250483, Val MAE: 1.5334644317626953\n",
      "Epoch 330/2000, Train Loss: 6.560396334171489, Val Loss: 7.7078341417116665, Val MAE: 1.5325477123260498\n",
      "Epoch 331/2000, Train Loss: 6.550377115244997, Val Loss: 7.697480025765014, Val MAE: 1.5316178798675537\n",
      "Epoch 332/2000, Train Loss: 6.540324881165688, Val Loss: 7.687573725536262, Val MAE: 1.5308263301849365\n",
      "Epoch 333/2000, Train Loss: 6.530570253947179, Val Loss: 7.676466165646298, Val MAE: 1.5298855304718018\n",
      "Epoch 334/2000, Train Loss: 6.52048071392931, Val Loss: 7.666137496494267, Val MAE: 1.5289900302886963\n",
      "Epoch 335/2000, Train Loss: 6.510499677549885, Val Loss: 7.655936328209426, Val MAE: 1.528151035308838\n",
      "Epoch 336/2000, Train Loss: 6.500633219652469, Val Loss: 7.645596401025988, Val MAE: 1.5272407531738281\n",
      "Epoch 337/2000, Train Loss: 6.4907227134395455, Val Loss: 7.634930198629425, Val MAE: 1.5263358354568481\n",
      "Epoch 338/2000, Train Loss: 6.4809208770238795, Val Loss: 7.624301992050589, Val MAE: 1.5253585577011108\n",
      "Epoch 339/2000, Train Loss: 6.4710019411492, Val Loss: 7.614294538759205, Val MAE: 1.5245252847671509\n",
      "Epoch 340/2000, Train Loss: 6.461324301492453, Val Loss: 7.604136220600507, Val MAE: 1.5236468315124512\n",
      "Epoch 341/2000, Train Loss: 6.451754749304278, Val Loss: 7.593641201212798, Val MAE: 1.5227453708648682\n",
      "Epoch 342/2000, Train Loss: 6.441725840638098, Val Loss: 7.583770852893183, Val MAE: 1.5219626426696777\n",
      "Epoch 343/2000, Train Loss: 6.432117768199556, Val Loss: 7.573475133168371, Val MAE: 1.5210922956466675\n",
      "Epoch 344/2000, Train Loss: 6.4224176576883325, Val Loss: 7.563617282330173, Val MAE: 1.5202394723892212\n",
      "Epoch 345/2000, Train Loss: 6.412900450163954, Val Loss: 7.553094167619536, Val MAE: 1.5194087028503418\n",
      "Epoch 346/2000, Train Loss: 6.403127750662582, Val Loss: 7.543339228997492, Val MAE: 1.5186082124710083\n",
      "Epoch 347/2000, Train Loss: 6.393595944347412, Val Loss: 7.532886793352153, Val MAE: 1.5177334547042847\n",
      "Epoch 348/2000, Train Loss: 6.384022374207236, Val Loss: 7.52296802636287, Val MAE: 1.5168765783309937\n",
      "Epoch 349/2000, Train Loss: 6.374668763947448, Val Loss: 7.513087652623653, Val MAE: 1.5159798860549927\n",
      "Epoch 350/2000, Train Loss: 6.365242925609916, Val Loss: 7.503206936055666, Val MAE: 1.5151212215423584\n",
      "Epoch 351/2000, Train Loss: 6.355841529862034, Val Loss: 7.4934974024965335, Val MAE: 1.514355182647705\n",
      "Epoch 352/2000, Train Loss: 6.346661056073404, Val Loss: 7.483461767435074, Val MAE: 1.513438105583191\n",
      "Epoch 353/2000, Train Loss: 6.337355832806283, Val Loss: 7.47336075628457, Val MAE: 1.5125974416732788\n",
      "Epoch 354/2000, Train Loss: 6.327847178596731, Val Loss: 7.463863956397527, Val MAE: 1.5118751525878906\n",
      "Epoch 355/2000, Train Loss: 6.31853453344039, Val Loss: 7.454412087083679, Val MAE: 1.5111045837402344\n",
      "Epoch 356/2000, Train Loss: 6.309445398166848, Val Loss: 7.444200905932956, Val MAE: 1.510258674621582\n",
      "Epoch 357/2000, Train Loss: 6.300102963053426, Val Loss: 7.434616924863156, Val MAE: 1.5094717741012573\n",
      "Epoch 358/2000, Train Loss: 6.290953706400514, Val Loss: 7.4248580569273805, Val MAE: 1.5087171792984009\n",
      "Epoch 359/2000, Train Loss: 6.281795639273027, Val Loss: 7.415431016316153, Val MAE: 1.5079537630081177\n",
      "Epoch 360/2000, Train Loss: 6.272812572252035, Val Loss: 7.405639471971009, Val MAE: 1.5071358680725098\n",
      "Epoch 361/2000, Train Loss: 6.263708005268338, Val Loss: 7.396296877465019, Val MAE: 1.506403923034668\n",
      "Epoch 362/2000, Train Loss: 6.254691635576987, Val Loss: 7.386998896729456, Val MAE: 1.5056743621826172\n",
      "Epoch 363/2000, Train Loss: 6.245658789500232, Val Loss: 7.377390865389615, Val MAE: 1.5048744678497314\n",
      "Epoch 364/2000, Train Loss: 6.2366972634239755, Val Loss: 7.36749967555069, Val MAE: 1.5041368007659912\n",
      "Epoch 365/2000, Train Loss: 6.227631784335528, Val Loss: 7.3579631953002655, Val MAE: 1.5033879280090332\n",
      "Epoch 366/2000, Train Loss: 6.218738835673278, Val Loss: 7.348682769153216, Val MAE: 1.5026774406433105\n",
      "Epoch 367/2000, Train Loss: 6.209868584111985, Val Loss: 7.339684934546686, Val MAE: 1.5019975900650024\n",
      "Epoch 368/2000, Train Loss: 6.201073679004342, Val Loss: 7.32999305039236, Val MAE: 1.5012372732162476\n",
      "Epoch 369/2000, Train Loss: 6.192198261835973, Val Loss: 7.320717542138818, Val MAE: 1.5005934238433838\n",
      "Epoch 370/2000, Train Loss: 6.183379455362378, Val Loss: 7.311307967423576, Val MAE: 1.4998397827148438\n",
      "Epoch 371/2000, Train Loss: 6.174605530316865, Val Loss: 7.3022088188014616, Val MAE: 1.4991538524627686\n",
      "Epoch 372/2000, Train Loss: 6.165920096819366, Val Loss: 7.292843681696343, Val MAE: 1.498370885848999\n",
      "Epoch 373/2000, Train Loss: 6.157220695161742, Val Loss: 7.283610285961465, Val MAE: 1.497687816619873\n",
      "Epoch 374/2000, Train Loss: 6.148554604482419, Val Loss: 7.2744224605290855, Val MAE: 1.49697744846344\n",
      "Epoch 375/2000, Train Loss: 6.140025412823817, Val Loss: 7.265399625448332, Val MAE: 1.4962544441223145\n",
      "Epoch 376/2000, Train Loss: 6.131465590367248, Val Loss: 7.256432007743071, Val MAE: 1.4955651760101318\n",
      "Epoch 377/2000, Train Loss: 6.123057057057633, Val Loss: 7.2472843082800305, Val MAE: 1.49484121799469\n",
      "Epoch 378/2000, Train Loss: 6.1145652835913955, Val Loss: 7.23850046792259, Val MAE: 1.4941760301589966\n",
      "Epoch 379/2000, Train Loss: 6.10613862196862, Val Loss: 7.229762217463696, Val MAE: 1.4935811758041382\n",
      "Epoch 380/2000, Train Loss: 6.09787128211227, Val Loss: 7.220532285534356, Val MAE: 1.4928406476974487\n",
      "Epoch 381/2000, Train Loss: 6.089480842145181, Val Loss: 7.211650307456108, Val MAE: 1.492187738418579\n",
      "Epoch 382/2000, Train Loss: 6.081159966498179, Val Loss: 7.203148376349717, Val MAE: 1.4915666580200195\n",
      "Epoch 383/2000, Train Loss: 6.072964284176572, Val Loss: 7.19431533776734, Val MAE: 1.4909158945083618\n",
      "Epoch 384/2000, Train Loss: 6.064598836234168, Val Loss: 7.185772465312318, Val MAE: 1.4902801513671875\n",
      "Epoch 385/2000, Train Loss: 6.0563531609756245, Val Loss: 7.176538694803029, Val MAE: 1.4895868301391602\n",
      "Epoch 386/2000, Train Loss: 6.048085404369781, Val Loss: 7.167339928856451, Val MAE: 1.488982081413269\n",
      "Epoch 387/2000, Train Loss: 6.039733469969256, Val Loss: 7.158972195974768, Val MAE: 1.4884302616119385\n",
      "Epoch 388/2000, Train Loss: 6.0316973984531295, Val Loss: 7.1506080287572455, Val MAE: 1.4878568649291992\n",
      "Epoch 389/2000, Train Loss: 6.023745417788118, Val Loss: 7.14173230620688, Val MAE: 1.4872043132781982\n",
      "Epoch 390/2000, Train Loss: 6.015664644720489, Val Loss: 7.133131589383295, Val MAE: 1.4866015911102295\n",
      "Epoch 391/2000, Train Loss: 6.007536889668222, Val Loss: 7.124874389641089, Val MAE: 1.486052393913269\n",
      "Epoch 392/2000, Train Loss: 5.999519273952688, Val Loss: 7.116003595802882, Val MAE: 1.4854522943496704\n",
      "Epoch 393/2000, Train Loss: 5.991433028463799, Val Loss: 7.107460728450997, Val MAE: 1.484840750694275\n",
      "Epoch 394/2000, Train Loss: 5.98368217802125, Val Loss: 7.098838024862008, Val MAE: 1.4842239618301392\n",
      "Epoch 395/2000, Train Loss: 5.975644041305616, Val Loss: 7.09065255930979, Val MAE: 1.4836256504058838\n",
      "Epoch 396/2000, Train Loss: 5.9677947730647105, Val Loss: 7.08236933254624, Val MAE: 1.4830267429351807\n",
      "Epoch 397/2000, Train Loss: 5.960083459146212, Val Loss: 7.074059693678601, Val MAE: 1.4824265241622925\n",
      "Epoch 398/2000, Train Loss: 5.952427873348688, Val Loss: 7.06555784799873, Val MAE: 1.4818586111068726\n",
      "Epoch 399/2000, Train Loss: 5.944566409050742, Val Loss: 7.05740832067924, Val MAE: 1.4813778400421143\n",
      "Epoch 400/2000, Train Loss: 5.936930827808535, Val Loss: 7.048993832240366, Val MAE: 1.4808392524719238\n",
      "Epoch 401/2000, Train Loss: 5.929272673311651, Val Loss: 7.041085779973089, Val MAE: 1.4802911281585693\n",
      "Epoch 402/2000, Train Loss: 5.921761906127682, Val Loss: 7.0328056349942125, Val MAE: 1.4797192811965942\n",
      "Epoch 403/2000, Train Loss: 5.914229229165051, Val Loss: 7.024717386965066, Val MAE: 1.4792394638061523\n",
      "Epoch 404/2000, Train Loss: 5.906755819892574, Val Loss: 7.016761418993342, Val MAE: 1.4786959886550903\n",
      "Epoch 405/2000, Train Loss: 5.899281582144516, Val Loss: 7.008807446144215, Val MAE: 1.4782651662826538\n",
      "Epoch 406/2000, Train Loss: 5.891783947689607, Val Loss: 7.0009013457249285, Val MAE: 1.4778176546096802\n",
      "Epoch 407/2000, Train Loss: 5.884460120765278, Val Loss: 6.993047163196622, Val MAE: 1.4773050546646118\n",
      "Epoch 408/2000, Train Loss: 5.877153360283356, Val Loss: 6.985247275498632, Val MAE: 1.476891279220581\n",
      "Epoch 409/2000, Train Loss: 5.869812339785806, Val Loss: 6.977435479425404, Val MAE: 1.4764326810836792\n",
      "Epoch 410/2000, Train Loss: 5.8625613804574535, Val Loss: 6.969653932403212, Val MAE: 1.4759924411773682\n",
      "Epoch 411/2000, Train Loss: 5.8552597801727835, Val Loss: 6.961728903937013, Val MAE: 1.4755098819732666\n",
      "Epoch 412/2000, Train Loss: 5.847938546098805, Val Loss: 6.954206744489604, Val MAE: 1.4750733375549316\n",
      "Epoch 413/2000, Train Loss: 5.840811694653726, Val Loss: 6.946508003118104, Val MAE: 1.4746589660644531\n",
      "Epoch 414/2000, Train Loss: 5.833677213041284, Val Loss: 6.938748014810151, Val MAE: 1.474189043045044\n",
      "Epoch 415/2000, Train Loss: 5.826530506004198, Val Loss: 6.930919685898578, Val MAE: 1.473719835281372\n",
      "Epoch 416/2000, Train Loss: 5.819469199574748, Val Loss: 6.9236872801429605, Val MAE: 1.473285436630249\n",
      "Epoch 417/2000, Train Loss: 5.812688940348076, Val Loss: 6.915716247709646, Val MAE: 1.4728517532348633\n",
      "Epoch 418/2000, Train Loss: 5.805524781225566, Val Loss: 6.908578123233906, Val MAE: 1.4724942445755005\n",
      "Epoch 419/2000, Train Loss: 5.798703187481897, Val Loss: 6.9011713020401455, Val MAE: 1.4721086025238037\n",
      "Epoch 420/2000, Train Loss: 5.791790495042476, Val Loss: 6.893765564650705, Val MAE: 1.4716722965240479\n",
      "Epoch 421/2000, Train Loss: 5.784963753853277, Val Loss: 6.886356464497847, Val MAE: 1.4712674617767334\n",
      "Epoch 422/2000, Train Loss: 5.778237813102561, Val Loss: 6.879104135073211, Val MAE: 1.4708620309829712\n",
      "Epoch 423/2000, Train Loss: 5.771684701477496, Val Loss: 6.871579065306546, Val MAE: 1.4704853296279907\n",
      "Epoch 424/2000, Train Loss: 5.764866862729844, Val Loss: 6.864728078552305, Val MAE: 1.4701485633850098\n",
      "Epoch 425/2000, Train Loss: 5.758349740138123, Val Loss: 6.8575332072907935, Val MAE: 1.469738483428955\n",
      "Epoch 426/2000, Train Loss: 5.751822527262535, Val Loss: 6.850557222043815, Val MAE: 1.4694150686264038\n",
      "Epoch 427/2000, Train Loss: 5.745122083388813, Val Loss: 6.843732969707822, Val MAE: 1.469124436378479\n",
      "Epoch 428/2000, Train Loss: 5.738756153533292, Val Loss: 6.836284916184536, Val MAE: 1.4687579870224\n",
      "Epoch 429/2000, Train Loss: 5.732102022565165, Val Loss: 6.829462273480141, Val MAE: 1.4684332609176636\n",
      "Epoch 430/2000, Train Loss: 5.725607210465343, Val Loss: 6.822891670955371, Val MAE: 1.4681469202041626\n",
      "Epoch 431/2000, Train Loss: 5.719321008246374, Val Loss: 6.81551311818296, Val MAE: 1.467793583869934\n",
      "Epoch 432/2000, Train Loss: 5.712818902354186, Val Loss: 6.808515616781907, Val MAE: 1.4675092697143555\n",
      "Epoch 433/2000, Train Loss: 5.706491436332319, Val Loss: 6.8015434226557, Val MAE: 1.4672576189041138\n",
      "Epoch 434/2000, Train Loss: 5.700119339279842, Val Loss: 6.795287903245181, Val MAE: 1.4669814109802246\n",
      "Epoch 435/2000, Train Loss: 5.693946141284714, Val Loss: 6.78838682817678, Val MAE: 1.4667466878890991\n",
      "Epoch 436/2000, Train Loss: 5.687579207520817, Val Loss: 6.781953863187195, Val MAE: 1.4665552377700806\n",
      "Epoch 437/2000, Train Loss: 5.681444533640214, Val Loss: 6.774886964526895, Val MAE: 1.4662203788757324\n",
      "Epoch 438/2000, Train Loss: 5.675243099466135, Val Loss: 6.768437987321044, Val MAE: 1.4660117626190186\n",
      "Epoch 439/2000, Train Loss: 5.669249209244788, Val Loss: 6.761563814972361, Val MAE: 1.4657573699951172\n",
      "Epoch 440/2000, Train Loss: 5.663162225262659, Val Loss: 6.755003546096691, Val MAE: 1.4654943943023682\n",
      "Epoch 441/2000, Train Loss: 5.657180420583419, Val Loss: 6.748389128034246, Val MAE: 1.465250849723816\n",
      "Epoch 442/2000, Train Loss: 5.651185346770325, Val Loss: 6.742002866243663, Val MAE: 1.4650168418884277\n",
      "Epoch 443/2000, Train Loss: 5.6452630899327305, Val Loss: 6.735832865515801, Val MAE: 1.4648042917251587\n",
      "Epoch 444/2000, Train Loss: 5.639523484911679, Val Loss: 6.729279065275029, Val MAE: 1.464566946029663\n",
      "Epoch 445/2000, Train Loss: 5.633674520244475, Val Loss: 6.722912390244334, Val MAE: 1.4643341302871704\n",
      "Epoch 446/2000, Train Loss: 5.627911208321248, Val Loss: 6.7166837232366, Val MAE: 1.4641424417495728\n",
      "Epoch 447/2000, Train Loss: 5.622115644488961, Val Loss: 6.710461943218969, Val MAE: 1.4639170169830322\n",
      "Epoch 448/2000, Train Loss: 5.616365144086618, Val Loss: 6.704332120206258, Val MAE: 1.4636976718902588\n",
      "Epoch 449/2000, Train Loss: 5.610696279442291, Val Loss: 6.698037780617198, Val MAE: 1.4635034799575806\n",
      "Epoch 450/2000, Train Loss: 5.604997702896885, Val Loss: 6.691959572777356, Val MAE: 1.4632632732391357\n",
      "Epoch 451/2000, Train Loss: 5.599320362993732, Val Loss: 6.685935713759024, Val MAE: 1.4631155729293823\n",
      "Epoch 452/2000, Train Loss: 5.593965988877913, Val Loss: 6.679349760486655, Val MAE: 1.4629309177398682\n",
      "Epoch 453/2000, Train Loss: 5.588198897517867, Val Loss: 6.673652622695655, Val MAE: 1.46277916431427\n",
      "Epoch 454/2000, Train Loss: 5.582765795230093, Val Loss: 6.6678592437138295, Val MAE: 1.4626383781433105\n",
      "Epoch 455/2000, Train Loss: 5.577304100681163, Val Loss: 6.661943149485, Val MAE: 1.4624600410461426\n",
      "Epoch 456/2000, Train Loss: 5.57191474008792, Val Loss: 6.656120866230906, Val MAE: 1.4623775482177734\n",
      "Epoch 457/2000, Train Loss: 5.5666370229551045, Val Loss: 6.649941238128159, Val MAE: 1.4623230695724487\n",
      "Epoch 458/2000, Train Loss: 5.5612219736294, Val Loss: 6.644486935987865, Val MAE: 1.4622853994369507\n",
      "Epoch 459/2000, Train Loss: 5.556118427258063, Val Loss: 6.638250878106241, Val MAE: 1.4621862173080444\n",
      "Epoch 460/2000, Train Loss: 5.55071778119673, Val Loss: 6.632688321581442, Val MAE: 1.4621620178222656\n",
      "Epoch 461/2000, Train Loss: 5.545441591952184, Val Loss: 6.627348390446134, Val MAE: 1.462103247642517\n",
      "Epoch 462/2000, Train Loss: 5.5404185530045815, Val Loss: 6.6214081606228055, Val MAE: 1.462099313735962\n",
      "Epoch 463/2000, Train Loss: 5.535216830731211, Val Loss: 6.6160096740477705, Val MAE: 1.4621341228485107\n",
      "Epoch 464/2000, Train Loss: 5.530067644783898, Val Loss: 6.6105311645628655, Val MAE: 1.4621490240097046\n",
      "Epoch 465/2000, Train Loss: 5.524956374732563, Val Loss: 6.604871593721925, Val MAE: 1.4621572494506836\n",
      "Epoch 466/2000, Train Loss: 5.5198806472110595, Val Loss: 6.599254294617535, Val MAE: 1.462192177772522\n",
      "Epoch 467/2000, Train Loss: 5.514830103956126, Val Loss: 6.593931524720911, Val MAE: 1.4622026681900024\n",
      "Epoch 468/2000, Train Loss: 5.509983537262804, Val Loss: 6.588098760949422, Val MAE: 1.4622266292572021\n",
      "Epoch 469/2000, Train Loss: 5.504951997166515, Val Loss: 6.582897603103559, Val MAE: 1.4622822999954224\n",
      "Epoch 470/2000, Train Loss: 5.500142600586581, Val Loss: 6.577606951537198, Val MAE: 1.4623061418533325\n",
      "Epoch 471/2000, Train Loss: 5.495314320636917, Val Loss: 6.572473530287612, Val MAE: 1.4623843431472778\n",
      "Epoch 472/2000, Train Loss: 5.490560484473478, Val Loss: 6.567468480092206, Val MAE: 1.4624358415603638\n",
      "Epoch 473/2000, Train Loss: 5.485863360632181, Val Loss: 6.562027704021702, Val MAE: 1.4624783992767334\n",
      "Epoch 474/2000, Train Loss: 5.481123891023608, Val Loss: 6.557081405226499, Val MAE: 1.462559461593628\n",
      "Epoch 475/2000, Train Loss: 5.476674046663256, Val Loss: 6.5516531678707635, Val MAE: 1.4626219272613525\n",
      "Epoch 476/2000, Train Loss: 5.4718949984113054, Val Loss: 6.546706521143652, Val MAE: 1.4626920223236084\n",
      "Epoch 477/2000, Train Loss: 5.467335661574353, Val Loss: 6.541663576274702, Val MAE: 1.4627913236618042\n",
      "Epoch 478/2000, Train Loss: 5.46280951013055, Val Loss: 6.5370153317304505, Val MAE: 1.4628922939300537\n",
      "Epoch 479/2000, Train Loss: 5.4584176030305835, Val Loss: 6.531799127181915, Val MAE: 1.4629802703857422\n",
      "Epoch 480/2000, Train Loss: 5.453797846597829, Val Loss: 6.52718410457242, Val MAE: 1.463085651397705\n",
      "Epoch 481/2000, Train Loss: 5.449436833445025, Val Loss: 6.522132705029559, Val MAE: 1.4632060527801514\n",
      "Epoch 482/2000, Train Loss: 5.444985621558982, Val Loss: 6.517142441378881, Val MAE: 1.4632710218429565\n",
      "Epoch 483/2000, Train Loss: 5.4405591905213635, Val Loss: 6.512261735249872, Val MAE: 1.4633705615997314\n",
      "Epoch 484/2000, Train Loss: 5.436208966292284, Val Loss: 6.5077212672323395, Val MAE: 1.4634557962417603\n",
      "Epoch 485/2000, Train Loss: 5.432051565890567, Val Loss: 6.502922696814145, Val MAE: 1.4636088609695435\n",
      "Epoch 486/2000, Train Loss: 5.427752629284727, Val Loss: 6.498466792784325, Val MAE: 1.4637421369552612\n",
      "Epoch 487/2000, Train Loss: 5.423660378788433, Val Loss: 6.493538514085828, Val MAE: 1.4638574123382568\n",
      "Epoch 488/2000, Train Loss: 5.419335594053593, Val Loss: 6.489037929740671, Val MAE: 1.4640039205551147\n",
      "Epoch 489/2000, Train Loss: 5.415161941194457, Val Loss: 6.484750430469644, Val MAE: 1.464111089706421\n",
      "Epoch 490/2000, Train Loss: 5.411099583055444, Val Loss: 6.47992472595548, Val MAE: 1.464288353919983\n",
      "Epoch 491/2000, Train Loss: 5.406993957160937, Val Loss: 6.475148243242747, Val MAE: 1.464457392692566\n",
      "Epoch 492/2000, Train Loss: 5.402802236269514, Val Loss: 6.470822735600276, Val MAE: 1.4646328687667847\n",
      "Epoch 493/2000, Train Loss: 5.398955548408545, Val Loss: 6.466250864202029, Val MAE: 1.464752435684204\n",
      "Epoch 494/2000, Train Loss: 5.394903814386897, Val Loss: 6.461857927886591, Val MAE: 1.464906930923462\n",
      "Epoch 495/2000, Train Loss: 5.390909061826029, Val Loss: 6.457800491929871, Val MAE: 1.4650905132293701\n",
      "Epoch 496/2000, Train Loss: 5.387126186871645, Val Loss: 6.453395547217702, Val MAE: 1.4652262926101685\n",
      "Epoch 497/2000, Train Loss: 5.38322223218179, Val Loss: 6.448991435133431, Val MAE: 1.4653681516647339\n",
      "Epoch 498/2000, Train Loss: 5.379344957187844, Val Loss: 6.444919714168327, Val MAE: 1.4655108451843262\n",
      "Epoch 499/2000, Train Loss: 5.375670393630017, Val Loss: 6.440328156907264, Val MAE: 1.465706467628479\n",
      "Epoch 500/2000, Train Loss: 5.3718302385153995, Val Loss: 6.436415005118063, Val MAE: 1.4658393859863281\n",
      "Epoch 501/2000, Train Loss: 5.368118996558351, Val Loss: 6.432057171447636, Val MAE: 1.465968132019043\n",
      "Epoch 502/2000, Train Loss: 5.364390496110221, Val Loss: 6.428032570097544, Val MAE: 1.4661346673965454\n",
      "Epoch 503/2000, Train Loss: 5.360736435777365, Val Loss: 6.424188195842586, Val MAE: 1.4663218259811401\n",
      "Epoch 504/2000, Train Loss: 5.357121004277729, Val Loss: 6.420163171238278, Val MAE: 1.4664711952209473\n",
      "Epoch 505/2000, Train Loss: 5.353723032934353, Val Loss: 6.415674726950796, Val MAE: 1.4666144847869873\n",
      "Epoch 506/2000, Train Loss: 5.350031333379266, Val Loss: 6.412178638352923, Val MAE: 1.466766357421875\n",
      "Epoch 507/2000, Train Loss: 5.3465910622521005, Val Loss: 6.408375543478417, Val MAE: 1.466936707496643\n",
      "Epoch 508/2000, Train Loss: 5.3431822623773755, Val Loss: 6.404462131225083, Val MAE: 1.4670486450195312\n",
      "Epoch 509/2000, Train Loss: 5.339606351558741, Val Loss: 6.400809503581426, Val MAE: 1.4672691822052002\n",
      "Epoch 510/2000, Train Loss: 5.336301060511189, Val Loss: 6.396566633390237, Val MAE: 1.4673956632614136\n",
      "Epoch 511/2000, Train Loss: 5.332900788447845, Val Loss: 6.392782749786769, Val MAE: 1.4675339460372925\n",
      "Epoch 512/2000, Train Loss: 5.329529864668267, Val Loss: 6.3892082884295345, Val MAE: 1.467727541923523\n",
      "Epoch 513/2000, Train Loss: 5.326252592633957, Val Loss: 6.385631817150606, Val MAE: 1.4679006338119507\n",
      "Epoch 514/2000, Train Loss: 5.3230044150081985, Val Loss: 6.381877071338973, Val MAE: 1.4680558443069458\n",
      "Epoch 515/2000, Train Loss: 5.319707198892652, Val Loss: 6.3783686198804475, Val MAE: 1.4682550430297852\n",
      "Epoch 516/2000, Train Loss: 5.316485727625496, Val Loss: 6.374664792765493, Val MAE: 1.4684659242630005\n",
      "Epoch 517/2000, Train Loss: 5.313501162505807, Val Loss: 6.370750036856083, Val MAE: 1.468635082244873\n",
      "Epoch 518/2000, Train Loss: 5.310127885840121, Val Loss: 6.367406957974173, Val MAE: 1.468817949295044\n",
      "Epoch 519/2000, Train Loss: 5.307037991101777, Val Loss: 6.363932085465895, Val MAE: 1.4690114259719849\n",
      "Epoch 520/2000, Train Loss: 5.30406762765814, Val Loss: 6.360390961374322, Val MAE: 1.4692102670669556\n",
      "Epoch 521/2000, Train Loss: 5.301032654470138, Val Loss: 6.357004068791866, Val MAE: 1.469416618347168\n",
      "Epoch 522/2000, Train Loss: 5.297985430861215, Val Loss: 6.354058928991834, Val MAE: 1.469628095626831\n",
      "Epoch 523/2000, Train Loss: 5.295156272149357, Val Loss: 6.3502160114784765, Val MAE: 1.4698179960250854\n",
      "Epoch 524/2000, Train Loss: 5.292086524167362, Val Loss: 6.347216214933624, Val MAE: 1.4700219631195068\n",
      "Epoch 525/2000, Train Loss: 5.289215885452938, Val Loss: 6.344055753660529, Val MAE: 1.4702084064483643\n",
      "Epoch 526/2000, Train Loss: 5.286371243541013, Val Loss: 6.340359673924642, Val MAE: 1.470445990562439\n",
      "Epoch 527/2000, Train Loss: 5.2834158024872915, Val Loss: 6.3373204301277255, Val MAE: 1.4706268310546875\n",
      "Epoch 528/2000, Train Loss: 5.280687500630631, Val Loss: 6.334336184985833, Val MAE: 1.4708086252212524\n",
      "Epoch 529/2000, Train Loss: 5.277845181754188, Val Loss: 6.331130404280473, Val MAE: 1.4710044860839844\n",
      "Epoch 530/2000, Train Loss: 5.275070431553178, Val Loss: 6.327761020358294, Val MAE: 1.4711946249008179\n",
      "Epoch 531/2000, Train Loss: 5.272240108570364, Val Loss: 6.324674097132193, Val MAE: 1.4714131355285645\n",
      "Epoch 532/2000, Train Loss: 5.269506087372717, Val Loss: 6.321689893938091, Val MAE: 1.4716041088104248\n",
      "Epoch 533/2000, Train Loss: 5.266881333757644, Val Loss: 6.318962005209433, Val MAE: 1.471797227859497\n",
      "Epoch 534/2000, Train Loss: 5.264333222634981, Val Loss: 6.315370549894359, Val MAE: 1.4720405340194702\n",
      "Epoch 535/2000, Train Loss: 5.261553791392371, Val Loss: 6.31284969830758, Val MAE: 1.4722319841384888\n",
      "Epoch 536/2000, Train Loss: 5.258994694467109, Val Loss: 6.30991092997871, Val MAE: 1.4724451303482056\n",
      "Epoch 537/2000, Train Loss: 5.256553765827099, Val Loss: 6.306683067895778, Val MAE: 1.4726468324661255\n",
      "Epoch 538/2000, Train Loss: 5.253891081632246, Val Loss: 6.303780281074243, Val MAE: 1.4728652238845825\n",
      "Epoch 539/2000, Train Loss: 5.2513620430298715, Val Loss: 6.300944418939825, Val MAE: 1.4730618000030518\n",
      "Epoch 540/2000, Train Loss: 5.24890027053932, Val Loss: 6.29825182116195, Val MAE: 1.4732555150985718\n",
      "Epoch 541/2000, Train Loss: 5.246475857312714, Val Loss: 6.295434464330542, Val MAE: 1.4734668731689453\n",
      "Epoch 542/2000, Train Loss: 5.243991752884183, Val Loss: 6.2928839803558505, Val MAE: 1.4736639261245728\n",
      "Epoch 543/2000, Train Loss: 5.24165808555566, Val Loss: 6.290139595327312, Val MAE: 1.4738621711730957\n",
      "Epoch 544/2000, Train Loss: 5.239357976125164, Val Loss: 6.2872785791142345, Val MAE: 1.4740638732910156\n",
      "Epoch 545/2000, Train Loss: 5.23695429604096, Val Loss: 6.284587359183456, Val MAE: 1.474298119544983\n",
      "Epoch 546/2000, Train Loss: 5.234631398508576, Val Loss: 6.282013220738058, Val MAE: 1.4744927883148193\n",
      "Epoch 547/2000, Train Loss: 5.232330309900318, Val Loss: 6.279754063854479, Val MAE: 1.474684476852417\n",
      "Epoch 548/2000, Train Loss: 5.230082008788419, Val Loss: 6.276904786898665, Val MAE: 1.474889874458313\n",
      "Epoch 549/2000, Train Loss: 5.22770354620255, Val Loss: 6.274196856438297, Val MAE: 1.4751031398773193\n",
      "Epoch 550/2000, Train Loss: 5.225584600887577, Val Loss: 6.271383302056626, Val MAE: 1.4752990007400513\n",
      "Epoch 551/2000, Train Loss: 5.2232358312684095, Val Loss: 6.269118692368677, Val MAE: 1.475511908531189\n",
      "Epoch 552/2000, Train Loss: 5.221056482973516, Val Loss: 6.266557850556015, Val MAE: 1.4757044315338135\n",
      "Epoch 553/2000, Train Loss: 5.218901737582549, Val Loss: 6.26417168699307, Val MAE: 1.4759057760238647\n",
      "Epoch 554/2000, Train Loss: 5.2168374757133, Val Loss: 6.261837547568426, Val MAE: 1.4761239290237427\n",
      "Epoch 555/2000, Train Loss: 5.214699004805455, Val Loss: 6.259622989247923, Val MAE: 1.476285696029663\n",
      "Epoch 556/2000, Train Loss: 5.2126858010284325, Val Loss: 6.256865989161681, Val MAE: 1.4765290021896362\n",
      "Epoch 557/2000, Train Loss: 5.2105094308404905, Val Loss: 6.25481466203928, Val MAE: 1.4766976833343506\n",
      "Epoch 558/2000, Train Loss: 5.20858333369708, Val Loss: 6.252442703875777, Val MAE: 1.4769071340560913\n",
      "Epoch 559/2000, Train Loss: 5.206572584047117, Val Loss: 6.250149559790957, Val MAE: 1.4771156311035156\n",
      "Epoch 560/2000, Train Loss: 5.204577562681473, Val Loss: 6.248056038601757, Val MAE: 1.4773006439208984\n",
      "Epoch 561/2000, Train Loss: 5.202571371965702, Val Loss: 6.245672902628167, Val MAE: 1.4775269031524658\n",
      "Epoch 562/2000, Train Loss: 5.200643152433238, Val Loss: 6.243344326745974, Val MAE: 1.477771520614624\n",
      "Epoch 563/2000, Train Loss: 5.198679988156261, Val Loss: 6.2409675543847145, Val MAE: 1.4779808521270752\n",
      "Epoch 564/2000, Train Loss: 5.196826680842636, Val Loss: 6.2387540149566245, Val MAE: 1.4781776666641235\n",
      "Epoch 565/2000, Train Loss: 5.194875214822481, Val Loss: 6.236807124357518, Val MAE: 1.4783926010131836\n",
      "Epoch 566/2000, Train Loss: 5.193018042287719, Val Loss: 6.234504522628163, Val MAE: 1.47859525680542\n",
      "Epoch 567/2000, Train Loss: 5.191124135516644, Val Loss: 6.232446120414015, Val MAE: 1.4787873029708862\n",
      "Epoch 568/2000, Train Loss: 5.189355039519274, Val Loss: 6.230249986766952, Val MAE: 1.4790016412734985\n",
      "Epoch 569/2000, Train Loss: 5.187503732004274, Val Loss: 6.228351846847632, Val MAE: 1.479174256324768\n",
      "Epoch 570/2000, Train Loss: 5.185800233575089, Val Loss: 6.2261536377545905, Val MAE: 1.479381799697876\n",
      "Epoch 571/2000, Train Loss: 5.18408085731865, Val Loss: 6.224707661936544, Val MAE: 1.4795812368392944\n",
      "Epoch 572/2000, Train Loss: 5.182337542986754, Val Loss: 6.222478569778677, Val MAE: 1.4797837734222412\n",
      "Epoch 573/2000, Train Loss: 5.180601140088742, Val Loss: 6.220631111566335, Val MAE: 1.4799915552139282\n",
      "Epoch 574/2000, Train Loss: 5.178926171128135, Val Loss: 6.218390282804835, Val MAE: 1.4802191257476807\n",
      "Epoch 575/2000, Train Loss: 5.177214809522829, Val Loss: 6.216590204671638, Val MAE: 1.4804282188415527\n",
      "Epoch 576/2000, Train Loss: 5.175527775113741, Val Loss: 6.21456005679418, Val MAE: 1.4806435108184814\n",
      "Epoch 577/2000, Train Loss: 5.1738683562997485, Val Loss: 6.21281524186265, Val MAE: 1.4808344841003418\n",
      "Epoch 578/2000, Train Loss: 5.172288111774809, Val Loss: 6.210860393431089, Val MAE: 1.4810463190078735\n",
      "Epoch 579/2000, Train Loss: 5.170660930089471, Val Loss: 6.20900076392987, Val MAE: 1.4812544584274292\n",
      "Epoch 580/2000, Train Loss: 5.169077843862376, Val Loss: 6.207152528509702, Val MAE: 1.48147714138031\n",
      "Epoch 581/2000, Train Loss: 5.167518362233859, Val Loss: 6.205242981037048, Val MAE: 1.4816721677780151\n",
      "Epoch 582/2000, Train Loss: 5.1659435816290316, Val Loss: 6.2036291220008515, Val MAE: 1.48188054561615\n",
      "Epoch 583/2000, Train Loss: 5.164457903882093, Val Loss: 6.201704238987949, Val MAE: 1.4821066856384277\n",
      "Epoch 584/2000, Train Loss: 5.162869037646722, Val Loss: 6.200143426350535, Val MAE: 1.4823157787322998\n",
      "Epoch 585/2000, Train Loss: 5.161370545577385, Val Loss: 6.198218925358498, Val MAE: 1.4825459718704224\n",
      "Epoch 586/2000, Train Loss: 5.159898430060722, Val Loss: 6.1964164839623725, Val MAE: 1.4827649593353271\n",
      "Epoch 587/2000, Train Loss: 5.158455700696577, Val Loss: 6.194838138986124, Val MAE: 1.4829764366149902\n",
      "Epoch 588/2000, Train Loss: 5.157005083618906, Val Loss: 6.193370343479391, Val MAE: 1.48317289352417\n",
      "Epoch 589/2000, Train Loss: 5.1557004695193696, Val Loss: 6.191431567889371, Val MAE: 1.4834357500076294\n",
      "Epoch 590/2000, Train Loss: 5.154260212830245, Val Loss: 6.189817411646451, Val MAE: 1.4836515188217163\n",
      "Epoch 591/2000, Train Loss: 5.1528428632590915, Val Loss: 6.188415015294944, Val MAE: 1.483866810798645\n",
      "Epoch 592/2000, Train Loss: 5.15149083670766, Val Loss: 6.187067463483713, Val MAE: 1.4840754270553589\n",
      "Epoch 593/2000, Train Loss: 5.150160602850891, Val Loss: 6.185347268128232, Val MAE: 1.4843233823776245\n",
      "Epoch 594/2000, Train Loss: 5.148876088171763, Val Loss: 6.183624646332983, Val MAE: 1.4845309257507324\n",
      "Epoch 595/2000, Train Loss: 5.147368748725137, Val Loss: 6.182285807108226, Val MAE: 1.4847530126571655\n",
      "Epoch 596/2000, Train Loss: 5.146104334239249, Val Loss: 6.180672042888322, Val MAE: 1.485013723373413\n",
      "Epoch 597/2000, Train Loss: 5.144772606885221, Val Loss: 6.179142425015365, Val MAE: 1.4852083921432495\n",
      "Epoch 598/2000, Train Loss: 5.143509254270087, Val Loss: 6.177452519740144, Val MAE: 1.4854774475097656\n",
      "Epoch 599/2000, Train Loss: 5.142105884451534, Val Loss: 6.176075259300127, Val MAE: 1.485672116279602\n",
      "Epoch 600/2000, Train Loss: 5.140919778103574, Val Loss: 6.174704627631462, Val MAE: 1.4859148263931274\n",
      "Epoch 601/2000, Train Loss: 5.139689839639772, Val Loss: 6.17314982465277, Val MAE: 1.4861708879470825\n",
      "Epoch 602/2000, Train Loss: 5.138330583247798, Val Loss: 6.171816232984197, Val MAE: 1.486377477645874\n",
      "Epoch 603/2000, Train Loss: 5.137137832672515, Val Loss: 6.1701217440709675, Val MAE: 1.486664891242981\n",
      "Epoch 604/2000, Train Loss: 5.135851108081144, Val Loss: 6.1687348367620825, Val MAE: 1.4869087934494019\n",
      "Epoch 605/2000, Train Loss: 5.134615505533821, Val Loss: 6.167504823779407, Val MAE: 1.4871288537979126\n",
      "Epoch 606/2000, Train Loss: 5.133367388522799, Val Loss: 6.166047270575615, Val MAE: 1.4873847961425781\n",
      "Epoch 607/2000, Train Loss: 5.132168752834128, Val Loss: 6.164676094708377, Val MAE: 1.4876071214675903\n",
      "Epoch 608/2000, Train Loss: 5.1309706326242015, Val Loss: 6.163114551199626, Val MAE: 1.4878473281860352\n",
      "Epoch 609/2000, Train Loss: 5.1297681598261455, Val Loss: 6.161801550078065, Val MAE: 1.4880820512771606\n",
      "Epoch 610/2000, Train Loss: 5.128681929030334, Val Loss: 6.160462739737066, Val MAE: 1.4883478879928589\n",
      "Epoch 611/2000, Train Loss: 5.127473138719556, Val Loss: 6.159342125466425, Val MAE: 1.4885503053665161\n",
      "Epoch 612/2000, Train Loss: 5.126339839961579, Val Loss: 6.157981047483339, Val MAE: 1.4887841939926147\n",
      "Epoch 613/2000, Train Loss: 5.12533585121025, Val Loss: 6.156495746684401, Val MAE: 1.489027500152588\n",
      "Epoch 614/2000, Train Loss: 5.124175351481384, Val Loss: 6.155295972954737, Val MAE: 1.4892206192016602\n",
      "Epoch 615/2000, Train Loss: 5.123137436191305, Val Loss: 6.154261549449947, Val MAE: 1.48944091796875\n",
      "Epoch 616/2000, Train Loss: 5.122049475411735, Val Loss: 6.15312330475817, Val MAE: 1.4896435737609863\n",
      "Epoch 617/2000, Train Loss: 5.120956211074631, Val Loss: 6.151834578036445, Val MAE: 1.4898651838302612\n",
      "Epoch 618/2000, Train Loss: 5.119947119315594, Val Loss: 6.150535832937449, Val MAE: 1.4901031255722046\n",
      "Epoch 619/2000, Train Loss: 5.1188387515286, Val Loss: 6.149445905464969, Val MAE: 1.490310549736023\n",
      "Epoch 620/2000, Train Loss: 5.117820713469816, Val Loss: 6.148423859721993, Val MAE: 1.490527629852295\n",
      "Epoch 621/2000, Train Loss: 5.116818681528348, Val Loss: 6.1468487403368295, Val MAE: 1.4907643795013428\n",
      "Epoch 622/2000, Train Loss: 5.115798697092931, Val Loss: 6.145578907777185, Val MAE: 1.491034984588623\n",
      "Epoch 623/2000, Train Loss: 5.114669070251564, Val Loss: 6.144805659067957, Val MAE: 1.4912153482437134\n",
      "Epoch 624/2000, Train Loss: 5.1137137243967965, Val Loss: 6.143563753325645, Val MAE: 1.4914660453796387\n",
      "Epoch 625/2000, Train Loss: 5.112685823363268, Val Loss: 6.142462822468313, Val MAE: 1.4916675090789795\n",
      "Epoch 626/2000, Train Loss: 5.111775753756976, Val Loss: 6.14128775006696, Val MAE: 1.4919337034225464\n",
      "Epoch 627/2000, Train Loss: 5.110820506341646, Val Loss: 6.140124693512917, Val MAE: 1.492205023765564\n",
      "Epoch 628/2000, Train Loss: 5.109841175365293, Val Loss: 6.139201250896878, Val MAE: 1.492382287979126\n",
      "Epoch 629/2000, Train Loss: 5.10897200072797, Val Loss: 6.138242096729474, Val MAE: 1.4926066398620605\n",
      "Epoch 630/2000, Train Loss: 5.1080674879748, Val Loss: 6.137152627110481, Val MAE: 1.4928734302520752\n",
      "Epoch 631/2000, Train Loss: 5.107160938037272, Val Loss: 6.136196368667361, Val MAE: 1.4930896759033203\n",
      "Epoch 632/2000, Train Loss: 5.1062601389336235, Val Loss: 6.135107469885317, Val MAE: 1.49331533908844\n",
      "Epoch 633/2000, Train Loss: 5.105344715149322, Val Loss: 6.134119008603978, Val MAE: 1.4935688972473145\n",
      "Epoch 634/2000, Train Loss: 5.104418637107218, Val Loss: 6.133128459833256, Val MAE: 1.4937974214553833\n",
      "Epoch 635/2000, Train Loss: 5.103517081787753, Val Loss: 6.132199806505686, Val MAE: 1.4940215349197388\n",
      "Epoch 636/2000, Train Loss: 5.102638878350899, Val Loss: 6.131239342567039, Val MAE: 1.4942501783370972\n",
      "Epoch 637/2000, Train Loss: 5.101765974993837, Val Loss: 6.130083248733658, Val MAE: 1.4944995641708374\n",
      "Epoch 638/2000, Train Loss: 5.100865962246442, Val Loss: 6.1292925939372145, Val MAE: 1.4947229623794556\n",
      "Epoch 639/2000, Train Loss: 5.100032145625382, Val Loss: 6.12819727663308, Val MAE: 1.4949384927749634\n",
      "Epoch 640/2000, Train Loss: 5.099224558912181, Val Loss: 6.127207515888835, Val MAE: 1.4951884746551514\n",
      "Epoch 641/2000, Train Loss: 5.098305657964858, Val Loss: 6.126447466138291, Val MAE: 1.4953707456588745\n",
      "Epoch 642/2000, Train Loss: 5.097532419949721, Val Loss: 6.125561704782591, Val MAE: 1.4955860376358032\n",
      "Epoch 643/2000, Train Loss: 5.096716245144267, Val Loss: 6.124466381456754, Val MAE: 1.4958136081695557\n",
      "Epoch 644/2000, Train Loss: 5.095890209006336, Val Loss: 6.123807769625971, Val MAE: 1.4960066080093384\n",
      "Epoch 645/2000, Train Loss: 5.095114600523171, Val Loss: 6.122600459583001, Val MAE: 1.4962722063064575\n",
      "Epoch 646/2000, Train Loss: 5.0942317065775296, Val Loss: 6.121813726241458, Val MAE: 1.496482014656067\n",
      "Epoch 647/2000, Train Loss: 5.0934725947666015, Val Loss: 6.1210718118164635, Val MAE: 1.4966665506362915\n",
      "Epoch 648/2000, Train Loss: 5.092669210325764, Val Loss: 6.120216141824853, Val MAE: 1.4968820810317993\n",
      "Epoch 649/2000, Train Loss: 5.091915978025193, Val Loss: 6.119277710988097, Val MAE: 1.497082233428955\n",
      "Epoch 650/2000, Train Loss: 5.091183695646314, Val Loss: 6.118409157411693, Val MAE: 1.4972978830337524\n",
      "Epoch 651/2000, Train Loss: 5.090363775312031, Val Loss: 6.117463116980579, Val MAE: 1.49752676486969\n",
      "Epoch 652/2000, Train Loss: 5.089552259522474, Val Loss: 6.116697766907411, Val MAE: 1.4977256059646606\n",
      "Epoch 653/2000, Train Loss: 5.088848633356574, Val Loss: 6.115921017139742, Val MAE: 1.4979320764541626\n",
      "Epoch 654/2000, Train Loss: 5.088073441429694, Val Loss: 6.115105124981436, Val MAE: 1.4981517791748047\n",
      "Epoch 655/2000, Train Loss: 5.087332456579471, Val Loss: 6.1142657496137165, Val MAE: 1.4983651638031006\n",
      "Epoch 656/2000, Train Loss: 5.08657684086594, Val Loss: 6.113331516311593, Val MAE: 1.4986076354980469\n",
      "Epoch 657/2000, Train Loss: 5.085828434319705, Val Loss: 6.1124713739303695, Val MAE: 1.4988449811935425\n",
      "Epoch 658/2000, Train Loss: 5.085098063540034, Val Loss: 6.111731363894188, Val MAE: 1.499048113822937\n",
      "Epoch 659/2000, Train Loss: 5.084456177159685, Val Loss: 6.1111718273326145, Val MAE: 1.4991992712020874\n",
      "Epoch 660/2000, Train Loss: 5.083730546156823, Val Loss: 6.1101776186938155, Val MAE: 1.499466061592102\n",
      "Epoch 661/2000, Train Loss: 5.082966071087112, Val Loss: 6.1095068841560245, Val MAE: 1.4996814727783203\n",
      "Epoch 662/2000, Train Loss: 5.082240188527532, Val Loss: 6.1086276638997745, Val MAE: 1.499895453453064\n",
      "Epoch 663/2000, Train Loss: 5.0815567290183985, Val Loss: 6.107970078514047, Val MAE: 1.500076413154602\n",
      "Epoch 664/2000, Train Loss: 5.080937070243749, Val Loss: 6.107119364587412, Val MAE: 1.5003081560134888\n",
      "Epoch 665/2000, Train Loss: 5.080186942214904, Val Loss: 6.106648660277667, Val MAE: 1.5004535913467407\n",
      "Epoch 666/2000, Train Loss: 5.07956160757105, Val Loss: 6.105738291797572, Val MAE: 1.5006966590881348\n",
      "Epoch 667/2000, Train Loss: 5.078856833750077, Val Loss: 6.10530315441628, Val MAE: 1.500860333442688\n",
      "Epoch 668/2000, Train Loss: 5.07814507507621, Val Loss: 6.1043033032384635, Val MAE: 1.5011094808578491\n",
      "Epoch 669/2000, Train Loss: 5.077512825327522, Val Loss: 6.103719374292518, Val MAE: 1.5013024806976318\n",
      "Epoch 670/2000, Train Loss: 5.076842047986567, Val Loss: 6.103018202602047, Val MAE: 1.5015246868133545\n",
      "Epoch 671/2000, Train Loss: 5.076215484347104, Val Loss: 6.1022652617260205, Val MAE: 1.501750111579895\n",
      "Epoch 672/2000, Train Loss: 5.0755337818708375, Val Loss: 6.101685928155298, Val MAE: 1.501893162727356\n",
      "Epoch 673/2000, Train Loss: 5.07496746896345, Val Loss: 6.101147541852846, Val MAE: 1.5020778179168701\n",
      "Epoch 674/2000, Train Loss: 5.074337984792997, Val Loss: 6.100510592738243, Val MAE: 1.502265453338623\n",
      "Epoch 675/2000, Train Loss: 5.073716046164836, Val Loss: 6.09994876639892, Val MAE: 1.5024045705795288\n",
      "Epoch 676/2000, Train Loss: 5.073064849674219, Val Loss: 6.099154260264684, Val MAE: 1.5026708841323853\n",
      "Epoch 677/2000, Train Loss: 5.072505067581103, Val Loss: 6.098311630626247, Val MAE: 1.5029202699661255\n",
      "Epoch 678/2000, Train Loss: 5.0718061363677736, Val Loss: 6.097806379913467, Val MAE: 1.5030479431152344\n",
      "Epoch 679/2000, Train Loss: 5.071218113458717, Val Loss: 6.097380947985061, Val MAE: 1.5032150745391846\n",
      "Epoch 680/2000, Train Loss: 5.070637898081898, Val Loss: 6.09654940991369, Val MAE: 1.5034629106521606\n",
      "Epoch 681/2000, Train Loss: 5.070047691536877, Val Loss: 6.095821920425108, Val MAE: 1.5036349296569824\n",
      "Epoch 682/2000, Train Loss: 5.069401741800664, Val Loss: 6.095397710596045, Val MAE: 1.503807783126831\n",
      "Epoch 683/2000, Train Loss: 5.068866972405481, Val Loss: 6.094912718113971, Val MAE: 1.5039489269256592\n",
      "Epoch 684/2000, Train Loss: 5.068315720055826, Val Loss: 6.094456284319701, Val MAE: 1.5041075944900513\n",
      "Epoch 685/2000, Train Loss: 5.067721974328039, Val Loss: 6.0936514071814, Val MAE: 1.5043094158172607\n",
      "Epoch 686/2000, Train Loss: 5.0671538356057635, Val Loss: 6.092986854278061, Val MAE: 1.504540205001831\n",
      "Epoch 687/2000, Train Loss: 5.066488960380492, Val Loss: 6.092406664094696, Val MAE: 1.504723310470581\n",
      "Epoch 688/2000, Train Loss: 5.0659531271824765, Val Loss: 6.091714838698302, Val MAE: 1.5049192905426025\n",
      "Epoch 689/2000, Train Loss: 5.0653337844959925, Val Loss: 6.091210280175078, Val MAE: 1.5050816535949707\n",
      "Epoch 690/2000, Train Loss: 5.064772847405902, Val Loss: 6.090656701322287, Val MAE: 1.505260705947876\n",
      "Epoch 691/2000, Train Loss: 5.064231573472727, Val Loss: 6.090131694323396, Val MAE: 1.5054259300231934\n",
      "Epoch 692/2000, Train Loss: 5.063666377790163, Val Loss: 6.0895761551514065, Val MAE: 1.5055989027023315\n",
      "Epoch 693/2000, Train Loss: 5.0631293593581335, Val Loss: 6.089096062803922, Val MAE: 1.5057305097579956\n",
      "Epoch 694/2000, Train Loss: 5.06258716073183, Val Loss: 6.088367864591618, Val MAE: 1.5059534311294556\n",
      "Epoch 695/2000, Train Loss: 5.062017253770628, Val Loss: 6.087876038804446, Val MAE: 1.5061562061309814\n",
      "Epoch 696/2000, Train Loss: 5.061427073501884, Val Loss: 6.087408190720702, Val MAE: 1.506308913230896\n",
      "Epoch 697/2000, Train Loss: 5.060933217430424, Val Loss: 6.08680322470322, Val MAE: 1.506500005722046\n",
      "Epoch 698/2000, Train Loss: 5.060338028053024, Val Loss: 6.0863431297140576, Val MAE: 1.5066425800323486\n",
      "Epoch 699/2000, Train Loss: 5.059809734214647, Val Loss: 6.085869737275659, Val MAE: 1.5068217515945435\n",
      "Epoch 700/2000, Train Loss: 5.059288147009752, Val Loss: 6.085211160044148, Val MAE: 1.507003903388977\n",
      "Epoch 701/2000, Train Loss: 5.058751468720274, Val Loss: 6.084841203097612, Val MAE: 1.5071734189987183\n",
      "Epoch 702/2000, Train Loss: 5.058191222928138, Val Loss: 6.084328961290725, Val MAE: 1.5073035955429077\n",
      "Epoch 703/2000, Train Loss: 5.057663269908541, Val Loss: 6.083880543198488, Val MAE: 1.5074609518051147\n",
      "Epoch 704/2000, Train Loss: 5.057169815323148, Val Loss: 6.0834385019867385, Val MAE: 1.5075993537902832\n",
      "Epoch 705/2000, Train Loss: 5.056669791668315, Val Loss: 6.082879916343787, Val MAE: 1.5078178644180298\n",
      "Epoch 706/2000, Train Loss: 5.056130336594543, Val Loss: 6.08227001395944, Val MAE: 1.5080170631408691\n",
      "Epoch 707/2000, Train Loss: 5.055625721933003, Val Loss: 6.081973357661946, Val MAE: 1.5081026554107666\n",
      "Epoch 708/2000, Train Loss: 5.055022603688789, Val Loss: 6.081263915214636, Val MAE: 1.5082911252975464\n",
      "Epoch 709/2000, Train Loss: 5.0545440554812044, Val Loss: 6.080875465314683, Val MAE: 1.5084389448165894\n",
      "Epoch 710/2000, Train Loss: 5.054002299687465, Val Loss: 6.080317254139953, Val MAE: 1.508609414100647\n",
      "Epoch 711/2000, Train Loss: 5.053497067919813, Val Loss: 6.079873945513, Val MAE: 1.5087847709655762\n",
      "Epoch 712/2000, Train Loss: 5.0529959244318485, Val Loss: 6.07938373323581, Val MAE: 1.5089529752731323\n",
      "Epoch 713/2000, Train Loss: 5.052499672099103, Val Loss: 6.078832411194501, Val MAE: 1.509110927581787\n",
      "Epoch 714/2000, Train Loss: 5.051988301053039, Val Loss: 6.078487566889149, Val MAE: 1.5092287063598633\n",
      "Epoch 715/2000, Train Loss: 5.051513391337093, Val Loss: 6.078022541758949, Val MAE: 1.509364128112793\n",
      "Epoch 716/2000, Train Loss: 5.051032666062613, Val Loss: 6.077522663220967, Val MAE: 1.5095274448394775\n",
      "Epoch 717/2000, Train Loss: 5.0506042151242445, Val Loss: 6.076842130047002, Val MAE: 1.5097447633743286\n",
      "Epoch 718/2000, Train Loss: 5.049982936494354, Val Loss: 6.076496987003986, Val MAE: 1.5098618268966675\n",
      "Epoch 719/2000, Train Loss: 5.04951868180904, Val Loss: 6.076071742462785, Val MAE: 1.5099767446517944\n",
      "Epoch 720/2000, Train Loss: 5.049086716032106, Val Loss: 6.075583762706143, Val MAE: 1.510149359703064\n",
      "Epoch 721/2000, Train Loss: 5.048535132137651, Val Loss: 6.075230422085279, Val MAE: 1.5102593898773193\n",
      "Epoch 722/2000, Train Loss: 5.048055662714681, Val Loss: 6.074840519628296, Val MAE: 1.5103812217712402\n",
      "Epoch 723/2000, Train Loss: 5.047612711135147, Val Loss: 6.074317420998665, Val MAE: 1.5105860233306885\n",
      "Epoch 724/2000, Train Loss: 5.047077494077203, Val Loss: 6.073832495163565, Val MAE: 1.5107110738754272\n",
      "Epoch 725/2000, Train Loss: 5.046619399633361, Val Loss: 6.0732934358593536, Val MAE: 1.5109052658081055\n",
      "Epoch 726/2000, Train Loss: 5.046175580357037, Val Loss: 6.072919551538278, Val MAE: 1.511027455329895\n",
      "Epoch 727/2000, Train Loss: 5.045664483956038, Val Loss: 6.072552082575347, Val MAE: 1.5111308097839355\n",
      "Epoch 728/2000, Train Loss: 5.045226685618272, Val Loss: 6.072071392038097, Val MAE: 1.5113190412521362\n",
      "Epoch 729/2000, Train Loss: 5.044820647958418, Val Loss: 6.071587873035914, Val MAE: 1.5114418268203735\n",
      "Epoch 730/2000, Train Loss: 5.044332009660174, Val Loss: 6.07141653438137, Val MAE: 1.5115301609039307\n",
      "Epoch 731/2000, Train Loss: 5.043881359131255, Val Loss: 6.070912132932715, Val MAE: 1.5116945505142212\n",
      "Epoch 732/2000, Train Loss: 5.043402494835505, Val Loss: 6.070521956233129, Val MAE: 1.511832594871521\n",
      "Epoch 733/2000, Train Loss: 5.042979124879142, Val Loss: 6.070103684822991, Val MAE: 1.5119919776916504\n",
      "Epoch 734/2000, Train Loss: 5.042489010085936, Val Loss: 6.06976995831483, Val MAE: 1.51211678981781\n",
      "Epoch 735/2000, Train Loss: 5.042081563553895, Val Loss: 6.069300834140549, Val MAE: 1.512239694595337\n",
      "Epoch 736/2000, Train Loss: 5.04160382256902, Val Loss: 6.068983630468584, Val MAE: 1.5123443603515625\n",
      "Epoch 737/2000, Train Loss: 5.041228447393235, Val Loss: 6.06859039668351, Val MAE: 1.5124870538711548\n",
      "Epoch 738/2000, Train Loss: 5.040723699804643, Val Loss: 6.068253044396231, Val MAE: 1.512586236000061\n",
      "Epoch 739/2000, Train Loss: 5.040269881825006, Val Loss: 6.067832295004636, Val MAE: 1.512755274772644\n",
      "Epoch 740/2000, Train Loss: 5.039864813296103, Val Loss: 6.067277970787597, Val MAE: 1.5129048824310303\n",
      "Epoch 741/2000, Train Loss: 5.039383511875592, Val Loss: 6.066914940737698, Val MAE: 1.5130517482757568\n",
      "Epoch 742/2000, Train Loss: 5.039006392032246, Val Loss: 6.066670192124909, Val MAE: 1.5130982398986816\n",
      "Epoch 743/2000, Train Loss: 5.0385665545780425, Val Loss: 6.06636959079602, Val MAE: 1.5131958723068237\n",
      "Epoch 744/2000, Train Loss: 5.038152046296353, Val Loss: 6.065788265062522, Val MAE: 1.5134375095367432\n",
      "Epoch 745/2000, Train Loss: 5.037632908195112, Val Loss: 6.065475938458965, Val MAE: 1.5134869813919067\n",
      "Epoch 746/2000, Train Loss: 5.03725652447396, Val Loss: 6.065124709924606, Val MAE: 1.5136266946792603\n",
      "Epoch 747/2000, Train Loss: 5.0367935449609496, Val Loss: 6.064706641721399, Val MAE: 1.5137763023376465\n",
      "Epoch 748/2000, Train Loss: 5.036354444401771, Val Loss: 6.064334609737135, Val MAE: 1.5139185190200806\n",
      "Epoch 749/2000, Train Loss: 5.035909869102837, Val Loss: 6.063902407068095, Val MAE: 1.5140589475631714\n",
      "Epoch 750/2000, Train Loss: 5.03551526432872, Val Loss: 6.063436001743356, Val MAE: 1.5142182111740112\n",
      "Epoch 751/2000, Train Loss: 5.035085668826605, Val Loss: 6.063180271383017, Val MAE: 1.5143152475357056\n",
      "Epoch 752/2000, Train Loss: 5.03465846330652, Val Loss: 6.062757606375707, Val MAE: 1.5144411325454712\n",
      "Epoch 753/2000, Train Loss: 5.034297831054638, Val Loss: 6.062369804063889, Val MAE: 1.5145577192306519\n",
      "Epoch 754/2000, Train Loss: 5.033829771332455, Val Loss: 6.062240925859915, Val MAE: 1.514617919921875\n",
      "Epoch 755/2000, Train Loss: 5.033426154954314, Val Loss: 6.0618979277676095, Val MAE: 1.5147552490234375\n",
      "Epoch 756/2000, Train Loss: 5.0330436194928385, Val Loss: 6.061520991872435, Val MAE: 1.5148499011993408\n",
      "Epoch 757/2000, Train Loss: 5.032606951807847, Val Loss: 6.061081227170278, Val MAE: 1.5150171518325806\n",
      "Epoch 758/2000, Train Loss: 5.032159739993573, Val Loss: 6.060656442013506, Val MAE: 1.515135645866394\n",
      "Epoch 759/2000, Train Loss: 5.031781208959545, Val Loss: 6.060369812870679, Val MAE: 1.5152450799942017\n",
      "Epoch 760/2000, Train Loss: 5.031344686180498, Val Loss: 6.059998120347115, Val MAE: 1.5153815746307373\n",
      "Epoch 761/2000, Train Loss: 5.030918348743618, Val Loss: 6.059680957500249, Val MAE: 1.515480637550354\n",
      "Epoch 762/2000, Train Loss: 5.030553568897217, Val Loss: 6.059390371384686, Val MAE: 1.5155949592590332\n",
      "Epoch 763/2000, Train Loss: 5.030107714561821, Val Loss: 6.05911860206764, Val MAE: 1.5157067775726318\n",
      "Epoch 764/2000, Train Loss: 5.029676022753724, Val Loss: 6.058728361476774, Val MAE: 1.515835165977478\n",
      "Epoch 765/2000, Train Loss: 5.029297470080408, Val Loss: 6.058555741322367, Val MAE: 1.5159153938293457\n",
      "Epoch 766/2000, Train Loss: 5.02888095166347, Val Loss: 6.058134453345652, Val MAE: 1.5160620212554932\n",
      "Epoch 767/2000, Train Loss: 5.028476138748651, Val Loss: 6.057576209919094, Val MAE: 1.5162535905838013\n",
      "Epoch 768/2000, Train Loss: 5.028035405002885, Val Loss: 6.05734885330886, Val MAE: 1.5163459777832031\n",
      "Epoch 769/2000, Train Loss: 5.0276622756760165, Val Loss: 6.057166163643745, Val MAE: 1.5164293050765991\n",
      "Epoch 770/2000, Train Loss: 5.02727507231108, Val Loss: 6.056834695886259, Val MAE: 1.5165214538574219\n",
      "Epoch 771/2000, Train Loss: 5.026837423129831, Val Loss: 6.056410524738978, Val MAE: 1.5166922807693481\n",
      "Epoch 772/2000, Train Loss: 5.02654196802569, Val Loss: 6.056187141431521, Val MAE: 1.516735315322876\n",
      "Epoch 773/2000, Train Loss: 5.0261799203325515, Val Loss: 6.055699053080114, Val MAE: 1.5169545412063599\n",
      "Epoch 774/2000, Train Loss: 5.025651632290412, Val Loss: 6.055552939539903, Val MAE: 1.5169460773468018\n",
      "Epoch 775/2000, Train Loss: 5.025244072152112, Val Loss: 6.055182504531455, Val MAE: 1.5171318054199219\n",
      "Epoch 776/2000, Train Loss: 5.024838231950757, Val Loss: 6.054804117916381, Val MAE: 1.5172269344329834\n",
      "Epoch 777/2000, Train Loss: 5.02441001170265, Val Loss: 6.054418260920538, Val MAE: 1.5173698663711548\n",
      "Epoch 778/2000, Train Loss: 5.024049247296548, Val Loss: 6.05406567525782, Val MAE: 1.5175057649612427\n",
      "Epoch 779/2000, Train Loss: 5.023649211834083, Val Loss: 6.053977025596247, Val MAE: 1.5175217390060425\n",
      "Epoch 780/2000, Train Loss: 5.023229993916021, Val Loss: 6.0535231611295925, Val MAE: 1.517684817314148\n",
      "Epoch 781/2000, Train Loss: 5.022873975855798, Val Loss: 6.053222732184684, Val MAE: 1.5178136825561523\n",
      "Epoch 782/2000, Train Loss: 5.0224569218664925, Val Loss: 6.052968188507916, Val MAE: 1.5178794860839844\n",
      "Epoch 783/2000, Train Loss: 5.022051832084718, Val Loss: 6.0526821896025575, Val MAE: 1.5180193185806274\n",
      "Epoch 784/2000, Train Loss: 5.0216895679987035, Val Loss: 6.052397982510802, Val MAE: 1.518099069595337\n",
      "Epoch 785/2000, Train Loss: 5.0212851003464465, Val Loss: 6.051914229376675, Val MAE: 1.5182465314865112\n",
      "Epoch 786/2000, Train Loss: 5.020887600544786, Val Loss: 6.051666578405524, Val MAE: 1.5183824300765991\n",
      "Epoch 787/2000, Train Loss: 5.020495612578801, Val Loss: 6.051245445564185, Val MAE: 1.5185099840164185\n",
      "Epoch 788/2000, Train Loss: 5.0200756666532795, Val Loss: 6.051035880430104, Val MAE: 1.5185885429382324\n",
      "Epoch 789/2000, Train Loss: 5.019672492721286, Val Loss: 6.050734176313224, Val MAE: 1.5186781883239746\n",
      "Epoch 790/2000, Train Loss: 5.01929017519835, Val Loss: 6.050426147163731, Val MAE: 1.5187938213348389\n",
      "Epoch 791/2000, Train Loss: 5.01887789112629, Val Loss: 6.050015514435834, Val MAE: 1.5189253091812134\n",
      "Epoch 792/2000, Train Loss: 5.01850769083156, Val Loss: 6.0497417196835555, Val MAE: 1.5190550088882446\n",
      "Epoch 793/2000, Train Loss: 5.018139488306772, Val Loss: 6.049406437637055, Val MAE: 1.5191398859024048\n",
      "Epoch 794/2000, Train Loss: 5.01770554485352, Val Loss: 6.049138613249341, Val MAE: 1.5192304849624634\n",
      "Epoch 795/2000, Train Loss: 5.017333234728252, Val Loss: 6.048863503622682, Val MAE: 1.5193188190460205\n",
      "Epoch 796/2000, Train Loss: 5.016929747026588, Val Loss: 6.048487049055426, Val MAE: 1.5194796323776245\n",
      "Epoch 797/2000, Train Loss: 5.016565583320259, Val Loss: 6.048233703697381, Val MAE: 1.5195791721343994\n",
      "Epoch 798/2000, Train Loss: 5.016202440524603, Val Loss: 6.048004344718097, Val MAE: 1.5196614265441895\n",
      "Epoch 799/2000, Train Loss: 5.015790484906016, Val Loss: 6.0476122309696185, Val MAE: 1.5197714567184448\n",
      "Epoch 800/2000, Train Loss: 5.015381301434732, Val Loss: 6.0472945074920785, Val MAE: 1.5199205875396729\n",
      "Epoch 801/2000, Train Loss: 5.015024552275721, Val Loss: 6.046983311335518, Val MAE: 1.5200250148773193\n",
      "Epoch 802/2000, Train Loss: 5.014629413474901, Val Loss: 6.046781715260793, Val MAE: 1.5201175212860107\n",
      "Epoch 803/2000, Train Loss: 5.014237201387724, Val Loss: 6.0464938250306535, Val MAE: 1.520181655883789\n",
      "Epoch 804/2000, Train Loss: 5.013856028428534, Val Loss: 6.04617208487367, Val MAE: 1.5203192234039307\n",
      "Epoch 805/2000, Train Loss: 5.0135275834576625, Val Loss: 6.04576715112549, Val MAE: 1.520460605621338\n",
      "Epoch 806/2000, Train Loss: 5.013128007830058, Val Loss: 6.045455013133892, Val MAE: 1.5205904245376587\n",
      "Epoch 807/2000, Train Loss: 5.012724279004906, Val Loss: 6.045271041662725, Val MAE: 1.5206265449523926\n",
      "Epoch 808/2000, Train Loss: 5.012429595573215, Val Loss: 6.045197302654182, Val MAE: 1.5206685066223145\n",
      "Epoch 809/2000, Train Loss: 5.012104291776784, Val Loss: 6.0446781558941485, Val MAE: 1.5208574533462524\n",
      "Epoch 810/2000, Train Loss: 5.011642997809709, Val Loss: 6.044479894719712, Val MAE: 1.5209451913833618\n",
      "Epoch 811/2000, Train Loss: 5.011258817762378, Val Loss: 6.044238100517286, Val MAE: 1.5210062265396118\n",
      "Epoch 812/2000, Train Loss: 5.01087582169321, Val Loss: 6.0439470182747055, Val MAE: 1.5211167335510254\n",
      "Epoch 813/2000, Train Loss: 5.0106104351519765, Val Loss: 6.043790818077245, Val MAE: 1.5211336612701416\n",
      "Epoch 814/2000, Train Loss: 5.010151580241743, Val Loss: 6.043292784731682, Val MAE: 1.5213367938995361\n",
      "Epoch 815/2000, Train Loss: 5.009785457020641, Val Loss: 6.043117528908874, Val MAE: 1.5214135646820068\n",
      "Epoch 816/2000, Train Loss: 5.009401726374943, Val Loss: 6.0428135578763, Val MAE: 1.5214868783950806\n",
      "Epoch 817/2000, Train Loss: 5.009066644325442, Val Loss: 6.042612082031492, Val MAE: 1.5216139554977417\n",
      "Epoch 818/2000, Train Loss: 5.008707632701633, Val Loss: 6.042446861324245, Val MAE: 1.5216870307922363\n",
      "Epoch 819/2000, Train Loss: 5.00835501085031, Val Loss: 6.042041960746458, Val MAE: 1.5218441486358643\n",
      "Epoch 820/2000, Train Loss: 5.007983164424061, Val Loss: 6.041942020393398, Val MAE: 1.5218558311462402\n",
      "Epoch 821/2000, Train Loss: 5.007592402941984, Val Loss: 6.041625502787224, Val MAE: 1.5219447612762451\n",
      "Epoch 822/2000, Train Loss: 5.007223611905857, Val Loss: 6.041272424569685, Val MAE: 1.522046685218811\n",
      "Epoch 823/2000, Train Loss: 5.006889684080498, Val Loss: 6.041010848667524, Val MAE: 1.5221439599990845\n",
      "Epoch 824/2000, Train Loss: 5.006521538745256, Val Loss: 6.040784709053497, Val MAE: 1.5221832990646362\n",
      "Epoch 825/2000, Train Loss: 5.00615022093589, Val Loss: 6.040447795431908, Val MAE: 1.522350549697876\n",
      "Epoch 826/2000, Train Loss: 5.005777563036357, Val Loss: 6.0401812990846695, Val MAE: 1.5224727392196655\n",
      "Epoch 827/2000, Train Loss: 5.0055115315670715, Val Loss: 6.03978655007604, Val MAE: 1.5226154327392578\n",
      "Epoch 828/2000, Train Loss: 5.00499660346651, Val Loss: 6.03959741086176, Val MAE: 1.5226839780807495\n",
      "Epoch 829/2000, Train Loss: 5.004630030457359, Val Loss: 6.03932159404232, Val MAE: 1.522782325744629\n",
      "Epoch 830/2000, Train Loss: 5.00429249505363, Val Loss: 6.0391386551399755, Val MAE: 1.5228421688079834\n",
      "Epoch 831/2000, Train Loss: 5.003981337941447, Val Loss: 6.038716279888806, Val MAE: 1.5229737758636475\n",
      "Epoch 832/2000, Train Loss: 5.003610978829803, Val Loss: 6.038582267010049, Val MAE: 1.5230547189712524\n",
      "Epoch 833/2000, Train Loss: 5.00323439591128, Val Loss: 6.038333361687726, Val MAE: 1.5231367349624634\n",
      "Epoch 834/2000, Train Loss: 5.0029208084366115, Val Loss: 6.038068468031818, Val MAE: 1.5232372283935547\n",
      "Epoch 835/2000, Train Loss: 5.002525236463624, Val Loss: 6.037861048562886, Val MAE: 1.523287057876587\n",
      "Epoch 836/2000, Train Loss: 5.002169857535216, Val Loss: 6.037684211583986, Val MAE: 1.5233581066131592\n",
      "Epoch 837/2000, Train Loss: 5.00183528980521, Val Loss: 6.037301612635181, Val MAE: 1.523472547531128\n",
      "Epoch 838/2000, Train Loss: 5.00148124323865, Val Loss: 6.0370973471092855, Val MAE: 1.5235774517059326\n",
      "Epoch 839/2000, Train Loss: 5.001127760066399, Val Loss: 6.0368304277119575, Val MAE: 1.5236550569534302\n",
      "Epoch 840/2000, Train Loss: 5.000929257471713, Val Loss: 6.0364337512483335, Val MAE: 1.5238261222839355\n",
      "Epoch 841/2000, Train Loss: 5.00040208037514, Val Loss: 6.036366063029798, Val MAE: 1.52382230758667\n",
      "Epoch 842/2000, Train Loss: 5.000084758577702, Val Loss: 6.036099599036452, Val MAE: 1.5239031314849854\n",
      "Epoch 843/2000, Train Loss: 4.999801183828851, Val Loss: 6.035846563030596, Val MAE: 1.5240086317062378\n",
      "Epoch 844/2000, Train Loss: 4.999433886096775, Val Loss: 6.035702894199384, Val MAE: 1.5240256786346436\n",
      "Epoch 845/2000, Train Loss: 4.999057891109968, Val Loss: 6.035446074727464, Val MAE: 1.5241144895553589\n",
      "Epoch 846/2000, Train Loss: 4.99868573555104, Val Loss: 6.035214122027567, Val MAE: 1.5242118835449219\n",
      "Epoch 847/2000, Train Loss: 4.998362989441116, Val Loss: 6.0349714733558155, Val MAE: 1.5242704153060913\n",
      "Epoch 848/2000, Train Loss: 4.998001085494674, Val Loss: 6.0347777952070105, Val MAE: 1.524336338043213\n",
      "Epoch 849/2000, Train Loss: 4.997615968388908, Val Loss: 6.034408604445523, Val MAE: 1.524466872215271\n",
      "Epoch 850/2000, Train Loss: 4.997283939411034, Val Loss: 6.034186977842083, Val MAE: 1.5245875120162964\n",
      "Epoch 851/2000, Train Loss: 4.996976769337585, Val Loss: 6.033897533808669, Val MAE: 1.5246487855911255\n",
      "Epoch 852/2000, Train Loss: 4.996613745171595, Val Loss: 6.0336825396508385, Val MAE: 1.5247082710266113\n",
      "Epoch 853/2000, Train Loss: 4.9962415509710825, Val Loss: 6.033372719402182, Val MAE: 1.5247992277145386\n",
      "Epoch 854/2000, Train Loss: 4.9959049789020655, Val Loss: 6.033235868566657, Val MAE: 1.5248764753341675\n",
      "Epoch 855/2000, Train Loss: 4.995561168491357, Val Loss: 6.0329061428161515, Val MAE: 1.525004506111145\n",
      "Epoch 856/2000, Train Loss: 4.995217194240329, Val Loss: 6.032715492052574, Val MAE: 1.5250526666641235\n",
      "Epoch 857/2000, Train Loss: 4.99487020362718, Val Loss: 6.032427473427498, Val MAE: 1.5251373052597046\n",
      "Epoch 858/2000, Train Loss: 4.994517232842345, Val Loss: 6.032233888564044, Val MAE: 1.5252580642700195\n",
      "Epoch 859/2000, Train Loss: 4.994185003314644, Val Loss: 6.03208580490661, Val MAE: 1.5253156423568726\n",
      "Epoch 860/2000, Train Loss: 4.993863979073746, Val Loss: 6.031828317331941, Val MAE: 1.5253703594207764\n",
      "Epoch 861/2000, Train Loss: 4.993548619689199, Val Loss: 6.031519634266422, Val MAE: 1.5255120992660522\n",
      "Epoch 862/2000, Train Loss: 4.993209488774428, Val Loss: 6.031370078864163, Val MAE: 1.5255608558654785\n",
      "Epoch 863/2000, Train Loss: 4.992855089023782, Val Loss: 6.031066460968697, Val MAE: 1.5256606340408325\n",
      "Epoch 864/2000, Train Loss: 4.992468663900366, Val Loss: 6.030831606012502, Val MAE: 1.5257554054260254\n",
      "Epoch 865/2000, Train Loss: 4.992135769737405, Val Loss: 6.03056963196356, Val MAE: 1.5257903337478638\n",
      "Epoch 866/2000, Train Loss: 4.99176641337489, Val Loss: 6.030403988206223, Val MAE: 1.5258744955062866\n",
      "Epoch 867/2000, Train Loss: 4.991447687535464, Val Loss: 6.030123871687341, Val MAE: 1.5259311199188232\n",
      "Epoch 868/2000, Train Loss: 4.991088813862112, Val Loss: 6.029930659761168, Val MAE: 1.5260415077209473\n",
      "Epoch 869/2000, Train Loss: 4.990749845241998, Val Loss: 6.029565845654435, Val MAE: 1.5261627435684204\n",
      "Epoch 870/2000, Train Loss: 4.990402662193756, Val Loss: 6.029428100749238, Val MAE: 1.5262062549591064\n",
      "Epoch 871/2000, Train Loss: 4.990017855379918, Val Loss: 6.02913652421677, Val MAE: 1.5262906551361084\n",
      "Epoch 872/2000, Train Loss: 4.989713098473448, Val Loss: 6.028879433257939, Val MAE: 1.526422142982483\n",
      "Epoch 873/2000, Train Loss: 4.989338472944411, Val Loss: 6.028571786945814, Val MAE: 1.5265034437179565\n",
      "Epoch 874/2000, Train Loss: 4.988995502601759, Val Loss: 6.028436164333396, Val MAE: 1.52651846408844\n",
      "Epoch 875/2000, Train Loss: 4.988646577590869, Val Loss: 6.028202096893363, Val MAE: 1.52658212184906\n",
      "Epoch 876/2000, Train Loss: 4.98838586416971, Val Loss: 6.0278561119347405, Val MAE: 1.52672278881073\n",
      "Epoch 877/2000, Train Loss: 4.987952077021668, Val Loss: 6.027758900841621, Val MAE: 1.5267590284347534\n",
      "Epoch 878/2000, Train Loss: 4.987707405074875, Val Loss: 6.0275442310392044, Val MAE: 1.5267733335494995\n",
      "Epoch 879/2000, Train Loss: 4.98729071605534, Val Loss: 6.027250364422798, Val MAE: 1.5269005298614502\n",
      "Epoch 880/2000, Train Loss: 4.986954392644923, Val Loss: 6.027030437368236, Val MAE: 1.526947259902954\n",
      "Epoch 881/2000, Train Loss: 4.986609996041371, Val Loss: 6.0268695672897445, Val MAE: 1.5270321369171143\n",
      "Epoch 882/2000, Train Loss: 4.986375786303701, Val Loss: 6.026563691358044, Val MAE: 1.5271365642547607\n",
      "Epoch 883/2000, Train Loss: 4.985934379614733, Val Loss: 6.026446940147713, Val MAE: 1.5271600484848022\n",
      "Epoch 884/2000, Train Loss: 4.985581873301749, Val Loss: 6.026095304791242, Val MAE: 1.5272576808929443\n",
      "Epoch 885/2000, Train Loss: 4.985234202210289, Val Loss: 6.025958639505792, Val MAE: 1.5272976160049438\n",
      "Epoch 886/2000, Train Loss: 4.984935741177255, Val Loss: 6.025818407739679, Val MAE: 1.527345061302185\n",
      "Epoch 887/2000, Train Loss: 4.984540807949666, Val Loss: 6.025482130785511, Val MAE: 1.5274473428726196\n",
      "Epoch 888/2000, Train Loss: 4.984257567450525, Val Loss: 6.025114965765444, Val MAE: 1.5275843143463135\n",
      "Epoch 889/2000, Train Loss: 4.983836674806171, Val Loss: 6.0249810802609955, Val MAE: 1.5276198387145996\n",
      "Epoch 890/2000, Train Loss: 4.983504183095317, Val Loss: 6.024735108222047, Val MAE: 1.5276918411254883\n",
      "Epoch 891/2000, Train Loss: 4.9831994453164326, Val Loss: 6.024490361344324, Val MAE: 1.5277435779571533\n",
      "Epoch 892/2000, Train Loss: 4.982848651212077, Val Loss: 6.024319248248453, Val MAE: 1.5278635025024414\n",
      "Epoch 893/2000, Train Loss: 4.982532895364869, Val Loss: 6.0239376215493845, Val MAE: 1.527955412864685\n",
      "Epoch 894/2000, Train Loss: 4.982151481282189, Val Loss: 6.023806124109111, Val MAE: 1.5280156135559082\n",
      "Epoch 895/2000, Train Loss: 4.981860880333948, Val Loss: 6.023661706545582, Val MAE: 1.5280630588531494\n",
      "Epoch 896/2000, Train Loss: 4.981513512965346, Val Loss: 6.023334294965823, Val MAE: 1.528130054473877\n",
      "Epoch 897/2000, Train Loss: 4.981155220075017, Val Loss: 6.023221954703331, Val MAE: 1.528167963027954\n",
      "Epoch 898/2000, Train Loss: 4.980832016448727, Val Loss: 6.0229044046304, Val MAE: 1.5282485485076904\n",
      "Epoch 899/2000, Train Loss: 4.980468586932512, Val Loss: 6.022660979669388, Val MAE: 1.5283472537994385\n",
      "Epoch 900/2000, Train Loss: 4.980224651107911, Val Loss: 6.0225470576384295, Val MAE: 1.528340220451355\n",
      "Epoch 901/2000, Train Loss: 4.979853630065918, Val Loss: 6.022167184989747, Val MAE: 1.528491735458374\n",
      "Epoch 902/2000, Train Loss: 4.979551002697195, Val Loss: 6.02204618225359, Val MAE: 1.528510332107544\n",
      "Epoch 903/2000, Train Loss: 4.979220856337339, Val Loss: 6.021636304790026, Val MAE: 1.5286959409713745\n",
      "Epoch 904/2000, Train Loss: 4.97883242803416, Val Loss: 6.021410882268866, Val MAE: 1.528740406036377\n",
      "Epoch 905/2000, Train Loss: 4.978462985887327, Val Loss: 6.021250219581878, Val MAE: 1.5287835597991943\n",
      "Epoch 906/2000, Train Loss: 4.978164747816237, Val Loss: 6.0210489214283145, Val MAE: 1.5287837982177734\n",
      "Epoch 907/2000, Train Loss: 4.977848538316823, Val Loss: 6.020772290964649, Val MAE: 1.5289443731307983\n",
      "Epoch 908/2000, Train Loss: 4.977485236317837, Val Loss: 6.020535705228374, Val MAE: 1.5289887189865112\n",
      "Epoch 909/2000, Train Loss: 4.977161443020961, Val Loss: 6.0204294258601045, Val MAE: 1.5290175676345825\n",
      "Epoch 910/2000, Train Loss: 4.976882575976404, Val Loss: 6.020166669806389, Val MAE: 1.5290837287902832\n",
      "Epoch 911/2000, Train Loss: 4.9765189907346015, Val Loss: 6.019923454278136, Val MAE: 1.5291271209716797\n",
      "Epoch 912/2000, Train Loss: 4.976188016671995, Val Loss: 6.019782643612117, Val MAE: 1.5291986465454102\n",
      "Epoch 913/2000, Train Loss: 4.975865733488259, Val Loss: 6.019484590177667, Val MAE: 1.5292433500289917\n",
      "Epoch 914/2000, Train Loss: 4.975543137307685, Val Loss: 6.019225222403056, Val MAE: 1.5293608903884888\n",
      "Epoch 915/2000, Train Loss: 4.975219405837345, Val Loss: 6.01903793154514, Val MAE: 1.5293869972229004\n",
      "Epoch 916/2000, Train Loss: 4.974904159286227, Val Loss: 6.018889968117622, Val MAE: 1.5294150114059448\n",
      "Epoch 917/2000, Train Loss: 4.9745466463762895, Val Loss: 6.018660634756088, Val MAE: 1.529450535774231\n",
      "Epoch 918/2000, Train Loss: 4.974192412977667, Val Loss: 6.018376937876009, Val MAE: 1.529541015625\n",
      "Epoch 919/2000, Train Loss: 4.973919496737191, Val Loss: 6.018099917124395, Val MAE: 1.5296591520309448\n",
      "Epoch 920/2000, Train Loss: 4.973563905666481, Val Loss: 6.0178989614934135, Val MAE: 1.5296814441680908\n",
      "Epoch 921/2000, Train Loss: 4.973240339582125, Val Loss: 6.017662719300349, Val MAE: 1.5297417640686035\n",
      "Epoch 922/2000, Train Loss: 4.972912439071186, Val Loss: 6.01739350451182, Val MAE: 1.5298430919647217\n",
      "Epoch 923/2000, Train Loss: 4.972550777793897, Val Loss: 6.017126863337543, Val MAE: 1.5299055576324463\n",
      "Epoch 924/2000, Train Loss: 4.972262938946147, Val Loss: 6.017014977050154, Val MAE: 1.5299687385559082\n",
      "Epoch 925/2000, Train Loss: 4.971955416075028, Val Loss: 6.016781382364769, Val MAE: 1.5299726724624634\n",
      "Epoch 926/2000, Train Loss: 4.97156781161044, Val Loss: 6.016486325696723, Val MAE: 1.5300689935684204\n",
      "Epoch 927/2000, Train Loss: 4.971249391812366, Val Loss: 6.016247923855913, Val MAE: 1.5301474332809448\n",
      "Epoch 928/2000, Train Loss: 4.970906159479769, Val Loss: 6.016076256355194, Val MAE: 1.5301767587661743\n",
      "Epoch 929/2000, Train Loss: 4.97056941352362, Val Loss: 6.0158065741192805, Val MAE: 1.5302797555923462\n",
      "Epoch 930/2000, Train Loss: 4.9702827431974, Val Loss: 6.015657518007984, Val MAE: 1.5302740335464478\n",
      "Epoch 931/2000, Train Loss: 4.969929086138015, Val Loss: 6.015289108638894, Val MAE: 1.5303936004638672\n",
      "Epoch 932/2000, Train Loss: 4.969563465643934, Val Loss: 6.015112534165382, Val MAE: 1.5304781198501587\n",
      "Epoch 933/2000, Train Loss: 4.969250287963273, Val Loss: 6.014969004751885, Val MAE: 1.5304723978042603\n",
      "Epoch 934/2000, Train Loss: 4.9689306984844235, Val Loss: 6.014771598863275, Val MAE: 1.530548334121704\n",
      "Epoch 935/2000, Train Loss: 4.968620477271428, Val Loss: 6.014533500557077, Val MAE: 1.5306587219238281\n",
      "Epoch 936/2000, Train Loss: 4.968277137128808, Val Loss: 6.014373293069944, Val MAE: 1.5306580066680908\n",
      "Epoch 937/2000, Train Loss: 4.967975614909415, Val Loss: 6.014103559394405, Val MAE: 1.5307612419128418\n",
      "Epoch 938/2000, Train Loss: 4.967612127430821, Val Loss: 6.013795364392947, Val MAE: 1.5308294296264648\n",
      "Epoch 939/2000, Train Loss: 4.967311428663603, Val Loss: 6.013509365385526, Val MAE: 1.5309728384017944\n",
      "Epoch 940/2000, Train Loss: 4.967038111709892, Val Loss: 6.01336632852685, Val MAE: 1.5309429168701172\n",
      "Epoch 941/2000, Train Loss: 4.9666026137056765, Val Loss: 6.013158900484647, Val MAE: 1.5310330390930176\n",
      "Epoch 942/2000, Train Loss: 4.966343616936929, Val Loss: 6.0129408097430455, Val MAE: 1.531072735786438\n",
      "Epoch 943/2000, Train Loss: 4.966059941333542, Val Loss: 6.012679397855719, Val MAE: 1.5312275886535645\n",
      "Epoch 944/2000, Train Loss: 4.965772034863019, Val Loss: 6.012510791625062, Val MAE: 1.5311579704284668\n",
      "Epoch 945/2000, Train Loss: 4.9653405424454995, Val Loss: 6.012179285696108, Val MAE: 1.531293511390686\n",
      "Epoch 946/2000, Train Loss: 4.965007528493625, Val Loss: 6.011994362285693, Val MAE: 1.531324863433838\n",
      "Epoch 947/2000, Train Loss: 4.964683663709817, Val Loss: 6.011779354859705, Val MAE: 1.531424641609192\n",
      "Epoch 948/2000, Train Loss: 4.964404961664828, Val Loss: 6.011451469300544, Val MAE: 1.5314747095108032\n",
      "Epoch 949/2000, Train Loss: 4.964037562112175, Val Loss: 6.011341253780339, Val MAE: 1.5315581560134888\n",
      "Epoch 950/2000, Train Loss: 4.963746731632919, Val Loss: 6.011071555622636, Val MAE: 1.531580924987793\n",
      "Epoch 951/2000, Train Loss: 4.963408502226324, Val Loss: 6.010774120280187, Val MAE: 1.5317167043685913\n",
      "Epoch 952/2000, Train Loss: 4.963129362758593, Val Loss: 6.0107780509207345, Val MAE: 1.531658411026001\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 953/2000, Train Loss: 4.962765835865583, Val Loss: 6.0103702561496055, Val MAE: 1.531796932220459\n",
      "Epoch 954/2000, Train Loss: 4.962405522406778, Val Loss: 6.010181783200943, Val MAE: 1.5318522453308105\n",
      "Epoch 955/2000, Train Loss: 4.9621282578287484, Val Loss: 6.0100794632549155, Val MAE: 1.5318716764450073\n",
      "Epoch 956/2000, Train Loss: 4.961791894037875, Val Loss: 6.009739297710053, Val MAE: 1.5320382118225098\n",
      "Epoch 957/2000, Train Loss: 4.961551437115167, Val Loss: 6.009640756943455, Val MAE: 1.5319842100143433\n",
      "Epoch 958/2000, Train Loss: 4.96113124117859, Val Loss: 6.00952420393898, Val MAE: 1.5320370197296143\n",
      "Epoch 959/2000, Train Loss: 4.960816297577498, Val Loss: 6.0091704148544025, Val MAE: 1.5321111679077148\n",
      "Epoch 960/2000, Train Loss: 4.960470697288575, Val Loss: 6.008980493022971, Val MAE: 1.5321719646453857\n",
      "Epoch 961/2000, Train Loss: 4.960175885953238, Val Loss: 6.008749062884344, Val MAE: 1.5322821140289307\n",
      "Epoch 962/2000, Train Loss: 4.95987935062359, Val Loss: 6.008530955608577, Val MAE: 1.532312035560608\n",
      "Epoch 963/2000, Train Loss: 4.959528436537114, Val Loss: 6.008324100750766, Val MAE: 1.5324110984802246\n",
      "Epoch 964/2000, Train Loss: 4.959245374175679, Val Loss: 6.008120537008325, Val MAE: 1.5324019193649292\n",
      "Epoch 965/2000, Train Loss: 4.958898981734265, Val Loss: 6.007899253539843, Val MAE: 1.532538652420044\n",
      "Epoch 966/2000, Train Loss: 4.958530630052959, Val Loss: 6.007660551634554, Val MAE: 1.5325942039489746\n",
      "Epoch 967/2000, Train Loss: 4.9582554880571905, Val Loss: 6.007516676228341, Val MAE: 1.5325579643249512\n",
      "Epoch 968/2000, Train Loss: 4.957942925936979, Val Loss: 6.007165563433138, Val MAE: 1.5327374935150146\n",
      "Epoch 969/2000, Train Loss: 4.957581039379249, Val Loss: 6.006979764732596, Val MAE: 1.5327476263046265\n",
      "Epoch 970/2000, Train Loss: 4.957285732658795, Val Loss: 6.006842747738917, Val MAE: 1.5328259468078613\n",
      "Epoch 971/2000, Train Loss: 4.956944984207277, Val Loss: 6.006505506291782, Val MAE: 1.532901406288147\n",
      "Epoch 972/2000, Train Loss: 4.956606676165057, Val Loss: 6.006334316648849, Val MAE: 1.5329265594482422\n",
      "Epoch 973/2000, Train Loss: 4.956344989362374, Val Loss: 6.006102609307798, Val MAE: 1.5329855680465698\n",
      "Epoch 974/2000, Train Loss: 4.955979431081243, Val Loss: 6.005921490184249, Val MAE: 1.5330846309661865\n",
      "Epoch 975/2000, Train Loss: 4.955655645899101, Val Loss: 6.005758937907546, Val MAE: 1.5331212282180786\n",
      "Epoch 976/2000, Train Loss: 4.955346279051548, Val Loss: 6.0054951059083415, Val MAE: 1.5331883430480957\n",
      "Epoch 977/2000, Train Loss: 4.955076402358143, Val Loss: 6.005382002420621, Val MAE: 1.533201813697815\n",
      "Epoch 978/2000, Train Loss: 4.954738620034687, Val Loss: 6.005210532105132, Val MAE: 1.5332671403884888\n",
      "Epoch 979/2000, Train Loss: 4.954383986116035, Val Loss: 6.004996612259786, Val MAE: 1.5333465337753296\n",
      "Epoch 980/2000, Train Loss: 4.954118929573937, Val Loss: 6.00479204809829, Val MAE: 1.5333362817764282\n",
      "Epoch 981/2000, Train Loss: 4.953750627740276, Val Loss: 6.004540356462949, Val MAE: 1.533475399017334\n",
      "Epoch 982/2000, Train Loss: 4.95348425448617, Val Loss: 6.004389578757221, Val MAE: 1.5335344076156616\n",
      "Epoch 983/2000, Train Loss: 4.953209342987456, Val Loss: 6.004046201501807, Val MAE: 1.5335801839828491\n",
      "Epoch 984/2000, Train Loss: 4.95289435417571, Val Loss: 6.003912889590002, Val MAE: 1.5336565971374512\n",
      "Epoch 985/2000, Train Loss: 4.952543744295111, Val Loss: 6.003810825413221, Val MAE: 1.5336861610412598\n",
      "Epoch 986/2000, Train Loss: 4.952192406213844, Val Loss: 6.003543066447729, Val MAE: 1.5337624549865723\n",
      "Epoch 987/2000, Train Loss: 4.951912699487646, Val Loss: 6.003361846894434, Val MAE: 1.533825397491455\n",
      "Epoch 988/2000, Train Loss: 4.951551332659235, Val Loss: 6.003204990535567, Val MAE: 1.5338279008865356\n",
      "Epoch 989/2000, Train Loss: 4.951305634778361, Val Loss: 6.002878457511941, Val MAE: 1.533972144126892\n",
      "Epoch 990/2000, Train Loss: 4.950969065414256, Val Loss: 6.002844976439868, Val MAE: 1.533945083618164\n",
      "Epoch 991/2000, Train Loss: 4.95059596468216, Val Loss: 6.0024912490011895, Val MAE: 1.5340484380722046\n",
      "Epoch 992/2000, Train Loss: 4.9502780908896815, Val Loss: 6.0023212702306985, Val MAE: 1.5340791940689087\n",
      "Epoch 993/2000, Train Loss: 4.949990859874064, Val Loss: 6.002174545232564, Val MAE: 1.5341241359710693\n",
      "Epoch 994/2000, Train Loss: 4.949649844022779, Val Loss: 6.001810996908031, Val MAE: 1.5342317819595337\n",
      "Epoch 995/2000, Train Loss: 4.949355911397084, Val Loss: 6.00166835727757, Val MAE: 1.5342718362808228\n",
      "Epoch 996/2000, Train Loss: 4.949035563770337, Val Loss: 6.001446463473855, Val MAE: 1.5343395471572876\n",
      "Epoch 997/2000, Train Loss: 4.948710594610032, Val Loss: 6.001254531414541, Val MAE: 1.5343877077102661\n",
      "Epoch 998/2000, Train Loss: 4.948451490417678, Val Loss: 6.000961442924526, Val MAE: 1.5345064401626587\n",
      "Epoch 999/2000, Train Loss: 4.948128188255347, Val Loss: 6.000945933877605, Val MAE: 1.5344734191894531\n",
      "Epoch 1000/2000, Train Loss: 4.94776318641304, Val Loss: 6.000682540544092, Val MAE: 1.5345633029937744\n",
      "Epoch 1001/2000, Train Loss: 4.947454306063041, Val Loss: 6.000497389737874, Val MAE: 1.5346379280090332\n",
      "Epoch 1002/2000, Train Loss: 4.9471500520768, Val Loss: 6.000164310409598, Val MAE: 1.5347286462783813\n",
      "Epoch 1003/2000, Train Loss: 4.94683114790646, Val Loss: 6.000192609960085, Val MAE: 1.5346676111221313\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1004/2000, Train Loss: 4.946556021753741, Val Loss: 5.999796186816202, Val MAE: 1.5348303318023682\n",
      "Epoch 1005/2000, Train Loss: 4.946186740355901, Val Loss: 5.9997520928513515, Val MAE: 1.5348706245422363\n",
      "Epoch 1006/2000, Train Loss: 4.945857429813527, Val Loss: 5.999549019826602, Val MAE: 1.534877061843872\n",
      "Epoch 1007/2000, Train Loss: 4.94559060425967, Val Loss: 5.999339894072651, Val MAE: 1.5349273681640625\n",
      "Epoch 1008/2000, Train Loss: 4.945274418718038, Val Loss: 5.999177345470207, Val MAE: 1.5349645614624023\n",
      "Epoch 1009/2000, Train Loss: 4.944971783814206, Val Loss: 5.9989084476885735, Val MAE: 1.5350573062896729\n",
      "Epoch 1010/2000, Train Loss: 4.944662438799148, Val Loss: 5.998673293076149, Val MAE: 1.535132884979248\n",
      "Epoch 1011/2000, Train Loss: 4.944345357545577, Val Loss: 5.998569571196216, Val MAE: 1.5351594686508179\n",
      "Epoch 1012/2000, Train Loss: 4.944004297642886, Val Loss: 5.998331274071785, Val MAE: 1.5352147817611694\n",
      "Epoch 1013/2000, Train Loss: 4.9437107514330405, Val Loss: 5.9981124907323755, Val MAE: 1.5352736711502075\n",
      "Epoch 1014/2000, Train Loss: 4.943407553328107, Val Loss: 5.998000384601828, Val MAE: 1.5353403091430664\n",
      "Epoch 1015/2000, Train Loss: 4.943132468135469, Val Loss: 5.997670813782574, Val MAE: 1.5354442596435547\n",
      "Epoch 1016/2000, Train Loss: 4.942773826698043, Val Loss: 5.9974839975164365, Val MAE: 1.5354846715927124\n",
      "Epoch 1017/2000, Train Loss: 4.942533155700955, Val Loss: 5.997265256839256, Val MAE: 1.5355629920959473\n",
      "Epoch 1018/2000, Train Loss: 4.942191886747289, Val Loss: 5.99713505294225, Val MAE: 1.5355695486068726\n",
      "Epoch 1019/2000, Train Loss: 4.941872405464877, Val Loss: 5.996999040041884, Val MAE: 1.5355602502822876\n",
      "Epoch 1020/2000, Train Loss: 4.941580074532879, Val Loss: 5.996771166994147, Val MAE: 1.5356099605560303\n",
      "Epoch 1021/2000, Train Loss: 4.941241816917927, Val Loss: 5.9965803317011215, Val MAE: 1.535723328590393\n",
      "Epoch 1022/2000, Train Loss: 4.940950673441833, Val Loss: 5.99637615027493, Val MAE: 1.53579843044281\n",
      "Epoch 1023/2000, Train Loss: 4.9406853943242055, Val Loss: 5.996054958603153, Val MAE: 1.5359021425247192\n",
      "Epoch 1024/2000, Train Loss: 4.94033128399903, Val Loss: 5.995898180628476, Val MAE: 1.5359361171722412\n",
      "Epoch 1025/2000, Train Loss: 4.940052993486921, Val Loss: 5.995713656691656, Val MAE: 1.53593909740448\n",
      "Epoch 1026/2000, Train Loss: 4.939718692383851, Val Loss: 5.995562578309072, Val MAE: 1.5359938144683838\n",
      "Epoch 1027/2000, Train Loss: 4.9394254591708435, Val Loss: 5.995318953713325, Val MAE: 1.5360530614852905\n",
      "Epoch 1028/2000, Train Loss: 4.939151554671833, Val Loss: 5.99507061953414, Val MAE: 1.5361493825912476\n",
      "Epoch 1029/2000, Train Loss: 4.938811379468229, Val Loss: 5.994946417947338, Val MAE: 1.5361824035644531\n",
      "Epoch 1030/2000, Train Loss: 4.938502729422076, Val Loss: 5.994739935618558, Val MAE: 1.5362612009048462\n",
      "Epoch 1031/2000, Train Loss: 4.938191051807744, Val Loss: 5.99457854052929, Val MAE: 1.5362722873687744\n",
      "Epoch 1032/2000, Train Loss: 4.937877966480472, Val Loss: 5.994385951390005, Val MAE: 1.5363050699234009\n",
      "Epoch 1033/2000, Train Loss: 4.937627318612567, Val Loss: 5.994164642201711, Val MAE: 1.5364136695861816\n",
      "Epoch 1034/2000, Train Loss: 4.937234069385251, Val Loss: 5.9940102298374045, Val MAE: 1.5364371538162231\n",
      "Epoch 1035/2000, Train Loss: 4.937006645882729, Val Loss: 5.9939375909632195, Val MAE: 1.5364022254943848\n",
      "Epoch 1036/2000, Train Loss: 4.936669276251399, Val Loss: 5.9937395404462945, Val MAE: 1.5364995002746582\n",
      "Epoch 1037/2000, Train Loss: 4.936362600790237, Val Loss: 5.993524726939528, Val MAE: 1.536596417427063\n",
      "Epoch 1038/2000, Train Loss: 4.936042304943214, Val Loss: 5.993266839891264, Val MAE: 1.5366647243499756\n",
      "Epoch 1039/2000, Train Loss: 4.935736131436241, Val Loss: 5.993095891932919, Val MAE: 1.5367165803909302\n",
      "Epoch 1040/2000, Train Loss: 4.935452009908964, Val Loss: 5.992844749191036, Val MAE: 1.5367776155471802\n",
      "Epoch 1041/2000, Train Loss: 4.935159811517601, Val Loss: 5.992809172362497, Val MAE: 1.5367906093597412\n",
      "Epoch 1042/2000, Train Loss: 4.934824679620455, Val Loss: 5.992583473659542, Val MAE: 1.5368610620498657\n",
      "Epoch 1043/2000, Train Loss: 4.934574781012883, Val Loss: 5.992304654766436, Val MAE: 1.5369377136230469\n",
      "Epoch 1044/2000, Train Loss: 4.9342688919079745, Val Loss: 5.992087684673805, Val MAE: 1.536992073059082\n",
      "Epoch 1045/2000, Train Loss: 4.933926396856818, Val Loss: 5.992009614632554, Val MAE: 1.5370303392410278\n",
      "Epoch 1046/2000, Train Loss: 4.933615367648281, Val Loss: 5.991741948748288, Val MAE: 1.5370774269104004\n",
      "Epoch 1047/2000, Train Loss: 4.93333428334958, Val Loss: 5.991514335348182, Val MAE: 1.5371973514556885\n",
      "Epoch 1048/2000, Train Loss: 4.932988035041277, Val Loss: 5.991395288950776, Val MAE: 1.5372211933135986\n",
      "Epoch 1049/2000, Train Loss: 4.932712457554847, Val Loss: 5.9911241051677155, Val MAE: 1.5372742414474487\n",
      "Epoch 1050/2000, Train Loss: 4.932443285683952, Val Loss: 5.990994478333486, Val MAE: 1.5372892618179321\n",
      "Epoch 1051/2000, Train Loss: 4.932130441093754, Val Loss: 5.990809615956594, Val MAE: 1.537352204322815\n",
      "Epoch 1052/2000, Train Loss: 4.93178897545442, Val Loss: 5.990683242882768, Val MAE: 1.5373867750167847\n",
      "Epoch 1053/2000, Train Loss: 4.931539217502217, Val Loss: 5.990412843553988, Val MAE: 1.5374845266342163\n",
      "Epoch 1054/2000, Train Loss: 4.931226309153404, Val Loss: 5.990292723048223, Val MAE: 1.5374958515167236\n",
      "Epoch 1055/2000, Train Loss: 4.930930459132264, Val Loss: 5.990053587988632, Val MAE: 1.5376012325286865\n",
      "Epoch 1056/2000, Train Loss: 4.930636595288591, Val Loss: 5.989892910604608, Val MAE: 1.5376477241516113\n",
      "Epoch 1057/2000, Train Loss: 4.930341496459862, Val Loss: 5.989741429890672, Val MAE: 1.537635326385498\n",
      "Epoch 1058/2000, Train Loss: 4.930001042843638, Val Loss: 5.989538160905446, Val MAE: 1.5377223491668701\n",
      "Epoch 1059/2000, Train Loss: 4.929780334089331, Val Loss: 5.989434900553259, Val MAE: 1.5377039909362793\n",
      "Epoch 1060/2000, Train Loss: 4.929404237668363, Val Loss: 5.989136982862264, Val MAE: 1.537834882736206\n",
      "Epoch 1061/2000, Train Loss: 4.929124748687497, Val Loss: 5.9890260700493645, Val MAE: 1.5378708839416504\n",
      "Epoch 1062/2000, Train Loss: 4.928873416090707, Val Loss: 5.988823902321188, Val MAE: 1.5379165410995483\n",
      "Epoch 1063/2000, Train Loss: 4.928583259134277, Val Loss: 5.988546992613845, Val MAE: 1.5380315780639648\n",
      "Epoch 1064/2000, Train Loss: 4.928276667138939, Val Loss: 5.988517109866011, Val MAE: 1.537994623184204\n",
      "Epoch 1065/2000, Train Loss: 4.927946474795596, Val Loss: 5.9882288611915016, Val MAE: 1.5380727052688599\n",
      "Epoch 1066/2000, Train Loss: 4.927623179975167, Val Loss: 5.988045318894191, Val MAE: 1.5381033420562744\n",
      "Epoch 1067/2000, Train Loss: 4.927422156403479, Val Loss: 5.98766955835362, Val MAE: 1.5382951498031616\n",
      "Epoch 1068/2000, Train Loss: 4.927009606670522, Val Loss: 5.987577114815581, Val MAE: 1.538273811340332\n",
      "Epoch 1069/2000, Train Loss: 4.926718823140792, Val Loss: 5.987374837880266, Val MAE: 1.5383087396621704\n",
      "Epoch 1070/2000, Train Loss: 4.9264372828713885, Val Loss: 5.987244938538499, Val MAE: 1.5383455753326416\n",
      "Epoch 1071/2000, Train Loss: 4.9261244587612305, Val Loss: 5.986989685117382, Val MAE: 1.5384421348571777\n",
      "Epoch 1072/2000, Train Loss: 4.925837366855318, Val Loss: 5.986920743364177, Val MAE: 1.5384703874588013\n",
      "Epoch 1073/2000, Train Loss: 4.925606753876376, Val Loss: 5.986731784188584, Val MAE: 1.538492202758789\n",
      "Epoch 1074/2000, Train Loss: 4.925241119471322, Val Loss: 5.986442888844503, Val MAE: 1.53861403465271\n",
      "Epoch 1075/2000, Train Loss: 4.924969323063979, Val Loss: 5.986180364064975, Val MAE: 1.538692593574524\n",
      "Epoch 1076/2000, Train Loss: 4.924671231830912, Val Loss: 5.986095666477125, Val MAE: 1.5386747121810913\n",
      "Epoch 1077/2000, Train Loss: 4.924351208793479, Val Loss: 5.985859531858196, Val MAE: 1.5386978387832642\n",
      "Epoch 1078/2000, Train Loss: 4.92404338606366, Val Loss: 5.9857593960549735, Val MAE: 1.53878653049469\n",
      "Epoch 1079/2000, Train Loss: 4.923725642106908, Val Loss: 5.985503807051541, Val MAE: 1.538865327835083\n",
      "Epoch 1080/2000, Train Loss: 4.923623875628801, Val Loss: 5.985528475617709, Val MAE: 1.5388058423995972\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1081/2000, Train Loss: 4.9231400029972265, Val Loss: 5.985167187166541, Val MAE: 1.538965106010437\n",
      "Epoch 1082/2000, Train Loss: 4.922827425806797, Val Loss: 5.984946267245567, Val MAE: 1.53902268409729\n",
      "Epoch 1083/2000, Train Loss: 4.922629463421468, Val Loss: 5.984842758880903, Val MAE: 1.5389961004257202\n",
      "Epoch 1084/2000, Train Loss: 4.922266028881846, Val Loss: 5.984606788174747, Val MAE: 1.5391175746917725\n",
      "Epoch 1085/2000, Train Loss: 4.921965493954947, Val Loss: 5.984483745408385, Val MAE: 1.5391486883163452\n",
      "Epoch 1086/2000, Train Loss: 4.92164711357129, Val Loss: 5.984209497701632, Val MAE: 1.5392184257507324\n",
      "Epoch 1087/2000, Train Loss: 4.921373693444547, Val Loss: 5.984079563658532, Val MAE: 1.5392136573791504\n",
      "Epoch 1088/2000, Train Loss: 4.92106160197884, Val Loss: 5.983855070724879, Val MAE: 1.5393041372299194\n",
      "Epoch 1089/2000, Train Loss: 4.920756469476165, Val Loss: 5.983663034357437, Val MAE: 1.5393588542938232\n",
      "Epoch 1090/2000, Train Loss: 4.920457330669731, Val Loss: 5.983465503952274, Val MAE: 1.5394139289855957\n",
      "Epoch 1091/2000, Train Loss: 4.9201974787627085, Val Loss: 5.983229577745477, Val MAE: 1.5395225286483765\n",
      "Epoch 1092/2000, Train Loss: 4.919878537689654, Val Loss: 5.98319896463662, Val MAE: 1.539469599723816\n",
      "Epoch 1093/2000, Train Loss: 4.919588407782333, Val Loss: 5.982915328381813, Val MAE: 1.5395921468734741\n",
      "Epoch 1094/2000, Train Loss: 4.9192979559906105, Val Loss: 5.98276670175056, Val MAE: 1.5396173000335693\n",
      "Epoch 1095/2000, Train Loss: 4.919004037662302, Val Loss: 5.982518954023923, Val MAE: 1.5397047996520996\n",
      "Epoch 1096/2000, Train Loss: 4.918725349914699, Val Loss: 5.982400700654069, Val MAE: 1.5397275686264038\n",
      "Epoch 1097/2000, Train Loss: 4.91841064434577, Val Loss: 5.982151045578799, Val MAE: 1.5397980213165283\n",
      "Epoch 1098/2000, Train Loss: 4.918174734378363, Val Loss: 5.982000119065585, Val MAE: 1.5398811101913452\n",
      "Epoch 1099/2000, Train Loss: 4.917820557203246, Val Loss: 5.981755049261328, Val MAE: 1.539879560470581\n",
      "Epoch 1100/2000, Train Loss: 4.917533457955433, Val Loss: 5.981633978755506, Val MAE: 1.5399377346038818\n",
      "Epoch 1101/2000, Train Loss: 4.917268153334359, Val Loss: 5.981498148343334, Val MAE: 1.5399690866470337\n",
      "Epoch 1102/2000, Train Loss: 4.917081286493731, Val Loss: 5.981338556498697, Val MAE: 1.5399601459503174\n",
      "Epoch 1103/2000, Train Loss: 4.916713502457308, Val Loss: 5.980949943400409, Val MAE: 1.5401496887207031\n",
      "Epoch 1104/2000, Train Loss: 4.9163601325049004, Val Loss: 5.980798128328911, Val MAE: 1.5401809215545654\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[0;32m      7\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mfull_cnn_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mseason\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2020-21\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2021-22\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_dense\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdrop_low_playtime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlow_playtime_cutoff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_FEATURES_DICT\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlarge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSTANDARD_CAT_FEATURES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstratify_by\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconv_activation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdense_activation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.000001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmae\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43mregularization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# only used if early stopping is turned on, threshold to define low val loss decrease\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# num of iterations before early stopping bc of low val loss decrease\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m                \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdraw_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstandardize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dean\\Code\\csci-567\\ml-premier-predictor\\final_project\\cnn2d\\..\\..\\final_project\\cnn2d\\model.py:380\u001b[0m, in \u001b[0;36mfull_cnn_pipeline\u001b[1;34m(data_dir, season, position, window_size, kernel_size, num_filters, num_dense, batch_size, epochs, drop_low_playtime, low_playtime_cutoff, num_features, cat_features, conv_activation, dense_activation, optimizer, learning_rate, loss, metrics, verbose, regularization, early_stopping, tolerance, patience, plot, draw_model, standardize, test_size, val_size, stratify_by)\u001b[0m\n\u001b[0;32m    364\u001b[0m (X_train, d_train, y_train, \n\u001b[0;32m    365\u001b[0m  X_val, d_val, y_val, \n\u001b[0;32m    366\u001b[0m  X_test, d_test, y_test, pipeline) \u001b[38;5;241m=\u001b[39m generate_datasets(data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m                             low_playtime_cutoff\u001b[38;5;241m=\u001b[39mlow_playtime_cutoff,\n\u001b[0;32m    377\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m#call build_train_cnn passing on all params \u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m model, expt_res \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_train_cnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseason\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_dense\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_dense\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_low_playtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_low_playtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_playtime_cutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_playtime_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraw_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstandardize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstandardize\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, expt_res\n",
      "File \u001b[1;32mc:\\Users\\Dean\\Code\\csci-567\\ml-premier-predictor\\final_project\\cnn2d\\..\\..\\final_project\\cnn2d\\model.py:272\u001b[0m, in \u001b[0;36mbuild_train_cnn\u001b[1;34m(X_train, d_train, y_train, X_val, d_val, y_val, X_test, d_test, y_test, season, position, window_size, kernel_size, num_filters, num_dense, batch_size, epochs, drop_low_playtime, low_playtime_cutoff, num_features, cat_features, conv_activation, dense_activation, optimizer, learning_rate, loss, metrics, verbose, regularization, early_stopping, tolerance, patience, plot, draw_model, standardize)\u001b[0m\n\u001b[0;32m    270\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y_batch)\n\u001b[0;32m    271\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 272\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    275\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[1;32mc:\\Users\\Dean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Dean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    217\u001b[0m         group,\n\u001b[0;32m    218\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m         state_steps,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Dean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:529\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    527\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 529\u001b[0m         device_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[0;32m    530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    534\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEED = 266\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_cnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=6,\n",
    "                kernel_size=3,\n",
    "                num_filters=6,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = NUM_FEATURES_DICT['GK']['large'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpremier.cnn.experiment import gridsearch_cnn\n",
    "\n",
    "#gridsearch_cnn(epochs=100, verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized) — {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
