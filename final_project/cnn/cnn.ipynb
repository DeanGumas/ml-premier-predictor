{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.cnn.model import build_train_cnn, full_cnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2502, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2502, 7)\n",
      "Shape of a given window (prior to preprocessing): (6, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[ 1.94211124e+00  1.28342792e+00  1.45289444e-01  7.49148695e-01\n",
      "  9.96594779e+00 -1.87854711e-01  0.00000000e+00  1.70261067e-03\n",
      "  2.66742338e-02  1.13507378e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.73976222  1.60627602  0.35239243  1.15661345 10.75598603  1.34612153\n",
      "  1.          0.04122756  0.16112951  0.03367173]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building CNN Architecture ======\n",
      "====== Done Building CNN Architecture ======\n",
      "Epoch 1/2000, Train Loss: 11.638805663407739, Val Loss: 8.701298748613445, Val MAE: 1.5895955562591553\n",
      "Epoch 2/2000, Train Loss: 11.60750271145411, Val Loss: 8.680992052124036, Val MAE: 1.5867793560028076\n",
      "Epoch 3/2000, Train Loss: 11.576110055560203, Val Loss: 8.660818235972515, Val MAE: 1.5839935541152954\n",
      "Epoch 4/2000, Train Loss: 11.545021015721066, Val Loss: 8.640143239468255, Val MAE: 1.581238031387329\n",
      "Epoch 5/2000, Train Loss: 11.512718361138974, Val Loss: 8.61987599289525, Val MAE: 1.5785233974456787\n",
      "Epoch 6/2000, Train Loss: 11.48075750644042, Val Loss: 8.599637490180944, Val MAE: 1.5759140253067017\n",
      "Epoch 7/2000, Train Loss: 11.450213841204324, Val Loss: 8.580316623457978, Val MAE: 1.573364496231079\n",
      "Epoch 8/2000, Train Loss: 11.419967071657426, Val Loss: 8.561165130014283, Val MAE: 1.5709013938903809\n",
      "Epoch 9/2000, Train Loss: 11.389490623810559, Val Loss: 8.54099667442662, Val MAE: 1.5683120489120483\n",
      "Epoch 10/2000, Train Loss: 11.358294712316392, Val Loss: 8.521778823645954, Val MAE: 1.5658715963363647\n",
      "Epoch 11/2000, Train Loss: 11.328437323809641, Val Loss: 8.502136534468939, Val MAE: 1.5635358095169067\n",
      "Epoch 12/2000, Train Loss: 11.297697216070167, Val Loss: 8.48261542203396, Val MAE: 1.5611099004745483\n",
      "Epoch 13/2000, Train Loss: 11.267393888579619, Val Loss: 8.463374592833162, Val MAE: 1.5588523149490356\n",
      "Epoch 14/2000, Train Loss: 11.237270557087829, Val Loss: 8.444256987849133, Val MAE: 1.5564966201782227\n",
      "Epoch 15/2000, Train Loss: 11.206900206964407, Val Loss: 8.424721515720973, Val MAE: 1.554192304611206\n",
      "Epoch 16/2000, Train Loss: 11.177191121808255, Val Loss: 8.405898181507254, Val MAE: 1.551902174949646\n",
      "Epoch 17/2000, Train Loss: 11.147774887446875, Val Loss: 8.386991128863906, Val MAE: 1.5497195720672607\n",
      "Epoch 18/2000, Train Loss: 11.118439655875145, Val Loss: 8.368542578462584, Val MAE: 1.5475374460220337\n",
      "Epoch 19/2000, Train Loss: 11.089027501501173, Val Loss: 8.349776151933062, Val MAE: 1.5453054904937744\n",
      "Epoch 20/2000, Train Loss: 11.059428094536207, Val Loss: 8.331470516911253, Val MAE: 1.543157696723938\n",
      "Epoch 21/2000, Train Loss: 11.030000269979118, Val Loss: 8.31220110140807, Val MAE: 1.54093599319458\n",
      "Epoch 22/2000, Train Loss: 10.999322753424885, Val Loss: 8.293255401402162, Val MAE: 1.538780927658081\n",
      "Epoch 23/2000, Train Loss: 10.968991051535431, Val Loss: 8.273628695763456, Val MAE: 1.5365245342254639\n",
      "Epoch 24/2000, Train Loss: 10.938845113142213, Val Loss: 8.254631514870011, Val MAE: 1.5342618227005005\n",
      "Epoch 25/2000, Train Loss: 10.909187738736902, Val Loss: 8.235612154719016, Val MAE: 1.5321965217590332\n",
      "Epoch 26/2000, Train Loss: 10.879040422400756, Val Loss: 8.215852810583979, Val MAE: 1.5298917293548584\n",
      "Epoch 27/2000, Train Loss: 10.848911041120925, Val Loss: 8.197259071087794, Val MAE: 1.5278308391571045\n",
      "Epoch 28/2000, Train Loss: 10.819556077105705, Val Loss: 8.178546848545569, Val MAE: 1.5257188081741333\n",
      "Epoch 29/2000, Train Loss: 10.78990935820333, Val Loss: 8.15947340125361, Val MAE: 1.5239719152450562\n",
      "Epoch 30/2000, Train Loss: 10.760473875048364, Val Loss: 8.140134510363078, Val MAE: 1.521750569343567\n",
      "Epoch 31/2000, Train Loss: 10.729273812903479, Val Loss: 8.120793869773175, Val MAE: 1.5195038318634033\n",
      "Epoch 32/2000, Train Loss: 10.698074620323233, Val Loss: 8.101455965331814, Val MAE: 1.5172706842422485\n",
      "Epoch 33/2000, Train Loss: 10.667786707123552, Val Loss: 8.081976555812899, Val MAE: 1.5150538682937622\n",
      "Epoch 34/2000, Train Loss: 10.638738336972892, Val Loss: 8.063599637218047, Val MAE: 1.5130221843719482\n",
      "Epoch 35/2000, Train Loss: 10.609926846166479, Val Loss: 8.045258855984518, Val MAE: 1.5109937191009521\n",
      "Epoch 36/2000, Train Loss: 10.580590112238434, Val Loss: 8.026823590591725, Val MAE: 1.5089902877807617\n",
      "Epoch 37/2000, Train Loss: 10.551609014720736, Val Loss: 8.007428083809708, Val MAE: 1.506828784942627\n",
      "Epoch 38/2000, Train Loss: 10.522054781098541, Val Loss: 7.989018080212857, Val MAE: 1.504831314086914\n",
      "Epoch 39/2000, Train Loss: 10.492907760909972, Val Loss: 7.9705200114835595, Val MAE: 1.502739429473877\n",
      "Epoch 40/2000, Train Loss: 10.462744246081549, Val Loss: 7.951356010515551, Val MAE: 1.50052011013031\n",
      "Epoch 41/2000, Train Loss: 10.433535078972618, Val Loss: 7.93290835744353, Val MAE: 1.4984533786773682\n",
      "Epoch 42/2000, Train Loss: 10.404366456524732, Val Loss: 7.9137121920983535, Val MAE: 1.4963053464889526\n",
      "Epoch 43/2000, Train Loss: 10.37517293078605, Val Loss: 7.895231932236659, Val MAE: 1.494316577911377\n",
      "Epoch 44/2000, Train Loss: 10.345950950922765, Val Loss: 7.876473279365918, Val MAE: 1.4921560287475586\n",
      "Epoch 45/2000, Train Loss: 10.3154029432113, Val Loss: 7.856771887412779, Val MAE: 1.4900524616241455\n",
      "Epoch 46/2000, Train Loss: 10.285328202262987, Val Loss: 7.838054789448758, Val MAE: 1.4878849983215332\n",
      "Epoch 47/2000, Train Loss: 10.256379440143766, Val Loss: 7.819757144227551, Val MAE: 1.4858802556991577\n",
      "Epoch 48/2000, Train Loss: 10.227873570236454, Val Loss: 7.801444728845936, Val MAE: 1.4837852716445923\n",
      "Epoch 49/2000, Train Loss: 10.198756830986675, Val Loss: 7.783243748489914, Val MAE: 1.4817898273468018\n",
      "Epoch 50/2000, Train Loss: 10.169910396559207, Val Loss: 7.764510608731043, Val MAE: 1.479623556137085\n",
      "Epoch 51/2000, Train Loss: 10.141017771478909, Val Loss: 7.746451104448111, Val MAE: 1.4775513410568237\n",
      "Epoch 52/2000, Train Loss: 10.11151232583552, Val Loss: 7.727793683956825, Val MAE: 1.47533118724823\n",
      "Epoch 53/2000, Train Loss: 10.082042807963356, Val Loss: 7.708916320968199, Val MAE: 1.473175048828125\n",
      "Epoch 54/2000, Train Loss: 10.052840910113973, Val Loss: 7.690305101870024, Val MAE: 1.4710639715194702\n",
      "Epoch 55/2000, Train Loss: 10.024278922837981, Val Loss: 7.671551341828927, Val MAE: 1.4689420461654663\n",
      "Epoch 56/2000, Train Loss: 9.99576785962326, Val Loss: 7.652462007004519, Val MAE: 1.4671134948730469\n",
      "Epoch 57/2000, Train Loss: 9.966700391174172, Val Loss: 7.6343585897970385, Val MAE: 1.4651488065719604\n",
      "Epoch 58/2000, Train Loss: 9.938269066125194, Val Loss: 7.61551918250978, Val MAE: 1.46296226978302\n",
      "Epoch 59/2000, Train Loss: 9.909744319307594, Val Loss: 7.597432484305794, Val MAE: 1.4609079360961914\n",
      "Epoch 60/2000, Train Loss: 9.881330456468435, Val Loss: 7.578305197257079, Val MAE: 1.4587498903274536\n",
      "Epoch 61/2000, Train Loss: 9.85251655563045, Val Loss: 7.559653108983528, Val MAE: 1.456931471824646\n",
      "Epoch 62/2000, Train Loss: 9.824251611650071, Val Loss: 7.541478101081603, Val MAE: 1.4548596143722534\n",
      "Epoch 63/2000, Train Loss: 9.795664648996764, Val Loss: 7.523083912047712, Val MAE: 1.4526878595352173\n",
      "Epoch 64/2000, Train Loss: 9.767294570904737, Val Loss: 7.504403008311606, Val MAE: 1.4504605531692505\n",
      "Epoch 65/2000, Train Loss: 9.738523574826514, Val Loss: 7.486137310990433, Val MAE: 1.4483195543289185\n",
      "Epoch 66/2000, Train Loss: 9.710098183614024, Val Loss: 7.466822719889671, Val MAE: 1.4460371732711792\n",
      "Epoch 67/2000, Train Loss: 9.681542001472561, Val Loss: 7.448461183388607, Val MAE: 1.443844199180603\n",
      "Epoch 68/2000, Train Loss: 9.652969331398424, Val Loss: 7.430346453515446, Val MAE: 1.4416617155075073\n",
      "Epoch 69/2000, Train Loss: 9.624644953651377, Val Loss: 7.4112737565649685, Val MAE: 1.4393523931503296\n",
      "Epoch 70/2000, Train Loss: 9.59525329408361, Val Loss: 7.39137449618979, Val MAE: 1.4369746446609497\n",
      "Epoch 71/2000, Train Loss: 9.5663429231786, Val Loss: 7.373369410002648, Val MAE: 1.4348504543304443\n",
      "Epoch 72/2000, Train Loss: 9.537355699888563, Val Loss: 7.354578318675305, Val MAE: 1.4325456619262695\n",
      "Epoch 73/2000, Train Loss: 9.508550656618224, Val Loss: 7.335362627319111, Val MAE: 1.4302774667739868\n",
      "Epoch 74/2000, Train Loss: 9.480410416543565, Val Loss: 7.3172107878650765, Val MAE: 1.4279841184616089\n",
      "Epoch 75/2000, Train Loss: 9.452421454593964, Val Loss: 7.299818020258246, Val MAE: 1.425958275794983\n",
      "Epoch 76/2000, Train Loss: 9.42480488159925, Val Loss: 7.2810493526393625, Val MAE: 1.4235538244247437\n",
      "Epoch 77/2000, Train Loss: 9.396018191302648, Val Loss: 7.262044349452114, Val MAE: 1.4212065935134888\n",
      "Epoch 78/2000, Train Loss: 9.3676209986776, Val Loss: 7.2436742440628725, Val MAE: 1.4188750982284546\n",
      "Epoch 79/2000, Train Loss: 9.338601255999167, Val Loss: 7.225002948193276, Val MAE: 1.4165475368499756\n",
      "Epoch 80/2000, Train Loss: 9.31035010753202, Val Loss: 7.2062814831528375, Val MAE: 1.414188027381897\n",
      "Epoch 81/2000, Train Loss: 9.281679104270573, Val Loss: 7.188519654489731, Val MAE: 1.4119747877120972\n",
      "Epoch 82/2000, Train Loss: 9.254008647038006, Val Loss: 7.169910036952888, Val MAE: 1.4096689224243164\n",
      "Epoch 83/2000, Train Loss: 9.226702876136294, Val Loss: 7.152090215956513, Val MAE: 1.4073905944824219\n",
      "Epoch 84/2000, Train Loss: 9.199102308889257, Val Loss: 7.134071370181958, Val MAE: 1.405147671699524\n",
      "Epoch 85/2000, Train Loss: 9.171685170447972, Val Loss: 7.11580457905762, Val MAE: 1.4027988910675049\n",
      "Epoch 86/2000, Train Loss: 9.144083339728546, Val Loss: 7.097902057208414, Val MAE: 1.4005942344665527\n",
      "Epoch 87/2000, Train Loss: 9.116911775544994, Val Loss: 7.080096811425733, Val MAE: 1.3984161615371704\n",
      "Epoch 88/2000, Train Loss: 9.089484447701816, Val Loss: 7.061744525533639, Val MAE: 1.3961189985275269\n",
      "Epoch 89/2000, Train Loss: 9.06153630239831, Val Loss: 7.042811335490533, Val MAE: 1.3937525749206543\n",
      "Epoch 90/2000, Train Loss: 9.034114115597273, Val Loss: 7.0246043087613925, Val MAE: 1.3914285898208618\n",
      "Epoch 91/2000, Train Loss: 9.006595816023475, Val Loss: 7.006580112611978, Val MAE: 1.3891421556472778\n",
      "Epoch 92/2000, Train Loss: 8.979381113557027, Val Loss: 6.988386724721347, Val MAE: 1.3867090940475464\n",
      "Epoch 93/2000, Train Loss: 8.952168655545126, Val Loss: 6.969903824350789, Val MAE: 1.3843052387237549\n",
      "Epoch 94/2000, Train Loss: 8.925743056443524, Val Loss: 6.952607519922441, Val MAE: 1.3821377754211426\n",
      "Epoch 95/2000, Train Loss: 8.899657004885627, Val Loss: 6.934460117478715, Val MAE: 1.3799985647201538\n",
      "Epoch 96/2000, Train Loss: 8.873301152458346, Val Loss: 6.91647890182652, Val MAE: 1.377719759941101\n",
      "Epoch 97/2000, Train Loss: 8.846950800480318, Val Loss: 6.899368508132248, Val MAE: 1.3755704164505005\n",
      "Epoch 98/2000, Train Loss: 8.820719140898968, Val Loss: 6.881816057338724, Val MAE: 1.3732972145080566\n",
      "Epoch 99/2000, Train Loss: 8.794638345005392, Val Loss: 6.864597159890296, Val MAE: 1.3710845708847046\n",
      "Epoch 100/2000, Train Loss: 8.76847010931781, Val Loss: 6.8465813729171385, Val MAE: 1.3687461614608765\n",
      "Epoch 101/2000, Train Loss: 8.74252700805664, Val Loss: 6.829420297784276, Val MAE: 1.3666611909866333\n",
      "Epoch 102/2000, Train Loss: 8.715883854124568, Val Loss: 6.811519489295077, Val MAE: 1.364314317703247\n",
      "Epoch 103/2000, Train Loss: 8.689525083111779, Val Loss: 6.794629178900184, Val MAE: 1.3621739149093628\n",
      "Epoch 104/2000, Train Loss: 8.663988799220672, Val Loss: 6.77700241829276, Val MAE: 1.3599696159362793\n",
      "Epoch 105/2000, Train Loss: 8.63651051854505, Val Loss: 6.758125411078158, Val MAE: 1.3575056791305542\n",
      "Epoch 106/2000, Train Loss: 8.610936142858042, Val Loss: 6.741159780449637, Val MAE: 1.3552640676498413\n",
      "Epoch 107/2000, Train Loss: 8.584270155737164, Val Loss: 6.723694890030608, Val MAE: 1.3530309200286865\n",
      "Epoch 108/2000, Train Loss: 8.558852568895555, Val Loss: 6.706643380096373, Val MAE: 1.3508986234664917\n",
      "Epoch 109/2000, Train Loss: 8.533785938514015, Val Loss: 6.6889932322051875, Val MAE: 1.348562240600586\n",
      "Epoch 110/2000, Train Loss: 8.508513591668013, Val Loss: 6.672347257883237, Val MAE: 1.3464834690093994\n",
      "Epoch 111/2000, Train Loss: 8.48248785774614, Val Loss: 6.654354066131123, Val MAE: 1.3442262411117554\n",
      "Epoch 112/2000, Train Loss: 8.45653629432378, Val Loss: 6.636979316774844, Val MAE: 1.3420087099075317\n",
      "Epoch 113/2000, Train Loss: 8.4302295803862, Val Loss: 6.619034277168873, Val MAE: 1.3398315906524658\n",
      "Epoch 114/2000, Train Loss: 8.405459307907515, Val Loss: 6.602333081433711, Val MAE: 1.337746024131775\n",
      "Epoch 115/2000, Train Loss: 8.381025071059995, Val Loss: 6.5852154075014075, Val MAE: 1.3356993198394775\n",
      "Epoch 116/2000, Train Loss: 8.355552775161076, Val Loss: 6.568076988688017, Val MAE: 1.3335014581680298\n",
      "Epoch 117/2000, Train Loss: 8.331488200338251, Val Loss: 6.551362447430358, Val MAE: 1.3314666748046875\n",
      "Epoch 118/2000, Train Loss: 8.307120650561888, Val Loss: 6.53469464642355, Val MAE: 1.3294684886932373\n",
      "Epoch 119/2000, Train Loss: 8.282380292988211, Val Loss: 6.517283937129683, Val MAE: 1.3274800777435303\n",
      "Epoch 120/2000, Train Loss: 8.257084550818725, Val Loss: 6.500438726263663, Val MAE: 1.3254259824752808\n",
      "Epoch 121/2000, Train Loss: 8.232888494773668, Val Loss: 6.483910664657908, Val MAE: 1.323516845703125\n",
      "Epoch 122/2000, Train Loss: 8.208493188731222, Val Loss: 6.466867030557157, Val MAE: 1.3215358257293701\n",
      "Epoch 123/2000, Train Loss: 8.183937213774088, Val Loss: 6.4496829461365, Val MAE: 1.3194478750228882\n",
      "Epoch 124/2000, Train Loss: 8.159850708018164, Val Loss: 6.433418421401811, Val MAE: 1.317561388015747\n",
      "Epoch 125/2000, Train Loss: 8.135570194597484, Val Loss: 6.416367640410821, Val MAE: 1.3155152797698975\n",
      "Epoch 126/2000, Train Loss: 8.111662865817305, Val Loss: 6.400210786414311, Val MAE: 1.3135861158370972\n",
      "Epoch 127/2000, Train Loss: 8.088360422663643, Val Loss: 6.3835601217942095, Val MAE: 1.3117583990097046\n",
      "Epoch 128/2000, Train Loss: 8.064845411230783, Val Loss: 6.366539957673531, Val MAE: 1.3098866939544678\n",
      "Epoch 129/2000, Train Loss: 8.040731474913258, Val Loss: 6.350904130931269, Val MAE: 1.3081525564193726\n",
      "Epoch 130/2000, Train Loss: 8.017412154613066, Val Loss: 6.334427335208619, Val MAE: 1.3062299489974976\n",
      "Epoch 131/2000, Train Loss: 7.993352419970964, Val Loss: 6.317332972460023, Val MAE: 1.3043090105056763\n",
      "Epoch 132/2000, Train Loss: 7.97019938228897, Val Loss: 6.301691182365451, Val MAE: 1.3024917840957642\n",
      "Epoch 133/2000, Train Loss: 7.9470257411333085, Val Loss: 6.2850007631720635, Val MAE: 1.3005728721618652\n",
      "Epoch 134/2000, Train Loss: 7.923136632510409, Val Loss: 6.268123114004907, Val MAE: 1.2985326051712036\n",
      "Epoch 135/2000, Train Loss: 7.899950187921201, Val Loss: 6.252365378341747, Val MAE: 1.2967238426208496\n",
      "Epoch 136/2000, Train Loss: 7.876445877665257, Val Loss: 6.2343507859723895, Val MAE: 1.2946038246154785\n",
      "Epoch 137/2000, Train Loss: 7.85248484669741, Val Loss: 6.2181243656819145, Val MAE: 1.2927120923995972\n",
      "Epoch 138/2000, Train Loss: 7.829290677281539, Val Loss: 6.201703450382929, Val MAE: 1.2910230159759521\n",
      "Epoch 139/2000, Train Loss: 7.806409038714508, Val Loss: 6.185275721516255, Val MAE: 1.2891616821289062\n",
      "Epoch 140/2000, Train Loss: 7.7825730295000115, Val Loss: 6.168469256112777, Val MAE: 1.2873064279556274\n",
      "Epoch 141/2000, Train Loss: 7.7597813936230935, Val Loss: 6.152828318084495, Val MAE: 1.2854015827178955\n",
      "Epoch 142/2000, Train Loss: 7.736685172022855, Val Loss: 6.13601792733823, Val MAE: 1.2834374904632568\n",
      "Epoch 143/2000, Train Loss: 7.714071167696121, Val Loss: 6.120350950991544, Val MAE: 1.281585454940796\n",
      "Epoch 144/2000, Train Loss: 7.690898780098117, Val Loss: 6.10393473400375, Val MAE: 1.2796097993850708\n",
      "Epoch 145/2000, Train Loss: 7.668114497833045, Val Loss: 6.087983601735392, Val MAE: 1.2777804136276245\n",
      "Epoch 146/2000, Train Loss: 7.645298064715423, Val Loss: 6.071983115690371, Val MAE: 1.2759209871292114\n",
      "Epoch 147/2000, Train Loss: 7.623232009452913, Val Loss: 6.056628316959117, Val MAE: 1.2741003036499023\n",
      "Epoch 148/2000, Train Loss: 7.600641671239602, Val Loss: 6.040284540950078, Val MAE: 1.2722735404968262\n",
      "Epoch 149/2000, Train Loss: 7.578813816507927, Val Loss: 6.025370335471853, Val MAE: 1.2705217599868774\n",
      "Epoch 150/2000, Train Loss: 7.5576239081865415, Val Loss: 6.010178340559341, Val MAE: 1.268818974494934\n",
      "Epoch 151/2000, Train Loss: 7.536659573562433, Val Loss: 5.995047411872587, Val MAE: 1.2670682668685913\n",
      "Epoch 152/2000, Train Loss: 7.51497441003784, Val Loss: 5.979657195593545, Val MAE: 1.265254020690918\n",
      "Epoch 153/2000, Train Loss: 7.493717838660024, Val Loss: 5.964138169325595, Val MAE: 1.2634408473968506\n",
      "Epoch 154/2000, Train Loss: 7.472420649418514, Val Loss: 5.94892025589063, Val MAE: 1.2617334127426147\n",
      "Epoch 155/2000, Train Loss: 7.45063235268017, Val Loss: 5.934062504103979, Val MAE: 1.259986162185669\n",
      "Epoch 156/2000, Train Loss: 7.429944090150913, Val Loss: 5.918659112504207, Val MAE: 1.2582098245620728\n",
      "Epoch 157/2000, Train Loss: 7.4080669967257995, Val Loss: 5.902748967808356, Val MAE: 1.2563674449920654\n",
      "Epoch 158/2000, Train Loss: 7.386356123428655, Val Loss: 5.888139469595993, Val MAE: 1.2545599937438965\n",
      "Epoch 159/2000, Train Loss: 7.364797039384596, Val Loss: 5.872109409719412, Val MAE: 1.2526363134384155\n",
      "Epoch 160/2000, Train Loss: 7.343670452708628, Val Loss: 5.857106312244075, Val MAE: 1.2509350776672363\n",
      "Epoch 161/2000, Train Loss: 7.322829843376191, Val Loss: 5.842152909632391, Val MAE: 1.2492196559906006\n",
      "Epoch 162/2000, Train Loss: 7.302217821900201, Val Loss: 5.82724176465761, Val MAE: 1.247463583946228\n",
      "Epoch 163/2000, Train Loss: 7.281219254678755, Val Loss: 5.812251872622122, Val MAE: 1.2457842826843262\n",
      "Epoch 164/2000, Train Loss: 7.260213012125955, Val Loss: 5.796979547715856, Val MAE: 1.244093894958496\n",
      "Epoch 165/2000, Train Loss: 7.239550911264148, Val Loss: 5.781836388274793, Val MAE: 1.2424017190933228\n",
      "Epoch 166/2000, Train Loss: 7.219262616120006, Val Loss: 5.7671870902826, Val MAE: 1.2408205270767212\n",
      "Epoch 167/2000, Train Loss: 7.198836708327646, Val Loss: 5.752596427142385, Val MAE: 1.239203929901123\n",
      "Epoch 168/2000, Train Loss: 7.178105118539149, Val Loss: 5.73766489346607, Val MAE: 1.2375589609146118\n",
      "Epoch 169/2000, Train Loss: 7.158082581624348, Val Loss: 5.723300064000206, Val MAE: 1.2359541654586792\n",
      "Epoch 170/2000, Train Loss: 7.138422249130187, Val Loss: 5.708624421271283, Val MAE: 1.2342931032180786\n",
      "Epoch 171/2000, Train Loss: 7.1181342928509865, Val Loss: 5.6942945803654945, Val MAE: 1.232715129852295\n",
      "Epoch 172/2000, Train Loss: 7.098396042470531, Val Loss: 5.67996665533544, Val MAE: 1.2311148643493652\n",
      "Epoch 173/2000, Train Loss: 7.077658574325582, Val Loss: 5.665243277813154, Val MAE: 1.229473352432251\n",
      "Epoch 174/2000, Train Loss: 7.057368269915537, Val Loss: 5.650379578384944, Val MAE: 1.227773666381836\n",
      "Epoch 175/2000, Train Loss: 7.038223600452273, Val Loss: 5.636766234553588, Val MAE: 1.2262680530548096\n",
      "Epoch 176/2000, Train Loss: 7.018957171705393, Val Loss: 5.621768256749107, Val MAE: 1.224853754043579\n",
      "Epoch 177/2000, Train Loss: 6.999425576855725, Val Loss: 5.608159299326811, Val MAE: 1.2233099937438965\n",
      "Epoch 178/2000, Train Loss: 6.9809314754957095, Val Loss: 5.595092950588254, Val MAE: 1.2218942642211914\n",
      "Epoch 179/2000, Train Loss: 6.9622528045116, Val Loss: 5.581588585525164, Val MAE: 1.2203088998794556\n",
      "Epoch 180/2000, Train Loss: 6.942160330763342, Val Loss: 5.567665142073172, Val MAE: 1.2187453508377075\n",
      "Epoch 181/2000, Train Loss: 6.922954614320959, Val Loss: 5.553485285546484, Val MAE: 1.2169862985610962\n",
      "Epoch 182/2000, Train Loss: 6.904568630448837, Val Loss: 5.540555003889668, Val MAE: 1.2154829502105713\n",
      "Epoch 183/2000, Train Loss: 6.887146643091251, Val Loss: 5.528333967610255, Val MAE: 1.2142648696899414\n",
      "Epoch 184/2000, Train Loss: 6.869632184748086, Val Loss: 5.514562554232131, Val MAE: 1.2127166986465454\n",
      "Epoch 185/2000, Train Loss: 6.851987859221577, Val Loss: 5.50150772436104, Val MAE: 1.2114559412002563\n",
      "Epoch 186/2000, Train Loss: 6.83461925714925, Val Loss: 5.489037739916697, Val MAE: 1.2101222276687622\n",
      "Epoch 187/2000, Train Loss: 6.817658250982111, Val Loss: 5.47627831236056, Val MAE: 1.2087955474853516\n",
      "Epoch 188/2000, Train Loss: 6.7999694429065025, Val Loss: 5.462983185498733, Val MAE: 1.2073922157287598\n",
      "Epoch 189/2000, Train Loss: 6.782468122656342, Val Loss: 5.450577785004311, Val MAE: 1.2061591148376465\n",
      "Epoch 190/2000, Train Loss: 6.765612917321669, Val Loss: 5.437933701150647, Val MAE: 1.2048321962356567\n",
      "Epoch 191/2000, Train Loss: 6.74891468954337, Val Loss: 5.4254737767764905, Val MAE: 1.2036288976669312\n",
      "Epoch 192/2000, Train Loss: 6.732255875659927, Val Loss: 5.4131464910348805, Val MAE: 1.2023539543151855\n",
      "Epoch 193/2000, Train Loss: 6.715517886458129, Val Loss: 5.400707884455525, Val MAE: 1.2011144161224365\n",
      "Epoch 194/2000, Train Loss: 6.699020637100717, Val Loss: 5.388171112425919, Val MAE: 1.1999125480651855\n",
      "Epoch 195/2000, Train Loss: 6.682415660718402, Val Loss: 5.376468930639855, Val MAE: 1.1987371444702148\n",
      "Epoch 196/2000, Train Loss: 6.665269191747119, Val Loss: 5.363527119763958, Val MAE: 1.197458267211914\n",
      "Epoch 197/2000, Train Loss: 6.648086772561882, Val Loss: 5.351089224169456, Val MAE: 1.1963034868240356\n",
      "Epoch 198/2000, Train Loss: 6.6320295592661305, Val Loss: 5.339467199765674, Val MAE: 1.1952323913574219\n",
      "Epoch 199/2000, Train Loss: 6.616108105033939, Val Loss: 5.327349760887895, Val MAE: 1.1943471431732178\n",
      "Epoch 200/2000, Train Loss: 6.600698924240539, Val Loss: 5.315189468860626, Val MAE: 1.1931818723678589\n",
      "Epoch 201/2000, Train Loss: 6.584779504438916, Val Loss: 5.3042434845207715, Val MAE: 1.1921559572219849\n",
      "Epoch 202/2000, Train Loss: 6.569231426861426, Val Loss: 5.292564737494653, Val MAE: 1.1910425424575806\n",
      "Epoch 203/2000, Train Loss: 6.553885536245607, Val Loss: 5.281210295641868, Val MAE: 1.1899839639663696\n",
      "Epoch 204/2000, Train Loss: 6.537890886322417, Val Loss: 5.26918814548064, Val MAE: 1.188849687576294\n",
      "Epoch 205/2000, Train Loss: 6.522652533355205, Val Loss: 5.257936069568781, Val MAE: 1.187811255455017\n",
      "Epoch 206/2000, Train Loss: 6.507374927826135, Val Loss: 5.246412338251908, Val MAE: 1.186801552772522\n",
      "Epoch 207/2000, Train Loss: 6.491383522475995, Val Loss: 5.235115404770248, Val MAE: 1.1857993602752686\n",
      "Epoch 208/2000, Train Loss: 6.476652388619525, Val Loss: 5.223595610774291, Val MAE: 1.1848766803741455\n",
      "Epoch 209/2000, Train Loss: 6.461844760876558, Val Loss: 5.213224912766279, Val MAE: 1.1840647459030151\n",
      "Epoch 210/2000, Train Loss: 6.4475682399651415, Val Loss: 5.201983852408768, Val MAE: 1.183101773262024\n",
      "Epoch 211/2000, Train Loss: 6.4322836159765115, Val Loss: 5.190910421253189, Val MAE: 1.1822102069854736\n",
      "Epoch 212/2000, Train Loss: 6.417874712795221, Val Loss: 5.180082251867793, Val MAE: 1.1813987493515015\n",
      "Epoch 213/2000, Train Loss: 6.403253632290554, Val Loss: 5.169249098689303, Val MAE: 1.1806635856628418\n",
      "Epoch 214/2000, Train Loss: 6.389709744826111, Val Loss: 5.15875884774691, Val MAE: 1.1800031661987305\n",
      "Epoch 215/2000, Train Loss: 6.375904266999243, Val Loss: 5.148604482412338, Val MAE: 1.1793006658554077\n",
      "Epoch 216/2000, Train Loss: 6.362088257090531, Val Loss: 5.138076831992921, Val MAE: 1.1787205934524536\n",
      "Epoch 217/2000, Train Loss: 6.348550248466523, Val Loss: 5.127581223382021, Val MAE: 1.1780421733856201\n",
      "Epoch 218/2000, Train Loss: 6.335174054266276, Val Loss: 5.117085683600992, Val MAE: 1.1773360967636108\n",
      "Epoch 219/2000, Train Loss: 6.321647913578084, Val Loss: 5.10745155048652, Val MAE: 1.1767605543136597\n",
      "Epoch 220/2000, Train Loss: 6.30858051760711, Val Loss: 5.097229149001907, Val MAE: 1.1760746240615845\n",
      "Epoch 221/2000, Train Loss: 6.294150305964973, Val Loss: 5.086343855551613, Val MAE: 1.1754060983657837\n",
      "Epoch 222/2000, Train Loss: 6.281034106638894, Val Loss: 5.076377357079053, Val MAE: 1.1747575998306274\n",
      "Epoch 223/2000, Train Loss: 6.268870883776865, Val Loss: 5.067119520315973, Val MAE: 1.1742808818817139\n",
      "Epoch 224/2000, Train Loss: 6.25628174095335, Val Loss: 5.057633413887752, Val MAE: 1.1737366914749146\n",
      "Epoch 225/2000, Train Loss: 6.2440565847153575, Val Loss: 5.048639823009414, Val MAE: 1.1733193397521973\n",
      "Epoch 226/2000, Train Loss: 6.231567963816256, Val Loss: 5.038774363676042, Val MAE: 1.172800898551941\n",
      "Epoch 227/2000, Train Loss: 6.218206494925304, Val Loss: 5.028478023046114, Val MAE: 1.172186255455017\n",
      "Epoch 228/2000, Train Loss: 6.205099907464865, Val Loss: 5.018359226501715, Val MAE: 1.171686053276062\n",
      "Epoch 229/2000, Train Loss: 6.192959513722475, Val Loss: 5.009496460731809, Val MAE: 1.1713541746139526\n",
      "Epoch 230/2000, Train Loss: 6.180682177378656, Val Loss: 5.000075841548405, Val MAE: 1.1709470748901367\n",
      "Epoch 231/2000, Train Loss: 6.168844579462482, Val Loss: 4.990764366360161, Val MAE: 1.1706093549728394\n",
      "Epoch 232/2000, Train Loss: 6.157355855569102, Val Loss: 4.98196650582153, Val MAE: 1.170322299003601\n",
      "Epoch 233/2000, Train Loss: 6.146507029940994, Val Loss: 4.973637959971203, Val MAE: 1.1701427698135376\n",
      "Epoch 234/2000, Train Loss: 6.135190537009828, Val Loss: 4.9645000740709735, Val MAE: 1.1697965860366821\n",
      "Epoch 235/2000, Train Loss: 6.124074002179435, Val Loss: 4.956310531907664, Val MAE: 1.1696118116378784\n",
      "Epoch 236/2000, Train Loss: 6.113271095697721, Val Loss: 4.947445526836425, Val MAE: 1.1693326234817505\n",
      "Epoch 237/2000, Train Loss: 6.102515730395246, Val Loss: 4.939657213263155, Val MAE: 1.1691844463348389\n",
      "Epoch 238/2000, Train Loss: 6.091844948799712, Val Loss: 4.931065129570839, Val MAE: 1.1689764261245728\n",
      "Epoch 239/2000, Train Loss: 6.0810432123523235, Val Loss: 4.922721413898421, Val MAE: 1.1687912940979004\n",
      "Epoch 240/2000, Train Loss: 6.069661782910413, Val Loss: 4.913824766856713, Val MAE: 1.1686041355133057\n",
      "Epoch 241/2000, Train Loss: 6.059216549147421, Val Loss: 4.906111259283278, Val MAE: 1.1686227321624756\n",
      "Epoch 242/2000, Train Loss: 6.048379368321866, Val Loss: 4.898330467751646, Val MAE: 1.16861891746521\n",
      "Epoch 243/2000, Train Loss: 6.038441670373628, Val Loss: 4.889850703926068, Val MAE: 1.1686291694641113\n",
      "Epoch 244/2000, Train Loss: 6.0279880792832605, Val Loss: 4.8824182653462325, Val MAE: 1.1686913967132568\n",
      "Epoch 245/2000, Train Loss: 6.0173999267646705, Val Loss: 4.873580685592308, Val MAE: 1.168692708015442\n",
      "Epoch 246/2000, Train Loss: 6.0074072589332, Val Loss: 4.866337294348582, Val MAE: 1.1687366962432861\n",
      "Epoch 247/2000, Train Loss: 5.997857131074047, Val Loss: 4.858812635016489, Val MAE: 1.1687439680099487\n",
      "Epoch 248/2000, Train Loss: 5.9883716979861426, Val Loss: 4.851597508829175, Val MAE: 1.168833613395691\n",
      "Epoch 249/2000, Train Loss: 5.979036733812022, Val Loss: 4.843539319903128, Val MAE: 1.1688332557678223\n",
      "Epoch 250/2000, Train Loss: 5.969402616350751, Val Loss: 4.836384747758156, Val MAE: 1.16889226436615\n",
      "Epoch 251/2000, Train Loss: 5.960415738570318, Val Loss: 4.829595979127124, Val MAE: 1.168984293937683\n",
      "Epoch 252/2000, Train Loss: 5.951596825546616, Val Loss: 4.822304439579877, Val MAE: 1.1689801216125488\n",
      "Epoch 253/2000, Train Loss: 5.943122400357571, Val Loss: 4.815621265434609, Val MAE: 1.1691585779190063\n",
      "Epoch 254/2000, Train Loss: 5.93480190958479, Val Loss: 4.808776451887812, Val MAE: 1.1692105531692505\n",
      "Epoch 255/2000, Train Loss: 5.926080338828761, Val Loss: 4.801585627306165, Val MAE: 1.1692792177200317\n",
      "Epoch 256/2000, Train Loss: 5.91718435932704, Val Loss: 4.794483199346018, Val MAE: 1.1695480346679688\n",
      "Epoch 257/2000, Train Loss: 5.908703821808545, Val Loss: 4.7875578084975245, Val MAE: 1.1696139574050903\n",
      "Epoch 258/2000, Train Loss: 5.900276352301382, Val Loss: 4.780940239078651, Val MAE: 1.169734239578247\n",
      "Epoch 259/2000, Train Loss: 5.892081220567308, Val Loss: 4.774245518956363, Val MAE: 1.1698908805847168\n",
      "Epoch 260/2000, Train Loss: 5.882591248852589, Val Loss: 4.767576887229765, Val MAE: 1.1698644161224365\n",
      "Epoch 261/2000, Train Loss: 5.875002818321114, Val Loss: 4.7607979871097985, Val MAE: 1.1700307130813599\n",
      "Epoch 262/2000, Train Loss: 5.866521540296288, Val Loss: 4.754055353870073, Val MAE: 1.1701598167419434\n",
      "Epoch 263/2000, Train Loss: 5.857582767942625, Val Loss: 4.7473356027713445, Val MAE: 1.1703232526779175\n",
      "Epoch 264/2000, Train Loss: 5.849956002131893, Val Loss: 4.741255884165839, Val MAE: 1.1704626083374023\n",
      "Epoch 265/2000, Train Loss: 5.842510380620437, Val Loss: 4.735057675199011, Val MAE: 1.1706409454345703\n",
      "Epoch 266/2000, Train Loss: 5.835083536025303, Val Loss: 4.729471856584465, Val MAE: 1.1708670854568481\n",
      "Epoch 267/2000, Train Loss: 5.827876270868723, Val Loss: 4.723575389889751, Val MAE: 1.171044111251831\n",
      "Epoch 268/2000, Train Loss: 5.819974935847351, Val Loss: 4.7169476963168995, Val MAE: 1.1712599992752075\n",
      "Epoch 269/2000, Train Loss: 5.8124012579102695, Val Loss: 4.711511808575138, Val MAE: 1.1715670824050903\n",
      "Epoch 270/2000, Train Loss: 5.805837614776906, Val Loss: 4.705816654198047, Val MAE: 1.1718088388442993\n",
      "Epoch 271/2000, Train Loss: 5.7989761724441315, Val Loss: 4.700207804823954, Val MAE: 1.1720761060714722\n",
      "Epoch 272/2000, Train Loss: 5.792275125585405, Val Loss: 4.694844420447828, Val MAE: 1.1723554134368896\n",
      "Epoch 273/2000, Train Loss: 5.785726487717441, Val Loss: 4.689178442022228, Val MAE: 1.172647476196289\n",
      "Epoch 274/2000, Train Loss: 5.778790684212176, Val Loss: 4.683921314805277, Val MAE: 1.173315405845642\n",
      "Epoch 275/2000, Train Loss: 5.772721521121676, Val Loss: 4.678830019167559, Val MAE: 1.1736477613449097\n",
      "Epoch 276/2000, Train Loss: 5.76643589102591, Val Loss: 4.673265564893409, Val MAE: 1.1739857196807861\n",
      "Epoch 277/2000, Train Loss: 5.760214453257286, Val Loss: 4.668521274074795, Val MAE: 1.174350380897522\n",
      "Epoch 278/2000, Train Loss: 5.754122667833455, Val Loss: 4.663303345242354, Val MAE: 1.1747174263000488\n",
      "Epoch 279/2000, Train Loss: 5.748036316724866, Val Loss: 4.6584993147357245, Val MAE: 1.1750743389129639\n",
      "Epoch 280/2000, Train Loss: 5.7420295572345585, Val Loss: 4.653211929490716, Val MAE: 1.1754766702651978\n",
      "Epoch 281/2000, Train Loss: 5.736166777571959, Val Loss: 4.64850589834799, Val MAE: 1.1758408546447754\n",
      "Epoch 282/2000, Train Loss: 5.730085784091717, Val Loss: 4.643200576035526, Val MAE: 1.1763067245483398\n",
      "Epoch 283/2000, Train Loss: 5.724344653060996, Val Loss: 4.638829432979343, Val MAE: 1.1767728328704834\n",
      "Epoch 284/2000, Train Loss: 5.718738853041805, Val Loss: 4.633912541940222, Val MAE: 1.1771849393844604\n",
      "Epoch 285/2000, Train Loss: 5.713090800850815, Val Loss: 4.62939494014021, Val MAE: 1.1776200532913208\n",
      "Epoch 286/2000, Train Loss: 5.707153328966092, Val Loss: 4.623835964301439, Val MAE: 1.1781415939331055\n",
      "Epoch 287/2000, Train Loss: 5.701577445383473, Val Loss: 4.619710794724817, Val MAE: 1.1786432266235352\n",
      "Epoch 288/2000, Train Loss: 5.695852404533603, Val Loss: 4.614676228101094, Val MAE: 1.1791324615478516\n",
      "Epoch 289/2000, Train Loss: 5.690274052898143, Val Loss: 4.609623189692892, Val MAE: 1.179542064666748\n",
      "Epoch 290/2000, Train Loss: 5.68517542166921, Val Loss: 4.605561875211676, Val MAE: 1.1800894737243652\n",
      "Epoch 291/2000, Train Loss: 5.680181542580778, Val Loss: 4.601637439095364, Val MAE: 1.1805511713027954\n",
      "Epoch 292/2000, Train Loss: 5.675209124121446, Val Loss: 4.597416299376197, Val MAE: 1.1810343265533447\n",
      "Epoch 293/2000, Train Loss: 5.670286018133487, Val Loss: 4.59306031040202, Val MAE: 1.1815663576126099\n",
      "Epoch 294/2000, Train Loss: 5.664880306458699, Val Loss: 4.58875331023312, Val MAE: 1.1821157932281494\n",
      "Epoch 295/2000, Train Loss: 5.659861942078398, Val Loss: 4.584577105792723, Val MAE: 1.1826070547103882\n",
      "Epoch 296/2000, Train Loss: 5.655165531590399, Val Loss: 4.580309570480988, Val MAE: 1.183172583580017\n",
      "Epoch 297/2000, Train Loss: 5.650320343602464, Val Loss: 4.576422505350563, Val MAE: 1.1836947202682495\n",
      "Epoch 298/2000, Train Loss: 5.645777827737856, Val Loss: 4.5726435707251385, Val MAE: 1.1841460466384888\n",
      "Epoch 299/2000, Train Loss: 5.641438833570545, Val Loss: 4.568727419396319, Val MAE: 1.1846234798431396\n",
      "Epoch 300/2000, Train Loss: 5.636226258252013, Val Loss: 4.564281727426399, Val MAE: 1.1852355003356934\n",
      "Epoch 301/2000, Train Loss: 5.631842809127773, Val Loss: 4.560843173197404, Val MAE: 1.1857187747955322\n",
      "Epoch 302/2000, Train Loss: 5.6275672249425215, Val Loss: 4.557112498986204, Val MAE: 1.186287760734558\n",
      "Epoch 303/2000, Train Loss: 5.623571661789996, Val Loss: 4.553976124132008, Val MAE: 1.1868364810943604\n",
      "Epoch 304/2000, Train Loss: 5.619895339820893, Val Loss: 4.550783999141042, Val MAE: 1.1873383522033691\n",
      "Epoch 305/2000, Train Loss: 5.61618219204803, Val Loss: 4.547715637697948, Val MAE: 1.1879241466522217\n",
      "Epoch 306/2000, Train Loss: 5.612303858049204, Val Loss: 4.544533543593771, Val MAE: 1.1884539127349854\n",
      "Epoch 307/2000, Train Loss: 5.608357335528008, Val Loss: 4.541562634190236, Val MAE: 1.1889547109603882\n",
      "Epoch 308/2000, Train Loss: 5.604557343383334, Val Loss: 4.538128274561852, Val MAE: 1.1894621849060059\n",
      "Epoch 309/2000, Train Loss: 5.600392264388552, Val Loss: 4.534082501610433, Val MAE: 1.190077304840088\n",
      "Epoch 310/2000, Train Loss: 5.596300358800635, Val Loss: 4.531075485918935, Val MAE: 1.190579891204834\n",
      "Epoch 311/2000, Train Loss: 5.5925893951788685, Val Loss: 4.527751111503192, Val MAE: 1.1911256313323975\n",
      "Epoch 312/2000, Train Loss: 5.588470637762927, Val Loss: 4.524046532117476, Val MAE: 1.1917201280593872\n",
      "Epoch 313/2000, Train Loss: 5.584730930343331, Val Loss: 4.520873755908857, Val MAE: 1.1922188997268677\n",
      "Epoch 314/2000, Train Loss: 5.581144644960943, Val Loss: 4.517853913450335, Val MAE: 1.1927334070205688\n",
      "Epoch 315/2000, Train Loss: 5.577569599229251, Val Loss: 4.514964987185058, Val MAE: 1.1933842897415161\n",
      "Epoch 316/2000, Train Loss: 5.5740485250990295, Val Loss: 4.511929445144698, Val MAE: 1.1941356658935547\n",
      "Epoch 317/2000, Train Loss: 5.570657019941907, Val Loss: 4.509251878451644, Val MAE: 1.1946369409561157\n",
      "Epoch 318/2000, Train Loss: 5.567345335655006, Val Loss: 4.5061622892425754, Val MAE: 1.1951439380645752\n",
      "Epoch 319/2000, Train Loss: 5.56394636113414, Val Loss: 4.503404266252293, Val MAE: 1.1957241296768188\n",
      "Epoch 320/2000, Train Loss: 5.560782279864742, Val Loss: 4.500666171300599, Val MAE: 1.196290373802185\n",
      "Epoch 321/2000, Train Loss: 5.557731240684498, Val Loss: 4.497639124846365, Val MAE: 1.1968668699264526\n",
      "Epoch 322/2000, Train Loss: 5.554381579524627, Val Loss: 4.49549100489363, Val MAE: 1.1973621845245361\n",
      "Epoch 323/2000, Train Loss: 5.551790648831239, Val Loss: 4.4929155146864455, Val MAE: 1.1978929042816162\n",
      "Epoch 324/2000, Train Loss: 5.548649859072557, Val Loss: 4.490426189282278, Val MAE: 1.198499321937561\n",
      "Epoch 325/2000, Train Loss: 5.5454180755297395, Val Loss: 4.4874502827802045, Val MAE: 1.1991559267044067\n",
      "Epoch 326/2000, Train Loss: 5.542302887022414, Val Loss: 4.4851076043379585, Val MAE: 1.19967782497406\n",
      "Epoch 327/2000, Train Loss: 5.5396506304333295, Val Loss: 4.482562216048635, Val MAE: 1.2003239393234253\n",
      "Epoch 328/2000, Train Loss: 5.5367673752700535, Val Loss: 4.480588930176468, Val MAE: 1.2009198665618896\n",
      "Epoch 329/2000, Train Loss: 5.534119436342972, Val Loss: 4.477963667087198, Val MAE: 1.2014718055725098\n",
      "Epoch 330/2000, Train Loss: 5.531401682094124, Val Loss: 4.476061479794228, Val MAE: 1.2020890712738037\n",
      "Epoch 331/2000, Train Loss: 5.528381247048139, Val Loss: 4.473457168335989, Val MAE: 1.2030742168426514\n",
      "Epoch 332/2000, Train Loss: 5.5255313697921835, Val Loss: 4.471120248676285, Val MAE: 1.2036926746368408\n",
      "Epoch 333/2000, Train Loss: 5.52288724900586, Val Loss: 4.468754988063978, Val MAE: 1.2042632102966309\n",
      "Epoch 334/2000, Train Loss: 5.519907178839964, Val Loss: 4.465928679305738, Val MAE: 1.2049453258514404\n",
      "Epoch 335/2000, Train Loss: 5.517171535398331, Val Loss: 4.463829497781795, Val MAE: 1.2055602073669434\n",
      "Epoch 336/2000, Train Loss: 5.514564731571535, Val Loss: 4.461795118368986, Val MAE: 1.2061233520507812\n",
      "Epoch 337/2000, Train Loss: 5.512077723461705, Val Loss: 4.45954577867675, Val MAE: 1.206709623336792\n",
      "Epoch 338/2000, Train Loss: 5.509740283725382, Val Loss: 4.457393307256417, Val MAE: 1.207336664199829\n",
      "Epoch 339/2000, Train Loss: 5.507416822531815, Val Loss: 4.455432474261194, Val MAE: 1.2078757286071777\n",
      "Epoch 340/2000, Train Loss: 5.504785846984532, Val Loss: 4.453802720122919, Val MAE: 1.208484172821045\n",
      "Epoch 341/2000, Train Loss: 5.5023666883906, Val Loss: 4.451385605546434, Val MAE: 1.2092008590698242\n",
      "Epoch 342/2000, Train Loss: 5.500177461062552, Val Loss: 4.449953461420818, Val MAE: 1.2098039388656616\n",
      "Epoch 343/2000, Train Loss: 5.498023825067031, Val Loss: 4.448038395676087, Val MAE: 1.2103590965270996\n",
      "Epoch 344/2000, Train Loss: 5.4957197776804785, Val Loss: 4.445930085559999, Val MAE: 1.211114764213562\n",
      "Epoch 345/2000, Train Loss: 5.4930211362230565, Val Loss: 4.444056524113408, Val MAE: 1.2117342948913574\n",
      "Epoch 346/2000, Train Loss: 5.491057057878864, Val Loss: 4.442267779876866, Val MAE: 1.2123671770095825\n",
      "Epoch 347/2000, Train Loss: 5.48907474928322, Val Loss: 4.440536895100995, Val MAE: 1.2130098342895508\n",
      "Epoch 348/2000, Train Loss: 5.487102383060262, Val Loss: 4.438764072086398, Val MAE: 1.2136057615280151\n",
      "Epoch 349/2000, Train Loss: 5.485032587010146, Val Loss: 4.437225616060373, Val MAE: 1.2142301797866821\n",
      "Epoch 350/2000, Train Loss: 5.483092810761492, Val Loss: 4.435503033425395, Val MAE: 1.214849829673767\n",
      "Epoch 351/2000, Train Loss: 5.481219112468704, Val Loss: 4.433747029222372, Val MAE: 1.2156834602355957\n",
      "Epoch 352/2000, Train Loss: 5.479756409354164, Val Loss: 4.432727471662788, Val MAE: 1.2161073684692383\n",
      "Epoch 353/2000, Train Loss: 5.478083940382081, Val Loss: 4.430889014899731, Val MAE: 1.2167783975601196\n",
      "Epoch 354/2000, Train Loss: 5.4761082391725955, Val Loss: 4.42960483273418, Val MAE: 1.2173101902008057\n",
      "Epoch 355/2000, Train Loss: 5.4743842745308475, Val Loss: 4.4280851267102195, Val MAE: 1.2179075479507446\n",
      "Epoch 356/2000, Train Loss: 5.472602152095013, Val Loss: 4.426540339169071, Val MAE: 1.2185466289520264\n",
      "Epoch 357/2000, Train Loss: 5.470775927746797, Val Loss: 4.425100017274459, Val MAE: 1.2191646099090576\n",
      "Epoch 358/2000, Train Loss: 5.468600316521594, Val Loss: 4.422870687354268, Val MAE: 1.2199757099151611\n",
      "Epoch 359/2000, Train Loss: 5.466841279732486, Val Loss: 4.421538308380157, Val MAE: 1.2205612659454346\n",
      "Epoch 360/2000, Train Loss: 5.464650167485893, Val Loss: 4.418984379766025, Val MAE: 1.2213906049728394\n",
      "Epoch 361/2000, Train Loss: 5.462953533214015, Val Loss: 4.41756798458381, Val MAE: 1.221979022026062\n",
      "Epoch 362/2000, Train Loss: 5.461364191616892, Val Loss: 4.416526891478873, Val MAE: 1.2224912643432617\n",
      "Epoch 363/2000, Train Loss: 5.459705650402054, Val Loss: 4.414970533010059, Val MAE: 1.2231537103652954\n",
      "Epoch 364/2000, Train Loss: 5.457918381964184, Val Loss: 4.413250114469547, Val MAE: 1.2238354682922363\n",
      "Epoch 365/2000, Train Loss: 5.456223288578774, Val Loss: 4.412033010479503, Val MAE: 1.2243748903274536\n",
      "Epoch 366/2000, Train Loss: 5.454721263176388, Val Loss: 4.410921156206938, Val MAE: 1.2248648405075073\n",
      "Epoch 367/2000, Train Loss: 5.452940470408875, Val Loss: 4.409564275840136, Val MAE: 1.2254910469055176\n",
      "Epoch 368/2000, Train Loss: 5.451479057929247, Val Loss: 4.408282449313505, Val MAE: 1.2260807752609253\n",
      "Epoch 369/2000, Train Loss: 5.450137558915075, Val Loss: 4.407286982543356, Val MAE: 1.2265955209732056\n",
      "Epoch 370/2000, Train Loss: 5.448677278741729, Val Loss: 4.406248728015761, Val MAE: 1.2272305488586426\n",
      "Epoch 371/2000, Train Loss: 5.447328578827339, Val Loss: 4.405110644930461, Val MAE: 1.227779746055603\n",
      "Epoch 372/2000, Train Loss: 5.445999136429629, Val Loss: 4.404039407691617, Val MAE: 1.2283178567886353\n",
      "Epoch 373/2000, Train Loss: 5.44450568117636, Val Loss: 4.402912155826261, Val MAE: 1.2289563417434692\n",
      "Epoch 374/2000, Train Loss: 5.442848359632686, Val Loss: 4.401444584178174, Val MAE: 1.229590654373169\n",
      "Epoch 375/2000, Train Loss: 5.441328817339086, Val Loss: 4.400435046358841, Val MAE: 1.2301863431930542\n",
      "Epoch 376/2000, Train Loss: 5.439925979767272, Val Loss: 4.39939582018167, Val MAE: 1.2307721376419067\n",
      "Epoch 377/2000, Train Loss: 5.438560008355688, Val Loss: 4.39821960053106, Val MAE: 1.2314318418502808\n",
      "Epoch 378/2000, Train Loss: 5.437303958948464, Val Loss: 4.3973780061668295, Val MAE: 1.2318997383117676\n",
      "Epoch 379/2000, Train Loss: 5.435989709891976, Val Loss: 4.396441973370361, Val MAE: 1.2324455976486206\n",
      "Epoch 380/2000, Train Loss: 5.434732690109812, Val Loss: 4.39536398110662, Val MAE: 1.2330665588378906\n",
      "Epoch 381/2000, Train Loss: 5.433573814066003, Val Loss: 4.394341271411715, Val MAE: 1.2336229085922241\n",
      "Epoch 382/2000, Train Loss: 5.432011836581185, Val Loss: 4.393019455952907, Val MAE: 1.2342625856399536\n",
      "Epoch 383/2000, Train Loss: 5.4309250236042805, Val Loss: 4.392053070394542, Val MAE: 1.234906792640686\n",
      "Epoch 384/2000, Train Loss: 5.42936690173932, Val Loss: 4.391058764190186, Val MAE: 1.2354987859725952\n",
      "Epoch 385/2000, Train Loss: 5.427933192550327, Val Loss: 4.389700379087699, Val MAE: 1.2361527681350708\n",
      "Epoch 386/2000, Train Loss: 5.426657301827436, Val Loss: 4.388592432333729, Val MAE: 1.2367631196975708\n",
      "Epoch 387/2000, Train Loss: 5.425294928343681, Val Loss: 4.387707580039351, Val MAE: 1.2373188734054565\n",
      "Epoch 388/2000, Train Loss: 5.424040473461798, Val Loss: 4.386675794000231, Val MAE: 1.237882375717163\n",
      "Epoch 389/2000, Train Loss: 5.422860511471208, Val Loss: 4.38571269811373, Val MAE: 1.2384639978408813\n",
      "Epoch 390/2000, Train Loss: 5.421578398229633, Val Loss: 4.384795056154409, Val MAE: 1.2390360832214355\n",
      "Epoch 391/2000, Train Loss: 5.420602989649676, Val Loss: 4.384293646242206, Val MAE: 1.239500880241394\n",
      "Epoch 392/2000, Train Loss: 5.419613827970207, Val Loss: 4.383639268072572, Val MAE: 1.240007996559143\n",
      "Epoch 393/2000, Train Loss: 5.418407467875018, Val Loss: 4.3825640745167656, Val MAE: 1.2406303882598877\n",
      "Epoch 394/2000, Train Loss: 5.417153463273068, Val Loss: 4.3817168342081585, Val MAE: 1.2412118911743164\n",
      "Epoch 395/2000, Train Loss: 5.415668605383248, Val Loss: 4.380597880494407, Val MAE: 1.2419663667678833\n",
      "Epoch 396/2000, Train Loss: 5.414427675928521, Val Loss: 4.3797158427595155, Val MAE: 1.2425323724746704\n",
      "Epoch 397/2000, Train Loss: 5.413179107728127, Val Loss: 4.378848304945653, Val MAE: 1.2431117296218872\n",
      "Epoch 398/2000, Train Loss: 5.412141620061453, Val Loss: 4.377975842028152, Val MAE: 1.2436702251434326\n",
      "Epoch 399/2000, Train Loss: 5.410910467765063, Val Loss: 4.3769830474234, Val MAE: 1.244276523590088\n",
      "Epoch 400/2000, Train Loss: 5.409713617335181, Val Loss: 4.376154430668185, Val MAE: 1.24488365650177\n",
      "Epoch 401/2000, Train Loss: 5.408439684270033, Val Loss: 4.3753120319815135, Val MAE: 1.245483636856079\n",
      "Epoch 402/2000, Train Loss: 5.407360692845918, Val Loss: 4.374555532528659, Val MAE: 1.2460532188415527\n",
      "Epoch 403/2000, Train Loss: 5.406094687764648, Val Loss: 4.373816621491289, Val MAE: 1.2465578317642212\n",
      "Epoch 404/2000, Train Loss: 5.405157531051817, Val Loss: 4.372966554596668, Val MAE: 1.2471866607666016\n",
      "Epoch 405/2000, Train Loss: 5.404030018292741, Val Loss: 4.37217057957424, Val MAE: 1.2477508783340454\n",
      "Epoch 406/2000, Train Loss: 5.402931102459272, Val Loss: 4.3714519962316425, Val MAE: 1.2482287883758545\n",
      "Epoch 407/2000, Train Loss: 5.401844587546029, Val Loss: 4.3706248191867285, Val MAE: 1.2488088607788086\n",
      "Epoch 408/2000, Train Loss: 5.400621231731275, Val Loss: 4.369730739776544, Val MAE: 1.2494953870773315\n",
      "Epoch 409/2000, Train Loss: 5.399548102913265, Val Loss: 4.369078609488142, Val MAE: 1.2499332427978516\n",
      "Epoch 410/2000, Train Loss: 5.398392984873566, Val Loss: 4.368383329637408, Val MAE: 1.2504841089248657\n",
      "Epoch 411/2000, Train Loss: 5.397343961322324, Val Loss: 4.367511759405061, Val MAE: 1.2511022090911865\n",
      "Epoch 412/2000, Train Loss: 5.396192373381864, Val Loss: 4.36661282655761, Val MAE: 1.251757025718689\n",
      "Epoch 413/2000, Train Loss: 5.394893195929973, Val Loss: 4.365685412710107, Val MAE: 1.2523980140686035\n",
      "Epoch 414/2000, Train Loss: 5.393934435889711, Val Loss: 4.364840976694437, Val MAE: 1.2529898881912231\n",
      "Epoch 415/2000, Train Loss: 5.392896885464279, Val Loss: 4.364175510476893, Val MAE: 1.2535046339035034\n",
      "Epoch 416/2000, Train Loss: 5.39194697117385, Val Loss: 4.363514784846719, Val MAE: 1.2540558576583862\n",
      "Epoch 417/2000, Train Loss: 5.390762098770452, Val Loss: 4.36255575446632, Val MAE: 1.2547316551208496\n",
      "Epoch 418/2000, Train Loss: 5.389643885710831, Val Loss: 4.361835421610066, Val MAE: 1.2552905082702637\n",
      "Epoch 419/2000, Train Loss: 5.388612436633582, Val Loss: 4.3611730203149826, Val MAE: 1.2557870149612427\n",
      "Epoch 420/2000, Train Loss: 5.38743768585909, Val Loss: 4.360149576884555, Val MAE: 1.256537675857544\n",
      "Epoch 421/2000, Train Loss: 5.386407961434554, Val Loss: 4.359395249293545, Val MAE: 1.2570627927780151\n",
      "Epoch 422/2000, Train Loss: 5.385450960337873, Val Loss: 4.358871891766083, Val MAE: 1.2574387788772583\n",
      "Epoch 423/2000, Train Loss: 5.384637919339267, Val Loss: 4.358326239238574, Val MAE: 1.2579866647720337\n",
      "Epoch 424/2000, Train Loss: 5.383774437768489, Val Loss: 4.3578560385178395, Val MAE: 1.2584772109985352\n",
      "Epoch 425/2000, Train Loss: 5.382806313632301, Val Loss: 4.357021644664561, Val MAE: 1.2591029405593872\n",
      "Epoch 426/2000, Train Loss: 5.3817594714533525, Val Loss: 4.356405557187523, Val MAE: 1.25957190990448\n",
      "Epoch 427/2000, Train Loss: 5.380894459409985, Val Loss: 4.355918403216235, Val MAE: 1.2601158618927002\n",
      "Epoch 428/2000, Train Loss: 5.380048256230969, Val Loss: 4.355352542484839, Val MAE: 1.2607022523880005\n",
      "Epoch 429/2000, Train Loss: 5.379183164930408, Val Loss: 4.3548754851649125, Val MAE: 1.2611567974090576\n",
      "Epoch 430/2000, Train Loss: 5.378086577597981, Val Loss: 4.354244199864508, Val MAE: 1.2617855072021484\n",
      "Epoch 431/2000, Train Loss: 5.377030605700479, Val Loss: 4.3536187515014735, Val MAE: 1.262320637702942\n",
      "Epoch 432/2000, Train Loss: 5.376144079857959, Val Loss: 4.35310385541653, Val MAE: 1.2628246545791626\n",
      "Epoch 433/2000, Train Loss: 5.375093821269362, Val Loss: 4.352443016560998, Val MAE: 1.2633916139602661\n",
      "Epoch 434/2000, Train Loss: 5.374282410866208, Val Loss: 4.351979050063711, Val MAE: 1.2638393640518188\n",
      "Epoch 435/2000, Train Loss: 5.373424737594183, Val Loss: 4.351509218915241, Val MAE: 1.264331579208374\n",
      "Epoch 436/2000, Train Loss: 5.372511581235958, Val Loss: 4.350948084574046, Val MAE: 1.2648125886917114\n",
      "Epoch 437/2000, Train Loss: 5.371667551480769, Val Loss: 4.350436206267575, Val MAE: 1.2652992010116577\n",
      "Epoch 438/2000, Train Loss: 5.370835554001289, Val Loss: 4.349883445839244, Val MAE: 1.2658519744873047\n",
      "Epoch 439/2000, Train Loss: 5.369806205563177, Val Loss: 4.3493655147514945, Val MAE: 1.2664408683776855\n",
      "Epoch 440/2000, Train Loss: 5.368719394155747, Val Loss: 4.348783810190328, Val MAE: 1.2669416666030884\n",
      "Epoch 441/2000, Train Loss: 5.367860160737704, Val Loss: 4.348283300671991, Val MAE: 1.2674736976623535\n",
      "Epoch 442/2000, Train Loss: 5.367047566854808, Val Loss: 4.347702420954629, Val MAE: 1.2680171728134155\n",
      "Epoch 443/2000, Train Loss: 5.366264767692079, Val Loss: 4.3474300055757285, Val MAE: 1.268298864364624\n",
      "Epoch 444/2000, Train Loss: 5.365416990438908, Val Loss: 4.346934681145225, Val MAE: 1.2688770294189453\n",
      "Epoch 445/2000, Train Loss: 5.364658547780165, Val Loss: 4.346353713475813, Val MAE: 1.2693315744400024\n",
      "Epoch 446/2000, Train Loss: 5.3637792878661665, Val Loss: 4.345916641603305, Val MAE: 1.269845962524414\n",
      "Epoch 447/2000, Train Loss: 5.3629833949954735, Val Loss: 4.345446864568342, Val MAE: 1.2702724933624268\n",
      "Epoch 448/2000, Train Loss: 5.36213271596862, Val Loss: 4.344944817132837, Val MAE: 1.2709155082702637\n",
      "Epoch 449/2000, Train Loss: 5.361263028225954, Val Loss: 4.344420553309711, Val MAE: 1.27147376537323\n",
      "Epoch 450/2000, Train Loss: 5.360473604021111, Val Loss: 4.3439795583955885, Val MAE: 1.2719202041625977\n",
      "Epoch 451/2000, Train Loss: 5.359670424558415, Val Loss: 4.343499464852603, Val MAE: 1.2723771333694458\n",
      "Epoch 452/2000, Train Loss: 5.358862146431911, Val Loss: 4.343097648963215, Val MAE: 1.2727978229522705\n",
      "Epoch 453/2000, Train Loss: 5.358084905644932, Val Loss: 4.342582180579816, Val MAE: 1.2733253240585327\n",
      "Epoch 454/2000, Train Loss: 5.357232003797508, Val Loss: 4.342167039063034, Val MAE: 1.273794412612915\n",
      "Epoch 455/2000, Train Loss: 5.356472752958011, Val Loss: 4.341850129661598, Val MAE: 1.2742514610290527\n",
      "Epoch 456/2000, Train Loss: 5.355623212889383, Val Loss: 4.341406251994644, Val MAE: 1.2747890949249268\n",
      "Epoch 457/2000, Train Loss: 5.354644571566194, Val Loss: 4.340682089751161, Val MAE: 1.275343418121338\n",
      "Epoch 458/2000, Train Loss: 5.353856767453203, Val Loss: 4.34029192159495, Val MAE: 1.2757891416549683\n",
      "Epoch 459/2000, Train Loss: 5.353140989184541, Val Loss: 4.339808292041614, Val MAE: 1.2762320041656494\n",
      "Epoch 460/2000, Train Loss: 5.352302152375353, Val Loss: 4.339387325508388, Val MAE: 1.276761770248413\n",
      "Epoch 461/2000, Train Loss: 5.351520687159722, Val Loss: 4.339043132151206, Val MAE: 1.2772053480148315\n",
      "Epoch 462/2000, Train Loss: 5.350831165070693, Val Loss: 4.33860222554113, Val MAE: 1.2776296138763428\n",
      "Epoch 463/2000, Train Loss: 5.349991086774393, Val Loss: 4.338233681903111, Val MAE: 1.2781429290771484\n",
      "Epoch 464/2000, Train Loss: 5.34926823068668, Val Loss: 4.337788508234062, Val MAE: 1.2785719633102417\n",
      "Epoch 465/2000, Train Loss: 5.3484824224598855, Val Loss: 4.337411783928946, Val MAE: 1.279145359992981\n",
      "Epoch 466/2000, Train Loss: 5.347630173412721, Val Loss: 4.337077825534062, Val MAE: 1.2795723676681519\n",
      "Epoch 467/2000, Train Loss: 5.346839055924435, Val Loss: 4.33673603492459, Val MAE: 1.280029296875\n",
      "Epoch 468/2000, Train Loss: 5.345922263053573, Val Loss: 4.335896148292099, Val MAE: 1.2807215452194214\n",
      "Epoch 469/2000, Train Loss: 5.345025147964705, Val Loss: 4.335338524500216, Val MAE: 1.2813626527786255\n",
      "Epoch 470/2000, Train Loss: 5.34431998150643, Val Loss: 4.334938443528386, Val MAE: 1.281760334968567\n",
      "Epoch 471/2000, Train Loss: 5.343512254015238, Val Loss: 4.334536401798406, Val MAE: 1.2823182344436646\n",
      "Epoch 472/2000, Train Loss: 5.342815096294217, Val Loss: 4.334159940340387, Val MAE: 1.2828166484832764\n",
      "Epoch 473/2000, Train Loss: 5.342140982723624, Val Loss: 4.333812278227543, Val MAE: 1.2831782102584839\n",
      "Epoch 474/2000, Train Loss: 5.341495010228461, Val Loss: 4.333456317646297, Val MAE: 1.2837224006652832\n",
      "Epoch 475/2000, Train Loss: 5.34060451877651, Val Loss: 4.3330386877763925, Val MAE: 1.2841345071792603\n",
      "Epoch 476/2000, Train Loss: 5.339982132413494, Val Loss: 4.332647818561614, Val MAE: 1.2846357822418213\n",
      "Epoch 477/2000, Train Loss: 5.339131969660318, Val Loss: 4.332313194941348, Val MAE: 1.2850229740142822\n",
      "Epoch 478/2000, Train Loss: 5.338513840126376, Val Loss: 4.331961622623008, Val MAE: 1.2855490446090698\n",
      "Epoch 479/2000, Train Loss: 5.337721473892476, Val Loss: 4.331648293536479, Val MAE: 1.2860254049301147\n",
      "Epoch 480/2000, Train Loss: 5.337035738791022, Val Loss: 4.331365094527485, Val MAE: 1.286337971687317\n",
      "Epoch 481/2000, Train Loss: 5.336402379916513, Val Loss: 4.331032970219146, Val MAE: 1.286784291267395\n",
      "Epoch 482/2000, Train Loss: 5.3356273687183045, Val Loss: 4.330744346061091, Val MAE: 1.2871595621109009\n",
      "Epoch 483/2000, Train Loss: 5.334958904002018, Val Loss: 4.330442552092507, Val MAE: 1.2875217199325562\n",
      "Epoch 484/2000, Train Loss: 5.334342814203518, Val Loss: 4.330060371639222, Val MAE: 1.287997841835022\n",
      "Epoch 485/2000, Train Loss: 5.333772752469398, Val Loss: 4.329687073685991, Val MAE: 1.2885143756866455\n",
      "Epoch 486/2000, Train Loss: 5.332936180009932, Val Loss: 4.329399096824991, Val MAE: 1.2890853881835938\n",
      "Epoch 487/2000, Train Loss: 5.332225897904167, Val Loss: 4.329119334774693, Val MAE: 1.2894753217697144\n",
      "Epoch 488/2000, Train Loss: 5.331526160725422, Val Loss: 4.328843969576002, Val MAE: 1.289963722229004\n",
      "Epoch 489/2000, Train Loss: 5.3308989121761865, Val Loss: 4.328590385510227, Val MAE: 1.290260672569275\n",
      "Epoch 490/2000, Train Loss: 5.33023271147096, Val Loss: 4.328208932656003, Val MAE: 1.2907390594482422\n",
      "Epoch 491/2000, Train Loss: 5.32947716161639, Val Loss: 4.328030301829961, Val MAE: 1.2909986972808838\n",
      "Epoch 492/2000, Train Loss: 5.328880513412941, Val Loss: 4.327761368723366, Val MAE: 1.2915109395980835\n",
      "Epoch 493/2000, Train Loss: 5.328188628645733, Val Loss: 4.327374363743414, Val MAE: 1.2920281887054443\n",
      "Epoch 494/2000, Train Loss: 5.327453894725163, Val Loss: 4.327039926704459, Val MAE: 1.2923555374145508\n",
      "Epoch 495/2000, Train Loss: 5.326862354291505, Val Loss: 4.326850368235055, Val MAE: 1.292658805847168\n",
      "Epoch 496/2000, Train Loss: 5.326317311088925, Val Loss: 4.326656834957168, Val MAE: 1.2930172681808472\n",
      "Epoch 497/2000, Train Loss: 5.325590499680555, Val Loss: 4.326379353488524, Val MAE: 1.2933828830718994\n",
      "Epoch 498/2000, Train Loss: 5.324963123212838, Val Loss: 4.326119240390973, Val MAE: 1.2937757968902588\n",
      "Epoch 499/2000, Train Loss: 5.324517309423056, Val Loss: 4.325741279618008, Val MAE: 1.294168472290039\n",
      "Epoch 500/2000, Train Loss: 5.323810297485284, Val Loss: 4.325536291571114, Val MAE: 1.2944258451461792\n",
      "Epoch 501/2000, Train Loss: 5.323058155530829, Val Loss: 4.325229111712749, Val MAE: 1.2948698997497559\n",
      "Epoch 502/2000, Train Loss: 5.322288064652576, Val Loss: 4.3249505216211785, Val MAE: 1.29543936252594\n",
      "Epoch 503/2000, Train Loss: 5.321598321967727, Val Loss: 4.324645092590587, Val MAE: 1.2958251237869263\n",
      "Epoch 504/2000, Train Loss: 5.321001408058073, Val Loss: 4.324338453327577, Val MAE: 1.2961602210998535\n",
      "Epoch 505/2000, Train Loss: 5.320282234629588, Val Loss: 4.324009162727303, Val MAE: 1.2966723442077637\n",
      "Epoch 506/2000, Train Loss: 5.3196895501021615, Val Loss: 4.32379136341294, Val MAE: 1.2971032857894897\n",
      "Epoch 507/2000, Train Loss: 5.3190116200146225, Val Loss: 4.323641992647817, Val MAE: 1.2972700595855713\n",
      "Epoch 508/2000, Train Loss: 5.3184077341812115, Val Loss: 4.323409947002028, Val MAE: 1.2976194620132446\n",
      "Epoch 509/2000, Train Loss: 5.317727913526538, Val Loss: 4.323428562612046, Val MAE: 1.2981829643249512\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 510/2000, Train Loss: 5.317096696294632, Val Loss: 4.32327325609256, Val MAE: 1.298478126525879\n",
      "Epoch 511/2000, Train Loss: 5.316502501230227, Val Loss: 4.323063538980296, Val MAE: 1.2987449169158936\n",
      "Epoch 512/2000, Train Loss: 5.31595998145185, Val Loss: 4.3227639026529205, Val MAE: 1.2990838289260864\n",
      "Epoch 513/2000, Train Loss: 5.315410859872722, Val Loss: 4.3224539300822835, Val MAE: 1.2994911670684814\n",
      "Epoch 514/2000, Train Loss: 5.3148031959378415, Val Loss: 4.3222965309939045, Val MAE: 1.2997422218322754\n",
      "Epoch 515/2000, Train Loss: 5.314165869755532, Val Loss: 4.322062381257222, Val MAE: 1.300026535987854\n",
      "Epoch 516/2000, Train Loss: 5.313610564801069, Val Loss: 4.321846323829936, Val MAE: 1.3003900051116943\n",
      "Epoch 517/2000, Train Loss: 5.313003066436892, Val Loss: 4.321585607810284, Val MAE: 1.3007960319519043\n",
      "Epoch 518/2000, Train Loss: 5.312375289290698, Val Loss: 4.321320452793377, Val MAE: 1.3014723062515259\n",
      "Epoch 519/2000, Train Loss: 5.311681466639608, Val Loss: 4.320839722818277, Val MAE: 1.301947832107544\n",
      "Epoch 520/2000, Train Loss: 5.310987045839328, Val Loss: 4.320798452163306, Val MAE: 1.3022100925445557\n",
      "Epoch 521/2000, Train Loss: 5.310385094568882, Val Loss: 4.320515405333887, Val MAE: 1.302680492401123\n",
      "Epoch 522/2000, Train Loss: 5.309593267971334, Val Loss: 4.320782855035752, Val MAE: 1.3031237125396729\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 523/2000, Train Loss: 5.308996619330656, Val Loss: 4.320577210000181, Val MAE: 1.3034731149673462\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 524/2000, Train Loss: 5.308567292978191, Val Loss: 4.320426085121988, Val MAE: 1.3037141561508179\n",
      "Epoch 525/2000, Train Loss: 5.308044864089712, Val Loss: 4.320165889680855, Val MAE: 1.3044261932373047\n",
      "Epoch 526/2000, Train Loss: 5.307332052789841, Val Loss: 4.319963281314204, Val MAE: 1.3047442436218262\n",
      "Epoch 527/2000, Train Loss: 5.306838018761561, Val Loss: 4.3198270766049855, Val MAE: 1.3049654960632324\n",
      "Epoch 528/2000, Train Loss: 5.306245926115536, Val Loss: 4.319891357703471, Val MAE: 1.3053791522979736\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 529/2000, Train Loss: 5.306049808313921, Val Loss: 4.319806275686879, Val MAE: 1.3055624961853027\n",
      "Epoch 530/2000, Train Loss: 5.305232657061503, Val Loss: 4.319589195734872, Val MAE: 1.3058764934539795\n",
      "Epoch 531/2000, Train Loss: 5.304737489999068, Val Loss: 4.319353497004885, Val MAE: 1.3061808347702026\n",
      "Epoch 532/2000, Train Loss: 5.30413620556064, Val Loss: 4.319175205221327, Val MAE: 1.3064032793045044\n",
      "Epoch 533/2000, Train Loss: 5.303649190646499, Val Loss: 4.318975294293381, Val MAE: 1.3067291975021362\n",
      "Epoch 534/2000, Train Loss: 5.303152252893435, Val Loss: 4.31890515492657, Val MAE: 1.3068971633911133\n",
      "Epoch 535/2000, Train Loss: 5.30257808201753, Val Loss: 4.318771715520874, Val MAE: 1.307250738143921\n",
      "Epoch 536/2000, Train Loss: 5.302021598880618, Val Loss: 4.318524114588114, Val MAE: 1.307592749595642\n",
      "Epoch 537/2000, Train Loss: 5.301589660156958, Val Loss: 4.318321143032059, Val MAE: 1.3078089952468872\n",
      "Epoch 538/2000, Train Loss: 5.300889174359139, Val Loss: 4.318145324207666, Val MAE: 1.308190107345581\n",
      "Epoch 539/2000, Train Loss: 5.300283217203666, Val Loss: 4.317861658334732, Val MAE: 1.3082683086395264\n",
      "Epoch 540/2000, Train Loss: 5.300000295750645, Val Loss: 4.317697888143419, Val MAE: 1.3086525201797485\n",
      "Epoch 541/2000, Train Loss: 5.299400615886238, Val Loss: 4.317484875598292, Val MAE: 1.3088617324829102\n",
      "Epoch 542/2000, Train Loss: 5.298858717973553, Val Loss: 4.317254884646633, Val MAE: 1.3093032836914062\n",
      "Epoch 543/2000, Train Loss: 5.298312829663343, Val Loss: 4.317178213408613, Val MAE: 1.3095110654830933\n",
      "Epoch 544/2000, Train Loss: 5.297726044226614, Val Loss: 4.31692156472544, Val MAE: 1.309921383857727\n",
      "Epoch 545/2000, Train Loss: 5.297306492985347, Val Loss: 4.3166627287864685, Val MAE: 1.3102587461471558\n",
      "Epoch 546/2000, Train Loss: 5.296568873455922, Val Loss: 4.316380540404733, Val MAE: 1.3106865882873535\n",
      "Epoch 547/2000, Train Loss: 5.29602306830511, Val Loss: 4.316109752514231, Val MAE: 1.3110567331314087\n",
      "Epoch 548/2000, Train Loss: 5.2955848611031975, Val Loss: 4.315998710797528, Val MAE: 1.3112176656723022\n",
      "Epoch 549/2000, Train Loss: 5.294891961070544, Val Loss: 4.315746579911766, Val MAE: 1.311981201171875\n",
      "Epoch 550/2000, Train Loss: 5.294362186576812, Val Loss: 4.315596097097622, Val MAE: 1.312167763710022\n",
      "Epoch 551/2000, Train Loss: 5.293734936086585, Val Loss: 4.315431459682194, Val MAE: 1.312474012374878\n",
      "Epoch 552/2000, Train Loss: 5.293251394740277, Val Loss: 4.315382345220235, Val MAE: 1.3127720355987549\n",
      "Epoch 553/2000, Train Loss: 5.292647736366924, Val Loss: 4.315040119426457, Val MAE: 1.3133429288864136\n",
      "Epoch 554/2000, Train Loss: 5.292016214459851, Val Loss: 4.314858747701945, Val MAE: 1.3136459589004517\n",
      "Epoch 555/2000, Train Loss: 5.2915088436092415, Val Loss: 4.314716503469962, Val MAE: 1.313799500465393\n",
      "Epoch 556/2000, Train Loss: 5.291035332018193, Val Loss: 4.314553247004982, Val MAE: 1.3140881061553955\n",
      "Epoch 557/2000, Train Loss: 5.290496568181622, Val Loss: 4.314420248532858, Val MAE: 1.3142762184143066\n",
      "Epoch 558/2000, Train Loss: 5.2901475034613705, Val Loss: 4.314224512464418, Val MAE: 1.314526915550232\n",
      "Epoch 559/2000, Train Loss: 5.289545415775545, Val Loss: 4.314109454802641, Val MAE: 1.3146920204162598\n",
      "Epoch 560/2000, Train Loss: 5.288966090255386, Val Loss: 4.3139000001385455, Val MAE: 1.3149938583374023\n",
      "Epoch 561/2000, Train Loss: 5.288418055389435, Val Loss: 4.313704874337189, Val MAE: 1.3151522874832153\n",
      "Epoch 562/2000, Train Loss: 5.287956431892219, Val Loss: 4.313488540921624, Val MAE: 1.315443992614746\n",
      "Epoch 563/2000, Train Loss: 5.287416025935552, Val Loss: 4.313194560065983, Val MAE: 1.315964698791504\n",
      "Epoch 564/2000, Train Loss: 5.286826822133368, Val Loss: 4.313049727678299, Val MAE: 1.3161250352859497\n",
      "Epoch 565/2000, Train Loss: 5.28645869478927, Val Loss: 4.312880008755706, Val MAE: 1.3162270784378052\n",
      "Epoch 566/2000, Train Loss: 5.285766235038416, Val Loss: 4.312663569412832, Val MAE: 1.3168683052062988\n",
      "Epoch 567/2000, Train Loss: 5.2853019402502674, Val Loss: 4.312508491033644, Val MAE: 1.31705641746521\n",
      "Epoch 568/2000, Train Loss: 5.284701607221664, Val Loss: 4.312253982767346, Val MAE: 1.3174502849578857\n",
      "Epoch 569/2000, Train Loss: 5.284246030785497, Val Loss: 4.312077276443872, Val MAE: 1.3177165985107422\n",
      "Epoch 570/2000, Train Loss: 5.283722396459799, Val Loss: 4.3119331896774415, Val MAE: 1.3179736137390137\n",
      "Epoch 571/2000, Train Loss: 5.283281277122136, Val Loss: 4.311728744760273, Val MAE: 1.3182703256607056\n",
      "Epoch 572/2000, Train Loss: 5.282744162441756, Val Loss: 4.311688995642925, Val MAE: 1.3189908266067505\n",
      "Epoch 573/2000, Train Loss: 5.282270941352586, Val Loss: 4.311507625138666, Val MAE: 1.3192116022109985\n",
      "Epoch 574/2000, Train Loss: 5.281679770031001, Val Loss: 4.311298966407776, Val MAE: 1.3196979761123657\n",
      "Epoch 575/2000, Train Loss: 5.2811843653385155, Val Loss: 4.311173302733053, Val MAE: 1.3200621604919434\n",
      "Epoch 576/2000, Train Loss: 5.2806739263651, Val Loss: 4.3109597437494385, Val MAE: 1.3203657865524292\n",
      "Epoch 577/2000, Train Loss: 5.280173026338685, Val Loss: 4.311002223181912, Val MAE: 1.3205748796463013\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 578/2000, Train Loss: 5.2796984986077335, Val Loss: 4.31082103824991, Val MAE: 1.320858120918274\n",
      "Epoch 579/2000, Train Loss: 5.279262213645506, Val Loss: 4.3107133931531685, Val MAE: 1.3210614919662476\n",
      "Epoch 580/2000, Train Loss: 5.278821190346533, Val Loss: 4.3106108659834375, Val MAE: 1.3211606740951538\n",
      "Epoch 581/2000, Train Loss: 5.278333744815117, Val Loss: 4.310512394276191, Val MAE: 1.3213496208190918\n",
      "Epoch 582/2000, Train Loss: 5.277887035176421, Val Loss: 4.3103065467725585, Val MAE: 1.32149076461792\n",
      "Epoch 583/2000, Train Loss: 5.277364728246971, Val Loss: 4.310155431820652, Val MAE: 1.3217930793762207\n",
      "Epoch 584/2000, Train Loss: 5.276734922859651, Val Loss: 4.309876233245444, Val MAE: 1.3218494653701782\n",
      "Epoch 585/2000, Train Loss: 5.276391812224887, Val Loss: 4.3096289882040395, Val MAE: 1.3220832347869873\n",
      "Epoch 586/2000, Train Loss: 5.275915546365477, Val Loss: 4.309588559593742, Val MAE: 1.3221769332885742\n",
      "Epoch 587/2000, Train Loss: 5.275377419134008, Val Loss: 4.309405077911737, Val MAE: 1.3223440647125244\n",
      "Epoch 588/2000, Train Loss: 5.274994231305581, Val Loss: 4.309304792627575, Val MAE: 1.3224246501922607\n",
      "Epoch 589/2000, Train Loss: 5.27455985562287, Val Loss: 4.309163670136234, Val MAE: 1.3226311206817627\n",
      "Epoch 590/2000, Train Loss: 5.2740363428113906, Val Loss: 4.309054921180245, Val MAE: 1.322700023651123\n",
      "Epoch 591/2000, Train Loss: 5.273631762293657, Val Loss: 4.308962654738914, Val MAE: 1.3228727579116821\n",
      "Epoch 592/2000, Train Loss: 5.273237179706992, Val Loss: 4.308852487614774, Val MAE: 1.323104739189148\n",
      "Epoch 593/2000, Train Loss: 5.272763219099679, Val Loss: 4.308712283973619, Val MAE: 1.3233158588409424\n",
      "Epoch 594/2000, Train Loss: 5.2723089933718885, Val Loss: 4.308668712321229, Val MAE: 1.3234930038452148\n",
      "Epoch 595/2000, Train Loss: 5.271918389946222, Val Loss: 4.308497106465768, Val MAE: 1.3235799074172974\n",
      "Epoch 596/2000, Train Loss: 5.271447900515883, Val Loss: 4.308395330267628, Val MAE: 1.323793888092041\n",
      "Epoch 597/2000, Train Loss: 5.271085312461109, Val Loss: 4.308350160131305, Val MAE: 1.3238707780838013\n",
      "Epoch 598/2000, Train Loss: 5.270624969513478, Val Loss: 4.30821038174817, Val MAE: 1.324018120765686\n",
      "Epoch 599/2000, Train Loss: 5.2701504427833505, Val Loss: 4.308186297998653, Val MAE: 1.3242502212524414\n",
      "Epoch 600/2000, Train Loss: 5.269640720683166, Val Loss: 4.308058862657997, Val MAE: 1.324442982673645\n",
      "Epoch 601/2000, Train Loss: 5.269282872835364, Val Loss: 4.3079834424135255, Val MAE: 1.324561357498169\n",
      "Epoch 602/2000, Train Loss: 5.268755476380948, Val Loss: 4.307776299330193, Val MAE: 1.3251630067825317\n",
      "Epoch 603/2000, Train Loss: 5.268182474043185, Val Loss: 4.307740050648141, Val MAE: 1.3250950574874878\n",
      "Epoch 604/2000, Train Loss: 5.26778867640602, Val Loss: 4.307571865395298, Val MAE: 1.3252859115600586\n",
      "Epoch 605/2000, Train Loss: 5.267279080682645, Val Loss: 4.307414990288066, Val MAE: 1.3253933191299438\n",
      "Epoch 606/2000, Train Loss: 5.266848218651283, Val Loss: 4.307277306562334, Val MAE: 1.3255940675735474\n",
      "Epoch 607/2000, Train Loss: 5.266202122013475, Val Loss: 4.307185895987383, Val MAE: 1.3261398077011108\n",
      "Epoch 608/2000, Train Loss: 5.2656874216314575, Val Loss: 4.307121198712371, Val MAE: 1.3263282775878906\n",
      "Epoch 609/2000, Train Loss: 5.265333352188404, Val Loss: 4.3069255799289765, Val MAE: 1.326484203338623\n",
      "Epoch 610/2000, Train Loss: 5.264836235318126, Val Loss: 4.3068981371057315, Val MAE: 1.326716661453247\n",
      "Epoch 611/2000, Train Loss: 5.264360937690476, Val Loss: 4.306744480883982, Val MAE: 1.3267968893051147\n",
      "Epoch 612/2000, Train Loss: 5.263747358904442, Val Loss: 4.306773135652692, Val MAE: 1.326945185661316\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 613/2000, Train Loss: 5.263272024135091, Val Loss: 4.306615726919625, Val MAE: 1.3271965980529785\n",
      "Epoch 614/2000, Train Loss: 5.262838847601592, Val Loss: 4.306459549139804, Val MAE: 1.327311396598816\n",
      "Epoch 615/2000, Train Loss: 5.262384684763981, Val Loss: 4.306405428920205, Val MAE: 1.3273429870605469\n",
      "Epoch 616/2000, Train Loss: 5.262003059141348, Val Loss: 4.30629256226885, Val MAE: 1.3275128602981567\n",
      "Epoch 617/2000, Train Loss: 5.26164074151131, Val Loss: 4.306188648939132, Val MAE: 1.3274779319763184\n",
      "Epoch 618/2000, Train Loss: 5.261183275134787, Val Loss: 4.306141531091975, Val MAE: 1.3277071714401245\n",
      "Epoch 619/2000, Train Loss: 5.260835657934968, Val Loss: 4.306005179459654, Val MAE: 1.3278498649597168\n",
      "Epoch 620/2000, Train Loss: 5.2601902326379415, Val Loss: 4.305989451765075, Val MAE: 1.3279722929000854\n",
      "Epoch 621/2000, Train Loss: 5.2597237448517635, Val Loss: 4.306057628496426, Val MAE: 1.3281629085540771\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 622/2000, Train Loss: 5.259072594920848, Val Loss: 4.305768556388345, Val MAE: 1.3283663988113403\n",
      "Epoch 623/2000, Train Loss: 5.2588149393268, Val Loss: 4.305710271024329, Val MAE: 1.3286142349243164\n",
      "Epoch 624/2000, Train Loss: 5.258281050818584, Val Loss: 4.305528644830223, Val MAE: 1.3287255764007568\n",
      "Epoch 625/2000, Train Loss: 5.257982081279987, Val Loss: 4.30548347722827, Val MAE: 1.3288829326629639\n",
      "Epoch 626/2000, Train Loss: 5.257523309909505, Val Loss: 4.305550299103804, Val MAE: 1.3291016817092896\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 627/2000, Train Loss: 5.2570863947717115, Val Loss: 4.3055350564126895, Val MAE: 1.329247236251831\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 628/2000, Train Loss: 5.256710643522451, Val Loss: 4.30535372406479, Val MAE: 1.3293209075927734\n",
      "Epoch 629/2000, Train Loss: 5.25622253539483, Val Loss: 4.305298406965151, Val MAE: 1.329660415649414\n",
      "Epoch 630/2000, Train Loss: 5.255825205767979, Val Loss: 4.305189459295724, Val MAE: 1.3297336101531982\n",
      "Epoch 631/2000, Train Loss: 5.255347997435398, Val Loss: 4.305062022500151, Val MAE: 1.3298919200897217\n",
      "Epoch 632/2000, Train Loss: 5.254891075952083, Val Loss: 4.304970938248897, Val MAE: 1.3300625085830688\n",
      "Epoch 633/2000, Train Loss: 5.254517657462421, Val Loss: 4.304922369896897, Val MAE: 1.3301146030426025\n",
      "Epoch 634/2000, Train Loss: 5.254180569176435, Val Loss: 4.3048387487572946, Val MAE: 1.330276608467102\n",
      "Epoch 635/2000, Train Loss: 5.253567023306398, Val Loss: 4.30474069761479, Val MAE: 1.3307785987854004\n",
      "Epoch 636/2000, Train Loss: 5.253153008938321, Val Loss: 4.304701201793716, Val MAE: 1.3307639360427856\n",
      "Epoch 637/2000, Train Loss: 5.252817430279472, Val Loss: 4.304595645816307, Val MAE: 1.3308773040771484\n",
      "Epoch 638/2000, Train Loss: 5.252346853105152, Val Loss: 4.304405895462186, Val MAE: 1.3309860229492188\n",
      "Epoch 639/2000, Train Loss: 5.251926518521768, Val Loss: 4.304406593587455, Val MAE: 1.3310574293136597\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 640/2000, Train Loss: 5.251442895006778, Val Loss: 4.304309361469088, Val MAE: 1.3311738967895508\n",
      "Epoch 641/2000, Train Loss: 5.251095535311964, Val Loss: 4.3042225873376445, Val MAE: 1.331189513206482\n",
      "Epoch 642/2000, Train Loss: 5.250635128897922, Val Loss: 4.304152477256895, Val MAE: 1.3312628269195557\n",
      "Epoch 643/2000, Train Loss: 5.250210844839606, Val Loss: 4.304085131329814, Val MAE: 1.3313474655151367\n",
      "Epoch 644/2000, Train Loss: 5.249770140186903, Val Loss: 4.303979216832814, Val MAE: 1.3314002752304077\n",
      "Epoch 645/2000, Train Loss: 5.249378973625732, Val Loss: 4.303851200323405, Val MAE: 1.3314306735992432\n",
      "Epoch 646/2000, Train Loss: 5.248927288899751, Val Loss: 4.303778591681653, Val MAE: 1.331604242324829\n",
      "Epoch 647/2000, Train Loss: 5.24849811046709, Val Loss: 4.303647306960399, Val MAE: 1.3318405151367188\n",
      "Epoch 648/2000, Train Loss: 5.247993913725565, Val Loss: 4.303266882802558, Val MAE: 1.331685185432434\n",
      "Epoch 649/2000, Train Loss: 5.247715809573312, Val Loss: 4.30314966106978, Val MAE: 1.3317008018493652\n",
      "Epoch 650/2000, Train Loss: 5.24724148037961, Val Loss: 4.303078654244191, Val MAE: 1.3318476676940918\n",
      "Epoch 651/2000, Train Loss: 5.246834713859506, Val Loss: 4.3029740550855955, Val MAE: 1.3319345712661743\n",
      "Epoch 652/2000, Train Loss: 5.246399501609026, Val Loss: 4.302855506187349, Val MAE: 1.3322150707244873\n",
      "Epoch 653/2000, Train Loss: 5.246031037502728, Val Loss: 4.3028445873204175, Val MAE: 1.3323901891708374\n",
      "Epoch 654/2000, Train Loss: 5.24549712513648, Val Loss: 4.302781978226084, Val MAE: 1.3325108289718628\n",
      "Epoch 655/2000, Train Loss: 5.245063319782066, Val Loss: 4.302753565865239, Val MAE: 1.332614541053772\n",
      "Epoch 656/2000, Train Loss: 5.244603696962872, Val Loss: 4.302671700809884, Val MAE: 1.3328287601470947\n",
      "Epoch 657/2000, Train Loss: 5.244217633877551, Val Loss: 4.302581194582887, Val MAE: 1.3330098390579224\n",
      "Epoch 658/2000, Train Loss: 5.24365866574455, Val Loss: 4.302506399811723, Val MAE: 1.3330284357070923\n",
      "Epoch 659/2000, Train Loss: 5.2432930773197395, Val Loss: 4.3024410439288525, Val MAE: 1.333151936531067\n",
      "Epoch 660/2000, Train Loss: 5.242845371911451, Val Loss: 4.302316346975762, Val MAE: 1.3330838680267334\n",
      "Epoch 661/2000, Train Loss: 5.242468042968895, Val Loss: 4.30221947947825, Val MAE: 1.3333380222320557\n",
      "Epoch 662/2000, Train Loss: 5.241965058801779, Val Loss: 4.302073984962749, Val MAE: 1.333266258239746\n",
      "Epoch 663/2000, Train Loss: 5.241519829634895, Val Loss: 4.3020592646805325, Val MAE: 1.3334041833877563\n",
      "Epoch 664/2000, Train Loss: 5.241077626252919, Val Loss: 4.302098609518817, Val MAE: 1.333326816558838\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 665/2000, Train Loss: 5.240709403638439, Val Loss: 4.30203257034144, Val MAE: 1.3335613012313843\n",
      "Epoch 666/2000, Train Loss: 5.240262683769094, Val Loss: 4.301880676211335, Val MAE: 1.3334944248199463\n",
      "Epoch 667/2000, Train Loss: 5.239861899185699, Val Loss: 4.3018180890815465, Val MAE: 1.333469271659851\n",
      "Epoch 668/2000, Train Loss: 5.239431883295647, Val Loss: 4.3017596524531445, Val MAE: 1.3335660696029663\n",
      "Epoch 669/2000, Train Loss: 5.238902794619267, Val Loss: 4.301757062607863, Val MAE: 1.3338232040405273\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 670/2000, Train Loss: 5.238362265086077, Val Loss: 4.301704032871667, Val MAE: 1.3342317342758179\n",
      "Epoch 671/2000, Train Loss: 5.237971455950588, Val Loss: 4.301701438990165, Val MAE: 1.3342294692993164\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 672/2000, Train Loss: 5.237459724235729, Val Loss: 4.301652644562909, Val MAE: 1.3342294692993164\n",
      "Epoch 673/2000, Train Loss: 5.237057594140155, Val Loss: 4.301527221043279, Val MAE: 1.334180235862732\n",
      "Epoch 674/2000, Train Loss: 5.236665261163802, Val Loss: 4.301395499847066, Val MAE: 1.3341214656829834\n",
      "Epoch 675/2000, Train Loss: 5.236251196703966, Val Loss: 4.301420755789975, Val MAE: 1.3343007564544678\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 676/2000, Train Loss: 5.235841167506887, Val Loss: 4.301315586820362, Val MAE: 1.334285855293274\n",
      "Epoch 677/2000, Train Loss: 5.235364986666992, Val Loss: 4.30124512441515, Val MAE: 1.3343242406845093\n",
      "Epoch 678/2000, Train Loss: 5.23498530239069, Val Loss: 4.301193375521757, Val MAE: 1.3344120979309082\n",
      "Epoch 679/2000, Train Loss: 5.234552688805672, Val Loss: 4.301127261392714, Val MAE: 1.3345237970352173\n",
      "Epoch 680/2000, Train Loss: 5.234081604536951, Val Loss: 4.301038806316421, Val MAE: 1.33446204662323\n",
      "Epoch 681/2000, Train Loss: 5.233676974938391, Val Loss: 4.300992874085434, Val MAE: 1.3345954418182373\n",
      "Epoch 682/2000, Train Loss: 5.233276311060467, Val Loss: 4.300912818336111, Val MAE: 1.3346666097640991\n",
      "Epoch 683/2000, Train Loss: 5.2328496606896655, Val Loss: 4.300746316891017, Val MAE: 1.334713101387024\n",
      "Epoch 684/2000, Train Loss: 5.23248878974604, Val Loss: 4.300743279306907, Val MAE: 1.3349159955978394\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 685/2000, Train Loss: 5.232155656717525, Val Loss: 4.300633133396389, Val MAE: 1.3348709344863892\n",
      "Epoch 686/2000, Train Loss: 5.231613551260173, Val Loss: 4.300583682379385, Val MAE: 1.3348597288131714\n",
      "Epoch 687/2000, Train Loss: 5.231192056974207, Val Loss: 4.30045033313158, Val MAE: 1.3348169326782227\n",
      "Epoch 688/2000, Train Loss: 5.230780186776067, Val Loss: 4.30032419717218, Val MAE: 1.3349577188491821\n",
      "Epoch 689/2000, Train Loss: 5.230512430742282, Val Loss: 4.300353160946388, Val MAE: 1.335064172744751\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 690/2000, Train Loss: 5.229968400952612, Val Loss: 4.300163311704876, Val MAE: 1.3350814580917358\n",
      "Epoch 691/2000, Train Loss: 5.229567575648812, Val Loss: 4.3002466383881455, Val MAE: 1.3360366821289062\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 692/2000, Train Loss: 5.229028999401077, Val Loss: 4.300231370728786, Val MAE: 1.3367100954055786\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 693/2000, Train Loss: 5.228683599946942, Val Loss: 4.3002460321103495, Val MAE: 1.3367509841918945\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 694/2000, Train Loss: 5.228505137049197, Val Loss: 4.300064654566172, Val MAE: 1.3369511365890503\n",
      "Epoch 695/2000, Train Loss: 5.228016893284615, Val Loss: 4.300153630784178, Val MAE: 1.3368929624557495\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 696/2000, Train Loss: 5.227532801586058, Val Loss: 4.299933233552092, Val MAE: 1.3369840383529663\n",
      "Epoch 697/2000, Train Loss: 5.227069164389994, Val Loss: 4.299877793206943, Val MAE: 1.3368979692459106\n",
      "Epoch 698/2000, Train Loss: 5.226693699447239, Val Loss: 4.29981406274743, Val MAE: 1.3372085094451904\n",
      "Epoch 699/2000, Train Loss: 5.226216934024883, Val Loss: 4.299764143575834, Val MAE: 1.337459683418274\n",
      "Epoch 700/2000, Train Loss: 5.225830222728941, Val Loss: 4.299666359997171, Val MAE: 1.337440848350525\n",
      "Epoch 701/2000, Train Loss: 5.225382284684614, Val Loss: 4.2996394040077694, Val MAE: 1.3374899625778198\n",
      "Epoch 702/2000, Train Loss: 5.225017757234612, Val Loss: 4.2996176107192605, Val MAE: 1.3375340700149536\n",
      "Epoch 703/2000, Train Loss: 5.22462821233224, Val Loss: 4.299576155219491, Val MAE: 1.3378174304962158\n",
      "Epoch 704/2000, Train Loss: 5.2240917497242485, Val Loss: 4.299404514070571, Val MAE: 1.3379498720169067\n",
      "Epoch 705/2000, Train Loss: 5.223823934025162, Val Loss: 4.299465525197232, Val MAE: 1.3381102085113525\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 706/2000, Train Loss: 5.223348265263572, Val Loss: 4.299325779901714, Val MAE: 1.3379647731781006\n",
      "Epoch 707/2000, Train Loss: 5.222998597081675, Val Loss: 4.299304265675582, Val MAE: 1.338114619255066\n",
      "Epoch 708/2000, Train Loss: 5.22274393383085, Val Loss: 4.299393002583286, Val MAE: 1.3380292654037476\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 709/2000, Train Loss: 5.222167494177333, Val Loss: 4.2992615657059225, Val MAE: 1.3381556272506714\n",
      "Epoch 710/2000, Train Loss: 5.221731727449995, Val Loss: 4.299196057056817, Val MAE: 1.3381627798080444\n",
      "Epoch 711/2000, Train Loss: 5.221311522630047, Val Loss: 4.299155394204958, Val MAE: 1.3383921384811401\n",
      "Epoch 712/2000, Train Loss: 5.220823106804567, Val Loss: 4.299283699444898, Val MAE: 1.3389302492141724\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 713/2000, Train Loss: 5.220583301062823, Val Loss: 4.299382049690081, Val MAE: 1.338871955871582\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 714/2000, Train Loss: 5.220075169666651, Val Loss: 4.299329147423347, Val MAE: 1.3388640880584717\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 715/2000, Train Loss: 5.219654396398743, Val Loss: 4.299265311036523, Val MAE: 1.3389930725097656\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 716/2000, Train Loss: 5.219247769225728, Val Loss: 4.29918199505393, Val MAE: 1.3392618894577026\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 717/2000, Train Loss: 5.2188230207849555, Val Loss: 4.299116260517301, Val MAE: 1.3392868041992188\n",
      "Epoch 718/2000, Train Loss: 5.218523023571703, Val Loss: 4.299028770050665, Val MAE: 1.339284896850586\n",
      "Epoch 719/2000, Train Loss: 5.218080071310337, Val Loss: 4.2989028896403125, Val MAE: 1.339524269104004\n",
      "Epoch 720/2000, Train Loss: 5.217787944255561, Val Loss: 4.298844582781078, Val MAE: 1.33951735496521\n",
      "Epoch 721/2000, Train Loss: 5.217349416041794, Val Loss: 4.298745704495062, Val MAE: 1.3395845890045166\n",
      "Epoch 722/2000, Train Loss: 5.216985394220339, Val Loss: 4.298756789691805, Val MAE: 1.3397895097732544\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 723/2000, Train Loss: 5.216651178926755, Val Loss: 4.29865092958991, Val MAE: 1.3398157358169556\n",
      "Epoch 724/2000, Train Loss: 5.2161584673613675, Val Loss: 4.298734966319377, Val MAE: 1.3399500846862793\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 725/2000, Train Loss: 5.215908722120515, Val Loss: 4.29876005738739, Val MAE: 1.3399596214294434\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 726/2000, Train Loss: 5.215479412751735, Val Loss: 4.298757532590956, Val MAE: 1.3403278589248657\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 727/2000, Train Loss: 5.215056281562091, Val Loss: 4.298754109782497, Val MAE: 1.3402435779571533\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 728/2000, Train Loss: 5.214749552218529, Val Loss: 4.298662901769473, Val MAE: 1.34023916721344\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 729/2000, Train Loss: 5.214296954619512, Val Loss: 4.298615909561398, Val MAE: 1.340293049812317\n",
      "Epoch 730/2000, Train Loss: 5.213956757085941, Val Loss: 4.29854443566067, Val MAE: 1.3402069807052612\n",
      "Epoch 731/2000, Train Loss: 5.213566600467651, Val Loss: 4.298435686047622, Val MAE: 1.3400574922561646\n",
      "Epoch 732/2000, Train Loss: 5.21328442682243, Val Loss: 4.298452023211427, Val MAE: 1.3401910066604614\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 733/2000, Train Loss: 5.212907624171903, Val Loss: 4.298423640559038, Val MAE: 1.3401349782943726\n",
      "Epoch 734/2000, Train Loss: 5.212528461678866, Val Loss: 4.298344648400629, Val MAE: 1.340301513671875\n",
      "Epoch 735/2000, Train Loss: 5.2120492920946715, Val Loss: 4.2982979786208295, Val MAE: 1.3406333923339844\n",
      "Epoch 736/2000, Train Loss: 5.21157689463332, Val Loss: 4.2983704259545785, Val MAE: 1.3406956195831299\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 737/2000, Train Loss: 5.211149069355495, Val Loss: 4.298286909381235, Val MAE: 1.34071946144104\n",
      "Epoch 738/2000, Train Loss: 5.2108214405369075, Val Loss: 4.298284764834277, Val MAE: 1.3407148122787476\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 739/2000, Train Loss: 5.210418360204192, Val Loss: 4.298203717395077, Val MAE: 1.3407230377197266\n",
      "Epoch 740/2000, Train Loss: 5.2101088301297604, Val Loss: 4.298155654650035, Val MAE: 1.3407976627349854\n",
      "Epoch 741/2000, Train Loss: 5.209654665704983, Val Loss: 4.298110890106892, Val MAE: 1.340722680091858\n",
      "Epoch 742/2000, Train Loss: 5.209454174119387, Val Loss: 4.298080757189924, Val MAE: 1.3407434225082397\n",
      "Epoch 743/2000, Train Loss: 5.209002571481215, Val Loss: 4.297863253176682, Val MAE: 1.3406109809875488\n",
      "Epoch 744/2000, Train Loss: 5.208622281130004, Val Loss: 4.297810433089264, Val MAE: 1.3407009840011597\n",
      "Epoch 745/2000, Train Loss: 5.208355089702632, Val Loss: 4.297748833143805, Val MAE: 1.3405919075012207\n",
      "Epoch 746/2000, Train Loss: 5.207895121063274, Val Loss: 4.297723903759258, Val MAE: 1.3410273790359497\n",
      "Epoch 747/2000, Train Loss: 5.2073351777230705, Val Loss: 4.297730427370297, Val MAE: 1.3410899639129639\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 748/2000, Train Loss: 5.2070771389447525, Val Loss: 4.297667717745924, Val MAE: 1.3410389423370361\n",
      "Epoch 749/2000, Train Loss: 5.206518891708498, Val Loss: 4.2976722086508445, Val MAE: 1.341196894645691\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 750/2000, Train Loss: 5.206245172945745, Val Loss: 4.297588268837591, Val MAE: 1.341168999671936\n",
      "Epoch 751/2000, Train Loss: 5.205876284409087, Val Loss: 4.297400376458806, Val MAE: 1.3411777019500732\n",
      "Epoch 752/2000, Train Loss: 5.205558127269978, Val Loss: 4.297465117850642, Val MAE: 1.3412377834320068\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 753/2000, Train Loss: 5.205105611394833, Val Loss: 4.2975809582105775, Val MAE: 1.342002034187317\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 754/2000, Train Loss: 5.204812498154116, Val Loss: 4.297482196317883, Val MAE: 1.3419023752212524\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 755/2000, Train Loss: 5.204481494798751, Val Loss: 4.2974956437358705, Val MAE: 1.3419266939163208\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 756/2000, Train Loss: 5.204108582422886, Val Loss: 4.297428924455417, Val MAE: 1.3419222831726074\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 757/2000, Train Loss: 5.203823645370463, Val Loss: 4.297409604431137, Val MAE: 1.3420021533966064\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 758/2000, Train Loss: 5.203335686972216, Val Loss: 4.29739871874569, Val MAE: 1.3420263528823853\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 759/2000, Train Loss: 5.202982812877265, Val Loss: 4.297296656914583, Val MAE: 1.34183669090271\n",
      "Epoch 760/2000, Train Loss: 5.20267437256903, Val Loss: 4.2972982875005465, Val MAE: 1.3419216871261597\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 761/2000, Train Loss: 5.202287612161934, Val Loss: 4.297247172059037, Val MAE: 1.3418654203414917\n",
      "Epoch 762/2000, Train Loss: 5.2019868118630335, Val Loss: 4.297171017879577, Val MAE: 1.3419697284698486\n",
      "Epoch 763/2000, Train Loss: 5.201672084295928, Val Loss: 4.297091536493752, Val MAE: 1.3419758081436157\n",
      "Epoch 764/2000, Train Loss: 5.201274003232027, Val Loss: 4.297111936347691, Val MAE: 1.3418322801589966\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 765/2000, Train Loss: 5.200882599829334, Val Loss: 4.29710005197938, Val MAE: 1.3420928716659546\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 766/2000, Train Loss: 5.200570517996758, Val Loss: 4.297005974824034, Val MAE: 1.34213125705719\n",
      "Epoch 767/2000, Train Loss: 5.200171916261619, Val Loss: 4.297027455493221, Val MAE: 1.3420536518096924\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 768/2000, Train Loss: 5.199811141922807, Val Loss: 4.296949375192011, Val MAE: 1.3420677185058594\n",
      "Epoch 769/2000, Train Loss: 5.199508460276486, Val Loss: 4.296871645244088, Val MAE: 1.3419768810272217\n",
      "Epoch 770/2000, Train Loss: 5.199126191831509, Val Loss: 4.296800579189315, Val MAE: 1.3419440984725952\n",
      "Epoch 771/2000, Train Loss: 5.198634092649256, Val Loss: 4.296867385201566, Val MAE: 1.3421335220336914\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 772/2000, Train Loss: 5.198518874687288, Val Loss: 4.297139295348971, Val MAE: 1.3422573804855347\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 773/2000, Train Loss: 5.198011071662567, Val Loss: 4.297017414973476, Val MAE: 1.3423595428466797\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 774/2000, Train Loss: 5.197635883715615, Val Loss: 4.296983402967453, Val MAE: 1.3425859212875366\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 775/2000, Train Loss: 5.19727192004306, Val Loss: 4.296980738733697, Val MAE: 1.342616319656372\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 776/2000, Train Loss: 5.196927945520047, Val Loss: 4.296863938081922, Val MAE: 1.3424880504608154\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 777/2000, Train Loss: 5.196614595066903, Val Loss: 4.2968987373385845, Val MAE: 1.3425397872924805\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 778/2000, Train Loss: 5.196217418202228, Val Loss: 4.2968406770642344, Val MAE: 1.3424735069274902\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 779/2000, Train Loss: 5.195868907175361, Val Loss: 4.296717536027037, Val MAE: 1.342342734336853\n",
      "Epoch 780/2000, Train Loss: 5.1955293399184495, Val Loss: 4.296671597507056, Val MAE: 1.342333436012268\n",
      "Epoch 781/2000, Train Loss: 5.1952578173280575, Val Loss: 4.296626140048184, Val MAE: 1.3421971797943115\n",
      "Epoch 782/2000, Train Loss: 5.1948815867244145, Val Loss: 4.2965988115062865, Val MAE: 1.342079520225525\n",
      "Epoch 783/2000, Train Loss: 5.194601363291411, Val Loss: 4.296418359382884, Val MAE: 1.342126727104187\n",
      "Epoch 784/2000, Train Loss: 5.194135123401314, Val Loss: 4.296410257422079, Val MAE: 1.3420639038085938\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 785/2000, Train Loss: 5.193833107864194, Val Loss: 4.296416647274663, Val MAE: 1.342093825340271\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 786/2000, Train Loss: 5.193370703602743, Val Loss: 4.296284106021791, Val MAE: 1.341945767402649\n",
      "Epoch 787/2000, Train Loss: 5.19308142746158, Val Loss: 4.296359707520702, Val MAE: 1.3418482542037964\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 788/2000, Train Loss: 5.192561592143458, Val Loss: 4.29629684639728, Val MAE: 1.3424267768859863\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 789/2000, Train Loss: 5.192161905797883, Val Loss: 4.296163976661802, Val MAE: 1.3424338102340698\n",
      "Epoch 790/2000, Train Loss: 5.191844420120868, Val Loss: 4.296099959067472, Val MAE: 1.3423867225646973\n",
      "Epoch 791/2000, Train Loss: 5.1914477461552195, Val Loss: 4.296128052097606, Val MAE: 1.3422691822052002\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 792/2000, Train Loss: 5.191318372858428, Val Loss: 4.295994193394353, Val MAE: 1.3421415090560913\n",
      "Epoch 793/2000, Train Loss: 5.190903940711934, Val Loss: 4.296081107432448, Val MAE: 1.3422223329544067\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 794/2000, Train Loss: 5.190362967338459, Val Loss: 4.296032455587012, Val MAE: 1.3423924446105957\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 795/2000, Train Loss: 5.190076888092823, Val Loss: 4.295893115509213, Val MAE: 1.3422693014144897\n",
      "Epoch 796/2000, Train Loss: 5.189750564344015, Val Loss: 4.295903001667008, Val MAE: 1.3425025939941406\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 797/2000, Train Loss: 5.189328893715847, Val Loss: 4.295899239112073, Val MAE: 1.3423200845718384\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 798/2000, Train Loss: 5.188906211057221, Val Loss: 4.295851868344104, Val MAE: 1.3426388502120972\n",
      "Epoch 799/2000, Train Loss: 5.1886593458448695, Val Loss: 4.295845806504798, Val MAE: 1.3425649404525757\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 800/2000, Train Loss: 5.188264968113789, Val Loss: 4.295792882648978, Val MAE: 1.3425450325012207\n",
      "Epoch 801/2000, Train Loss: 5.18795101548794, Val Loss: 4.295801495238552, Val MAE: 1.3425145149230957\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 802/2000, Train Loss: 5.1875664466433475, Val Loss: 4.295698953988984, Val MAE: 1.3425270318984985\n",
      "Epoch 803/2000, Train Loss: 5.187229916716205, Val Loss: 4.29558623212529, Val MAE: 1.3426460027694702\n",
      "Epoch 804/2000, Train Loss: 5.186850489237253, Val Loss: 4.295479831592305, Val MAE: 1.342446208000183\n",
      "Epoch 805/2000, Train Loss: 5.186407678638012, Val Loss: 4.295294746169894, Val MAE: 1.342560052871704\n",
      "Epoch 806/2000, Train Loss: 5.186187215220168, Val Loss: 4.295127873768018, Val MAE: 1.3424041271209717\n",
      "Epoch 807/2000, Train Loss: 5.185755138804825, Val Loss: 4.295065250359182, Val MAE: 1.3423000574111938\n",
      "Epoch 808/2000, Train Loss: 5.185452719232929, Val Loss: 4.295089490066363, Val MAE: 1.3422961235046387\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 809/2000, Train Loss: 5.18516160675111, Val Loss: 4.294988692839315, Val MAE: 1.3422346115112305\n",
      "Epoch 810/2000, Train Loss: 5.1847878265898455, Val Loss: 4.294891610812014, Val MAE: 1.3420978784561157\n",
      "Epoch 811/2000, Train Loss: 5.184588143751126, Val Loss: 4.294918700231342, Val MAE: 1.3419678211212158\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 812/2000, Train Loss: 5.184133420805757, Val Loss: 4.294923455274011, Val MAE: 1.3418163061141968\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 813/2000, Train Loss: 5.183819219409368, Val Loss: 4.294815384356055, Val MAE: 1.3418934345245361\n",
      "Epoch 814/2000, Train Loss: 5.183480273593556, Val Loss: 4.294889069260575, Val MAE: 1.3418487310409546\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 815/2000, Train Loss: 5.183156578964585, Val Loss: 4.294757046990507, Val MAE: 1.3415887355804443\n",
      "Epoch 816/2000, Train Loss: 5.182812050207337, Val Loss: 4.2948693190972635, Val MAE: 1.3418620824813843\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 817/2000, Train Loss: 5.182322120407705, Val Loss: 4.294879629902952, Val MAE: 1.341896891593933\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 818/2000, Train Loss: 5.182043590652409, Val Loss: 4.294848045589417, Val MAE: 1.3418024778366089\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 819/2000, Train Loss: 5.181718174120465, Val Loss: 4.294824446186306, Val MAE: 1.3418055772781372\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 820/2000, Train Loss: 5.1814587248875945, Val Loss: 4.294746884586305, Val MAE: 1.3417187929153442\n",
      "Epoch 821/2000, Train Loss: 5.181082138051171, Val Loss: 4.294789313331363, Val MAE: 1.341566562652588\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 822/2000, Train Loss: 5.180834160367055, Val Loss: 4.294845187710965, Val MAE: 1.342047095298767\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 823/2000, Train Loss: 5.180459664343494, Val Loss: 4.29477119854116, Val MAE: 1.3417683839797974\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 824/2000, Train Loss: 5.180151223165856, Val Loss: 4.294722600152173, Val MAE: 1.3418174982070923\n",
      "Epoch 825/2000, Train Loss: 5.179832290216088, Val Loss: 4.29464380431363, Val MAE: 1.3417541980743408\n",
      "Epoch 826/2000, Train Loss: 5.1794074561896775, Val Loss: 4.294607991965737, Val MAE: 1.3418129682540894\n",
      "Epoch 827/2000, Train Loss: 5.179032509491919, Val Loss: 4.294625122105981, Val MAE: 1.3416800498962402\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 828/2000, Train Loss: 5.178751981403967, Val Loss: 4.294435305435827, Val MAE: 1.3415188789367676\n",
      "Epoch 829/2000, Train Loss: 5.178420877521365, Val Loss: 4.294371418314656, Val MAE: 1.3417186737060547\n",
      "Epoch 830/2000, Train Loss: 5.1782434429857105, Val Loss: 4.29436427091989, Val MAE: 1.3415465354919434\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 831/2000, Train Loss: 5.177672623779266, Val Loss: 4.29432861180756, Val MAE: 1.3415334224700928\n",
      "Epoch 832/2000, Train Loss: 5.177405236877774, Val Loss: 4.294387480733901, Val MAE: 1.3413885831832886\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 833/2000, Train Loss: 5.177283734770611, Val Loss: 4.294248235976602, Val MAE: 1.341100811958313\n",
      "Epoch 834/2000, Train Loss: 5.176735444825896, Val Loss: 4.294196375383167, Val MAE: 1.341120958328247\n",
      "Epoch 835/2000, Train Loss: 5.176423386932228, Val Loss: 4.294179411525802, Val MAE: 1.341091513633728\n",
      "Epoch 836/2000, Train Loss: 5.176174664594425, Val Loss: 4.294228961007802, Val MAE: 1.341008186340332\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 837/2000, Train Loss: 5.175735850832355, Val Loss: 4.294192401583739, Val MAE: 1.341049313545227\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 838/2000, Train Loss: 5.175480268541799, Val Loss: 4.294211289732475, Val MAE: 1.3411821126937866\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 839/2000, Train Loss: 5.175051876858099, Val Loss: 4.294133494031711, Val MAE: 1.3409088850021362\n",
      "Epoch 840/2000, Train Loss: 5.174741944270347, Val Loss: 4.294076748344842, Val MAE: 1.3408175706863403\n",
      "Epoch 841/2000, Train Loss: 5.1743958488086506, Val Loss: 4.294188203961831, Val MAE: 1.3416682481765747\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 842/2000, Train Loss: 5.174102356629585, Val Loss: 4.294078147082817, Val MAE: 1.3413344621658325\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 843/2000, Train Loss: 5.1737731786361385, Val Loss: 4.294116290676312, Val MAE: 1.3413060903549194\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 844/2000, Train Loss: 5.173415867452544, Val Loss: 4.293991992605014, Val MAE: 1.3412595987319946\n",
      "Epoch 845/2000, Train Loss: 5.173010528168491, Val Loss: 4.293961460008396, Val MAE: 1.341025471687317\n",
      "Epoch 846/2000, Train Loss: 5.172765234917443, Val Loss: 4.293899528670499, Val MAE: 1.340822458267212\n",
      "Epoch 847/2000, Train Loss: 5.172346659884201, Val Loss: 4.293988731151491, Val MAE: 1.3409662246704102\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 848/2000, Train Loss: 5.172019445912324, Val Loss: 4.2938853958929615, Val MAE: 1.3408265113830566\n",
      "Epoch 849/2000, Train Loss: 5.171739870689941, Val Loss: 4.293780103396243, Val MAE: 1.34080970287323\n",
      "Epoch 850/2000, Train Loss: 5.171362127021986, Val Loss: 4.293770262716324, Val MAE: 1.3410704135894775\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 851/2000, Train Loss: 5.171021862625267, Val Loss: 4.293686992915597, Val MAE: 1.3409066200256348\n",
      "Epoch 852/2000, Train Loss: 5.170789567877512, Val Loss: 4.293789805389765, Val MAE: 1.3410053253173828\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 853/2000, Train Loss: 5.170251591881709, Val Loss: 4.293633097504068, Val MAE: 1.3408533334732056\n",
      "Epoch 854/2000, Train Loss: 5.169944035958403, Val Loss: 4.293543440807523, Val MAE: 1.340690016746521\n",
      "Epoch 855/2000, Train Loss: 5.169702936868655, Val Loss: 4.293396703230115, Val MAE: 1.3405468463897705\n",
      "Epoch 856/2000, Train Loss: 5.169307848067103, Val Loss: 4.293417555610026, Val MAE: 1.3404823541641235\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 857/2000, Train Loss: 5.169007243747789, Val Loss: 4.2933346228806055, Val MAE: 1.3403366804122925\n",
      "Epoch 858/2000, Train Loss: 5.168685383841981, Val Loss: 4.293206463132318, Val MAE: 1.34019935131073\n",
      "Epoch 859/2000, Train Loss: 5.1683889515849595, Val Loss: 4.293118198931687, Val MAE: 1.3399837017059326\n",
      "Epoch 860/2000, Train Loss: 5.168108452935393, Val Loss: 4.293130875619378, Val MAE: 1.3398685455322266\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 861/2000, Train Loss: 5.16773168752766, Val Loss: 4.293046124948291, Val MAE: 1.339838981628418\n",
      "Epoch 862/2000, Train Loss: 5.167254952595062, Val Loss: 4.293206778098279, Val MAE: 1.3398157358169556\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 863/2000, Train Loss: 5.167044834980648, Val Loss: 4.29331647425186, Val MAE: 1.3398489952087402\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 864/2000, Train Loss: 5.166700774326092, Val Loss: 4.293355692043079, Val MAE: 1.3395284414291382\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 865/2000, Train Loss: 5.166510928113878, Val Loss: 4.293179952582037, Val MAE: 1.3391882181167603\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 866/2000, Train Loss: 5.166100216486075, Val Loss: 4.293308949376654, Val MAE: 1.3393936157226562\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 867/2000, Train Loss: 5.165820872443987, Val Loss: 4.293202200039166, Val MAE: 1.3391427993774414\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 868/2000, Train Loss: 5.16543705527907, Val Loss: 4.293181718662968, Val MAE: 1.3391094207763672\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 869/2000, Train Loss: 5.165137686270063, Val Loss: 4.293080738069504, Val MAE: 1.3389374017715454\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 870/2000, Train Loss: 5.164864624209772, Val Loss: 4.292974627205706, Val MAE: 1.3388172388076782\n",
      "Epoch 871/2000, Train Loss: 5.164468923126179, Val Loss: 4.293105623618824, Val MAE: 1.3388185501098633\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 872/2000, Train Loss: 5.16412745240262, Val Loss: 4.293268233819271, Val MAE: 1.3397160768508911\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 873/2000, Train Loss: 5.163838982925809, Val Loss: 4.293176142859647, Val MAE: 1.3397741317749023\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 874/2000, Train Loss: 5.163507400083089, Val Loss: 4.293091993773078, Val MAE: 1.3396071195602417\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 875/2000, Train Loss: 5.1631487907192275, Val Loss: 4.293127236732348, Val MAE: 1.3397585153579712\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 876/2000, Train Loss: 5.162937134767161, Val Loss: 4.293163455236615, Val MAE: 1.3400150537490845\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 877/2000, Train Loss: 5.162575546970523, Val Loss: 4.293143954474156, Val MAE: 1.3400647640228271\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 878/2000, Train Loss: 5.162288466496254, Val Loss: 4.293057833600232, Val MAE: 1.3399304151535034\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 879/2000, Train Loss: 5.161946245215156, Val Loss: 4.293044306725029, Val MAE: 1.3398168087005615\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 880/2000, Train Loss: 5.161625493969894, Val Loss: 4.293012847534315, Val MAE: 1.3397363424301147\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 881/2000, Train Loss: 5.161392101115741, Val Loss: 4.29291199924439, Val MAE: 1.3398584127426147\n",
      "Epoch 882/2000, Train Loss: 5.160984524554767, Val Loss: 4.292980536231845, Val MAE: 1.3396227359771729\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 883/2000, Train Loss: 5.160685367791268, Val Loss: 4.292834055095207, Val MAE: 1.339467167854309\n",
      "Epoch 884/2000, Train Loss: 5.160427545110115, Val Loss: 4.29284757356944, Val MAE: 1.339544653892517\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 885/2000, Train Loss: 5.160050765549312, Val Loss: 4.292883614808556, Val MAE: 1.3395920991897583\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 886/2000, Train Loss: 5.159690229993015, Val Loss: 4.292841043106214, Val MAE: 1.339608073234558\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 887/2000, Train Loss: 5.159348844895531, Val Loss: 4.292890317186596, Val MAE: 1.3399089574813843\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 888/2000, Train Loss: 5.15919478389269, Val Loss: 4.292952043146599, Val MAE: 1.339970588684082\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 889/2000, Train Loss: 5.158647272124219, Val Loss: 4.292838714066453, Val MAE: 1.3399748802185059\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 890/2000, Train Loss: 5.158359708423692, Val Loss: 4.292842997763101, Val MAE: 1.3400053977966309\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 891/2000, Train Loss: 5.158105597890701, Val Loss: 4.292749155975702, Val MAE: 1.3399863243103027\n",
      "Epoch 892/2000, Train Loss: 5.157790508813742, Val Loss: 4.292651545062778, Val MAE: 1.3398630619049072\n",
      "Epoch 893/2000, Train Loss: 5.157326922649606, Val Loss: 4.292774954133146, Val MAE: 1.340011715888977\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 894/2000, Train Loss: 5.156975937989544, Val Loss: 4.292683461613542, Val MAE: 1.3399385213851929\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 895/2000, Train Loss: 5.156732920390456, Val Loss: 4.292608261671592, Val MAE: 1.339769721031189\n",
      "Epoch 896/2000, Train Loss: 5.156403870937216, Val Loss: 4.292605185180198, Val MAE: 1.3398513793945312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 897/2000, Train Loss: 5.156115836455023, Val Loss: 4.292669387501994, Val MAE: 1.3397032022476196\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 898/2000, Train Loss: 5.155763465643252, Val Loss: 4.292587135016449, Val MAE: 1.339542269706726\n",
      "Epoch 899/2000, Train Loss: 5.1554562104120345, Val Loss: 4.2926271363506165, Val MAE: 1.3399608135223389\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 900/2000, Train Loss: 5.155123155508468, Val Loss: 4.292596829078329, Val MAE: 1.3397599458694458\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 901/2000, Train Loss: 5.154723298112929, Val Loss: 4.292780929005991, Val MAE: 1.339962363243103\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 902/2000, Train Loss: 5.154622200992732, Val Loss: 4.292626683355317, Val MAE: 1.3397587537765503\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 903/2000, Train Loss: 5.1542334449825, Val Loss: 4.292709057959985, Val MAE: 1.3397769927978516\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 904/2000, Train Loss: 5.153960377099232, Val Loss: 4.292750327746699, Val MAE: 1.339852213859558\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 905/2000, Train Loss: 5.15359112462648, Val Loss: 4.292566249689718, Val MAE: 1.339687466621399\n",
      "Epoch 906/2000, Train Loss: 5.153436878845197, Val Loss: 4.292603156604166, Val MAE: 1.339804768562317\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 907/2000, Train Loss: 5.153052633458918, Val Loss: 4.292449602320438, Val MAE: 1.3394075632095337\n",
      "Epoch 908/2000, Train Loss: 5.152905256890539, Val Loss: 4.292420412377109, Val MAE: 1.33958101272583\n",
      "Epoch 909/2000, Train Loss: 5.152465237984421, Val Loss: 4.292503858692064, Val MAE: 1.3396632671356201\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 910/2000, Train Loss: 5.1522254909880285, Val Loss: 4.2924754522447515, Val MAE: 1.3396728038787842\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 911/2000, Train Loss: 5.151888235614937, Val Loss: 4.292398417699994, Val MAE: 1.33967125415802\n",
      "Epoch 912/2000, Train Loss: 5.151582147243712, Val Loss: 4.292481259802195, Val MAE: 1.3395650386810303\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 913/2000, Train Loss: 5.151229282570662, Val Loss: 4.292300399128846, Val MAE: 1.3394807577133179\n",
      "Epoch 914/2000, Train Loss: 5.150983388582434, Val Loss: 4.292256033702159, Val MAE: 1.3393610715866089\n",
      "Epoch 915/2000, Train Loss: 5.1506873868213745, Val Loss: 4.2921993676602375, Val MAE: 1.3393914699554443\n",
      "Epoch 916/2000, Train Loss: 5.150405772505493, Val Loss: 4.29212172810487, Val MAE: 1.3393441438674927\n",
      "Epoch 917/2000, Train Loss: 5.150107912486735, Val Loss: 4.2921527475822625, Val MAE: 1.339400291442871\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 918/2000, Train Loss: 5.1497959950724, Val Loss: 4.292141019687878, Val MAE: 1.3396024703979492\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 919/2000, Train Loss: 5.149527953470093, Val Loss: 4.292084046752434, Val MAE: 1.3393632173538208\n",
      "Epoch 920/2000, Train Loss: 5.149114577708769, Val Loss: 4.292177486278879, Val MAE: 1.3394060134887695\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 921/2000, Train Loss: 5.148961883013009, Val Loss: 4.292436969890369, Val MAE: 1.3396108150482178\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 922/2000, Train Loss: 5.148590530436915, Val Loss: 4.292322244090358, Val MAE: 1.3394267559051514\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 923/2000, Train Loss: 5.148193376196935, Val Loss: 4.2922607766361685, Val MAE: 1.33957040309906\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 924/2000, Train Loss: 5.147919336523421, Val Loss: 4.292321599937799, Val MAE: 1.3396021127700806\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 925/2000, Train Loss: 5.147605302699385, Val Loss: 4.292155291386477, Val MAE: 1.3396413326263428\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 926/2000, Train Loss: 5.147306642546906, Val Loss: 4.291992373494652, Val MAE: 1.3396223783493042\n",
      "Epoch 927/2000, Train Loss: 5.147054690357141, Val Loss: 4.292123629586903, Val MAE: 1.3395076990127563\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 928/2000, Train Loss: 5.146740770728164, Val Loss: 4.292211678835351, Val MAE: 1.3394296169281006\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 929/2000, Train Loss: 5.1464564272959485, Val Loss: 4.292249691486359, Val MAE: 1.33961021900177\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 930/2000, Train Loss: 5.146028516270736, Val Loss: 4.292108143877796, Val MAE: 1.3394930362701416\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 931/2000, Train Loss: 5.145874780262827, Val Loss: 4.292125230746007, Val MAE: 1.3394086360931396\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 932/2000, Train Loss: 5.145558000096149, Val Loss: 4.292046299130898, Val MAE: 1.339238166809082\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 933/2000, Train Loss: 5.145238065654905, Val Loss: 4.292001030201049, Val MAE: 1.3391464948654175\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 934/2000, Train Loss: 5.14495149002571, Val Loss: 4.291848351495473, Val MAE: 1.338811993598938\n",
      "Epoch 935/2000, Train Loss: 5.144709897655969, Val Loss: 4.291823490538935, Val MAE: 1.338889479637146\n",
      "Epoch 936/2000, Train Loss: 5.144413995856023, Val Loss: 4.291751149273294, Val MAE: 1.338592290878296\n",
      "Epoch 937/2000, Train Loss: 5.144246494753875, Val Loss: 4.291659168981192, Val MAE: 1.3384003639221191\n",
      "Epoch 938/2000, Train Loss: 5.143932888595188, Val Loss: 4.291665842260901, Val MAE: 1.3384228944778442\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 939/2000, Train Loss: 5.143474246624205, Val Loss: 4.2918134718898715, Val MAE: 1.3385318517684937\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 940/2000, Train Loss: 5.143228924986142, Val Loss: 4.29172684207676, Val MAE: 1.338315486907959\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 941/2000, Train Loss: 5.142932788326103, Val Loss: 4.291759011360604, Val MAE: 1.3383784294128418\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 942/2000, Train Loss: 5.142529452732655, Val Loss: 4.291799244918223, Val MAE: 1.3385112285614014\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 943/2000, Train Loss: 5.1422992980706645, Val Loss: 4.291790197871801, Val MAE: 1.3386470079421997\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 944/2000, Train Loss: 5.141931207726736, Val Loss: 4.291741462955325, Val MAE: 1.3384957313537598\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 945/2000, Train Loss: 5.141695795745021, Val Loss: 4.292044861954967, Val MAE: 1.3385305404663086\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 946/2000, Train Loss: 5.141324155573484, Val Loss: 4.291971035313418, Val MAE: 1.3383638858795166\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 947/2000, Train Loss: 5.141095765410156, Val Loss: 4.291835290709819, Val MAE: 1.338224172592163\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 948/2000, Train Loss: 5.140737807855839, Val Loss: 4.291832497317022, Val MAE: 1.3383268117904663\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 949/2000, Train Loss: 5.140487553872765, Val Loss: 4.29172258761924, Val MAE: 1.338069200515747\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 950/2000, Train Loss: 5.140183611673125, Val Loss: 4.291706970967645, Val MAE: 1.3381115198135376\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 951/2000, Train Loss: 5.139957946223514, Val Loss: 4.291687601006876, Val MAE: 1.3379991054534912\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 952/2000, Train Loss: 5.1397152059032924, Val Loss: 4.291577733877137, Val MAE: 1.3382407426834106\n",
      "Epoch 953/2000, Train Loss: 5.139392877530858, Val Loss: 4.291703405483501, Val MAE: 1.3381651639938354\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 954/2000, Train Loss: 5.139210206035681, Val Loss: 4.291564834352553, Val MAE: 1.3378618955612183\n",
      "Epoch 955/2000, Train Loss: 5.138725358401419, Val Loss: 4.2916054596112465, Val MAE: 1.33790123462677\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 956/2000, Train Loss: 5.138585505812471, Val Loss: 4.291525537291849, Val MAE: 1.3376564979553223\n",
      "Epoch 957/2000, Train Loss: 5.138139602994822, Val Loss: 4.291572937205082, Val MAE: 1.3377763032913208\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 958/2000, Train Loss: 5.137880247677683, Val Loss: 4.2914879458626425, Val MAE: 1.3377033472061157\n",
      "Epoch 959/2000, Train Loss: 5.137588798433663, Val Loss: 4.291588443846215, Val MAE: 1.337756872177124\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 960/2000, Train Loss: 5.137318384857643, Val Loss: 4.291595116234201, Val MAE: 1.3378958702087402\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 961/2000, Train Loss: 5.136970870368678, Val Loss: 4.2915039552947665, Val MAE: 1.3378888368606567\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 962/2000, Train Loss: 5.136761744277933, Val Loss: 4.291468698302592, Val MAE: 1.3376820087432861\n",
      "Epoch 963/2000, Train Loss: 5.136413701030261, Val Loss: 4.291620711313458, Val MAE: 1.3382582664489746\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 964/2000, Train Loss: 5.136313228950087, Val Loss: 4.291527205095516, Val MAE: 1.337947964668274\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 965/2000, Train Loss: 5.135991776935766, Val Loss: 4.291360373478236, Val MAE: 1.3379697799682617\n",
      "Epoch 966/2000, Train Loss: 5.135671946539808, Val Loss: 4.291415859113528, Val MAE: 1.3379942178726196\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 967/2000, Train Loss: 5.1353675332451125, Val Loss: 4.291348583069373, Val MAE: 1.3377816677093506\n",
      "Epoch 968/2000, Train Loss: 5.135112835723639, Val Loss: 4.291288727803493, Val MAE: 1.3378849029541016\n",
      "Epoch 969/2000, Train Loss: 5.134789988014397, Val Loss: 4.291234981028114, Val MAE: 1.3377912044525146\n",
      "Epoch 970/2000, Train Loss: 5.134557421608567, Val Loss: 4.291136088474529, Val MAE: 1.3375258445739746\n",
      "Epoch 971/2000, Train Loss: 5.134256136142415, Val Loss: 4.291186473970338, Val MAE: 1.3373713493347168\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 972/2000, Train Loss: 5.133952268741752, Val Loss: 4.291072098992941, Val MAE: 1.3373500108718872\n",
      "Epoch 973/2000, Train Loss: 5.133701896408682, Val Loss: 4.291047891153125, Val MAE: 1.3372503519058228\n",
      "Epoch 974/2000, Train Loss: 5.133377314018265, Val Loss: 4.291148279971025, Val MAE: 1.3374155759811401\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 975/2000, Train Loss: 5.1331812545273, Val Loss: 4.2910936301148785, Val MAE: 1.337433099746704\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 976/2000, Train Loss: 5.13276638784111, Val Loss: 4.291092242828504, Val MAE: 1.3373677730560303\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 977/2000, Train Loss: 5.1327247774908304, Val Loss: 4.291179724143246, Val MAE: 1.337364673614502\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 978/2000, Train Loss: 5.132362682589682, Val Loss: 4.291348603485138, Val MAE: 1.3373647928237915\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 979/2000, Train Loss: 5.1320015454470225, Val Loss: 4.2912965128740925, Val MAE: 1.3373360633850098\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 980/2000, Train Loss: 5.13180151752612, Val Loss: 4.291163094259622, Val MAE: 1.337230920791626\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 981/2000, Train Loss: 5.131505748310115, Val Loss: 4.291225831996737, Val MAE: 1.3371518850326538\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 982/2000, Train Loss: 5.131196434818097, Val Loss: 4.291103753517932, Val MAE: 1.3372185230255127\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 983/2000, Train Loss: 5.130815366957857, Val Loss: 4.29112693587626, Val MAE: 1.3371654748916626\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 984/2000, Train Loss: 5.130588424092555, Val Loss: 4.291088573482093, Val MAE: 1.3371058702468872\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 985/2000, Train Loss: 5.13028635359603, Val Loss: 4.290907557789735, Val MAE: 1.337007999420166\n",
      "Epoch 986/2000, Train Loss: 5.13008604994299, Val Loss: 4.290969552862363, Val MAE: 1.3370476961135864\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 987/2000, Train Loss: 5.129736706877337, Val Loss: 4.290830588716221, Val MAE: 1.3372358083724976\n",
      "Epoch 988/2000, Train Loss: 5.129520642061894, Val Loss: 4.290728917459803, Val MAE: 1.336992859840393\n",
      "Epoch 989/2000, Train Loss: 5.129215380229976, Val Loss: 4.290687267752144, Val MAE: 1.3369381427764893\n",
      "Epoch 990/2000, Train Loss: 5.128976452544716, Val Loss: 4.290693888044733, Val MAE: 1.3369957208633423\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 991/2000, Train Loss: 5.128781407950206, Val Loss: 4.290642769411793, Val MAE: 1.3369724750518799\n",
      "Epoch 992/2000, Train Loss: 5.1284583850663905, Val Loss: 4.290720072787578, Val MAE: 1.3370323181152344\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 993/2000, Train Loss: 5.128191361757276, Val Loss: 4.290678062119822, Val MAE: 1.3369334936141968\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 994/2000, Train Loss: 5.127873282399397, Val Loss: 4.290719747824932, Val MAE: 1.3368644714355469\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 995/2000, Train Loss: 5.127771571015729, Val Loss: 4.290685701839567, Val MAE: 1.3367421627044678\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 996/2000, Train Loss: 5.127287397552863, Val Loss: 4.290517952498488, Val MAE: 1.3369423151016235\n",
      "Epoch 997/2000, Train Loss: 5.127049669967092, Val Loss: 4.290673503819413, Val MAE: 1.337114930152893\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 998/2000, Train Loss: 5.126801794872517, Val Loss: 4.290822568465406, Val MAE: 1.3376737833023071\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 999/2000, Train Loss: 5.1264969037701675, Val Loss: 4.290906769084179, Val MAE: 1.338232398033142\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1000/2000, Train Loss: 5.126239065399325, Val Loss: 4.290654713197017, Val MAE: 1.3379403352737427\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1001/2000, Train Loss: 5.125989105193553, Val Loss: 4.290607961613362, Val MAE: 1.3378467559814453\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1002/2000, Train Loss: 5.125704105544123, Val Loss: 4.290506867958805, Val MAE: 1.3379429578781128\n",
      "Epoch 1003/2000, Train Loss: 5.125452469938322, Val Loss: 4.290452235749387, Val MAE: 1.338019847869873\n",
      "Epoch 1004/2000, Train Loss: 5.125280915962954, Val Loss: 4.290364109531162, Val MAE: 1.337899088859558\n",
      "Epoch 1005/2000, Train Loss: 5.125100146478681, Val Loss: 4.290177427924524, Val MAE: 1.337579607963562\n",
      "Epoch 1006/2000, Train Loss: 5.124777932341742, Val Loss: 4.290340686172951, Val MAE: 1.3381561040878296\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1007/2000, Train Loss: 5.12440855046949, Val Loss: 4.290242404214979, Val MAE: 1.3381868600845337\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1008/2000, Train Loss: 5.124206863328552, Val Loss: 4.290092468308652, Val MAE: 1.3379175662994385\n",
      "Epoch 1009/2000, Train Loss: 5.124024939666448, Val Loss: 4.290017111329582, Val MAE: 1.3378312587738037\n",
      "Epoch 1010/2000, Train Loss: 5.123698194444261, Val Loss: 4.290147309603654, Val MAE: 1.337964653968811\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1011/2000, Train Loss: 5.123468599697952, Val Loss: 4.290058179164496, Val MAE: 1.3386317491531372\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1012/2000, Train Loss: 5.123173241456781, Val Loss: 4.290029610939852, Val MAE: 1.3384740352630615\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1013/2000, Train Loss: 5.12291640342495, Val Loss: 4.290003264889004, Val MAE: 1.338380217552185\n",
      "Epoch 1014/2000, Train Loss: 5.122760848403786, Val Loss: 4.289977913719462, Val MAE: 1.338076114654541\n",
      "Epoch 1015/2000, Train Loss: 5.1224756661393105, Val Loss: 4.289757712150183, Val MAE: 1.3381078243255615\n",
      "Epoch 1016/2000, Train Loss: 5.122216311368075, Val Loss: 4.28967370221934, Val MAE: 1.3378392457962036\n",
      "Epoch 1017/2000, Train Loss: 5.121998111784377, Val Loss: 4.289687778020468, Val MAE: 1.337836742401123\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1018/2000, Train Loss: 5.121773005502357, Val Loss: 4.289588157680091, Val MAE: 1.3379878997802734\n",
      "Epoch 1019/2000, Train Loss: 5.121646478994569, Val Loss: 4.28969584661206, Val MAE: 1.3379707336425781\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1020/2000, Train Loss: 5.12126188658006, Val Loss: 4.289616227478493, Val MAE: 1.3377892971038818\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1021/2000, Train Loss: 5.121039640305, Val Loss: 4.289617730876592, Val MAE: 1.3376680612564087\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1022/2000, Train Loss: 5.120743027535675, Val Loss: 4.289652458138353, Val MAE: 1.3377960920333862\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1023/2000, Train Loss: 5.120566748699307, Val Loss: 4.289464400743875, Val MAE: 1.3377598524093628\n",
      "Epoch 1024/2000, Train Loss: 5.120243079626738, Val Loss: 4.289413317592126, Val MAE: 1.3379120826721191\n",
      "Epoch 1025/2000, Train Loss: 5.119990612801008, Val Loss: 4.289377915202163, Val MAE: 1.3378148078918457\n",
      "Epoch 1026/2000, Train Loss: 5.119728085790916, Val Loss: 4.289349601165516, Val MAE: 1.3376188278198242\n",
      "Epoch 1027/2000, Train Loss: 5.1195231771837095, Val Loss: 4.2894243380685495, Val MAE: 1.3377442359924316\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1028/2000, Train Loss: 5.119281246024848, Val Loss: 4.289327704155539, Val MAE: 1.3376226425170898\n",
      "Epoch 1029/2000, Train Loss: 5.118929366081994, Val Loss: 4.289635471940979, Val MAE: 1.3375823497772217\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1030/2000, Train Loss: 5.11860168513967, Val Loss: 4.289689280339114, Val MAE: 1.3376728296279907\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1031/2000, Train Loss: 5.118277339267278, Val Loss: 4.289639299873292, Val MAE: 1.337801456451416\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1032/2000, Train Loss: 5.118022429571061, Val Loss: 4.289695211376731, Val MAE: 1.3377537727355957\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1033/2000, Train Loss: 5.117989969706438, Val Loss: 4.289840929480049, Val MAE: 1.3377223014831543\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1034/2000, Train Loss: 5.117500202626677, Val Loss: 4.289801376776432, Val MAE: 1.3377610445022583\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1035/2000, Train Loss: 5.11727399256692, Val Loss: 4.289756707788452, Val MAE: 1.3378674983978271\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1036/2000, Train Loss: 5.1170135806017685, Val Loss: 4.289887759816928, Val MAE: 1.338478446006775\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1037/2000, Train Loss: 5.116682683212463, Val Loss: 4.289896777905817, Val MAE: 1.3381876945495605\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1038/2000, Train Loss: 5.116493221438077, Val Loss: 4.289956856196321, Val MAE: 1.338192343711853\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1039/2000, Train Loss: 5.116280525963212, Val Loss: 4.289854710824846, Val MAE: 1.3381037712097168\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1040/2000, Train Loss: 5.116176702274235, Val Loss: 4.289860284281528, Val MAE: 1.337902545928955\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1041/2000, Train Loss: 5.115880499701325, Val Loss: 4.2896493834304055, Val MAE: 1.3378887176513672\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1042/2000, Train Loss: 5.115604539041684, Val Loss: 4.289597319617985, Val MAE: 1.3378695249557495\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1043/2000, Train Loss: 5.115337061914369, Val Loss: 4.289618395210251, Val MAE: 1.3378441333770752\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1044/2000, Train Loss: 5.115057378481654, Val Loss: 4.289697354750371, Val MAE: 1.3378804922103882\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 1045/2000, Train Loss: 5.114760796121212, Val Loss: 4.2895123519766045, Val MAE: 1.3376529216766357\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 1046/2000, Train Loss: 5.114646256859623, Val Loss: 4.289386156224829, Val MAE: 1.3373562097549438\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 1047/2000, Train Loss: 5.114442539344488, Val Loss: 4.289301057124701, Val MAE: 1.3372102975845337\n",
      "Epoch 1048/2000, Train Loss: 5.1141204400496045, Val Loss: 4.289325648029958, Val MAE: 1.3371294736862183\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1049/2000, Train Loss: 5.1138530168960346, Val Loss: 4.289192353224191, Val MAE: 1.337239146232605\n",
      "Epoch 1050/2000, Train Loss: 5.113691703273613, Val Loss: 4.289199073671356, Val MAE: 1.3369476795196533\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1051/2000, Train Loss: 5.113393757689436, Val Loss: 4.28919729440231, Val MAE: 1.3371760845184326\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1052/2000, Train Loss: 5.1131175583382635, Val Loss: 4.289067589626537, Val MAE: 1.3372420072555542\n",
      "Epoch 1053/2000, Train Loss: 5.112898971162625, Val Loss: 4.289067687105945, Val MAE: 1.3374234437942505\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1054/2000, Train Loss: 5.112629746161452, Val Loss: 4.289040235693999, Val MAE: 1.3373312950134277\n",
      "Epoch 1055/2000, Train Loss: 5.112409441247886, Val Loss: 4.289123940655566, Val MAE: 1.3374725580215454\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1056/2000, Train Loss: 5.112134542199941, Val Loss: 4.289020700248208, Val MAE: 1.337343454360962\n",
      "Epoch 1057/2000, Train Loss: 5.111940830582032, Val Loss: 4.289002476010736, Val MAE: 1.3373887538909912\n",
      "Epoch 1058/2000, Train Loss: 5.111668826686155, Val Loss: 4.288956540588319, Val MAE: 1.3373669385910034\n",
      "Epoch 1059/2000, Train Loss: 5.111414007448277, Val Loss: 4.2889188763194195, Val MAE: 1.337283968925476\n",
      "Epoch 1060/2000, Train Loss: 5.111079133834735, Val Loss: 4.288979908988232, Val MAE: 1.3372304439544678\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1061/2000, Train Loss: 5.110901936100166, Val Loss: 4.2889328044699875, Val MAE: 1.3368388414382935\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1062/2000, Train Loss: 5.1106534114201, Val Loss: 4.28887877239017, Val MAE: 1.336909294128418\n",
      "Epoch 1063/2000, Train Loss: 5.110370812495334, Val Loss: 4.288913160562515, Val MAE: 1.336868166923523\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1064/2000, Train Loss: 5.110261786097264, Val Loss: 4.288994229403068, Val MAE: 1.336763858795166\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1065/2000, Train Loss: 5.109905379436395, Val Loss: 4.288978945880424, Val MAE: 1.3368531465530396\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1066/2000, Train Loss: 5.109660999988442, Val Loss: 4.289062589313102, Val MAE: 1.3367559909820557\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1067/2000, Train Loss: 5.109385809710747, Val Loss: 4.2890427985998585, Val MAE: 1.3366929292678833\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1068/2000, Train Loss: 5.109140225310824, Val Loss: 4.289046842939272, Val MAE: 1.3367018699645996\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1069/2000, Train Loss: 5.108951255456725, Val Loss: 4.288986626197034, Val MAE: 1.336685299873352\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1070/2000, Train Loss: 5.108529506253744, Val Loss: 4.2890904070354825, Val MAE: 1.3366684913635254\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1071/2000, Train Loss: 5.108383520325618, Val Loss: 4.289290548168768, Val MAE: 1.3367409706115723\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1072/2000, Train Loss: 5.108264946625183, Val Loss: 4.289241578119007, Val MAE: 1.3369966745376587\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1073/2000, Train Loss: 5.107799492746066, Val Loss: 4.289236714567725, Val MAE: 1.3367745876312256\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1074/2000, Train Loss: 5.107471079645196, Val Loss: 4.289119880781399, Val MAE: 1.3366097211837769\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1075/2000, Train Loss: 5.1072645659686104, Val Loss: 4.289091134963073, Val MAE: 1.3366557359695435\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1076/2000, Train Loss: 5.107138901584992, Val Loss: 4.288953168514206, Val MAE: 1.336658000946045\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1077/2000, Train Loss: 5.106803417852808, Val Loss: 4.288652043032834, Val MAE: 1.336577296257019\n",
      "Epoch 1078/2000, Train Loss: 5.106500034123214, Val Loss: 4.288813046254511, Val MAE: 1.336753487586975\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1079/2000, Train Loss: 5.1062258055163205, Val Loss: 4.288788042106027, Val MAE: 1.336873173713684\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1080/2000, Train Loss: 5.10594354660896, Val Loss: 4.288727945324005, Val MAE: 1.3366029262542725\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1081/2000, Train Loss: 5.105707907126749, Val Loss: 4.288702350385546, Val MAE: 1.336567997932434\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1082/2000, Train Loss: 5.105444800740504, Val Loss: 4.288598032635966, Val MAE: 1.3366193771362305\n",
      "Epoch 1083/2000, Train Loss: 5.105108048893218, Val Loss: 4.288766472076807, Val MAE: 1.3370643854141235\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1084/2000, Train Loss: 5.104871287598216, Val Loss: 4.288849452161413, Val MAE: 1.3370403051376343\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1085/2000, Train Loss: 5.1047203139663555, Val Loss: 4.288613704058129, Val MAE: 1.3367376327514648\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1086/2000, Train Loss: 5.104407334667371, Val Loss: 4.28856573316056, Val MAE: 1.3366636037826538\n",
      "Epoch 1087/2000, Train Loss: 5.104127931273371, Val Loss: 4.288612101772639, Val MAE: 1.3366619348526\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1088/2000, Train Loss: 5.1038937083416425, Val Loss: 4.288629019542003, Val MAE: 1.3366726636886597\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1089/2000, Train Loss: 5.103550372647787, Val Loss: 4.288597539700861, Val MAE: 1.3365651369094849\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1090/2000, Train Loss: 5.103454266668498, Val Loss: 4.2885866522789, Val MAE: 1.336585521697998\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1091/2000, Train Loss: 5.103155902974823, Val Loss: 4.288760559014448, Val MAE: 1.3369975090026855\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1092/2000, Train Loss: 5.102929956230412, Val Loss: 4.288782633899704, Val MAE: 1.3369470834732056\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1093/2000, Train Loss: 5.102677993567375, Val Loss: 4.288773015634281, Val MAE: 1.3370457887649536\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1094/2000, Train Loss: 5.102329320801243, Val Loss: 4.289011636822242, Val MAE: 1.3376331329345703\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1095/2000, Train Loss: 5.102197220379656, Val Loss: 4.288993080441407, Val MAE: 1.3374744653701782\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1096/2000, Train Loss: 5.1019150453797835, Val Loss: 4.288947575955879, Val MAE: 1.3374255895614624\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1097/2000, Train Loss: 5.101724819056538, Val Loss: 4.288908166772737, Val MAE: 1.337380051612854\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1098/2000, Train Loss: 5.101552666931981, Val Loss: 4.288931751814414, Val MAE: 1.3377633094787598\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1099/2000, Train Loss: 5.101146197868979, Val Loss: 4.288978300413747, Val MAE: 1.3377776145935059\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1100/2000, Train Loss: 5.101097609003655, Val Loss: 4.289025738154809, Val MAE: 1.3382353782653809\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1101/2000, Train Loss: 5.100891885149268, Val Loss: 4.28910287684343, Val MAE: 1.3381049633026123\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1102/2000, Train Loss: 5.100485806791883, Val Loss: 4.289035294703611, Val MAE: 1.338058590888977\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 1103/2000, Train Loss: 5.100211061971967, Val Loss: 4.289025546152761, Val MAE: 1.3378971815109253\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 1104/2000, Train Loss: 5.100001911618817, Val Loss: 4.2893071062452215, Val MAE: 1.3394486904144287\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 1105/2000, Train Loss: 5.0998311646313, Val Loss: 4.289308789536709, Val MAE: 1.339342713356018\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 1106/2000, Train Loss: 5.09960257117428, Val Loss: 4.289391046668601, Val MAE: 1.3392839431762695\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.111608982086182\n",
      "Test Mean Absolute Error (MAE): 1.5874624626886376\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(CNNModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (conv1d): Conv1d(6, 64, kernel_size=(2,), stride=(1,))\n",
       "   (dense1): Linear(in_features=577, out_features=64, bias=True)\n",
       "   (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=577, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'test_mae': 1.5874624626886376})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 444\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_cnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=6,\n",
    "                kernel_size=2,\n",
    "                num_filters=64,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = ['total_points', 'ict_index', 'clean_sheets', 'goals_conceded', 'bps', 'matchup_difficulty', 'goals_scored', 'assists', 'yellow_cards', 'red_cards'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpremier.cnn.experiment import gridsearch_cnn\n",
    "\n",
    "#gridsearch_cnn(epochs=100, verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized)  {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
