{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.cnn.model import build_train_cnn, full_cnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2502, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2502, 7)\n",
      "Shape of a given window (prior to preprocessing): (6, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.89197166e+00 1.17762692e+00 1.49940968e-01 7.31404959e-01\n",
      " 9.55312869e+00 9.44510035e-03 0.00000000e+00 1.18063754e-03\n",
      " 2.59740260e-02 1.18063754e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.67958644  1.49942966  0.35701355  1.17399741 10.40055847  1.36100371\n",
      "  1.          0.03434012  0.15905778  0.03434012]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building CNN Architecture ======\n",
      "====== Done Building CNN Architecture ======\n",
      "Epoch 1/2000, Train Loss: 10.626029165238352, Val Loss: 11.294272726490384, Val MAE: 1.9654769897460938\n",
      "Epoch 2/2000, Train Loss: 10.593105482954233, Val Loss: 11.262298141207014, Val MAE: 1.9615912437438965\n",
      "Epoch 3/2000, Train Loss: 10.560963584942945, Val Loss: 11.230528681051164, Val MAE: 1.9577713012695312\n",
      "Epoch 4/2000, Train Loss: 10.52865130003484, Val Loss: 11.19927704334259, Val MAE: 1.9540678262710571\n",
      "Epoch 5/2000, Train Loss: 10.496746410267646, Val Loss: 11.167742357367562, Val MAE: 1.950272560119629\n",
      "Epoch 6/2000, Train Loss: 10.464742586542084, Val Loss: 11.135924728143783, Val MAE: 1.9464269876480103\n",
      "Epoch 7/2000, Train Loss: 10.432838800427607, Val Loss: 11.105538941565014, Val MAE: 1.942838430404663\n",
      "Epoch 8/2000, Train Loss: 10.401720846321417, Val Loss: 11.074123181047893, Val MAE: 1.938997507095337\n",
      "Epoch 9/2000, Train Loss: 10.370406872462823, Val Loss: 11.04319229012444, Val MAE: 1.935300588607788\n",
      "Epoch 10/2000, Train Loss: 10.338480787990123, Val Loss: 11.01319552603222, Val MAE: 1.9317848682403564\n",
      "Epoch 11/2000, Train Loss: 10.307285310855537, Val Loss: 10.980983492873964, Val MAE: 1.9278290271759033\n",
      "Epoch 12/2000, Train Loss: 10.275247813279941, Val Loss: 10.95047642503466, Val MAE: 1.9242244958877563\n",
      "Epoch 13/2000, Train Loss: 10.24349105038663, Val Loss: 10.91827788807097, Val MAE: 1.9203494787216187\n",
      "Epoch 14/2000, Train Loss: 10.211471390825064, Val Loss: 10.887642111097064, Val MAE: 1.9166529178619385\n",
      "Epoch 15/2000, Train Loss: 10.179762583021049, Val Loss: 10.855624454362053, Val MAE: 1.912757158279419\n",
      "Epoch 16/2000, Train Loss: 10.148215330874466, Val Loss: 10.825447235788618, Val MAE: 1.9092458486557007\n",
      "Epoch 17/2000, Train Loss: 10.117287136099403, Val Loss: 10.79402319306419, Val MAE: 1.9053826332092285\n",
      "Epoch 18/2000, Train Loss: 10.085634035183109, Val Loss: 10.763240280605498, Val MAE: 1.9016145467758179\n",
      "Epoch 19/2000, Train Loss: 10.054261738221635, Val Loss: 10.731860952717918, Val MAE: 1.8979074954986572\n",
      "Epoch 20/2000, Train Loss: 10.022667236489538, Val Loss: 10.70130175068265, Val MAE: 1.8941996097564697\n",
      "Epoch 21/2000, Train Loss: 9.991233283609194, Val Loss: 10.669860175677709, Val MAE: 1.890332579612732\n",
      "Epoch 22/2000, Train Loss: 9.959231161432307, Val Loss: 10.638821428730374, Val MAE: 1.8865514993667603\n",
      "Epoch 23/2000, Train Loss: 9.927361733150079, Val Loss: 10.608043094476065, Val MAE: 1.8828266859054565\n",
      "Epoch 24/2000, Train Loss: 9.895680579547317, Val Loss: 10.577158099129086, Val MAE: 1.8790512084960938\n",
      "Epoch 25/2000, Train Loss: 9.86404060342248, Val Loss: 10.545489302703313, Val MAE: 1.8752965927124023\n",
      "Epoch 26/2000, Train Loss: 9.832493756486599, Val Loss: 10.514116803805033, Val MAE: 1.8713728189468384\n",
      "Epoch 27/2000, Train Loss: 9.800243740861944, Val Loss: 10.482945873623802, Val MAE: 1.867497444152832\n",
      "Epoch 28/2000, Train Loss: 9.768493985927996, Val Loss: 10.451622406641642, Val MAE: 1.8636808395385742\n",
      "Epoch 29/2000, Train Loss: 9.736799755536955, Val Loss: 10.420946524256752, Val MAE: 1.8598840236663818\n",
      "Epoch 30/2000, Train Loss: 9.70558613479894, Val Loss: 10.390450534366426, Val MAE: 1.8561450242996216\n",
      "Epoch 31/2000, Train Loss: 9.674342947719127, Val Loss: 10.35837577638172, Val MAE: 1.852095127105713\n",
      "Epoch 32/2000, Train Loss: 9.642364538943314, Val Loss: 10.327878174327669, Val MAE: 1.8482942581176758\n",
      "Epoch 33/2000, Train Loss: 9.611272158172135, Val Loss: 10.296825749533516, Val MAE: 1.8443857431411743\n",
      "Epoch 34/2000, Train Loss: 9.579811331583516, Val Loss: 10.266557460739499, Val MAE: 1.8406175374984741\n",
      "Epoch 35/2000, Train Loss: 9.548607184621275, Val Loss: 10.23525341351827, Val MAE: 1.8366862535476685\n",
      "Epoch 36/2000, Train Loss: 9.517030298962075, Val Loss: 10.204739133516947, Val MAE: 1.8328821659088135\n",
      "Epoch 37/2000, Train Loss: 9.485240554271531, Val Loss: 10.17282585870652, Val MAE: 1.828829050064087\n",
      "Epoch 38/2000, Train Loss: 9.453582447237626, Val Loss: 10.141675690809885, Val MAE: 1.8248647451400757\n",
      "Epoch 39/2000, Train Loss: 9.422517112010961, Val Loss: 10.110464524655114, Val MAE: 1.8209228515625\n",
      "Epoch 40/2000, Train Loss: 9.39105364970327, Val Loss: 10.080321289244152, Val MAE: 1.8171123266220093\n",
      "Epoch 41/2000, Train Loss: 9.359980135105225, Val Loss: 10.048837809335618, Val MAE: 1.8130576610565186\n",
      "Epoch 42/2000, Train Loss: 9.328509391278912, Val Loss: 10.018539039861588, Val MAE: 1.8091652393341064\n",
      "Epoch 43/2000, Train Loss: 9.297523095005827, Val Loss: 9.987396206174578, Val MAE: 1.8052215576171875\n",
      "Epoch 44/2000, Train Loss: 9.265917483438725, Val Loss: 9.956694353194464, Val MAE: 1.8012285232543945\n",
      "Epoch 45/2000, Train Loss: 9.234631453649953, Val Loss: 9.924908432222548, Val MAE: 1.7970792055130005\n",
      "Epoch 46/2000, Train Loss: 9.20269776601213, Val Loss: 9.894323469627471, Val MAE: 1.793076992034912\n",
      "Epoch 47/2000, Train Loss: 9.17145589578303, Val Loss: 9.863060329641614, Val MAE: 1.789004921913147\n",
      "Epoch 48/2000, Train Loss: 9.140454718693356, Val Loss: 9.83235597326642, Val MAE: 1.78509521484375\n",
      "Epoch 49/2000, Train Loss: 9.109271302377891, Val Loss: 9.801859530664625, Val MAE: 1.7810825109481812\n",
      "Epoch 50/2000, Train Loss: 9.078102150151693, Val Loss: 9.770665529228392, Val MAE: 1.7769452333450317\n",
      "Epoch 51/2000, Train Loss: 9.04700661512625, Val Loss: 9.740038041557584, Val MAE: 1.7730538845062256\n",
      "Epoch 52/2000, Train Loss: 9.016132635189212, Val Loss: 9.708701602050237, Val MAE: 1.7689263820648193\n",
      "Epoch 53/2000, Train Loss: 8.985061356311792, Val Loss: 9.678981500012535, Val MAE: 1.7650847434997559\n",
      "Epoch 54/2000, Train Loss: 8.95424284602086, Val Loss: 9.648256140095848, Val MAE: 1.761120080947876\n",
      "Epoch 55/2000, Train Loss: 8.92361010096137, Val Loss: 9.617801998342786, Val MAE: 1.757105827331543\n",
      "Epoch 56/2000, Train Loss: 8.89308460542278, Val Loss: 9.5870238670281, Val MAE: 1.7531020641326904\n",
      "Epoch 57/2000, Train Loss: 8.862276330317025, Val Loss: 9.555867447739555, Val MAE: 1.7490373849868774\n",
      "Epoch 58/2000, Train Loss: 8.831610783198993, Val Loss: 9.526721057437715, Val MAE: 1.7452291250228882\n",
      "Epoch 59/2000, Train Loss: 8.801280002836112, Val Loss: 9.496535362232299, Val MAE: 1.7412352561950684\n",
      "Epoch 60/2000, Train Loss: 8.770844649192476, Val Loss: 9.466611391022091, Val MAE: 1.7373148202896118\n",
      "Epoch 61/2000, Train Loss: 8.74036744686713, Val Loss: 9.43569890374229, Val MAE: 1.7332690954208374\n",
      "Epoch 62/2000, Train Loss: 8.709440290507544, Val Loss: 9.405736308722268, Val MAE: 1.729340672492981\n",
      "Epoch 63/2000, Train Loss: 8.678805466263183, Val Loss: 9.375860590310324, Val MAE: 1.7255182266235352\n",
      "Epoch 64/2000, Train Loss: 8.64871940088205, Val Loss: 9.344968804291316, Val MAE: 1.7215877771377563\n",
      "Epoch 65/2000, Train Loss: 8.618208937651683, Val Loss: 9.315347397611255, Val MAE: 1.7177011966705322\n",
      "Epoch 66/2000, Train Loss: 8.588053413439537, Val Loss: 9.285446741751262, Val MAE: 1.7139192819595337\n",
      "Epoch 67/2000, Train Loss: 8.558074364709249, Val Loss: 9.255406920398984, Val MAE: 1.7100023031234741\n",
      "Epoch 68/2000, Train Loss: 8.527759763182303, Val Loss: 9.225177725156149, Val MAE: 1.706178903579712\n",
      "Epoch 69/2000, Train Loss: 8.497532352237675, Val Loss: 9.195820785704113, Val MAE: 1.702411413192749\n",
      "Epoch 70/2000, Train Loss: 8.467393458814312, Val Loss: 9.165317563783555, Val MAE: 1.6985691785812378\n",
      "Epoch 71/2000, Train Loss: 8.437156342316749, Val Loss: 9.135727971792221, Val MAE: 1.6948068141937256\n",
      "Epoch 72/2000, Train Loss: 8.407595207391909, Val Loss: 9.105435602721714, Val MAE: 1.6909688711166382\n",
      "Epoch 73/2000, Train Loss: 8.377578744767247, Val Loss: 9.076785995846702, Val MAE: 1.6874140501022339\n",
      "Epoch 74/2000, Train Loss: 8.348199103884031, Val Loss: 9.046790665104275, Val MAE: 1.683593988418579\n",
      "Epoch 75/2000, Train Loss: 8.318482220761362, Val Loss: 9.01773641364915, Val MAE: 1.679870843887329\n",
      "Epoch 76/2000, Train Loss: 8.28920116091649, Val Loss: 8.988000299249377, Val MAE: 1.6761891841888428\n",
      "Epoch 77/2000, Train Loss: 8.260065084115743, Val Loss: 8.959030237935838, Val MAE: 1.6724464893341064\n",
      "Epoch 78/2000, Train Loss: 8.230645333089681, Val Loss: 8.930357552710033, Val MAE: 1.6689317226409912\n",
      "Epoch 79/2000, Train Loss: 8.201197087512535, Val Loss: 8.901129444440206, Val MAE: 1.6652759313583374\n",
      "Epoch 80/2000, Train Loss: 8.172096225444873, Val Loss: 8.870906656696683, Val MAE: 1.6614240407943726\n",
      "Epoch 81/2000, Train Loss: 8.143170518834769, Val Loss: 8.84275573633966, Val MAE: 1.6578961610794067\n",
      "Epoch 82/2000, Train Loss: 8.114204709721553, Val Loss: 8.814778000116348, Val MAE: 1.6545090675354004\n",
      "Epoch 83/2000, Train Loss: 8.085904414293964, Val Loss: 8.786566132590885, Val MAE: 1.6511192321777344\n",
      "Epoch 84/2000, Train Loss: 8.057509322428738, Val Loss: 8.758323131572633, Val MAE: 1.647645115852356\n",
      "Epoch 85/2000, Train Loss: 8.029307243686132, Val Loss: 8.729660335041228, Val MAE: 1.6441373825073242\n",
      "Epoch 86/2000, Train Loss: 8.000785418054448, Val Loss: 8.701381148326965, Val MAE: 1.6407169103622437\n",
      "Epoch 87/2000, Train Loss: 7.97246664098355, Val Loss: 8.67301981931641, Val MAE: 1.6373189687728882\n",
      "Epoch 88/2000, Train Loss: 7.944117492277632, Val Loss: 8.645164562123162, Val MAE: 1.6340982913970947\n",
      "Epoch 89/2000, Train Loss: 7.916187862415072, Val Loss: 8.616740767444883, Val MAE: 1.6307035684585571\n",
      "Epoch 90/2000, Train Loss: 7.887929682334825, Val Loss: 8.588876356681189, Val MAE: 1.627436637878418\n",
      "Epoch 91/2000, Train Loss: 7.859206246724418, Val Loss: 8.561400064400264, Val MAE: 1.62406325340271\n",
      "Epoch 92/2000, Train Loss: 7.8312228149688465, Val Loss: 8.532454937696457, Val MAE: 1.6205257177352905\n",
      "Epoch 93/2000, Train Loss: 7.8030648345839655, Val Loss: 8.505750707217626, Val MAE: 1.6173553466796875\n",
      "Epoch 94/2000, Train Loss: 7.775761065597426, Val Loss: 8.478073863756089, Val MAE: 1.6140142679214478\n",
      "Epoch 95/2000, Train Loss: 7.748286139645597, Val Loss: 8.451307244244076, Val MAE: 1.610804557800293\n",
      "Epoch 96/2000, Train Loss: 7.721125729161358, Val Loss: 8.423604342199507, Val MAE: 1.607619285583496\n",
      "Epoch 97/2000, Train Loss: 7.693702236385373, Val Loss: 8.397086990731102, Val MAE: 1.604478359222412\n",
      "Epoch 98/2000, Train Loss: 7.666568028405624, Val Loss: 8.370334437915258, Val MAE: 1.6012736558914185\n",
      "Epoch 99/2000, Train Loss: 7.639561798407766, Val Loss: 8.343404948711395, Val MAE: 1.5980918407440186\n",
      "Epoch 100/2000, Train Loss: 7.612839196397488, Val Loss: 8.31690010144597, Val MAE: 1.5950276851654053\n",
      "Epoch 101/2000, Train Loss: 7.586286369264546, Val Loss: 8.29002420675187, Val MAE: 1.5919044017791748\n",
      "Epoch 102/2000, Train Loss: 7.559494282866399, Val Loss: 8.263749888965062, Val MAE: 1.588858962059021\n",
      "Epoch 103/2000, Train Loss: 7.532983807481731, Val Loss: 8.237781541688102, Val MAE: 1.585907220840454\n",
      "Epoch 104/2000, Train Loss: 7.506576579783968, Val Loss: 8.211673671290988, Val MAE: 1.5829122066497803\n",
      "Epoch 105/2000, Train Loss: 7.480192981754943, Val Loss: 8.185536295175552, Val MAE: 1.579943299293518\n",
      "Epoch 106/2000, Train Loss: 7.453835628600988, Val Loss: 8.160307532265072, Val MAE: 1.5770710706710815\n",
      "Epoch 107/2000, Train Loss: 7.427464490548848, Val Loss: 8.134049578791572, Val MAE: 1.5740571022033691\n",
      "Epoch 108/2000, Train Loss: 7.400787303747007, Val Loss: 8.108037209226971, Val MAE: 1.5711215734481812\n",
      "Epoch 109/2000, Train Loss: 7.374822665000668, Val Loss: 8.080963911045165, Val MAE: 1.5679585933685303\n",
      "Epoch 110/2000, Train Loss: 7.3485009609728165, Val Loss: 8.056111917609261, Val MAE: 1.5650725364685059\n",
      "Epoch 111/2000, Train Loss: 7.323160096183314, Val Loss: 8.031178243103481, Val MAE: 1.5623136758804321\n",
      "Epoch 112/2000, Train Loss: 7.297879913125623, Val Loss: 8.005913970016298, Val MAE: 1.5593712329864502\n",
      "Epoch 113/2000, Train Loss: 7.272301092134376, Val Loss: 7.981750120719274, Val MAE: 1.5565770864486694\n",
      "Epoch 114/2000, Train Loss: 7.247014140040313, Val Loss: 7.956294031370254, Val MAE: 1.5535930395126343\n",
      "Epoch 115/2000, Train Loss: 7.221783387644167, Val Loss: 7.931874939373562, Val MAE: 1.5508527755737305\n",
      "Epoch 116/2000, Train Loss: 7.196817963012351, Val Loss: 7.907219750540597, Val MAE: 1.5480295419692993\n",
      "Epoch 117/2000, Train Loss: 7.171539769354257, Val Loss: 7.883180212406885, Val MAE: 1.545426368713379\n",
      "Epoch 118/2000, Train Loss: 7.1464646366991005, Val Loss: 7.857710247948056, Val MAE: 1.542577862739563\n",
      "Epoch 119/2000, Train Loss: 7.121488798488515, Val Loss: 7.833697414114361, Val MAE: 1.5399388074874878\n",
      "Epoch 120/2000, Train Loss: 7.097161594205582, Val Loss: 7.809045283567338, Val MAE: 1.5372099876403809\n",
      "Epoch 121/2000, Train Loss: 7.072727530396035, Val Loss: 7.785680707011904, Val MAE: 1.5346623659133911\n",
      "Epoch 122/2000, Train Loss: 7.048819174719462, Val Loss: 7.762147578455153, Val MAE: 1.532012939453125\n",
      "Epoch 123/2000, Train Loss: 7.024486942250907, Val Loss: 7.738448923542386, Val MAE: 1.5293166637420654\n",
      "Epoch 124/2000, Train Loss: 7.000384063747605, Val Loss: 7.715030549537568, Val MAE: 1.5266774892807007\n",
      "Epoch 125/2000, Train Loss: 6.97681343336872, Val Loss: 7.691342617784228, Val MAE: 1.5240437984466553\n",
      "Epoch 126/2000, Train Loss: 6.952661658880237, Val Loss: 7.668884650582359, Val MAE: 1.5218113660812378\n",
      "Epoch 127/2000, Train Loss: 6.9290488987782775, Val Loss: 7.6464928814343045, Val MAE: 1.5193780660629272\n",
      "Epoch 128/2000, Train Loss: 6.9061344444793775, Val Loss: 7.623038005261194, Val MAE: 1.5169084072113037\n",
      "Epoch 129/2000, Train Loss: 6.883038432036535, Val Loss: 7.600562759808132, Val MAE: 1.5144377946853638\n",
      "Epoch 130/2000, Train Loss: 6.8597018984376295, Val Loss: 7.5781484714576175, Val MAE: 1.511984944343567\n",
      "Epoch 131/2000, Train Loss: 6.836879225476673, Val Loss: 7.555495331684749, Val MAE: 1.5095398426055908\n",
      "Epoch 132/2000, Train Loss: 6.813959990973533, Val Loss: 7.53380997408004, Val MAE: 1.5071930885314941\n",
      "Epoch 133/2000, Train Loss: 6.791444236031708, Val Loss: 7.511489043633143, Val MAE: 1.5047767162322998\n",
      "Epoch 134/2000, Train Loss: 6.768874997312495, Val Loss: 7.490344816730136, Val MAE: 1.5026365518569946\n",
      "Epoch 135/2000, Train Loss: 6.746746084753986, Val Loss: 7.468513197842098, Val MAE: 1.5003111362457275\n",
      "Epoch 136/2000, Train Loss: 6.724667653714652, Val Loss: 7.445700575908025, Val MAE: 1.497933030128479\n",
      "Epoch 137/2000, Train Loss: 6.702181780876327, Val Loss: 7.424967914819717, Val MAE: 1.495811104774475\n",
      "Epoch 138/2000, Train Loss: 6.680500117948933, Val Loss: 7.403802244436173, Val MAE: 1.4936411380767822\n",
      "Epoch 139/2000, Train Loss: 6.658196680636601, Val Loss: 7.382301300764084, Val MAE: 1.491562843322754\n",
      "Epoch 140/2000, Train Loss: 6.636301288147403, Val Loss: 7.360605024156117, Val MAE: 1.4893697500228882\n",
      "Epoch 141/2000, Train Loss: 6.614628003581791, Val Loss: 7.3397905840760185, Val MAE: 1.4873002767562866\n",
      "Epoch 142/2000, Train Loss: 6.593156690826201, Val Loss: 7.3192339809167954, Val MAE: 1.4851969480514526\n",
      "Epoch 143/2000, Train Loss: 6.571706461133002, Val Loss: 7.298333710148221, Val MAE: 1.4830983877182007\n",
      "Epoch 144/2000, Train Loss: 6.550241407117319, Val Loss: 7.2777173448176615, Val MAE: 1.4811742305755615\n",
      "Epoch 145/2000, Train Loss: 6.529101876176799, Val Loss: 7.257030566533406, Val MAE: 1.4790562391281128\n",
      "Epoch 146/2000, Train Loss: 6.508079594718384, Val Loss: 7.236636641479674, Val MAE: 1.4771356582641602\n",
      "Epoch 147/2000, Train Loss: 6.487406306946059, Val Loss: 7.217558017798832, Val MAE: 1.4755054712295532\n",
      "Epoch 148/2000, Train Loss: 6.467141635663419, Val Loss: 7.197294808569408, Val MAE: 1.4734399318695068\n",
      "Epoch 149/2000, Train Loss: 6.4467201696968885, Val Loss: 7.177773022935504, Val MAE: 1.4715465307235718\n",
      "Epoch 150/2000, Train Loss: 6.426448202603963, Val Loss: 7.158710353431248, Val MAE: 1.4697133302688599\n",
      "Epoch 151/2000, Train Loss: 6.406700091234216, Val Loss: 7.139369699217024, Val MAE: 1.4678393602371216\n",
      "Epoch 152/2000, Train Loss: 6.3870830845261155, Val Loss: 7.120907986447925, Val MAE: 1.4662766456604004\n",
      "Epoch 153/2000, Train Loss: 6.367339888143607, Val Loss: 7.100784260602224, Val MAE: 1.464368462562561\n",
      "Epoch 154/2000, Train Loss: 6.347606679112014, Val Loss: 7.081670566683724, Val MAE: 1.462555170059204\n",
      "Epoch 155/2000, Train Loss: 6.327923808346682, Val Loss: 7.063614686330159, Val MAE: 1.4610044956207275\n",
      "Epoch 156/2000, Train Loss: 6.308285799954607, Val Loss: 7.044815516188031, Val MAE: 1.4592690467834473\n",
      "Epoch 157/2000, Train Loss: 6.28918969513499, Val Loss: 7.02639926189468, Val MAE: 1.4575231075286865\n",
      "Epoch 158/2000, Train Loss: 6.270576929928385, Val Loss: 7.009364981026876, Val MAE: 1.4562981128692627\n",
      "Epoch 159/2000, Train Loss: 6.252518451432415, Val Loss: 6.991216401259105, Val MAE: 1.4547128677368164\n",
      "Epoch 160/2000, Train Loss: 6.234326940833766, Val Loss: 6.973581647589093, Val MAE: 1.4531066417694092\n",
      "Epoch 161/2000, Train Loss: 6.215975400927372, Val Loss: 6.9567060598305295, Val MAE: 1.4516947269439697\n",
      "Epoch 162/2000, Train Loss: 6.197981572117556, Val Loss: 6.939116693678356, Val MAE: 1.450160026550293\n",
      "Epoch 163/2000, Train Loss: 6.179649633312091, Val Loss: 6.921318980909529, Val MAE: 1.448734164237976\n",
      "Epoch 164/2000, Train Loss: 6.161997156519816, Val Loss: 6.9044436273120695, Val MAE: 1.4472438097000122\n",
      "Epoch 165/2000, Train Loss: 6.14430500119294, Val Loss: 6.88810768581572, Val MAE: 1.4458459615707397\n",
      "Epoch 166/2000, Train Loss: 6.127004984571836, Val Loss: 6.870677123467128, Val MAE: 1.444307565689087\n",
      "Epoch 167/2000, Train Loss: 6.1095407217614905, Val Loss: 6.854374347698121, Val MAE: 1.4429880380630493\n",
      "Epoch 168/2000, Train Loss: 6.092530677281583, Val Loss: 6.838729260932832, Val MAE: 1.4417643547058105\n",
      "Epoch 169/2000, Train Loss: 6.075681664879803, Val Loss: 6.82236663500468, Val MAE: 1.4403499364852905\n",
      "Epoch 170/2000, Train Loss: 6.05842869305308, Val Loss: 6.806561495576586, Val MAE: 1.4391260147094727\n",
      "Epoch 171/2000, Train Loss: 6.041677920034809, Val Loss: 6.79044092127255, Val MAE: 1.4378716945648193\n",
      "Epoch 172/2000, Train Loss: 6.025181238170403, Val Loss: 6.774684495869137, Val MAE: 1.4366830587387085\n",
      "Epoch 173/2000, Train Loss: 6.00904500400398, Val Loss: 6.759373923142751, Val MAE: 1.4355435371398926\n",
      "Epoch 174/2000, Train Loss: 5.9931063396469995, Val Loss: 6.7437391976515455, Val MAE: 1.4343857765197754\n",
      "Epoch 175/2000, Train Loss: 5.976852343684359, Val Loss: 6.728868632089524, Val MAE: 1.433440089225769\n",
      "Epoch 176/2000, Train Loss: 5.961095902411995, Val Loss: 6.71385350397655, Val MAE: 1.4324662685394287\n",
      "Epoch 177/2000, Train Loss: 5.945554902758686, Val Loss: 6.698615538222449, Val MAE: 1.431512475013733\n",
      "Epoch 178/2000, Train Loss: 5.929948863172733, Val Loss: 6.684602362768991, Val MAE: 1.4308433532714844\n",
      "Epoch 179/2000, Train Loss: 5.914822314244567, Val Loss: 6.6703200581527895, Val MAE: 1.4301806688308716\n",
      "Epoch 180/2000, Train Loss: 5.899926078336363, Val Loss: 6.655901839335759, Val MAE: 1.4295169115066528\n",
      "Epoch 181/2000, Train Loss: 5.8849848269239295, Val Loss: 6.641418434324718, Val MAE: 1.4288084506988525\n",
      "Epoch 182/2000, Train Loss: 5.87018278186513, Val Loss: 6.627714039314361, Val MAE: 1.4283825159072876\n",
      "Epoch 183/2000, Train Loss: 5.855675108846388, Val Loss: 6.614281823237737, Val MAE: 1.4278700351715088\n",
      "Epoch 184/2000, Train Loss: 5.841176223015079, Val Loss: 6.60107302239963, Val MAE: 1.4273567199707031\n",
      "Epoch 185/2000, Train Loss: 5.826972665840547, Val Loss: 6.586921738726752, Val MAE: 1.426877737045288\n",
      "Epoch 186/2000, Train Loss: 5.812459741896399, Val Loss: 6.573981296448481, Val MAE: 1.426482081413269\n",
      "Epoch 187/2000, Train Loss: 5.798151897946937, Val Loss: 6.56074848203432, Val MAE: 1.4260103702545166\n",
      "Epoch 188/2000, Train Loss: 5.78416678195948, Val Loss: 6.54776835725421, Val MAE: 1.4256117343902588\n",
      "Epoch 189/2000, Train Loss: 5.770234820873007, Val Loss: 6.533846779948189, Val MAE: 1.4251638650894165\n",
      "Epoch 190/2000, Train Loss: 5.756514120841732, Val Loss: 6.521452661071505, Val MAE: 1.4248946905136108\n",
      "Epoch 191/2000, Train Loss: 5.74316939850284, Val Loss: 6.508724507831392, Val MAE: 1.4245961904525757\n",
      "Epoch 192/2000, Train Loss: 5.730134524142288, Val Loss: 6.496245203983216, Val MAE: 1.4245272874832153\n",
      "Epoch 193/2000, Train Loss: 5.716754269028248, Val Loss: 6.484324485063553, Val MAE: 1.4243534803390503\n",
      "Epoch 194/2000, Train Loss: 5.7037507589848655, Val Loss: 6.471541250035877, Val MAE: 1.4241292476654053\n",
      "Epoch 195/2000, Train Loss: 5.6907317120534575, Val Loss: 6.459691520248141, Val MAE: 1.424062967300415\n",
      "Epoch 196/2000, Train Loss: 5.67802358581586, Val Loss: 6.448214069718406, Val MAE: 1.4239835739135742\n",
      "Epoch 197/2000, Train Loss: 5.665420380230515, Val Loss: 6.436271641935621, Val MAE: 1.4238353967666626\n",
      "Epoch 198/2000, Train Loss: 5.652457787388639, Val Loss: 6.42487336908068, Val MAE: 1.4237369298934937\n",
      "Epoch 199/2000, Train Loss: 5.640193470442413, Val Loss: 6.412900195235298, Val MAE: 1.4236470460891724\n",
      "Epoch 200/2000, Train Loss: 5.6282034000993955, Val Loss: 6.401418471620197, Val MAE: 1.4236291646957397\n",
      "Epoch 201/2000, Train Loss: 5.616141402670964, Val Loss: 6.391073673963547, Val MAE: 1.4236820936203003\n",
      "Epoch 202/2000, Train Loss: 5.604461972905147, Val Loss: 6.380064074482236, Val MAE: 1.42372727394104\n",
      "Epoch 203/2000, Train Loss: 5.593226498541946, Val Loss: 6.369469875381107, Val MAE: 1.4237607717514038\n",
      "Epoch 204/2000, Train Loss: 5.582202697506408, Val Loss: 6.3594880657536645, Val MAE: 1.423785924911499\n",
      "Epoch 205/2000, Train Loss: 5.5710341210762095, Val Loss: 6.348663216545468, Val MAE: 1.4238693714141846\n",
      "Epoch 206/2000, Train Loss: 5.559816283466786, Val Loss: 6.338684101899465, Val MAE: 1.4239675998687744\n",
      "Epoch 207/2000, Train Loss: 5.549056382542437, Val Loss: 6.328801836286273, Val MAE: 1.4241316318511963\n",
      "Epoch 208/2000, Train Loss: 5.538426845803415, Val Loss: 6.3186783123584025, Val MAE: 1.4243104457855225\n",
      "Epoch 209/2000, Train Loss: 5.527577870823601, Val Loss: 6.3085748283636, Val MAE: 1.42447829246521\n",
      "Epoch 210/2000, Train Loss: 5.517049959583578, Val Loss: 6.299559320722308, Val MAE: 1.4246262311935425\n",
      "Epoch 211/2000, Train Loss: 5.506812586939049, Val Loss: 6.289519590990884, Val MAE: 1.4248785972595215\n",
      "Epoch 212/2000, Train Loss: 5.496481060813613, Val Loss: 6.280758634919212, Val MAE: 1.4251331090927124\n",
      "Epoch 213/2000, Train Loss: 5.48685449672855, Val Loss: 6.271106871820631, Val MAE: 1.4253349304199219\n",
      "Epoch 214/2000, Train Loss: 5.4766409487919345, Val Loss: 6.262130684795833, Val MAE: 1.4255415201187134\n",
      "Epoch 215/2000, Train Loss: 5.46670386148945, Val Loss: 6.252591299159186, Val MAE: 1.4257230758666992\n",
      "Epoch 216/2000, Train Loss: 5.456823733696985, Val Loss: 6.243893150772367, Val MAE: 1.4258924722671509\n",
      "Epoch 217/2000, Train Loss: 5.447605965840295, Val Loss: 6.234973548423676, Val MAE: 1.4262430667877197\n",
      "Epoch 218/2000, Train Loss: 5.438231570428114, Val Loss: 6.227510230881827, Val MAE: 1.426487684249878\n",
      "Epoch 219/2000, Train Loss: 5.429220751075045, Val Loss: 6.218962016559782, Val MAE: 1.4268306493759155\n",
      "Epoch 220/2000, Train Loss: 5.419961432643939, Val Loss: 6.210010197900591, Val MAE: 1.4271049499511719\n",
      "Epoch 221/2000, Train Loss: 5.4110747627882425, Val Loss: 6.202254982221694, Val MAE: 1.4274100065231323\n",
      "Epoch 222/2000, Train Loss: 5.4023072496792155, Val Loss: 6.194576978683472, Val MAE: 1.4277445077896118\n",
      "Epoch 223/2000, Train Loss: 5.393934178587748, Val Loss: 6.1870614886283875, Val MAE: 1.4281038045883179\n",
      "Epoch 224/2000, Train Loss: 5.385378989581497, Val Loss: 6.179327963363557, Val MAE: 1.428395390510559\n",
      "Epoch 225/2000, Train Loss: 5.3767736889579565, Val Loss: 6.171552554482505, Val MAE: 1.4287692308425903\n",
      "Epoch 226/2000, Train Loss: 5.368524957611127, Val Loss: 6.164390554030736, Val MAE: 1.429082989692688\n",
      "Epoch 227/2000, Train Loss: 5.360593314819847, Val Loss: 6.156552382877895, Val MAE: 1.4294196367263794\n",
      "Epoch 228/2000, Train Loss: 5.352637638662363, Val Loss: 6.149538903009324, Val MAE: 1.4297574758529663\n",
      "Epoch 229/2000, Train Loss: 5.3446396019294, Val Loss: 6.142498916103726, Val MAE: 1.4300963878631592\n",
      "Epoch 230/2000, Train Loss: 5.336646069256307, Val Loss: 6.135251052322841, Val MAE: 1.4306066036224365\n",
      "Epoch 231/2000, Train Loss: 5.328860839768424, Val Loss: 6.1284347133977075, Val MAE: 1.430949091911316\n",
      "Epoch 232/2000, Train Loss: 5.321405809588089, Val Loss: 6.121814724944887, Val MAE: 1.4313783645629883\n",
      "Epoch 233/2000, Train Loss: 5.3141643768977715, Val Loss: 6.115625939198902, Val MAE: 1.4317376613616943\n",
      "Epoch 234/2000, Train Loss: 5.306839153023466, Val Loss: 6.108977517911366, Val MAE: 1.4322080612182617\n",
      "Epoch 235/2000, Train Loss: 5.299819238095089, Val Loss: 6.102518828142257, Val MAE: 1.4326485395431519\n",
      "Epoch 236/2000, Train Loss: 5.2930204932208795, Val Loss: 6.096230924129486, Val MAE: 1.433074712753296\n",
      "Epoch 237/2000, Train Loss: 5.286248088387407, Val Loss: 6.0907309509459, Val MAE: 1.433527946472168\n",
      "Epoch 238/2000, Train Loss: 5.279863522318422, Val Loss: 6.084957738717397, Val MAE: 1.4340306520462036\n",
      "Epoch 239/2000, Train Loss: 5.2732929678998985, Val Loss: 6.078942849522545, Val MAE: 1.4343862533569336\n",
      "Epoch 240/2000, Train Loss: 5.266614279390558, Val Loss: 6.073114850691387, Val MAE: 1.434936761856079\n",
      "Epoch 241/2000, Train Loss: 5.260307349674457, Val Loss: 6.067500006584894, Val MAE: 1.4354491233825684\n",
      "Epoch 242/2000, Train Loss: 5.253916547395951, Val Loss: 6.062301103557859, Val MAE: 1.435959815979004\n",
      "Epoch 243/2000, Train Loss: 5.247774367608203, Val Loss: 6.056299724749157, Val MAE: 1.4364162683486938\n",
      "Epoch 244/2000, Train Loss: 5.241458944272255, Val Loss: 6.05130055972508, Val MAE: 1.436942219734192\n",
      "Epoch 245/2000, Train Loss: 5.235674174447322, Val Loss: 6.046074339321682, Val MAE: 1.437448263168335\n",
      "Epoch 246/2000, Train Loss: 5.229874622502347, Val Loss: 6.040837534836361, Val MAE: 1.4379181861877441\n",
      "Epoch 247/2000, Train Loss: 5.224184732679252, Val Loss: 6.0360697990372065, Val MAE: 1.4384161233901978\n",
      "Epoch 248/2000, Train Loss: 5.21844805245339, Val Loss: 6.031283978904996, Val MAE: 1.4388765096664429\n",
      "Epoch 249/2000, Train Loss: 5.212766460202141, Val Loss: 6.026390975429898, Val MAE: 1.439391851425171\n",
      "Epoch 250/2000, Train Loss: 5.207743890867246, Val Loss: 6.021508848383313, Val MAE: 1.439974069595337\n",
      "Epoch 251/2000, Train Loss: 5.202172001427084, Val Loss: 6.01750678959347, Val MAE: 1.4404311180114746\n",
      "Epoch 252/2000, Train Loss: 5.197047556741281, Val Loss: 6.012864592529478, Val MAE: 1.440992832183838\n",
      "Epoch 253/2000, Train Loss: 5.191830810606059, Val Loss: 6.008431799355007, Val MAE: 1.4414994716644287\n",
      "Epoch 254/2000, Train Loss: 5.18675111950873, Val Loss: 6.004237627699261, Val MAE: 1.4419893026351929\n",
      "Epoch 255/2000, Train Loss: 5.181946470555869, Val Loss: 5.999907130286807, Val MAE: 1.4425770044326782\n",
      "Epoch 256/2000, Train Loss: 5.177056775274667, Val Loss: 5.996033346369153, Val MAE: 1.443064570426941\n",
      "Epoch 257/2000, Train Loss: 5.17228676569983, Val Loss: 5.991942588772092, Val MAE: 1.4436222314834595\n",
      "Epoch 258/2000, Train Loss: 5.167627031947729, Val Loss: 5.988214531115124, Val MAE: 1.4440969228744507\n",
      "Epoch 259/2000, Train Loss: 5.163002420379682, Val Loss: 5.9838618494215465, Val MAE: 1.4446837902069092\n",
      "Epoch 260/2000, Train Loss: 5.158435501399935, Val Loss: 5.980334971632276, Val MAE: 1.445176124572754\n",
      "Epoch 261/2000, Train Loss: 5.153961756838058, Val Loss: 5.976559898683003, Val MAE: 1.4458147287368774\n",
      "Epoch 262/2000, Train Loss: 5.149734756674182, Val Loss: 5.9728882511456804, Val MAE: 1.4463894367218018\n",
      "Epoch 263/2000, Train Loss: 5.14550824024109, Val Loss: 5.969635367393494, Val MAE: 1.446986198425293\n",
      "Epoch 264/2000, Train Loss: 5.141620434894212, Val Loss: 5.966003646453221, Val MAE: 1.4475979804992676\n",
      "Epoch 265/2000, Train Loss: 5.137355172499615, Val Loss: 5.96251702876318, Val MAE: 1.4481992721557617\n",
      "Epoch 266/2000, Train Loss: 5.133249806079945, Val Loss: 5.959315701609566, Val MAE: 1.4487720727920532\n",
      "Epoch 267/2000, Train Loss: 5.129410885957467, Val Loss: 5.955808287575131, Val MAE: 1.4493860006332397\n",
      "Epoch 268/2000, Train Loss: 5.12540830947112, Val Loss: 5.9529354856127785, Val MAE: 1.4499871730804443\n",
      "Epoch 269/2000, Train Loss: 5.1217151064798765, Val Loss: 5.949509549708593, Val MAE: 1.4505460262298584\n",
      "Epoch 270/2000, Train Loss: 5.117893691123457, Val Loss: 5.946731635502407, Val MAE: 1.4511035680770874\n",
      "Epoch 271/2000, Train Loss: 5.114139749233746, Val Loss: 5.943584240618206, Val MAE: 1.4517157077789307\n",
      "Epoch 272/2000, Train Loss: 5.110425743801133, Val Loss: 5.940445672898066, Val MAE: 1.4523003101348877\n",
      "Epoch 273/2000, Train Loss: 5.106831768503983, Val Loss: 5.9375300620283396, Val MAE: 1.452909231185913\n",
      "Epoch 274/2000, Train Loss: 5.103408302339411, Val Loss: 5.934564705405917, Val MAE: 1.453528881072998\n",
      "Epoch 275/2000, Train Loss: 5.099947059608481, Val Loss: 5.931503102892921, Val MAE: 1.4541847705841064\n",
      "Epoch 276/2000, Train Loss: 5.096246385776105, Val Loss: 5.928903849351974, Val MAE: 1.4547735452651978\n",
      "Epoch 277/2000, Train Loss: 5.093232379141244, Val Loss: 5.926695053066526, Val MAE: 1.4553613662719727\n",
      "Epoch 278/2000, Train Loss: 5.090419837550821, Val Loss: 5.924378414948781, Val MAE: 1.4559024572372437\n",
      "Epoch 279/2000, Train Loss: 5.087512444037476, Val Loss: 5.921936672358286, Val MAE: 1.4565294981002808\n",
      "Epoch 280/2000, Train Loss: 5.084319253230128, Val Loss: 5.919381760415577, Val MAE: 1.4571044445037842\n",
      "Epoch 281/2000, Train Loss: 5.0811595694806915, Val Loss: 5.9166941585994905, Val MAE: 1.45773184299469\n",
      "Epoch 282/2000, Train Loss: 5.078243783902382, Val Loss: 5.9144554776804785, Val MAE: 1.4583066701889038\n",
      "Epoch 283/2000, Train Loss: 5.0755399339458, Val Loss: 5.912259714944022, Val MAE: 1.458822250366211\n",
      "Epoch 284/2000, Train Loss: 5.072716598954624, Val Loss: 5.910193312735784, Val MAE: 1.4594206809997559\n",
      "Epoch 285/2000, Train Loss: 5.070001232943515, Val Loss: 5.908064314297268, Val MAE: 1.4600247144699097\n",
      "Epoch 286/2000, Train Loss: 5.06737138185918, Val Loss: 5.905627545856294, Val MAE: 1.4606107473373413\n",
      "Epoch 287/2000, Train Loss: 5.064504652332688, Val Loss: 5.903325570481164, Val MAE: 1.4612376689910889\n",
      "Epoch 288/2000, Train Loss: 5.061769961974517, Val Loss: 5.901444523107438, Val MAE: 1.4617912769317627\n",
      "Epoch 289/2000, Train Loss: 5.059363925910971, Val Loss: 5.899693121512731, Val MAE: 1.462278127670288\n",
      "Epoch 290/2000, Train Loss: 5.0569426747740405, Val Loss: 5.89776131936482, Val MAE: 1.4628210067749023\n",
      "Epoch 291/2000, Train Loss: 5.054513128923596, Val Loss: 5.895658078647795, Val MAE: 1.4634718894958496\n",
      "Epoch 292/2000, Train Loss: 5.051996122798731, Val Loss: 5.8937838815507435, Val MAE: 1.4639933109283447\n",
      "Epoch 293/2000, Train Loss: 5.049702241617466, Val Loss: 5.8918296836671376, Val MAE: 1.464685082435608\n",
      "Epoch 294/2000, Train Loss: 5.047314075556057, Val Loss: 5.89010430375735, Val MAE: 1.4652600288391113\n",
      "Epoch 295/2000, Train Loss: 5.045033271289174, Val Loss: 5.888165428524926, Val MAE: 1.465903639793396\n",
      "Epoch 296/2000, Train Loss: 5.042923625668955, Val Loss: 5.886542140018372, Val MAE: 1.4665281772613525\n",
      "Epoch 297/2000, Train Loss: 5.04080160716693, Val Loss: 5.88518051022575, Val MAE: 1.4669510126113892\n",
      "Epoch 298/2000, Train Loss: 5.03869675917417, Val Loss: 5.8834454871359325, Val MAE: 1.4675791263580322\n",
      "Epoch 299/2000, Train Loss: 5.03667358350014, Val Loss: 5.8816862886860255, Val MAE: 1.4682066440582275\n",
      "Epoch 300/2000, Train Loss: 5.03457047169232, Val Loss: 5.880109740155084, Val MAE: 1.4688540697097778\n",
      "Epoch 301/2000, Train Loss: 5.032529605292467, Val Loss: 5.878447045882543, Val MAE: 1.4694138765335083\n",
      "Epoch 302/2000, Train Loss: 5.030538501120757, Val Loss: 5.876984881503241, Val MAE: 1.4700117111206055\n",
      "Epoch 303/2000, Train Loss: 5.028544078256583, Val Loss: 5.875261863072713, Val MAE: 1.470644235610962\n",
      "Epoch 304/2000, Train Loss: 5.026420877245485, Val Loss: 5.873862821431387, Val MAE: 1.471161127090454\n",
      "Epoch 305/2000, Train Loss: 5.024576429251387, Val Loss: 5.872579309202376, Val MAE: 1.4716931581497192\n",
      "Epoch 306/2000, Train Loss: 5.022680444004505, Val Loss: 5.871122747659683, Val MAE: 1.4722341299057007\n",
      "Epoch 307/2000, Train Loss: 5.020792627536359, Val Loss: 5.869753294047856, Val MAE: 1.4727925062179565\n",
      "Epoch 308/2000, Train Loss: 5.01906413117316, Val Loss: 5.868479014862151, Val MAE: 1.473348617553711\n",
      "Epoch 309/2000, Train Loss: 5.017245725112506, Val Loss: 5.8670950049445745, Val MAE: 1.4739227294921875\n",
      "Epoch 310/2000, Train Loss: 5.01547286675242, Val Loss: 5.8657085072426565, Val MAE: 1.4744603633880615\n",
      "Epoch 311/2000, Train Loss: 5.0138974337719056, Val Loss: 5.8643003177075155, Val MAE: 1.4750486612319946\n",
      "Epoch 312/2000, Train Loss: 5.012100176683434, Val Loss: 5.863286207119624, Val MAE: 1.475521445274353\n",
      "Epoch 313/2000, Train Loss: 5.010488545104378, Val Loss: 5.862173311767124, Val MAE: 1.4759896993637085\n",
      "Epoch 314/2000, Train Loss: 5.008864253465144, Val Loss: 5.860758631002335, Val MAE: 1.4766710996627808\n",
      "Epoch 315/2000, Train Loss: 5.00718275986202, Val Loss: 5.85965788648242, Val MAE: 1.4771450757980347\n",
      "Epoch 316/2000, Train Loss: 5.005704658492159, Val Loss: 5.858350515365601, Val MAE: 1.4776731729507446\n",
      "Epoch 317/2000, Train Loss: 5.004085143967643, Val Loss: 5.857393260513033, Val MAE: 1.478192687034607\n",
      "Epoch 318/2000, Train Loss: 5.002565510350323, Val Loss: 5.856187616075788, Val MAE: 1.478641152381897\n",
      "Epoch 319/2000, Train Loss: 5.001112396525395, Val Loss: 5.854910404909225, Val MAE: 1.4791887998580933\n",
      "Epoch 320/2000, Train Loss: 4.999576397776099, Val Loss: 5.853724736542929, Val MAE: 1.4797823429107666\n",
      "Epoch 321/2000, Train Loss: 4.998125531609539, Val Loss: 5.852808604637782, Val MAE: 1.4802520275115967\n",
      "Epoch 322/2000, Train Loss: 4.9967676971123485, Val Loss: 5.8518691545441035, Val MAE: 1.480677843093872\n",
      "Epoch 323/2000, Train Loss: 4.995389302081215, Val Loss: 5.850849533364887, Val MAE: 1.48116135597229\n",
      "Epoch 324/2000, Train Loss: 4.994059916443818, Val Loss: 5.849746586311431, Val MAE: 1.4816738367080688\n",
      "Epoch 325/2000, Train Loss: 4.992623762284079, Val Loss: 5.848853704475221, Val MAE: 1.4821579456329346\n",
      "Epoch 326/2000, Train Loss: 4.991285102155548, Val Loss: 5.847974428108761, Val MAE: 1.4826486110687256\n",
      "Epoch 327/2000, Train Loss: 4.989974443599771, Val Loss: 5.84686461516789, Val MAE: 1.4831864833831787\n",
      "Epoch 328/2000, Train Loss: 4.988803363149022, Val Loss: 5.845820778892154, Val MAE: 1.4836786985397339\n",
      "Epoch 329/2000, Train Loss: 4.9874259991773595, Val Loss: 5.84512509334655, Val MAE: 1.4840967655181885\n",
      "Epoch 330/2000, Train Loss: 4.986245929438237, Val Loss: 5.844039539496104, Val MAE: 1.4846532344818115\n",
      "Epoch 331/2000, Train Loss: 4.985093859254221, Val Loss: 5.843252952609744, Val MAE: 1.4850391149520874\n",
      "Epoch 332/2000, Train Loss: 4.983800591131192, Val Loss: 5.8423550469534735, Val MAE: 1.4855148792266846\n",
      "Epoch 333/2000, Train Loss: 4.982621371830132, Val Loss: 5.841412081604912, Val MAE: 1.4859809875488281\n",
      "Epoch 334/2000, Train Loss: 4.981443906536559, Val Loss: 5.840648666733787, Val MAE: 1.4863933324813843\n",
      "Epoch 335/2000, Train Loss: 4.980305376947347, Val Loss: 5.839874527284077, Val MAE: 1.4868367910385132\n",
      "Epoch 336/2000, Train Loss: 4.979086133421561, Val Loss: 5.838985846156166, Val MAE: 1.4873110055923462\n",
      "Epoch 337/2000, Train Loss: 4.977925931448997, Val Loss: 5.838150325275603, Val MAE: 1.487723708152771\n",
      "Epoch 338/2000, Train Loss: 4.976890320670285, Val Loss: 5.837442957219624, Val MAE: 1.488163948059082\n",
      "Epoch 339/2000, Train Loss: 4.975636497034845, Val Loss: 5.836637479918344, Val MAE: 1.4886385202407837\n",
      "Epoch 340/2000, Train Loss: 4.974493092643189, Val Loss: 5.835749743949799, Val MAE: 1.4890612363815308\n",
      "Epoch 341/2000, Train Loss: 4.9733473067559375, Val Loss: 5.835015278486979, Val MAE: 1.489506483078003\n",
      "Epoch 342/2000, Train Loss: 4.9722971936375195, Val Loss: 5.834288341658456, Val MAE: 1.4899225234985352\n",
      "Epoch 343/2000, Train Loss: 4.970932000194518, Val Loss: 5.833560326269695, Val MAE: 1.49038827419281\n",
      "Epoch 344/2000, Train Loss: 4.969940594456596, Val Loss: 5.832764804363251, Val MAE: 1.4907697439193726\n",
      "Epoch 345/2000, Train Loss: 4.968932354231647, Val Loss: 5.832000597601845, Val MAE: 1.49131441116333\n",
      "Epoch 346/2000, Train Loss: 4.967809697973038, Val Loss: 5.831259244964237, Val MAE: 1.4917324781417847\n",
      "Epoch 347/2000, Train Loss: 4.9665927355649275, Val Loss: 5.830550271840322, Val MAE: 1.4921376705169678\n",
      "Epoch 348/2000, Train Loss: 4.965687800697951, Val Loss: 5.82977557182312, Val MAE: 1.4926397800445557\n",
      "Epoch 349/2000, Train Loss: 4.964684382816632, Val Loss: 5.829156715245474, Val MAE: 1.4929182529449463\n",
      "Epoch 350/2000, Train Loss: 4.963741282650043, Val Loss: 5.828589638074239, Val MAE: 1.493288516998291\n",
      "Epoch 351/2000, Train Loss: 4.9627258760804684, Val Loss: 5.82775325008801, Val MAE: 1.493769884109497\n",
      "Epoch 352/2000, Train Loss: 4.961750117906899, Val Loss: 5.827121474913189, Val MAE: 1.4941045045852661\n",
      "Epoch 353/2000, Train Loss: 4.9610749568858505, Val Loss: 5.8263349916253775, Val MAE: 1.4946060180664062\n",
      "Epoch 354/2000, Train Loss: 4.959746002646528, Val Loss: 5.825847094967251, Val MAE: 1.4947725534439087\n",
      "Epoch 355/2000, Train Loss: 4.958968309824827, Val Loss: 5.825115047750019, Val MAE: 1.4952350854873657\n",
      "Epoch 356/2000, Train Loss: 4.958041941665628, Val Loss: 5.8246450665451235, Val MAE: 1.4955322742462158\n",
      "Epoch 357/2000, Train Loss: 4.957059764055002, Val Loss: 5.823893842243013, Val MAE: 1.49594247341156\n",
      "Epoch 358/2000, Train Loss: 4.956158402272105, Val Loss: 5.823311056409564, Val MAE: 1.4962658882141113\n",
      "Epoch 359/2000, Train Loss: 4.95520665110923, Val Loss: 5.822668979565303, Val MAE: 1.4966115951538086\n",
      "Epoch 360/2000, Train Loss: 4.954393292011428, Val Loss: 5.821942354951586, Val MAE: 1.4970626831054688\n",
      "Epoch 361/2000, Train Loss: 4.953425611864528, Val Loss: 5.821360292888823, Val MAE: 1.4973195791244507\n",
      "Epoch 362/2000, Train Loss: 4.952701183233006, Val Loss: 5.820785416024072, Val MAE: 1.4978461265563965\n",
      "Epoch 363/2000, Train Loss: 4.951536605489271, Val Loss: 5.820154581751142, Val MAE: 1.4981756210327148\n",
      "Epoch 364/2000, Train Loss: 4.950740421440437, Val Loss: 5.819620417697089, Val MAE: 1.4985036849975586\n",
      "Epoch 365/2000, Train Loss: 4.949898500872936, Val Loss: 5.818861761263439, Val MAE: 1.4989064931869507\n",
      "Epoch 366/2000, Train Loss: 4.948992116831254, Val Loss: 5.8183089310214635, Val MAE: 1.4991406202316284\n",
      "Epoch 367/2000, Train Loss: 4.948145605949488, Val Loss: 5.817737123795918, Val MAE: 1.4994337558746338\n",
      "Epoch 368/2000, Train Loss: 4.9472965437535334, Val Loss: 5.817132091238385, Val MAE: 1.4998433589935303\n",
      "Epoch 369/2000, Train Loss: 4.946491794458398, Val Loss: 5.816645975623812, Val MAE: 1.5001623630523682\n",
      "Epoch 370/2000, Train Loss: 4.945690623459594, Val Loss: 5.816156407197316, Val MAE: 1.5004459619522095\n",
      "Epoch 371/2000, Train Loss: 4.944860413985797, Val Loss: 5.815525140081133, Val MAE: 1.5007085800170898\n",
      "Epoch 372/2000, Train Loss: 4.944049350633608, Val Loss: 5.81492456084206, Val MAE: 1.5010851621627808\n",
      "Epoch 373/2000, Train Loss: 4.943309892215917, Val Loss: 5.814398464702425, Val MAE: 1.5013501644134521\n",
      "Epoch 374/2000, Train Loss: 4.942445407633385, Val Loss: 5.81374743722734, Val MAE: 1.5017571449279785\n",
      "Epoch 375/2000, Train Loss: 4.941689111954402, Val Loss: 5.813217377378827, Val MAE: 1.5020568370819092\n",
      "Epoch 376/2000, Train Loss: 4.94067314750553, Val Loss: 5.812697671708607, Val MAE: 1.502441167831421\n",
      "Epoch 377/2000, Train Loss: 4.939893860406701, Val Loss: 5.812138160069783, Val MAE: 1.5027495622634888\n",
      "Epoch 378/2000, Train Loss: 4.939127710877756, Val Loss: 5.811514248450597, Val MAE: 1.5029469728469849\n",
      "Epoch 379/2000, Train Loss: 4.9382767616777725, Val Loss: 5.811036768413725, Val MAE: 1.5033003091812134\n",
      "Epoch 380/2000, Train Loss: 4.937558851894438, Val Loss: 5.810515471867153, Val MAE: 1.5036901235580444\n",
      "Epoch 381/2000, Train Loss: 4.936751008538164, Val Loss: 5.809946625005631, Val MAE: 1.5039443969726562\n",
      "Epoch 382/2000, Train Loss: 4.936003239770198, Val Loss: 5.809329870201292, Val MAE: 1.5041816234588623\n",
      "Epoch 383/2000, Train Loss: 4.935187186105295, Val Loss: 5.808799674113591, Val MAE: 1.5045377016067505\n",
      "Epoch 384/2000, Train Loss: 4.93445297558654, Val Loss: 5.808345666953495, Val MAE: 1.504791498184204\n",
      "Epoch 385/2000, Train Loss: 4.933698953459058, Val Loss: 5.807645786376226, Val MAE: 1.5051579475402832\n",
      "Epoch 386/2000, Train Loss: 4.932876150429837, Val Loss: 5.807279073056721, Val MAE: 1.50551438331604\n",
      "Epoch 387/2000, Train Loss: 4.932227815996608, Val Loss: 5.806715251434417, Val MAE: 1.505837082862854\n",
      "Epoch 388/2000, Train Loss: 4.931508350775844, Val Loss: 5.806169231732686, Val MAE: 1.5061753988265991\n",
      "Epoch 389/2000, Train Loss: 4.930737151077335, Val Loss: 5.805788333926882, Val MAE: 1.5063233375549316\n",
      "Epoch 390/2000, Train Loss: 4.930104228101765, Val Loss: 5.805226409719104, Val MAE: 1.5065983533859253\n",
      "Epoch 391/2000, Train Loss: 4.929352209497406, Val Loss: 5.804765860239665, Val MAE: 1.5068886280059814\n",
      "Epoch 392/2000, Train Loss: 4.928646831737755, Val Loss: 5.80437528803235, Val MAE: 1.507131576538086\n",
      "Epoch 393/2000, Train Loss: 4.9279610939187295, Val Loss: 5.80387120729401, Val MAE: 1.5074268579483032\n",
      "Epoch 394/2000, Train Loss: 4.927254275979713, Val Loss: 5.803385269074213, Val MAE: 1.5076696872711182\n",
      "Epoch 395/2000, Train Loss: 4.926640174284641, Val Loss: 5.802933035861878, Val MAE: 1.5078551769256592\n",
      "Epoch 396/2000, Train Loss: 4.92595241486774, Val Loss: 5.802563457261948, Val MAE: 1.5080525875091553\n",
      "Epoch 397/2000, Train Loss: 4.925110401320357, Val Loss: 5.802086070889518, Val MAE: 1.5083580017089844\n",
      "Epoch 398/2000, Train Loss: 4.924589718010261, Val Loss: 5.8018359541893005, Val MAE: 1.5086300373077393\n",
      "Epoch 399/2000, Train Loss: 4.923632440849487, Val Loss: 5.80133267385619, Val MAE: 1.5089596509933472\n",
      "Epoch 400/2000, Train Loss: 4.923051047896801, Val Loss: 5.800883278960273, Val MAE: 1.5091218948364258\n",
      "Epoch 401/2000, Train Loss: 4.922378027893088, Val Loss: 5.80044732775007, Val MAE: 1.5093313455581665\n",
      "Epoch 402/2000, Train Loss: 4.921784060630206, Val Loss: 5.800028287229084, Val MAE: 1.5095579624176025\n",
      "Epoch 403/2000, Train Loss: 4.921028493657939, Val Loss: 5.799569829588845, Val MAE: 1.5098942518234253\n",
      "Epoch 404/2000, Train Loss: 4.920438192795295, Val Loss: 5.799004532042003, Val MAE: 1.5101321935653687\n",
      "Epoch 405/2000, Train Loss: 4.91978490873856, Val Loss: 5.7986606785229275, Val MAE: 1.5103096961975098\n",
      "Epoch 406/2000, Train Loss: 4.919127103146192, Val Loss: 5.798182322865441, Val MAE: 1.5105630159378052\n",
      "Epoch 407/2000, Train Loss: 4.91851058093718, Val Loss: 5.797766627300353, Val MAE: 1.5108253955841064\n",
      "Epoch 408/2000, Train Loss: 4.91782027722582, Val Loss: 5.797268433230264, Val MAE: 1.5110366344451904\n",
      "Epoch 409/2000, Train Loss: 4.917239376789087, Val Loss: 5.796783461457207, Val MAE: 1.5112097263336182\n",
      "Epoch 410/2000, Train Loss: 4.916640715471276, Val Loss: 5.796336378370013, Val MAE: 1.5114939212799072\n",
      "Epoch 411/2000, Train Loss: 4.916058261909001, Val Loss: 5.796053709018798, Val MAE: 1.5117534399032593\n",
      "Epoch 412/2000, Train Loss: 4.9153285759628575, Val Loss: 5.795649616491227, Val MAE: 1.5118966102600098\n",
      "Epoch 413/2000, Train Loss: 4.914765208329065, Val Loss: 5.795110632975896, Val MAE: 1.5121421813964844\n",
      "Epoch 414/2000, Train Loss: 4.914022145721909, Val Loss: 5.794713356665203, Val MAE: 1.5123696327209473\n",
      "Epoch 415/2000, Train Loss: 4.91345130573375, Val Loss: 5.794413350877308, Val MAE: 1.5126023292541504\n",
      "Epoch 416/2000, Train Loss: 4.9128517647219985, Val Loss: 5.793975280863898, Val MAE: 1.5128071308135986\n",
      "Epoch 417/2000, Train Loss: 4.912164021950683, Val Loss: 5.793460535151618, Val MAE: 1.5130910873413086\n",
      "Epoch 418/2000, Train Loss: 4.9115864305805585, Val Loss: 5.7931280334790545, Val MAE: 1.5132410526275635\n",
      "Epoch 419/2000, Train Loss: 4.911021218649591, Val Loss: 5.792791796582086, Val MAE: 1.5136138200759888\n",
      "Epoch 420/2000, Train Loss: 4.910358088645343, Val Loss: 5.792280194305238, Val MAE: 1.5137484073638916\n",
      "Epoch 421/2000, Train Loss: 4.9097241871111486, Val Loss: 5.791874715260097, Val MAE: 1.5139585733413696\n",
      "Epoch 422/2000, Train Loss: 4.909177745178825, Val Loss: 5.791575336740131, Val MAE: 1.5142431259155273\n",
      "Epoch 423/2000, Train Loss: 4.908571609152725, Val Loss: 5.791129459937413, Val MAE: 1.514384150505066\n",
      "Epoch 424/2000, Train Loss: 4.907950173312081, Val Loss: 5.790546031225295, Val MAE: 1.5145487785339355\n",
      "Epoch 425/2000, Train Loss: 4.907319327167462, Val Loss: 5.790148148934047, Val MAE: 1.5147463083267212\n",
      "Epoch 426/2000, Train Loss: 4.906725722072155, Val Loss: 5.789743202073233, Val MAE: 1.5149619579315186\n",
      "Epoch 427/2000, Train Loss: 4.906141430770056, Val Loss: 5.789286091214135, Val MAE: 1.5153294801712036\n",
      "Epoch 428/2000, Train Loss: 4.905481813993037, Val Loss: 5.788927833239238, Val MAE: 1.5153906345367432\n",
      "Epoch 429/2000, Train Loss: 4.904960887220246, Val Loss: 5.788449791215715, Val MAE: 1.5156190395355225\n",
      "Epoch 430/2000, Train Loss: 4.904344749383429, Val Loss: 5.788172816946393, Val MAE: 1.515824317932129\n",
      "Epoch 431/2000, Train Loss: 4.903892340882036, Val Loss: 5.787754068771998, Val MAE: 1.5159878730773926\n",
      "Epoch 432/2000, Train Loss: 4.903330751131216, Val Loss: 5.787340811320713, Val MAE: 1.5161828994750977\n",
      "Epoch 433/2000, Train Loss: 4.902725479330432, Val Loss: 5.786954563288462, Val MAE: 1.5161412954330444\n",
      "Epoch 434/2000, Train Loss: 4.90216825643951, Val Loss: 5.786636911687397, Val MAE: 1.5164676904678345\n",
      "Epoch 435/2000, Train Loss: 4.901449686717584, Val Loss: 5.786374744914827, Val MAE: 1.5166336297988892\n",
      "Epoch 436/2000, Train Loss: 4.90092487362107, Val Loss: 5.78604701587132, Val MAE: 1.5168066024780273\n",
      "Epoch 437/2000, Train Loss: 4.900499399021078, Val Loss: 5.785771649508249, Val MAE: 1.5170738697052002\n",
      "Epoch 438/2000, Train Loss: 4.899812703912786, Val Loss: 5.785220852919987, Val MAE: 1.5171537399291992\n",
      "Epoch 439/2000, Train Loss: 4.8992284302650955, Val Loss: 5.784902650685537, Val MAE: 1.5173219442367554\n",
      "Epoch 440/2000, Train Loss: 4.8986468624496995, Val Loss: 5.784442925737018, Val MAE: 1.5174518823623657\n",
      "Epoch 441/2000, Train Loss: 4.898130131877528, Val Loss: 5.78415125040781, Val MAE: 1.5176503658294678\n",
      "Epoch 442/2000, Train Loss: 4.897565108596522, Val Loss: 5.783705570868084, Val MAE: 1.5177966356277466\n",
      "Epoch 443/2000, Train Loss: 4.896991833981405, Val Loss: 5.783410438469478, Val MAE: 1.5180507898330688\n",
      "Epoch 444/2000, Train Loss: 4.8964140358025, Val Loss: 5.783135232471285, Val MAE: 1.518274188041687\n",
      "Epoch 445/2000, Train Loss: 4.895885141343088, Val Loss: 5.782796500694184, Val MAE: 1.518389344215393\n",
      "Epoch 446/2000, Train Loss: 4.895324296776431, Val Loss: 5.782385154849007, Val MAE: 1.518540382385254\n",
      "Epoch 447/2000, Train Loss: 4.894870226743023, Val Loss: 5.782181806507564, Val MAE: 1.518829584121704\n",
      "Epoch 448/2000, Train Loss: 4.894325710990701, Val Loss: 5.781807283560435, Val MAE: 1.5188738107681274\n",
      "Epoch 449/2000, Train Loss: 4.893723006309004, Val Loss: 5.781599220775423, Val MAE: 1.519084095954895\n",
      "Epoch 450/2000, Train Loss: 4.8932321451614875, Val Loss: 5.781067423877262, Val MAE: 1.5192619562149048\n",
      "Epoch 451/2000, Train Loss: 4.892608978852566, Val Loss: 5.78078771630923, Val MAE: 1.5194560289382935\n",
      "Epoch 452/2000, Train Loss: 4.892183706689789, Val Loss: 5.780385927075431, Val MAE: 1.5197086334228516\n",
      "Epoch 453/2000, Train Loss: 4.891640043056903, Val Loss: 5.779955768869037, Val MAE: 1.5197957754135132\n",
      "Epoch 454/2000, Train Loss: 4.891087278156253, Val Loss: 5.779791055690675, Val MAE: 1.519977331161499\n",
      "Epoch 455/2000, Train Loss: 4.890564343488771, Val Loss: 5.779385328292847, Val MAE: 1.5201596021652222\n",
      "Epoch 456/2000, Train Loss: 4.889995071001887, Val Loss: 5.778945372218177, Val MAE: 1.520374059677124\n",
      "Epoch 457/2000, Train Loss: 4.889587087590873, Val Loss: 5.778542170921962, Val MAE: 1.520446538925171\n",
      "Epoch 458/2000, Train Loss: 4.8890180164062755, Val Loss: 5.778525859117508, Val MAE: 1.5206854343414307\n",
      "Epoch 459/2000, Train Loss: 4.888506891529046, Val Loss: 5.778179558969679, Val MAE: 1.5208467245101929\n",
      "Epoch 460/2000, Train Loss: 4.88804391409009, Val Loss: 5.7777949046520956, Val MAE: 1.5208569765090942\n",
      "Epoch 461/2000, Train Loss: 4.887423828727604, Val Loss: 5.777664536521549, Val MAE: 1.5210837125778198\n",
      "Epoch 462/2000, Train Loss: 4.886929742708193, Val Loss: 5.777173347416378, Val MAE: 1.521215558052063\n",
      "Epoch 463/2000, Train Loss: 4.886401645891757, Val Loss: 5.776872941425869, Val MAE: 1.5213336944580078\n",
      "Epoch 464/2000, Train Loss: 4.885996120436739, Val Loss: 5.776589170807884, Val MAE: 1.5214612483978271\n",
      "Epoch 465/2000, Train Loss: 4.885438442230225, Val Loss: 5.776247451702754, Val MAE: 1.521572470664978\n",
      "Epoch 466/2000, Train Loss: 4.884928319955242, Val Loss: 5.7759457258951095, Val MAE: 1.5217268466949463\n",
      "Epoch 467/2000, Train Loss: 4.884511955870559, Val Loss: 5.775518326532273, Val MAE: 1.5217936038970947\n",
      "Epoch 468/2000, Train Loss: 4.883984433915282, Val Loss: 5.775190171741304, Val MAE: 1.5218969583511353\n",
      "Epoch 469/2000, Train Loss: 4.883475065567597, Val Loss: 5.774951905012131, Val MAE: 1.522086501121521\n",
      "Epoch 470/2000, Train Loss: 4.882865315263799, Val Loss: 5.774725882779984, Val MAE: 1.5223195552825928\n",
      "Epoch 471/2000, Train Loss: 4.88242553016867, Val Loss: 5.774391478016263, Val MAE: 1.5224307775497437\n",
      "Epoch 472/2000, Train Loss: 4.882006561134026, Val Loss: 5.774122350272679, Val MAE: 1.522631287574768\n",
      "Epoch 473/2000, Train Loss: 4.881452046597458, Val Loss: 5.773846881730216, Val MAE: 1.5226603746414185\n",
      "Epoch 474/2000, Train Loss: 4.8808530705267685, Val Loss: 5.773431319566, Val MAE: 1.5229345560073853\n",
      "Epoch 475/2000, Train Loss: 4.880444677323313, Val Loss: 5.7732067661626, Val MAE: 1.5231815576553345\n",
      "Epoch 476/2000, Train Loss: 4.879891979005004, Val Loss: 5.772930871872675, Val MAE: 1.523268699645996\n",
      "Epoch 477/2000, Train Loss: 4.879460743015004, Val Loss: 5.7725655124301, Val MAE: 1.5233237743377686\n",
      "Epoch 478/2000, Train Loss: 4.878894888631716, Val Loss: 5.772247517392749, Val MAE: 1.5234802961349487\n",
      "Epoch 479/2000, Train Loss: 4.878427750973507, Val Loss: 5.7719987872101015, Val MAE: 1.5235397815704346\n",
      "Epoch 480/2000, Train Loss: 4.87794537537525, Val Loss: 5.771667578390667, Val MAE: 1.5237345695495605\n",
      "Epoch 481/2000, Train Loss: 4.877449540728742, Val Loss: 5.771411171981266, Val MAE: 1.5238051414489746\n",
      "Epoch 482/2000, Train Loss: 4.877060927187943, Val Loss: 5.771142054171789, Val MAE: 1.5239940881729126\n",
      "Epoch 483/2000, Train Loss: 4.8764606389743825, Val Loss: 5.77095937586966, Val MAE: 1.5239465236663818\n",
      "Epoch 484/2000, Train Loss: 4.8759988142842134, Val Loss: 5.770542786234901, Val MAE: 1.5240356922149658\n",
      "Epoch 485/2000, Train Loss: 4.8755198802867294, Val Loss: 5.7702954823062536, Val MAE: 1.5241106748580933\n",
      "Epoch 486/2000, Train Loss: 4.875025523818598, Val Loss: 5.769910108475458, Val MAE: 1.5240939855575562\n",
      "Epoch 487/2000, Train Loss: 4.874589922061592, Val Loss: 5.769671257053103, Val MAE: 1.5242815017700195\n",
      "Epoch 488/2000, Train Loss: 4.874097846290793, Val Loss: 5.769353870834623, Val MAE: 1.524391531944275\n",
      "Epoch 489/2000, Train Loss: 4.873526835138942, Val Loss: 5.768960900249935, Val MAE: 1.5245774984359741\n",
      "Epoch 490/2000, Train Loss: 4.8731294969577545, Val Loss: 5.768644768567312, Val MAE: 1.5247209072113037\n",
      "Epoch 491/2000, Train Loss: 4.87275824795657, Val Loss: 5.7683768059526175, Val MAE: 1.5247223377227783\n",
      "Epoch 492/2000, Train Loss: 4.87234050262462, Val Loss: 5.76833300221534, Val MAE: 1.5250047445297241\n",
      "Epoch 493/2000, Train Loss: 4.871733680262384, Val Loss: 5.767984239828019, Val MAE: 1.5251471996307373\n",
      "Epoch 494/2000, Train Loss: 4.8712661653042515, Val Loss: 5.767685047217777, Val MAE: 1.5252203941345215\n",
      "Epoch 495/2000, Train Loss: 4.870779343485328, Val Loss: 5.767481159596216, Val MAE: 1.5253320932388306\n",
      "Epoch 496/2000, Train Loss: 4.870328100847424, Val Loss: 5.767218382585616, Val MAE: 1.525456190109253\n",
      "Epoch 497/2000, Train Loss: 4.869891402751669, Val Loss: 5.766915500164032, Val MAE: 1.5255450010299683\n",
      "Epoch 498/2000, Train Loss: 4.869415986689257, Val Loss: 5.7665966507934385, Val MAE: 1.5256222486495972\n",
      "Epoch 499/2000, Train Loss: 4.86883772277025, Val Loss: 5.766402583746683, Val MAE: 1.5256413221359253\n",
      "Epoch 500/2000, Train Loss: 4.868423154559223, Val Loss: 5.766053692215965, Val MAE: 1.5258142948150635\n",
      "Epoch 501/2000, Train Loss: 4.867929166731948, Val Loss: 5.7659441488129755, Val MAE: 1.525951623916626\n",
      "Epoch 502/2000, Train Loss: 4.86766138803135, Val Loss: 5.76583347717921, Val MAE: 1.5261248350143433\n",
      "Epoch 503/2000, Train Loss: 4.867055702276727, Val Loss: 5.765526929071972, Val MAE: 1.5262600183486938\n",
      "Epoch 504/2000, Train Loss: 4.866605134541628, Val Loss: 5.7651221596059345, Val MAE: 1.5261865854263306\n",
      "Epoch 505/2000, Train Loss: 4.866171035006621, Val Loss: 5.76495316198894, Val MAE: 1.5264122486114502\n",
      "Epoch 506/2000, Train Loss: 4.865723723585078, Val Loss: 5.764594858600979, Val MAE: 1.5264761447906494\n",
      "Epoch 507/2000, Train Loss: 4.865324375659689, Val Loss: 5.764248408022381, Val MAE: 1.5263773202896118\n",
      "Epoch 508/2000, Train Loss: 4.864924728113774, Val Loss: 5.763988361472175, Val MAE: 1.5265905857086182\n",
      "Epoch 509/2000, Train Loss: 4.864401972344295, Val Loss: 5.763753898087002, Val MAE: 1.5265520811080933\n",
      "Epoch 510/2000, Train Loss: 4.863961048287632, Val Loss: 5.763380193994159, Val MAE: 1.5266387462615967\n",
      "Epoch 511/2000, Train Loss: 4.863445014560172, Val Loss: 5.763146491277785, Val MAE: 1.526726484298706\n",
      "Epoch 512/2000, Train Loss: 4.863022523134821, Val Loss: 5.7629824706486295, Val MAE: 1.5268045663833618\n",
      "Epoch 513/2000, Train Loss: 4.862699091686685, Val Loss: 5.762635836998622, Val MAE: 1.526874303817749\n",
      "Epoch 514/2000, Train Loss: 4.8621523397093265, Val Loss: 5.762401278529849, Val MAE: 1.5268638134002686\n",
      "Epoch 515/2000, Train Loss: 4.861728332610998, Val Loss: 5.7621584222430275, Val MAE: 1.5270594358444214\n",
      "Epoch 516/2000, Train Loss: 4.861305967877042, Val Loss: 5.76197750227792, Val MAE: 1.5271272659301758\n",
      "Epoch 517/2000, Train Loss: 4.860997980168912, Val Loss: 5.761915598596845, Val MAE: 1.5272185802459717\n",
      "Epoch 518/2000, Train Loss: 4.860431122342689, Val Loss: 5.761547050305775, Val MAE: 1.5273388624191284\n",
      "Epoch 519/2000, Train Loss: 4.859981102398656, Val Loss: 5.761542754513877, Val MAE: 1.527591347694397\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 520/2000, Train Loss: 4.859511348189017, Val Loss: 5.761242828198841, Val MAE: 1.5277868509292603\n",
      "Epoch 521/2000, Train Loss: 4.859103061248284, Val Loss: 5.760953000613621, Val MAE: 1.5279655456542969\n",
      "Epoch 522/2000, Train Loss: 4.85864464352262, Val Loss: 5.760659721635637, Val MAE: 1.5279512405395508\n",
      "Epoch 523/2000, Train Loss: 4.8582320230131595, Val Loss: 5.7604535193670365, Val MAE: 1.5279420614242554\n",
      "Epoch 524/2000, Train Loss: 4.857824969863354, Val Loss: 5.760209337586448, Val MAE: 1.5280183553695679\n",
      "Epoch 525/2000, Train Loss: 4.857505808428077, Val Loss: 5.759971732185001, Val MAE: 1.5281873941421509\n",
      "Epoch 526/2000, Train Loss: 4.8570456454380615, Val Loss: 5.75968768057369, Val MAE: 1.528157353401184\n",
      "Epoch 527/2000, Train Loss: 4.856490475502606, Val Loss: 5.7595207180295676, Val MAE: 1.5283252000808716\n",
      "Epoch 528/2000, Train Loss: 4.85619082538298, Val Loss: 5.7593249423163275, Val MAE: 1.528409481048584\n",
      "Epoch 529/2000, Train Loss: 4.855701096051166, Val Loss: 5.759039765312558, Val MAE: 1.528440237045288\n",
      "Epoch 530/2000, Train Loss: 4.855388790327, Val Loss: 5.758886604082017, Val MAE: 1.5285532474517822\n",
      "Epoch 531/2000, Train Loss: 4.854899537916412, Val Loss: 5.758684703281948, Val MAE: 1.5284830331802368\n",
      "Epoch 532/2000, Train Loss: 4.8545264679891265, Val Loss: 5.758372340883527, Val MAE: 1.5286554098129272\n",
      "Epoch 533/2000, Train Loss: 4.854104961427546, Val Loss: 5.758215941133953, Val MAE: 1.528733730316162\n",
      "Epoch 534/2000, Train Loss: 4.8537834521241185, Val Loss: 5.757984294777825, Val MAE: 1.5288496017456055\n",
      "Epoch 535/2000, Train Loss: 4.853229339771782, Val Loss: 5.757809656006949, Val MAE: 1.5288060903549194\n",
      "Epoch 536/2000, Train Loss: 4.852968960453997, Val Loss: 5.757567045234499, Val MAE: 1.5289195775985718\n",
      "Epoch 537/2000, Train Loss: 4.852481614719493, Val Loss: 5.757232521261487, Val MAE: 1.52898371219635\n",
      "Epoch 538/2000, Train Loss: 4.852091506102526, Val Loss: 5.757238044625237, Val MAE: 1.5290745496749878\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 539/2000, Train Loss: 4.851685452696635, Val Loss: 5.757107220944905, Val MAE: 1.5292093753814697\n",
      "Epoch 540/2000, Train Loss: 4.851267307870647, Val Loss: 5.756746919382186, Val MAE: 1.5292326211929321\n",
      "Epoch 541/2000, Train Loss: 4.850875171182522, Val Loss: 5.75657121908097, Val MAE: 1.5293422937393188\n",
      "Epoch 542/2000, Train Loss: 4.850476619386875, Val Loss: 5.756118902138302, Val MAE: 1.5292590856552124\n",
      "Epoch 543/2000, Train Loss: 4.850014375530614, Val Loss: 5.756065655322302, Val MAE: 1.5294890403747559\n",
      "Epoch 544/2000, Train Loss: 4.849793494840603, Val Loss: 5.755853056907654, Val MAE: 1.5295355319976807\n",
      "Epoch 545/2000, Train Loss: 4.849253958135801, Val Loss: 5.755597886585054, Val MAE: 1.5294214487075806\n",
      "Epoch 546/2000, Train Loss: 4.848876702264267, Val Loss: 5.755342205365499, Val MAE: 1.5294904708862305\n",
      "Epoch 547/2000, Train Loss: 4.848496714162894, Val Loss: 5.755195963950384, Val MAE: 1.5295984745025635\n",
      "Epoch 548/2000, Train Loss: 4.848023499016701, Val Loss: 5.754970998991103, Val MAE: 1.5296911001205444\n",
      "Epoch 549/2000, Train Loss: 4.847675937019047, Val Loss: 5.754777457032885, Val MAE: 1.529700517654419\n",
      "Epoch 550/2000, Train Loss: 4.847289657054734, Val Loss: 5.754681737650008, Val MAE: 1.5297772884368896\n",
      "Epoch 551/2000, Train Loss: 4.846965901438036, Val Loss: 5.754382956595648, Val MAE: 1.5298981666564941\n",
      "Epoch 552/2000, Train Loss: 4.846468529344782, Val Loss: 5.7540818481218245, Val MAE: 1.5298616886138916\n",
      "Epoch 553/2000, Train Loss: 4.846039977665512, Val Loss: 5.753990647338686, Val MAE: 1.529945731163025\n",
      "Epoch 554/2000, Train Loss: 4.845740719137474, Val Loss: 5.753814651852562, Val MAE: 1.530027151107788\n",
      "Epoch 555/2000, Train Loss: 4.845271192417495, Val Loss: 5.753508147739229, Val MAE: 1.530051827430725\n",
      "Epoch 556/2000, Train Loss: 4.844851260460986, Val Loss: 5.753362982046037, Val MAE: 1.5300743579864502\n",
      "Epoch 557/2000, Train Loss: 4.8444661260155595, Val Loss: 5.753155489762624, Val MAE: 1.530029296875\n",
      "Epoch 558/2000, Train Loss: 4.844102426375589, Val Loss: 5.752998752253396, Val MAE: 1.5302592515945435\n",
      "Epoch 559/2000, Train Loss: 4.843744531841305, Val Loss: 5.752805891491118, Val MAE: 1.5303899049758911\n",
      "Epoch 560/2000, Train Loss: 4.843293544435703, Val Loss: 5.752677483218057, Val MAE: 1.5304762125015259\n",
      "Epoch 561/2000, Train Loss: 4.842934770880028, Val Loss: 5.752479876790728, Val MAE: 1.5304679870605469\n",
      "Epoch 562/2000, Train Loss: 4.842566123970473, Val Loss: 5.752278611773536, Val MAE: 1.5304867029190063\n",
      "Epoch 563/2000, Train Loss: 4.842268402721045, Val Loss: 5.752135046890804, Val MAE: 1.5306344032287598\n",
      "Epoch 564/2000, Train Loss: 4.8419102906171965, Val Loss: 5.751927316188812, Val MAE: 1.5306912660598755\n",
      "Epoch 565/2000, Train Loss: 4.841506025014374, Val Loss: 5.7518811992236545, Val MAE: 1.5307021141052246\n",
      "Epoch 566/2000, Train Loss: 4.840977520801453, Val Loss: 5.75162539879481, Val MAE: 1.5306777954101562\n",
      "Epoch 567/2000, Train Loss: 4.840643209193758, Val Loss: 5.751481005123684, Val MAE: 1.5308016538619995\n",
      "Epoch 568/2000, Train Loss: 4.840253563962971, Val Loss: 5.751262613705227, Val MAE: 1.530854344367981\n",
      "Epoch 569/2000, Train Loss: 4.839934153684608, Val Loss: 5.751154207047962, Val MAE: 1.5308805704116821\n",
      "Epoch 570/2000, Train Loss: 4.839597642505791, Val Loss: 5.750885920865195, Val MAE: 1.5308133363723755\n",
      "Epoch 571/2000, Train Loss: 4.8391967465410115, Val Loss: 5.7506432504881, Val MAE: 1.53091299533844\n",
      "Epoch 572/2000, Train Loss: 4.838765955105823, Val Loss: 5.7505371655736655, Val MAE: 1.5309908390045166\n",
      "Epoch 573/2000, Train Loss: 4.8385177230296925, Val Loss: 5.7504302603857855, Val MAE: 1.5309324264526367\n",
      "Epoch 574/2000, Train Loss: 4.838076940712707, Val Loss: 5.750315141110193, Val MAE: 1.5310617685317993\n",
      "Epoch 575/2000, Train Loss: 4.837614440447185, Val Loss: 5.750052088782901, Val MAE: 1.5311307907104492\n",
      "Epoch 576/2000, Train Loss: 4.837293826642594, Val Loss: 5.749781287851787, Val MAE: 1.5311261415481567\n",
      "Epoch 577/2000, Train Loss: 4.836931419641578, Val Loss: 5.749714735008421, Val MAE: 1.5312020778656006\n",
      "Epoch 578/2000, Train Loss: 4.836507040623717, Val Loss: 5.749532918135325, Val MAE: 1.531339406967163\n",
      "Epoch 579/2000, Train Loss: 4.83618087593692, Val Loss: 5.749436313197727, Val MAE: 1.5312964916229248\n",
      "Epoch 580/2000, Train Loss: 4.835822428399316, Val Loss: 5.749374332882109, Val MAE: 1.5314363241195679\n",
      "Epoch 581/2000, Train Loss: 4.835439873346993, Val Loss: 5.749169727166493, Val MAE: 1.5313720703125\n",
      "Epoch 582/2000, Train Loss: 4.8351883874793655, Val Loss: 5.748914159479595, Val MAE: 1.5314050912857056\n",
      "Epoch 583/2000, Train Loss: 4.834759178215425, Val Loss: 5.748574875649952, Val MAE: 1.531462550163269\n",
      "Epoch 584/2000, Train Loss: 4.834231965800771, Val Loss: 5.748441707520258, Val MAE: 1.5315378904342651\n",
      "Epoch 585/2000, Train Loss: 4.83393488481788, Val Loss: 5.748423800581977, Val MAE: 1.5315624475479126\n",
      "Epoch 586/2000, Train Loss: 4.833546228906499, Val Loss: 5.748170401368823, Val MAE: 1.5316567420959473\n",
      "Epoch 587/2000, Train Loss: 4.833191727885743, Val Loss: 5.747950721354711, Val MAE: 1.5318665504455566\n",
      "Epoch 588/2000, Train Loss: 4.832857635360847, Val Loss: 5.747860252857208, Val MAE: 1.5319081544876099\n",
      "Epoch 589/2000, Train Loss: 4.83251925415986, Val Loss: 5.747585390295301, Val MAE: 1.5319181680679321\n",
      "Epoch 590/2000, Train Loss: 4.8321405201940175, Val Loss: 5.747435237680163, Val MAE: 1.5319511890411377\n",
      "Epoch 591/2000, Train Loss: 4.83185608195317, Val Loss: 5.747236586752392, Val MAE: 1.531919240951538\n",
      "Epoch 592/2000, Train Loss: 4.831507839504183, Val Loss: 5.747031847635905, Val MAE: 1.531963586807251\n",
      "Epoch 593/2000, Train Loss: 4.831200666925298, Val Loss: 5.746905741237459, Val MAE: 1.5321940183639526\n",
      "Epoch 594/2000, Train Loss: 4.830784083757817, Val Loss: 5.746689330963862, Val MAE: 1.5320972204208374\n",
      "Epoch 595/2000, Train Loss: 4.8303909564051875, Val Loss: 5.746486408369882, Val MAE: 1.5321738719940186\n",
      "Epoch 596/2000, Train Loss: 4.830023304531705, Val Loss: 5.746420346555256, Val MAE: 1.532219648361206\n",
      "Epoch 597/2000, Train Loss: 4.8296653023896, Val Loss: 5.746190340746017, Val MAE: 1.5322870016098022\n",
      "Epoch 598/2000, Train Loss: 4.829297311887754, Val Loss: 5.746055228369577, Val MAE: 1.5324256420135498\n",
      "Epoch 599/2000, Train Loss: 4.82894935123675, Val Loss: 5.746034795329685, Val MAE: 1.5324227809906006\n",
      "Epoch 600/2000, Train Loss: 4.828626075819955, Val Loss: 5.7455456881296065, Val MAE: 1.532437801361084\n",
      "Epoch 601/2000, Train Loss: 4.828312791789369, Val Loss: 5.745548554829189, Val MAE: 1.5328007936477661\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 602/2000, Train Loss: 4.827929764439593, Val Loss: 5.745386569272904, Val MAE: 1.5328012704849243\n",
      "Epoch 603/2000, Train Loss: 4.827487275697953, Val Loss: 5.745232227302733, Val MAE: 1.5327904224395752\n",
      "Epoch 604/2000, Train Loss: 4.827322955864609, Val Loss: 5.745199183622996, Val MAE: 1.5329221487045288\n",
      "Epoch 605/2000, Train Loss: 4.826850528945708, Val Loss: 5.745041725181398, Val MAE: 1.5329424142837524\n",
      "Epoch 606/2000, Train Loss: 4.826544338624467, Val Loss: 5.744865408965519, Val MAE: 1.5329790115356445\n",
      "Epoch 607/2000, Train Loss: 4.826227831067084, Val Loss: 5.744705540793283, Val MAE: 1.5330727100372314\n",
      "Epoch 608/2000, Train Loss: 4.825806466413318, Val Loss: 5.744585814930144, Val MAE: 1.5330393314361572\n",
      "Epoch 609/2000, Train Loss: 4.825466028221571, Val Loss: 5.744513551394145, Val MAE: 1.5329811573028564\n",
      "Epoch 610/2000, Train Loss: 4.825233537488327, Val Loss: 5.7442457136653715, Val MAE: 1.5331562757492065\n",
      "Epoch 611/2000, Train Loss: 4.824781992203426, Val Loss: 5.744263870375497, Val MAE: 1.5331112146377563\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 612/2000, Train Loss: 4.824559510678935, Val Loss: 5.7442841842061, Val MAE: 1.5331071615219116\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 613/2000, Train Loss: 4.824136590755877, Val Loss: 5.744039944240025, Val MAE: 1.5330981016159058\n",
      "Epoch 614/2000, Train Loss: 4.823759874268546, Val Loss: 5.743833056518009, Val MAE: 1.5331555604934692\n",
      "Epoch 615/2000, Train Loss: 4.823442681720126, Val Loss: 5.743724354675838, Val MAE: 1.5332173109054565\n",
      "Epoch 616/2000, Train Loss: 4.82315747929561, Val Loss: 5.74355750708353, Val MAE: 1.533251404762268\n",
      "Epoch 617/2000, Train Loss: 4.822831450043344, Val Loss: 5.743484227430253, Val MAE: 1.5331834554672241\n",
      "Epoch 618/2000, Train Loss: 4.822380474155141, Val Loss: 5.743197006838662, Val MAE: 1.5332739353179932\n",
      "Epoch 619/2000, Train Loss: 4.82202471496356, Val Loss: 5.742999051298414, Val MAE: 1.5333048105239868\n",
      "Epoch 620/2000, Train Loss: 4.821640857015914, Val Loss: 5.742899752798534, Val MAE: 1.5333956480026245\n",
      "Epoch 621/2000, Train Loss: 4.821368962652424, Val Loss: 5.742785198347909, Val MAE: 1.5334080457687378\n",
      "Epoch 622/2000, Train Loss: 4.8210005817359525, Val Loss: 5.742607133729117, Val MAE: 1.5333762168884277\n",
      "Epoch 623/2000, Train Loss: 4.820694256904936, Val Loss: 5.742521365483602, Val MAE: 1.5333526134490967\n",
      "Epoch 624/2000, Train Loss: 4.820355851660326, Val Loss: 5.74247472910654, Val MAE: 1.5334045886993408\n",
      "Epoch 625/2000, Train Loss: 4.820109062033413, Val Loss: 5.742349973746708, Val MAE: 1.5335232019424438\n",
      "Epoch 626/2000, Train Loss: 4.819691590765132, Val Loss: 5.742146250747499, Val MAE: 1.5334978103637695\n",
      "Epoch 627/2000, Train Loss: 4.819344046421885, Val Loss: 5.742042796952384, Val MAE: 1.5334665775299072\n",
      "Epoch 628/2000, Train Loss: 4.818906536559628, Val Loss: 5.741894103231884, Val MAE: 1.5334854125976562\n",
      "Epoch 629/2000, Train Loss: 4.8185906746491725, Val Loss: 5.7417727793966025, Val MAE: 1.533539056777954\n",
      "Epoch 630/2000, Train Loss: 4.818270400818044, Val Loss: 5.741552020822253, Val MAE: 1.5334694385528564\n",
      "Epoch 631/2000, Train Loss: 4.817908328746371, Val Loss: 5.741424356188093, Val MAE: 1.5335780382156372\n",
      "Epoch 632/2000, Train Loss: 4.817583195077011, Val Loss: 5.741268643311092, Val MAE: 1.533650517463684\n",
      "Epoch 633/2000, Train Loss: 4.817254120271196, Val Loss: 5.741141330628168, Val MAE: 1.5336347818374634\n",
      "Epoch 634/2000, Train Loss: 4.816900542744124, Val Loss: 5.741124842848096, Val MAE: 1.5334992408752441\n",
      "Epoch 635/2000, Train Loss: 4.816541640143132, Val Loss: 5.740973600319454, Val MAE: 1.5336605310440063\n",
      "Epoch 636/2000, Train Loss: 4.816213037466633, Val Loss: 5.740885087421963, Val MAE: 1.5336605310440063\n",
      "Epoch 637/2000, Train Loss: 4.816017922124338, Val Loss: 5.740769170579457, Val MAE: 1.5337562561035156\n",
      "Epoch 638/2000, Train Loss: 4.815533720724001, Val Loss: 5.740611976101285, Val MAE: 1.5336523056030273\n",
      "Epoch 639/2000, Train Loss: 4.815181991445328, Val Loss: 5.740448105902899, Val MAE: 1.533757209777832\n",
      "Epoch 640/2000, Train Loss: 4.8150492077653935, Val Loss: 5.740388680072058, Val MAE: 1.5336802005767822\n",
      "Epoch 641/2000, Train Loss: 4.814618690646081, Val Loss: 5.740474431287675, Val MAE: 1.5338454246520996\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 642/2000, Train Loss: 4.814315390687735, Val Loss: 5.740221900599344, Val MAE: 1.5338000059127808\n",
      "Epoch 643/2000, Train Loss: 4.813927775545753, Val Loss: 5.74020436831883, Val MAE: 1.5337427854537964\n",
      "Epoch 644/2000, Train Loss: 4.8136340270425775, Val Loss: 5.739942278180804, Val MAE: 1.5338016748428345\n",
      "Epoch 645/2000, Train Loss: 4.813375275293089, Val Loss: 5.739940331095741, Val MAE: 1.5338414907455444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 646/2000, Train Loss: 4.8130862231987654, Val Loss: 5.739621477467673, Val MAE: 1.5339494943618774\n",
      "Epoch 647/2000, Train Loss: 4.812750493101408, Val Loss: 5.739599034899757, Val MAE: 1.5339715480804443\n",
      "Epoch 648/2000, Train Loss: 4.812289842429383, Val Loss: 5.739440898100535, Val MAE: 1.5340150594711304\n",
      "Epoch 649/2000, Train Loss: 4.812056550858555, Val Loss: 5.739379950932094, Val MAE: 1.5339548587799072\n",
      "Epoch 650/2000, Train Loss: 4.8117294619214555, Val Loss: 5.739200101012275, Val MAE: 1.5339293479919434\n",
      "Epoch 651/2000, Train Loss: 4.811413310983117, Val Loss: 5.739105937026796, Val MAE: 1.5338832139968872\n",
      "Epoch 652/2000, Train Loss: 4.811143729851511, Val Loss: 5.73888840845653, Val MAE: 1.5337659120559692\n",
      "Epoch 653/2000, Train Loss: 4.810727507842781, Val Loss: 5.738760681379409, Val MAE: 1.5339910984039307\n",
      "Epoch 654/2000, Train Loss: 4.810404957769956, Val Loss: 5.738828219118572, Val MAE: 1.533876657485962\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 655/2000, Train Loss: 4.8100939708300805, Val Loss: 5.7385506970541815, Val MAE: 1.5340434312820435\n",
      "Epoch 656/2000, Train Loss: 4.809781332110148, Val Loss: 5.738515890779949, Val MAE: 1.534102439880371\n",
      "Epoch 657/2000, Train Loss: 4.809508356960603, Val Loss: 5.738334309487116, Val MAE: 1.5339632034301758\n",
      "Epoch 658/2000, Train Loss: 4.8092367267406875, Val Loss: 5.738304010459355, Val MAE: 1.534135341644287\n",
      "Epoch 659/2000, Train Loss: 4.80901433584216, Val Loss: 5.738258387361254, Val MAE: 1.5339722633361816\n",
      "Epoch 660/2000, Train Loss: 4.808593559332391, Val Loss: 5.738176328795297, Val MAE: 1.533941388130188\n",
      "Epoch 661/2000, Train Loss: 4.808232704910472, Val Loss: 5.738091670331501, Val MAE: 1.534100890159607\n",
      "Epoch 662/2000, Train Loss: 4.807975765680224, Val Loss: 5.738019239334833, Val MAE: 1.5339903831481934\n",
      "Epoch 663/2000, Train Loss: 4.807615510499359, Val Loss: 5.737814903259277, Val MAE: 1.534008264541626\n",
      "Epoch 664/2000, Train Loss: 4.807380899219486, Val Loss: 5.737791793687003, Val MAE: 1.5339940786361694\n",
      "Epoch 665/2000, Train Loss: 4.807009718986425, Val Loss: 5.737679138070061, Val MAE: 1.5340750217437744\n",
      "Epoch 666/2000, Train Loss: 4.8067012203092805, Val Loss: 5.737693920021965, Val MAE: 1.5339349508285522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 667/2000, Train Loss: 4.806371282286974, Val Loss: 5.737487055006481, Val MAE: 1.5339913368225098\n",
      "Epoch 668/2000, Train Loss: 4.806080459035166, Val Loss: 5.737482939447675, Val MAE: 1.5340136289596558\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 669/2000, Train Loss: 4.805849137864429, Val Loss: 5.737278078283582, Val MAE: 1.5339306592941284\n",
      "Epoch 670/2000, Train Loss: 4.805384736051008, Val Loss: 5.737234175205231, Val MAE: 1.5341519117355347\n",
      "Epoch 671/2000, Train Loss: 4.805167814235929, Val Loss: 5.737094124158223, Val MAE: 1.5340229272842407\n",
      "Epoch 672/2000, Train Loss: 4.804810450342714, Val Loss: 5.737026972430093, Val MAE: 1.5340410470962524\n",
      "Epoch 673/2000, Train Loss: 4.8045076819501915, Val Loss: 5.737093652997698, Val MAE: 1.5340616703033447\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 674/2000, Train Loss: 4.804151872653719, Val Loss: 5.736948961303348, Val MAE: 1.5340198278427124\n",
      "Epoch 675/2000, Train Loss: 4.803846115453959, Val Loss: 5.736814629463923, Val MAE: 1.5341123342514038\n",
      "Epoch 676/2000, Train Loss: 4.803577806112292, Val Loss: 5.736638849689847, Val MAE: 1.5340520143508911\n",
      "Epoch 677/2000, Train Loss: 4.803390188849359, Val Loss: 5.736597762221382, Val MAE: 1.5340583324432373\n",
      "Epoch 678/2000, Train Loss: 4.803036997449415, Val Loss: 5.736388016314733, Val MAE: 1.5341120958328247\n",
      "Epoch 679/2000, Train Loss: 4.802708366525863, Val Loss: 5.736399292945862, Val MAE: 1.5340843200683594\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 680/2000, Train Loss: 4.802318769550458, Val Loss: 5.736337139492943, Val MAE: 1.5340590476989746\n",
      "Epoch 681/2000, Train Loss: 4.802120728284247, Val Loss: 5.7361190886724565, Val MAE: 1.5340383052825928\n",
      "Epoch 682/2000, Train Loss: 4.801743143704445, Val Loss: 5.736118932565053, Val MAE: 1.5340021848678589\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 683/2000, Train Loss: 4.801570280146364, Val Loss: 5.736070096492767, Val MAE: 1.5339446067810059\n",
      "Epoch 684/2000, Train Loss: 4.801284062173034, Val Loss: 5.73601800487155, Val MAE: 1.5340145826339722\n",
      "Epoch 685/2000, Train Loss: 4.8009815054652725, Val Loss: 5.735858956972758, Val MAE: 1.5338894128799438\n",
      "Epoch 686/2000, Train Loss: 4.800680292342042, Val Loss: 5.735806169964018, Val MAE: 1.5340029001235962\n",
      "Epoch 687/2000, Train Loss: 4.800357872407426, Val Loss: 5.735599336170015, Val MAE: 1.534314513206482\n",
      "Epoch 688/2000, Train Loss: 4.799904319900383, Val Loss: 5.735615937482743, Val MAE: 1.5343427658081055\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 689/2000, Train Loss: 4.799676190982921, Val Loss: 5.735482218719664, Val MAE: 1.5343517065048218\n",
      "Epoch 690/2000, Train Loss: 4.799434960476939, Val Loss: 5.7354551042829245, Val MAE: 1.5342587232589722\n",
      "Epoch 691/2000, Train Loss: 4.799176045298072, Val Loss: 5.735289516903105, Val MAE: 1.5342626571655273\n",
      "Epoch 692/2000, Train Loss: 4.79873404617202, Val Loss: 5.735116703169687, Val MAE: 1.5343507528305054\n",
      "Epoch 693/2000, Train Loss: 4.798452465927651, Val Loss: 5.735090286958785, Val MAE: 1.5344030857086182\n",
      "Epoch 694/2000, Train Loss: 4.798221226976016, Val Loss: 5.734902983620053, Val MAE: 1.5343527793884277\n",
      "Epoch 695/2000, Train Loss: 4.797885628782307, Val Loss: 5.734934236322131, Val MAE: 1.534348487854004\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 696/2000, Train Loss: 4.797633176461934, Val Loss: 5.734734117984772, Val MAE: 1.5344579219818115\n",
      "Epoch 697/2000, Train Loss: 4.7972355652259, Val Loss: 5.734596425578708, Val MAE: 1.5342817306518555\n",
      "Epoch 698/2000, Train Loss: 4.797043151344331, Val Loss: 5.734602533635639, Val MAE: 1.534315586090088\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 699/2000, Train Loss: 4.796737352782816, Val Loss: 5.7343041726521085, Val MAE: 1.5342210531234741\n",
      "Epoch 700/2000, Train Loss: 4.796392312339062, Val Loss: 5.734235238461268, Val MAE: 1.5343925952911377\n",
      "Epoch 701/2000, Train Loss: 4.796129836013859, Val Loss: 5.734341212681362, Val MAE: 1.5343072414398193\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 702/2000, Train Loss: 4.795956596837225, Val Loss: 5.7340838341485885, Val MAE: 1.5343599319458008\n",
      "Epoch 703/2000, Train Loss: 4.795539558690425, Val Loss: 5.73417203766959, Val MAE: 1.5344423055648804\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 704/2000, Train Loss: 4.795196640138397, Val Loss: 5.734065421989986, Val MAE: 1.5344165563583374\n",
      "Epoch 705/2000, Train Loss: 4.794942583452663, Val Loss: 5.733947589283898, Val MAE: 1.534543752670288\n",
      "Epoch 706/2000, Train Loss: 4.7947094258872, Val Loss: 5.733936199120113, Val MAE: 1.5344206094741821\n",
      "Epoch 707/2000, Train Loss: 4.794517889681925, Val Loss: 5.733806632813954, Val MAE: 1.5344197750091553\n",
      "Epoch 708/2000, Train Loss: 4.794183809094772, Val Loss: 5.733571869986398, Val MAE: 1.5344041585922241\n",
      "Epoch 709/2000, Train Loss: 4.793870611150443, Val Loss: 5.733633716901143, Val MAE: 1.5342888832092285\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 710/2000, Train Loss: 4.793504982640276, Val Loss: 5.733482999461038, Val MAE: 1.5345139503479004\n",
      "Epoch 711/2000, Train Loss: 4.7933074802538576, Val Loss: 5.733530183633168, Val MAE: 1.534762978553772\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 712/2000, Train Loss: 4.7929340612737015, Val Loss: 5.733352800210317, Val MAE: 1.5346406698226929\n",
      "Epoch 713/2000, Train Loss: 4.792667069455296, Val Loss: 5.733390833650317, Val MAE: 1.5347776412963867\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 714/2000, Train Loss: 4.792363238099264, Val Loss: 5.733379051798866, Val MAE: 1.534706473350525\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 715/2000, Train Loss: 4.792152019919058, Val Loss: 5.733271309307644, Val MAE: 1.5347025394439697\n",
      "Epoch 716/2000, Train Loss: 4.79187714229686, Val Loss: 5.733320880503881, Val MAE: 1.5347158908843994\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 717/2000, Train Loss: 4.791586046662754, Val Loss: 5.733257719448635, Val MAE: 1.5347765684127808\n",
      "Epoch 718/2000, Train Loss: 4.791343401113922, Val Loss: 5.733171389216468, Val MAE: 1.5346338748931885\n",
      "Epoch 719/2000, Train Loss: 4.791229520765447, Val Loss: 5.732875491891589, Val MAE: 1.5348764657974243\n",
      "Epoch 720/2000, Train Loss: 4.790648994391997, Val Loss: 5.73298537447339, Val MAE: 1.5348345041275024\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 721/2000, Train Loss: 4.79038971805438, Val Loss: 5.732957365966978, Val MAE: 1.5348381996154785\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 722/2000, Train Loss: 4.790105612893367, Val Loss: 5.732815816288903, Val MAE: 1.5349128246307373\n",
      "Epoch 723/2000, Train Loss: 4.78986281030437, Val Loss: 5.7327315126146585, Val MAE: 1.5349149703979492\n",
      "Epoch 724/2000, Train Loss: 4.7896267564071415, Val Loss: 5.732701023419698, Val MAE: 1.5348726511001587\n",
      "Epoch 725/2000, Train Loss: 4.789274742640292, Val Loss: 5.732674400011699, Val MAE: 1.5348761081695557\n",
      "Epoch 726/2000, Train Loss: 4.78903206682676, Val Loss: 5.732497453689575, Val MAE: 1.5348299741744995\n",
      "Epoch 727/2000, Train Loss: 4.788731153996606, Val Loss: 5.732461568855104, Val MAE: 1.5348436832427979\n",
      "Epoch 728/2000, Train Loss: 4.788406678416329, Val Loss: 5.7324403666314625, Val MAE: 1.534812569618225\n",
      "Epoch 729/2000, Train Loss: 4.7881283948384485, Val Loss: 5.73234840801784, Val MAE: 1.5347400903701782\n",
      "Epoch 730/2000, Train Loss: 4.787903383521334, Val Loss: 5.73228779293242, Val MAE: 1.534670114517212\n",
      "Epoch 731/2000, Train Loss: 4.787694238977473, Val Loss: 5.732210383528755, Val MAE: 1.5346033573150635\n",
      "Epoch 732/2000, Train Loss: 4.787323846803566, Val Loss: 5.732200409684863, Val MAE: 1.534672498703003\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 733/2000, Train Loss: 4.787131419807294, Val Loss: 5.7321580449740095, Val MAE: 1.5346732139587402\n",
      "Epoch 734/2000, Train Loss: 4.786794209177638, Val Loss: 5.732057491938273, Val MAE: 1.5346444845199585\n",
      "Epoch 735/2000, Train Loss: 4.786508280736603, Val Loss: 5.7320534246308465, Val MAE: 1.5346755981445312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 736/2000, Train Loss: 4.786242249654277, Val Loss: 5.731937604291098, Val MAE: 1.5346914529800415\n",
      "Epoch 737/2000, Train Loss: 4.7859853433789254, Val Loss: 5.731838734376998, Val MAE: 1.5345872640609741\n",
      "Epoch 738/2000, Train Loss: 4.785598694017138, Val Loss: 5.731875868070693, Val MAE: 1.5345418453216553\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 739/2000, Train Loss: 4.78543122127463, Val Loss: 5.7319304176739285, Val MAE: 1.5346530675888062\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 740/2000, Train Loss: 4.785140999281524, Val Loss: 5.731810390949249, Val MAE: 1.5346075296401978\n",
      "Epoch 741/2000, Train Loss: 4.784897251425072, Val Loss: 5.73164888506844, Val MAE: 1.534520149230957\n",
      "Epoch 742/2000, Train Loss: 4.784563270375154, Val Loss: 5.731682780243101, Val MAE: 1.5343663692474365\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 743/2000, Train Loss: 4.784365211454534, Val Loss: 5.731617592629933, Val MAE: 1.5346585512161255\n",
      "Epoch 744/2000, Train Loss: 4.784042720566011, Val Loss: 5.731403611955189, Val MAE: 1.5345999002456665\n",
      "Epoch 745/2000, Train Loss: 4.783727955750922, Val Loss: 5.731494157087235, Val MAE: 1.534601092338562\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 746/2000, Train Loss: 4.783502908956853, Val Loss: 5.731418603942508, Val MAE: 1.53448486328125\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 747/2000, Train Loss: 4.783202204280579, Val Loss: 5.731419753460657, Val MAE: 1.5344775915145874\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 748/2000, Train Loss: 4.783041528317084, Val Loss: 5.731238702932994, Val MAE: 1.5345582962036133\n",
      "Epoch 749/2000, Train Loss: 4.782744174615621, Val Loss: 5.731140999566941, Val MAE: 1.5344876050949097\n",
      "Epoch 750/2000, Train Loss: 4.782427198291666, Val Loss: 5.731067234561557, Val MAE: 1.5343776941299438\n",
      "Epoch 751/2000, Train Loss: 4.782113096777912, Val Loss: 5.731018881003062, Val MAE: 1.5343010425567627\n",
      "Epoch 752/2000, Train Loss: 4.781832760917115, Val Loss: 5.730843563874562, Val MAE: 1.5346097946166992\n",
      "Epoch 753/2000, Train Loss: 4.781571825401403, Val Loss: 5.730784470126743, Val MAE: 1.5345510244369507\n",
      "Epoch 754/2000, Train Loss: 4.7811927536478835, Val Loss: 5.730678586732774, Val MAE: 1.5345834493637085\n",
      "Epoch 755/2000, Train Loss: 4.781012615801081, Val Loss: 5.730611301603771, Val MAE: 1.5344904661178589\n",
      "Epoch 756/2000, Train Loss: 4.78075231248132, Val Loss: 5.730679307665143, Val MAE: 1.534582257270813\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 757/2000, Train Loss: 4.780531072145793, Val Loss: 5.730519272032238, Val MAE: 1.5346118211746216\n",
      "Epoch 758/2000, Train Loss: 4.780218300093044, Val Loss: 5.730438726288932, Val MAE: 1.5346125364303589\n",
      "Epoch 759/2000, Train Loss: 4.779927334260873, Val Loss: 5.730466956184024, Val MAE: 1.5345306396484375\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 760/2000, Train Loss: 4.779701428621881, Val Loss: 5.730364478769756, Val MAE: 1.534485936164856\n",
      "Epoch 761/2000, Train Loss: 4.77944089058256, Val Loss: 5.730434105509803, Val MAE: 1.5345438718795776\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 762/2000, Train Loss: 4.779129637649601, Val Loss: 5.730195300919669, Val MAE: 1.5343714952468872\n",
      "Epoch 763/2000, Train Loss: 4.778918771380598, Val Loss: 5.730362719013577, Val MAE: 1.5344663858413696\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 764/2000, Train Loss: 4.778557007672588, Val Loss: 5.730304141839345, Val MAE: 1.5342886447906494\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 765/2000, Train Loss: 4.778367113663548, Val Loss: 5.730138636770702, Val MAE: 1.5343443155288696\n",
      "Epoch 766/2000, Train Loss: 4.778102481987312, Val Loss: 5.730245431264241, Val MAE: 1.534238576889038\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 767/2000, Train Loss: 4.777794882339887, Val Loss: 5.730055545057569, Val MAE: 1.5343271493911743\n",
      "Epoch 768/2000, Train Loss: 4.777506223014111, Val Loss: 5.730066307953426, Val MAE: 1.5343583822250366\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 769/2000, Train Loss: 4.77729255011455, Val Loss: 5.729950552894955, Val MAE: 1.534285545349121\n",
      "Epoch 770/2000, Train Loss: 4.777051490267846, Val Loss: 5.730001250902812, Val MAE: 1.5343003273010254\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 771/2000, Train Loss: 4.776810251943483, Val Loss: 5.729880889256795, Val MAE: 1.5341696739196777\n",
      "Epoch 772/2000, Train Loss: 4.7765927272724, Val Loss: 5.729907819202968, Val MAE: 1.5343049764633179\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 773/2000, Train Loss: 4.7762177895087286, Val Loss: 5.729814884208498, Val MAE: 1.5341702699661255\n",
      "Epoch 774/2000, Train Loss: 4.776006283645738, Val Loss: 5.7297638058662415, Val MAE: 1.5342729091644287\n",
      "Epoch 775/2000, Train Loss: 4.775692350269205, Val Loss: 5.729837196213858, Val MAE: 1.5341691970825195\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 776/2000, Train Loss: 4.775391208099882, Val Loss: 5.729690903709049, Val MAE: 1.534192442893982\n",
      "Epoch 777/2000, Train Loss: 4.7751464100583485, Val Loss: 5.729713465486254, Val MAE: 1.5340917110443115\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 778/2000, Train Loss: 4.774938991106784, Val Loss: 5.729613568101611, Val MAE: 1.5342320203781128\n",
      "Epoch 779/2000, Train Loss: 4.774584958516324, Val Loss: 5.729531906899952, Val MAE: 1.5341682434082031\n",
      "Epoch 780/2000, Train Loss: 4.774320898002227, Val Loss: 5.729491100424812, Val MAE: 1.534071922302246\n",
      "Epoch 781/2000, Train Loss: 4.774130958932411, Val Loss: 5.729376216729482, Val MAE: 1.534069299697876\n",
      "Epoch 782/2000, Train Loss: 4.773850735555415, Val Loss: 5.729377922557649, Val MAE: 1.5341612100601196\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 783/2000, Train Loss: 4.773536045225113, Val Loss: 5.729455916654496, Val MAE: 1.5341917276382446\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 784/2000, Train Loss: 4.773317576799809, Val Loss: 5.729321774982271, Val MAE: 1.5340042114257812\n",
      "Epoch 785/2000, Train Loss: 4.77306908814628, Val Loss: 5.729258182502928, Val MAE: 1.5341085195541382\n",
      "Epoch 786/2000, Train Loss: 4.772759412676726, Val Loss: 5.7291630534898665, Val MAE: 1.5340038537979126\n",
      "Epoch 787/2000, Train Loss: 4.772477792247562, Val Loss: 5.72904988697597, Val MAE: 1.5340638160705566\n",
      "Epoch 788/2000, Train Loss: 4.772256124843495, Val Loss: 5.729051496301379, Val MAE: 1.534124493598938\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 789/2000, Train Loss: 4.7719410572132706, Val Loss: 5.728951380366371, Val MAE: 1.534022331237793\n",
      "Epoch 790/2000, Train Loss: 4.771738921805733, Val Loss: 5.728998150144305, Val MAE: 1.5339406728744507\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 791/2000, Train Loss: 4.771429499046422, Val Loss: 5.7289556832540605, Val MAE: 1.534063458442688\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 792/2000, Train Loss: 4.771257535022472, Val Loss: 5.728913613728115, Val MAE: 1.534040927886963\n",
      "Epoch 793/2000, Train Loss: 4.770987331783149, Val Loss: 5.728861502238682, Val MAE: 1.5341672897338867\n",
      "Epoch 794/2000, Train Loss: 4.770752048290667, Val Loss: 5.728830757595244, Val MAE: 1.534092903137207\n",
      "Epoch 795/2000, Train Loss: 4.770436336358612, Val Loss: 5.7288182235899425, Val MAE: 1.5340746641159058\n",
      "Epoch 796/2000, Train Loss: 4.770238037001767, Val Loss: 5.728701435384297, Val MAE: 1.5340063571929932\n",
      "Epoch 797/2000, Train Loss: 4.769989446120807, Val Loss: 5.728761258579436, Val MAE: 1.5341291427612305\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 798/2000, Train Loss: 4.769757303095335, Val Loss: 5.728733020169394, Val MAE: 1.534226417541504\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 799/2000, Train Loss: 4.7694160154743495, Val Loss: 5.728718380133311, Val MAE: 1.5340207815170288\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 800/2000, Train Loss: 4.769283906024669, Val Loss: 5.728526106902531, Val MAE: 1.5340852737426758\n",
      "Epoch 801/2000, Train Loss: 4.768930274441481, Val Loss: 5.728542211509886, Val MAE: 1.533934235572815\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 802/2000, Train Loss: 4.768866901337176, Val Loss: 5.728659229619162, Val MAE: 1.534026861190796\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 803/2000, Train Loss: 4.7683625530624925, Val Loss: 5.728601892789205, Val MAE: 1.5339475870132446\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 804/2000, Train Loss: 4.768348160516392, Val Loss: 5.728505872544789, Val MAE: 1.534019112586975\n",
      "Epoch 805/2000, Train Loss: 4.767970833018401, Val Loss: 5.728386640548706, Val MAE: 1.533766269683838\n",
      "Epoch 806/2000, Train Loss: 4.767797323140843, Val Loss: 5.7285039368129915, Val MAE: 1.5340183973312378\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 807/2000, Train Loss: 4.767502862744674, Val Loss: 5.728292306264241, Val MAE: 1.5338810682296753\n",
      "Epoch 808/2000, Train Loss: 4.767123643702949, Val Loss: 5.72829220408485, Val MAE: 1.5339643955230713\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 809/2000, Train Loss: 4.766972548786777, Val Loss: 5.728289967491513, Val MAE: 1.5338551998138428\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 810/2000, Train Loss: 4.766754598308181, Val Loss: 5.728192261287144, Val MAE: 1.5339090824127197\n",
      "Epoch 811/2000, Train Loss: 4.7665553610819185, Val Loss: 5.728101784274692, Val MAE: 1.5340009927749634\n",
      "Epoch 812/2000, Train Loss: 4.766181171024468, Val Loss: 5.728094864459265, Val MAE: 1.5338952541351318\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 813/2000, Train Loss: 4.7659754040210975, Val Loss: 5.72788899853116, Val MAE: 1.5340538024902344\n",
      "Epoch 814/2000, Train Loss: 4.765685452728917, Val Loss: 5.72794307697387, Val MAE: 1.5339792966842651\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 815/2000, Train Loss: 4.765484190794242, Val Loss: 5.727643458616166, Val MAE: 1.5341131687164307\n",
      "Epoch 816/2000, Train Loss: 4.7652911830520095, Val Loss: 5.727693960780189, Val MAE: 1.5341397523880005\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 817/2000, Train Loss: 4.765062835219885, Val Loss: 5.7275472567194985, Val MAE: 1.5340278148651123\n",
      "Epoch 818/2000, Train Loss: 4.764668105855815, Val Loss: 5.727582996799832, Val MAE: 1.534065842628479\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 819/2000, Train Loss: 4.764638878562723, Val Loss: 5.727587495531354, Val MAE: 1.534179449081421\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 820/2000, Train Loss: 4.764172187813246, Val Loss: 5.727564834413075, Val MAE: 1.5341036319732666\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 821/2000, Train Loss: 4.764029906061708, Val Loss: 5.727589689549946, Val MAE: 1.5341331958770752\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 822/2000, Train Loss: 4.763740054306762, Val Loss: 5.727557497365134, Val MAE: 1.5340986251831055\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 823/2000, Train Loss: 4.763515435421921, Val Loss: 5.7276065548261, Val MAE: 1.5341241359710693\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 824/2000, Train Loss: 4.7632146681986, Val Loss: 5.7275578039033075, Val MAE: 1.534179925918579\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 825/2000, Train Loss: 4.763101339676484, Val Loss: 5.727527800060454, Val MAE: 1.5341418981552124\n",
      "Epoch 826/2000, Train Loss: 4.762816521949929, Val Loss: 5.727375107152121, Val MAE: 1.5342069864273071\n",
      "Epoch 827/2000, Train Loss: 4.76260769854816, Val Loss: 5.727280645143418, Val MAE: 1.5341509580612183\n",
      "Epoch 828/2000, Train Loss: 4.76228139666139, Val Loss: 5.727122173422859, Val MAE: 1.5342817306518555\n",
      "Epoch 829/2000, Train Loss: 4.762016686465071, Val Loss: 5.727061342625391, Val MAE: 1.5340688228607178\n",
      "Epoch 830/2000, Train Loss: 4.76181130853123, Val Loss: 5.727026056675684, Val MAE: 1.5342650413513184\n",
      "Epoch 831/2000, Train Loss: 4.761569754361098, Val Loss: 5.727081290313175, Val MAE: 1.5342854261398315\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 832/2000, Train Loss: 4.761300004419723, Val Loss: 5.727035428796496, Val MAE: 1.5341724157333374\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 833/2000, Train Loss: 4.761157426914813, Val Loss: 5.7269918237413675, Val MAE: 1.5342164039611816\n",
      "Epoch 834/2000, Train Loss: 4.760824520934284, Val Loss: 5.726949958574204, Val MAE: 1.5341851711273193\n",
      "Epoch 835/2000, Train Loss: 4.760617475415486, Val Loss: 5.726868410905202, Val MAE: 1.5339581966400146\n",
      "Epoch 836/2000, Train Loss: 4.760358906935569, Val Loss: 5.727033561184292, Val MAE: 1.5341488122940063\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 837/2000, Train Loss: 4.760222565640179, Val Loss: 5.727026593117487, Val MAE: 1.534071445465088\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 838/2000, Train Loss: 4.759982362957028, Val Loss: 5.726850966612498, Val MAE: 1.5341603755950928\n",
      "Epoch 839/2000, Train Loss: 4.759911164242054, Val Loss: 5.726779707840511, Val MAE: 1.533967137336731\n",
      "Epoch 840/2000, Train Loss: 4.759414789875068, Val Loss: 5.726838611421131, Val MAE: 1.533979892730713\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 841/2000, Train Loss: 4.7592306216109455, Val Loss: 5.726623458521707, Val MAE: 1.5339380502700806\n",
      "Epoch 842/2000, Train Loss: 4.758994779734753, Val Loss: 5.726691507157826, Val MAE: 1.5339781045913696\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 843/2000, Train Loss: 4.7587429184167105, Val Loss: 5.726503968238831, Val MAE: 1.5339820384979248\n",
      "Epoch 844/2000, Train Loss: 4.758476376029097, Val Loss: 5.7265096335184005, Val MAE: 1.5339195728302002\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 845/2000, Train Loss: 4.758249055515392, Val Loss: 5.72663190251305, Val MAE: 1.5339511632919312\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 846/2000, Train Loss: 4.757983524136886, Val Loss: 5.726496548879714, Val MAE: 1.5338331460952759\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 847/2000, Train Loss: 4.757773073419025, Val Loss: 5.726515554246449, Val MAE: 1.5339090824127197\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 848/2000, Train Loss: 4.757640259894733, Val Loss: 5.726427844592503, Val MAE: 1.5338610410690308\n",
      "Epoch 849/2000, Train Loss: 4.757414448916996, Val Loss: 5.72637650228682, Val MAE: 1.5338118076324463\n",
      "Epoch 850/2000, Train Loss: 4.7571099473660015, Val Loss: 5.726430373532431, Val MAE: 1.5338741540908813\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 851/2000, Train Loss: 4.756920303377009, Val Loss: 5.7263346172514415, Val MAE: 1.533819317817688\n",
      "Epoch 852/2000, Train Loss: 4.756618449987242, Val Loss: 5.726349876040504, Val MAE: 1.5338881015777588\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 853/2000, Train Loss: 4.756336894122771, Val Loss: 5.726282732827323, Val MAE: 1.5340027809143066\n",
      "Epoch 854/2000, Train Loss: 4.756147722263094, Val Loss: 5.726166605949402, Val MAE: 1.534059762954712\n",
      "Epoch 855/2000, Train Loss: 4.756023514926517, Val Loss: 5.726175651663826, Val MAE: 1.5341789722442627\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 856/2000, Train Loss: 4.755714530164668, Val Loss: 5.7260882797695345, Val MAE: 1.5340853929519653\n",
      "Epoch 857/2000, Train Loss: 4.755410913351729, Val Loss: 5.725984891255696, Val MAE: 1.5340458154678345\n",
      "Epoch 858/2000, Train Loss: 4.755244196554165, Val Loss: 5.726062862646012, Val MAE: 1.534144401550293\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 859/2000, Train Loss: 4.755036484875699, Val Loss: 5.725939756348019, Val MAE: 1.533988356590271\n",
      "Epoch 860/2000, Train Loss: 4.754746611262915, Val Loss: 5.726007665906634, Val MAE: 1.5341466665267944\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 861/2000, Train Loss: 4.754635950742219, Val Loss: 5.7258945391291665, Val MAE: 1.534104824066162\n",
      "Epoch 862/2000, Train Loss: 4.754351675090064, Val Loss: 5.725802878538768, Val MAE: 1.5341650247573853\n",
      "Epoch 863/2000, Train Loss: 4.75403634785597, Val Loss: 5.725658819788978, Val MAE: 1.534187912940979\n",
      "Epoch 864/2000, Train Loss: 4.75381937349801, Val Loss: 5.725638591107868, Val MAE: 1.5342715978622437\n",
      "Epoch 865/2000, Train Loss: 4.753599271787743, Val Loss: 5.72551653498695, Val MAE: 1.534407138824463\n",
      "Epoch 866/2000, Train Loss: 4.75336164824549, Val Loss: 5.725500390643165, Val MAE: 1.5343900918960571\n",
      "Epoch 867/2000, Train Loss: 4.7531408732297225, Val Loss: 5.725558150382269, Val MAE: 1.534334421157837\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 868/2000, Train Loss: 4.753054676674653, Val Loss: 5.725433647632599, Val MAE: 1.534429669380188\n",
      "Epoch 869/2000, Train Loss: 4.752673948769509, Val Loss: 5.725381354490916, Val MAE: 1.534501552581787\n",
      "Epoch 870/2000, Train Loss: 4.752390136503534, Val Loss: 5.725422544138772, Val MAE: 1.534462332725525\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 871/2000, Train Loss: 4.752164645659066, Val Loss: 5.725447674592336, Val MAE: 1.5345171689987183\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 872/2000, Train Loss: 4.751967398504949, Val Loss: 5.725396741004217, Val MAE: 1.5345951318740845\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 873/2000, Train Loss: 4.7516807894444435, Val Loss: 5.725155702659062, Val MAE: 1.5345617532730103\n",
      "Epoch 874/2000, Train Loss: 4.751578943685685, Val Loss: 5.725175167833056, Val MAE: 1.5345346927642822\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 875/2000, Train Loss: 4.751289948421742, Val Loss: 5.72507894890649, Val MAE: 1.5344027280807495\n",
      "Epoch 876/2000, Train Loss: 4.751109261438776, Val Loss: 5.72504894222532, Val MAE: 1.5343838930130005\n",
      "Epoch 877/2000, Train Loss: 4.750888681209979, Val Loss: 5.725132777577355, Val MAE: 1.5341981649398804\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 878/2000, Train Loss: 4.750745829809536, Val Loss: 5.7249680161476135, Val MAE: 1.5342134237289429\n",
      "Epoch 879/2000, Train Loss: 4.750473716195111, Val Loss: 5.724980107375553, Val MAE: 1.5342084169387817\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 880/2000, Train Loss: 4.750175482799707, Val Loss: 5.724955042203267, Val MAE: 1.5342985391616821\n",
      "Epoch 881/2000, Train Loss: 4.7501264280929885, Val Loss: 5.724972889536903, Val MAE: 1.534324049949646\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 882/2000, Train Loss: 4.749744086743074, Val Loss: 5.724890041918981, Val MAE: 1.5342109203338623\n",
      "Epoch 883/2000, Train Loss: 4.74951703054108, Val Loss: 5.7248758900733225, Val MAE: 1.534306526184082\n",
      "Epoch 884/2000, Train Loss: 4.749280497789047, Val Loss: 5.7247855095636275, Val MAE: 1.5342549085617065\n",
      "Epoch 885/2000, Train Loss: 4.749219870701832, Val Loss: 5.725167907419658, Val MAE: 1.5342018604278564\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 886/2000, Train Loss: 4.748866724194525, Val Loss: 5.725080773943946, Val MAE: 1.5340547561645508\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 887/2000, Train Loss: 4.748546066337983, Val Loss: 5.725010219074431, Val MAE: 1.534279227256775\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 888/2000, Train Loss: 4.748293494303909, Val Loss: 5.724950089341118, Val MAE: 1.53424870967865\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 889/2000, Train Loss: 4.74812826552075, Val Loss: 5.724864321095603, Val MAE: 1.5342650413513184\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 890/2000, Train Loss: 4.7478811816537, Val Loss: 5.72489321231842, Val MAE: 1.5342867374420166\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 891/2000, Train Loss: 4.747629038537675, Val Loss: 5.724782869929359, Val MAE: 1.534276008605957\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 892/2000, Train Loss: 4.747438190013632, Val Loss: 5.724790329024906, Val MAE: 1.5342392921447754\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 893/2000, Train Loss: 4.7471418764090165, Val Loss: 5.724859697478158, Val MAE: 1.5342206954956055\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 894/2000, Train Loss: 4.7470067589844565, Val Loss: 5.72487489382426, Val MAE: 1.5343939065933228\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 895/2000, Train Loss: 4.746765099392287, Val Loss: 5.724636384419033, Val MAE: 1.534411072731018\n",
      "Epoch 896/2000, Train Loss: 4.746575731484611, Val Loss: 5.724674310003008, Val MAE: 1.5343239307403564\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 897/2000, Train Loss: 4.746459101588164, Val Loss: 5.724609048593612, Val MAE: 1.5341554880142212\n",
      "Epoch 898/2000, Train Loss: 4.746111181121956, Val Loss: 5.724654739811307, Val MAE: 1.534186601638794\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 899/2000, Train Loss: 4.745946562004359, Val Loss: 5.724586458433242, Val MAE: 1.5342636108398438\n",
      "Epoch 900/2000, Train Loss: 4.745783261755122, Val Loss: 5.724408200808933, Val MAE: 1.534134864807129\n",
      "Epoch 901/2000, Train Loss: 4.74543039805462, Val Loss: 5.724397159758068, Val MAE: 1.5342646837234497\n",
      "Epoch 902/2000, Train Loss: 4.74518203466332, Val Loss: 5.724446943828037, Val MAE: 1.5341248512268066\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 903/2000, Train Loss: 4.745140209857096, Val Loss: 5.7243298632758, Val MAE: 1.53413987159729\n",
      "Epoch 904/2000, Train Loss: 4.744830395842473, Val Loss: 5.724474069618044, Val MAE: 1.5342042446136475\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 905/2000, Train Loss: 4.744566840748861, Val Loss: 5.724372585614522, Val MAE: 1.5340741872787476\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 906/2000, Train Loss: 4.744434681866839, Val Loss: 5.724209442025139, Val MAE: 1.5342519283294678\n",
      "Epoch 907/2000, Train Loss: 4.744182587846883, Val Loss: 5.724299930390858, Val MAE: 1.5344431400299072\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 908/2000, Train Loss: 4.743869019496256, Val Loss: 5.724240371159145, Val MAE: 1.5344016551971436\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 909/2000, Train Loss: 4.743813807940786, Val Loss: 5.724190149988447, Val MAE: 1.5342875719070435\n",
      "Epoch 910/2000, Train Loss: 4.743450084089056, Val Loss: 5.724128666378203, Val MAE: 1.5343118906021118\n",
      "Epoch 911/2000, Train Loss: 4.7432392302622075, Val Loss: 5.72395228204273, Val MAE: 1.5342857837677002\n",
      "Epoch 912/2000, Train Loss: 4.743122722265246, Val Loss: 5.724005653744652, Val MAE: 1.5343347787857056\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 913/2000, Train Loss: 4.742843265425839, Val Loss: 5.724031962099529, Val MAE: 1.5341734886169434\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 914/2000, Train Loss: 4.742682251170256, Val Loss: 5.724019890739804, Val MAE: 1.5342661142349243\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 915/2000, Train Loss: 4.742311243614122, Val Loss: 5.723776715142386, Val MAE: 1.5342962741851807\n",
      "Epoch 916/2000, Train Loss: 4.742152966632493, Val Loss: 5.723882862499782, Val MAE: 1.5343148708343506\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 917/2000, Train Loss: 4.741887185738352, Val Loss: 5.723839135397048, Val MAE: 1.5343728065490723\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 918/2000, Train Loss: 4.7417541236231795, Val Loss: 5.7238352071671255, Val MAE: 1.534299612045288\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 919/2000, Train Loss: 4.741475259131201, Val Loss: 5.723615470386687, Val MAE: 1.5343995094299316\n",
      "Epoch 920/2000, Train Loss: 4.741282567318807, Val Loss: 5.723610105968657, Val MAE: 1.5344280004501343\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 921/2000, Train Loss: 4.741102888486617, Val Loss: 5.723589221636455, Val MAE: 1.5344561338424683\n",
      "Epoch 922/2000, Train Loss: 4.740939662217758, Val Loss: 5.723672926425934, Val MAE: 1.5346131324768066\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 923/2000, Train Loss: 4.740686644115637, Val Loss: 5.723608723708561, Val MAE: 1.5346357822418213\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 924/2000, Train Loss: 4.740368213572858, Val Loss: 5.723580916722615, Val MAE: 1.5346254110336304\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 925/2000, Train Loss: 4.740207709950015, Val Loss: 5.7236163928395225, Val MAE: 1.5345667600631714\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 926/2000, Train Loss: 4.739945597641895, Val Loss: 5.723612705866496, Val MAE: 1.5345789194107056\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 927/2000, Train Loss: 4.739716533208936, Val Loss: 5.723522010303679, Val MAE: 1.534548282623291\n",
      "Epoch 928/2000, Train Loss: 4.739465070544244, Val Loss: 5.723424956912086, Val MAE: 1.5344445705413818\n",
      "Epoch 929/2000, Train Loss: 4.7393064127318105, Val Loss: 5.723471945240384, Val MAE: 1.5343544483184814\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 930/2000, Train Loss: 4.739079988053218, Val Loss: 5.723346775486355, Val MAE: 1.5344412326812744\n",
      "Epoch 931/2000, Train Loss: 4.73904136205762, Val Loss: 5.723351623330798, Val MAE: 1.5343748331069946\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 932/2000, Train Loss: 4.738721566072472, Val Loss: 5.723247880027408, Val MAE: 1.5345937013626099\n",
      "Epoch 933/2000, Train Loss: 4.7384474603682545, Val Loss: 5.723192910353343, Val MAE: 1.5345635414123535\n",
      "Epoch 934/2000, Train Loss: 4.738185889966397, Val Loss: 5.723190591448829, Val MAE: 1.534479022026062\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 935/2000, Train Loss: 4.7381450196751755, Val Loss: 5.723155887353988, Val MAE: 1.5344182252883911\n",
      "Epoch 936/2000, Train Loss: 4.7377995324908255, Val Loss: 5.72328393799918, Val MAE: 1.5344854593276978\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 937/2000, Train Loss: 4.73752580543163, Val Loss: 5.7231822070621305, Val MAE: 1.534360647201538\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 938/2000, Train Loss: 4.73740000112773, Val Loss: 5.723107811950502, Val MAE: 1.5344197750091553\n",
      "Epoch 939/2000, Train Loss: 4.737193526602935, Val Loss: 5.722996689024425, Val MAE: 1.5343087911605835\n",
      "Epoch 940/2000, Train Loss: 4.736979799983532, Val Loss: 5.723015410559518, Val MAE: 1.5342919826507568\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 941/2000, Train Loss: 4.736770779188665, Val Loss: 5.723091074398586, Val MAE: 1.534324288368225\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 942/2000, Train Loss: 4.736663770608405, Val Loss: 5.72290909857977, Val MAE: 1.5342785120010376\n",
      "Epoch 943/2000, Train Loss: 4.736380665191306, Val Loss: 5.723045948005858, Val MAE: 1.5342934131622314\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 944/2000, Train Loss: 4.736203878991863, Val Loss: 5.723158938544137, Val MAE: 1.5341931581497192\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 945/2000, Train Loss: 4.735858461920734, Val Loss: 5.722938747633071, Val MAE: 1.5343900918960571\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 946/2000, Train Loss: 4.735775073257253, Val Loss: 5.723064510595231, Val MAE: 1.534258484840393\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 947/2000, Train Loss: 4.735534361286459, Val Loss: 5.722989195869083, Val MAE: 1.5342864990234375\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 948/2000, Train Loss: 4.735361000546615, Val Loss: 5.722800592581431, Val MAE: 1.5345237255096436\n",
      "Epoch 949/2000, Train Loss: 4.735087347635933, Val Loss: 5.722911164874122, Val MAE: 1.534420371055603\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 950/2000, Train Loss: 4.734819465362807, Val Loss: 5.722825118473598, Val MAE: 1.534442663192749\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 951/2000, Train Loss: 4.734655099796139, Val Loss: 5.722889795189812, Val MAE: 1.5343960523605347\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 952/2000, Train Loss: 4.734417540061961, Val Loss: 5.722820330233801, Val MAE: 1.5344278812408447\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 953/2000, Train Loss: 4.734251853777424, Val Loss: 5.722708801428477, Val MAE: 1.534419059753418\n",
      "Epoch 954/2000, Train Loss: 4.733999408686952, Val Loss: 5.722743230206626, Val MAE: 1.5345284938812256\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 955/2000, Train Loss: 4.733795307923439, Val Loss: 5.722651935759044, Val MAE: 1.5344525575637817\n",
      "Epoch 956/2000, Train Loss: 4.733684777877227, Val Loss: 5.722611007236299, Val MAE: 1.5342621803283691\n",
      "Epoch 957/2000, Train Loss: 4.733425210325943, Val Loss: 5.722610337393625, Val MAE: 1.5343700647354126\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 958/2000, Train Loss: 4.733191175084188, Val Loss: 5.7224962285586765, Val MAE: 1.5343989133834839\n",
      "Epoch 959/2000, Train Loss: 4.732996218295292, Val Loss: 5.722571971870604, Val MAE: 1.5343406200408936\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 960/2000, Train Loss: 4.732887484795284, Val Loss: 5.722411303293137, Val MAE: 1.534300684928894\n",
      "Epoch 961/2000, Train Loss: 4.732687370733414, Val Loss: 5.7224858573504855, Val MAE: 1.5343328714370728\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 962/2000, Train Loss: 4.732351199528058, Val Loss: 5.722356029919216, Val MAE: 1.5342432260513306\n",
      "Epoch 963/2000, Train Loss: 4.732242769852007, Val Loss: 5.722739398479462, Val MAE: 1.5343255996704102\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 964/2000, Train Loss: 4.731926796970987, Val Loss: 5.7228018414406545, Val MAE: 1.5343090295791626\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 965/2000, Train Loss: 4.731716928764526, Val Loss: 5.722668602353051, Val MAE: 1.5343457460403442\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 966/2000, Train Loss: 4.731533598664449, Val Loss: 5.722761355695271, Val MAE: 1.5342955589294434\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 967/2000, Train Loss: 4.731270345209011, Val Loss: 5.722694981665838, Val MAE: 1.5344960689544678\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 968/2000, Train Loss: 4.731095828862049, Val Loss: 5.7227022505941845, Val MAE: 1.5343812704086304\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 969/2000, Train Loss: 4.7308217164323425, Val Loss: 5.722734005678268, Val MAE: 1.5342955589294434\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 970/2000, Train Loss: 4.730732733843525, Val Loss: 5.72265503803889, Val MAE: 1.5344151258468628\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 971/2000, Train Loss: 4.730565150466726, Val Loss: 5.722514373915536, Val MAE: 1.5342832803726196\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 972/2000, Train Loss: 4.730262433860466, Val Loss: 5.722690128144764, Val MAE: 1.5344557762145996\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 973/2000, Train Loss: 4.73002855599515, Val Loss: 5.722634440376645, Val MAE: 1.5343111753463745\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 974/2000, Train Loss: 4.729857175071081, Val Loss: 5.722651379449027, Val MAE: 1.5342836380004883\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 975/2000, Train Loss: 4.72962633393211, Val Loss: 5.722714293570745, Val MAE: 1.5343068838119507\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 976/2000, Train Loss: 4.729523405874061, Val Loss: 5.722822192169371, Val MAE: 1.5341631174087524\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 977/2000, Train Loss: 4.7292480616710755, Val Loss: 5.722642069771176, Val MAE: 1.5342541933059692\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 978/2000, Train Loss: 4.729070681610969, Val Loss: 5.722573127065386, Val MAE: 1.5343645811080933\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 979/2000, Train Loss: 4.72887630338225, Val Loss: 5.72251277026676, Val MAE: 1.5343762636184692\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 980/2000, Train Loss: 4.728669493423698, Val Loss: 5.722555106594449, Val MAE: 1.5342864990234375\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 981/2000, Train Loss: 4.728487708168406, Val Loss: 5.722417927923656, Val MAE: 1.5342504978179932\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 982/2000, Train Loss: 4.728289195109154, Val Loss: 5.722536745525542, Val MAE: 1.5342563390731812\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 5.272824764251709\n",
      "Test Mean Absolute Error (MAE): 1.4836689957951024\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(CNNModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (conv1d): Conv1d(6, 64, kernel_size=(2,), stride=(1,))\n",
       "   (dense1): Linear(in_features=577, out_features=64, bias=True)\n",
       "   (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=577, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'test_mae': 1.4836689957951024})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 266\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_cnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=6,\n",
    "                kernel_size=2,\n",
    "                num_filters=64,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = ['total_points', 'ict_index', 'clean_sheets', 'goals_conceded', 'bps', 'matchup_difficulty', 'goals_scored', 'assists', 'yellow_cards', 'red_cards'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpremier.cnn.experiment import gridsearch_cnn\n",
    "\n",
    "#gridsearch_cnn(epochs=100, verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized) — {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
