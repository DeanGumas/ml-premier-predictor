{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.cnn.model import build_train_cnn, full_cnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2502, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2502, 7)\n",
      "Shape of a given window (prior to preprocessing): (6, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[ 1.91043017e+00  1.20188568e+00  1.49675899e-01  7.44843842e-01\n",
      "  9.65822039e+00 -5.24454920e-02  0.00000000e+00  1.76782557e-03\n",
      "  2.06246317e-02  1.17855038e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.71102698  1.51262882  0.35675345  1.18885945 10.44252919  1.38395817\n",
      "  1.          0.04200834  0.14212409  0.03430979]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building CNN Architecture ======\n",
      "====== Done Building CNN Architecture ======\n",
      "Epoch 1/2000, Train Loss: 10.887290988146294, Val Loss: 11.992705892429713, Val MAE: 2.063058853149414\n",
      "Epoch 2/2000, Train Loss: 10.857867767070902, Val Loss: 11.959288430542596, Val MAE: 2.0588693618774414\n",
      "Epoch 3/2000, Train Loss: 10.828526490847047, Val Loss: 11.925663105266265, Val MAE: 2.0544769763946533\n",
      "Epoch 4/2000, Train Loss: 10.799405418089961, Val Loss: 11.891691847987023, Val MAE: 2.050225257873535\n",
      "Epoch 5/2000, Train Loss: 10.769799355970648, Val Loss: 11.858840244296733, Val MAE: 2.0461103916168213\n",
      "Epoch 6/2000, Train Loss: 10.740641757185236, Val Loss: 11.825304229683969, Val MAE: 2.0418968200683594\n",
      "Epoch 7/2000, Train Loss: 10.711248751512134, Val Loss: 11.792281764149324, Val MAE: 2.03767728805542\n",
      "Epoch 8/2000, Train Loss: 10.682067070772412, Val Loss: 11.758968250299818, Val MAE: 2.0334551334381104\n",
      "Epoch 9/2000, Train Loss: 10.65285364328179, Val Loss: 11.72484364949839, Val MAE: 2.0291919708251953\n",
      "Epoch 10/2000, Train Loss: 10.623366978703713, Val Loss: 11.691314312708435, Val MAE: 2.0249407291412354\n",
      "Epoch 11/2000, Train Loss: 10.593717658964367, Val Loss: 11.658264835513217, Val MAE: 2.0206966400146484\n",
      "Epoch 12/2000, Train Loss: 10.564284586386307, Val Loss: 11.623761396185817, Val MAE: 2.0162296295166016\n",
      "Epoch 13/2000, Train Loss: 10.534421412396146, Val Loss: 11.5899830613647, Val MAE: 2.0119290351867676\n",
      "Epoch 14/2000, Train Loss: 10.504712366621566, Val Loss: 11.556514275814221, Val MAE: 2.007640838623047\n",
      "Epoch 15/2000, Train Loss: 10.475238379586507, Val Loss: 11.523072606779277, Val MAE: 2.003364324569702\n",
      "Epoch 16/2000, Train Loss: 10.445275689916322, Val Loss: 11.489724167582752, Val MAE: 1.99901282787323\n",
      "Epoch 17/2000, Train Loss: 10.415386948260013, Val Loss: 11.455247362149988, Val MAE: 1.9945529699325562\n",
      "Epoch 18/2000, Train Loss: 10.385560391403938, Val Loss: 11.420955793393793, Val MAE: 1.9901231527328491\n",
      "Epoch 19/2000, Train Loss: 10.355374170809377, Val Loss: 11.387688726051469, Val MAE: 1.985878825187683\n",
      "Epoch 20/2000, Train Loss: 10.325643487751778, Val Loss: 11.353130061039844, Val MAE: 1.9812862873077393\n",
      "Epoch 21/2000, Train Loss: 10.295920144664327, Val Loss: 11.318832079148695, Val MAE: 1.9769188165664673\n",
      "Epoch 22/2000, Train Loss: 10.26632177351563, Val Loss: 11.28464464544011, Val MAE: 1.972504734992981\n",
      "Epoch 23/2000, Train Loss: 10.236138637429974, Val Loss: 11.251615475399692, Val MAE: 1.9682838916778564\n",
      "Epoch 24/2000, Train Loss: 10.206479976581571, Val Loss: 11.217309365566644, Val MAE: 1.963884949684143\n",
      "Epoch 25/2000, Train Loss: 10.176012408976652, Val Loss: 11.182893196931138, Val MAE: 1.9594237804412842\n",
      "Epoch 26/2000, Train Loss: 10.145876830099, Val Loss: 11.14911970049812, Val MAE: 1.9551184177398682\n",
      "Epoch 27/2000, Train Loss: 10.115659529016858, Val Loss: 11.113351781873904, Val MAE: 1.9504289627075195\n",
      "Epoch 28/2000, Train Loss: 10.085308991713394, Val Loss: 11.079624239583884, Val MAE: 1.9460421800613403\n",
      "Epoch 29/2000, Train Loss: 10.05544682743682, Val Loss: 11.045275542935618, Val MAE: 1.9416747093200684\n",
      "Epoch 30/2000, Train Loss: 10.025105230410615, Val Loss: 11.01013795447476, Val MAE: 1.9370684623718262\n",
      "Epoch 31/2000, Train Loss: 9.994971857198438, Val Loss: 10.975665688588048, Val MAE: 1.9326316118240356\n",
      "Epoch 32/2000, Train Loss: 9.964814020998457, Val Loss: 10.941814840882333, Val MAE: 1.9281859397888184\n",
      "Epoch 33/2000, Train Loss: 9.934761872180829, Val Loss: 10.90765279249234, Val MAE: 1.9236787557601929\n",
      "Epoch 34/2000, Train Loss: 9.904847429639265, Val Loss: 10.872576387806308, Val MAE: 1.9190744161605835\n",
      "Epoch 35/2000, Train Loss: 9.874668766963994, Val Loss: 10.838108949983402, Val MAE: 1.9146575927734375\n",
      "Epoch 36/2000, Train Loss: 9.844675946285992, Val Loss: 10.803417782386799, Val MAE: 1.910073161125183\n",
      "Epoch 37/2000, Train Loss: 9.814464767745982, Val Loss: 10.769884403073727, Val MAE: 1.905639886856079\n",
      "Epoch 38/2000, Train Loss: 9.7845998290221, Val Loss: 10.735026748981953, Val MAE: 1.9010286331176758\n",
      "Epoch 39/2000, Train Loss: 9.754531598338604, Val Loss: 10.700062144457986, Val MAE: 1.8964041471481323\n",
      "Epoch 40/2000, Train Loss: 9.724751840587404, Val Loss: 10.66576083089917, Val MAE: 1.8917773962020874\n",
      "Epoch 41/2000, Train Loss: 9.694707838269206, Val Loss: 10.632227276021826, Val MAE: 1.8874024152755737\n",
      "Epoch 42/2000, Train Loss: 9.66472004266963, Val Loss: 10.59734467659231, Val MAE: 1.882665991783142\n",
      "Epoch 43/2000, Train Loss: 9.634649599882014, Val Loss: 10.562193484548251, Val MAE: 1.877824068069458\n",
      "Epoch 44/2000, Train Loss: 9.60453448741894, Val Loss: 10.52809424278688, Val MAE: 1.8733348846435547\n",
      "Epoch 45/2000, Train Loss: 9.574332307376633, Val Loss: 10.492859742505937, Val MAE: 1.8684477806091309\n",
      "Epoch 46/2000, Train Loss: 9.544297564625321, Val Loss: 10.45875727135221, Val MAE: 1.8638672828674316\n",
      "Epoch 47/2000, Train Loss: 9.51428421864117, Val Loss: 10.424316081526014, Val MAE: 1.8592529296875\n",
      "Epoch 48/2000, Train Loss: 9.484530138180844, Val Loss: 10.389545554186956, Val MAE: 1.854457139968872\n",
      "Epoch 49/2000, Train Loss: 9.454399069961237, Val Loss: 10.355854791090668, Val MAE: 1.8498953580856323\n",
      "Epoch 50/2000, Train Loss: 9.424274054654125, Val Loss: 10.321244143818246, Val MAE: 1.8452402353286743\n",
      "Epoch 51/2000, Train Loss: 9.39411715048455, Val Loss: 10.286471459570281, Val MAE: 1.840425729751587\n",
      "Epoch 52/2000, Train Loss: 9.364176835080924, Val Loss: 10.25230385475886, Val MAE: 1.8357332944869995\n",
      "Epoch 53/2000, Train Loss: 9.334278379838958, Val Loss: 10.217909013967825, Val MAE: 1.8309378623962402\n",
      "Epoch 54/2000, Train Loss: 9.30472184032894, Val Loss: 10.18277161167963, Val MAE: 1.8260949850082397\n",
      "Epoch 55/2000, Train Loss: 9.274596428384585, Val Loss: 10.149300455732345, Val MAE: 1.8214902877807617\n",
      "Epoch 56/2000, Train Loss: 9.245105761528686, Val Loss: 10.114569643271706, Val MAE: 1.816583514213562\n",
      "Epoch 57/2000, Train Loss: 9.215308426300629, Val Loss: 10.080829452493994, Val MAE: 1.8119605779647827\n",
      "Epoch 58/2000, Train Loss: 9.185587809842543, Val Loss: 10.045987007287914, Val MAE: 1.8071186542510986\n",
      "Epoch 59/2000, Train Loss: 9.155792468538761, Val Loss: 10.01127914182802, Val MAE: 1.802366852760315\n",
      "Epoch 60/2000, Train Loss: 9.12584286823313, Val Loss: 9.977550325448131, Val MAE: 1.7976995706558228\n",
      "Epoch 61/2000, Train Loss: 9.09613304134828, Val Loss: 9.943094151957524, Val MAE: 1.7928937673568726\n",
      "Epoch 62/2000, Train Loss: 9.066906061246309, Val Loss: 9.908953313816863, Val MAE: 1.7881149053573608\n",
      "Epoch 63/2000, Train Loss: 9.037318735706563, Val Loss: 9.874781843247774, Val MAE: 1.7833091020584106\n",
      "Epoch 64/2000, Train Loss: 9.007849241796302, Val Loss: 9.841322068371369, Val MAE: 1.778555154800415\n",
      "Epoch 65/2000, Train Loss: 8.978624048575307, Val Loss: 9.808645065233042, Val MAE: 1.7740157842636108\n",
      "Epoch 66/2000, Train Loss: 8.949689164118059, Val Loss: 9.773859664551424, Val MAE: 1.769202709197998\n",
      "Epoch 67/2000, Train Loss: 8.9201881172454, Val Loss: 9.741049466120643, Val MAE: 1.7647720575332642\n",
      "Epoch 68/2000, Train Loss: 8.89086305549831, Val Loss: 9.707365367877715, Val MAE: 1.7601674795150757\n",
      "Epoch 69/2000, Train Loss: 8.861788271179844, Val Loss: 9.673856383072788, Val MAE: 1.7556768655776978\n",
      "Epoch 70/2000, Train Loss: 8.832467741944436, Val Loss: 9.639901759591387, Val MAE: 1.7510749101638794\n",
      "Epoch 71/2000, Train Loss: 8.803625273251516, Val Loss: 9.606774925254285, Val MAE: 1.7466533184051514\n",
      "Epoch 72/2000, Train Loss: 8.774597884398963, Val Loss: 9.573722561664697, Val MAE: 1.7423937320709229\n",
      "Epoch 73/2000, Train Loss: 8.745792781526484, Val Loss: 9.540849934534286, Val MAE: 1.738075613975525\n",
      "Epoch 74/2000, Train Loss: 8.71689102164798, Val Loss: 9.507432871418422, Val MAE: 1.733691692352295\n",
      "Epoch 75/2000, Train Loss: 8.688127917693082, Val Loss: 9.474658923146716, Val MAE: 1.729341983795166\n",
      "Epoch 76/2000, Train Loss: 8.659610690238356, Val Loss: 9.44138790438641, Val MAE: 1.7249994277954102\n",
      "Epoch 77/2000, Train Loss: 8.630957247085089, Val Loss: 9.408417393470508, Val MAE: 1.7205756902694702\n",
      "Epoch 78/2000, Train Loss: 8.602705176995725, Val Loss: 9.376240100631838, Val MAE: 1.7163835763931274\n",
      "Epoch 79/2000, Train Loss: 8.574163987886562, Val Loss: 9.344329635077226, Val MAE: 1.7122313976287842\n",
      "Epoch 80/2000, Train Loss: 8.54589994888252, Val Loss: 9.31203968477199, Val MAE: 1.7080451250076294\n",
      "Epoch 81/2000, Train Loss: 8.517852736664523, Val Loss: 9.279230468877982, Val MAE: 1.7038630247116089\n",
      "Epoch 82/2000, Train Loss: 8.490098376076126, Val Loss: 9.247090576470057, Val MAE: 1.6997029781341553\n",
      "Epoch 83/2000, Train Loss: 8.462034278147494, Val Loss: 9.215053528923233, Val MAE: 1.6956733465194702\n",
      "Epoch 84/2000, Train Loss: 8.434300621993772, Val Loss: 9.183274778753907, Val MAE: 1.6917181015014648\n",
      "Epoch 85/2000, Train Loss: 8.406441063716494, Val Loss: 9.151852917629622, Val MAE: 1.6876970529556274\n",
      "Epoch 86/2000, Train Loss: 8.378979343331087, Val Loss: 9.11959623801166, Val MAE: 1.6835319995880127\n",
      "Epoch 87/2000, Train Loss: 8.351217179295045, Val Loss: 9.087471373589988, Val MAE: 1.6794122457504272\n",
      "Epoch 88/2000, Train Loss: 8.32371888506337, Val Loss: 9.055954161550087, Val MAE: 1.6754704713821411\n",
      "Epoch 89/2000, Train Loss: 8.296590221842605, Val Loss: 9.024748379714923, Val MAE: 1.6714187860488892\n",
      "Epoch 90/2000, Train Loss: 8.26910434383309, Val Loss: 8.993689974667419, Val MAE: 1.6674913167953491\n",
      "Epoch 91/2000, Train Loss: 8.24194116679654, Val Loss: 8.962391725627404, Val MAE: 1.6635501384735107\n",
      "Epoch 92/2000, Train Loss: 8.214713958348295, Val Loss: 8.930150196813502, Val MAE: 1.659398078918457\n",
      "Epoch 93/2000, Train Loss: 8.187367513261993, Val Loss: 8.899602134575332, Val MAE: 1.6555933952331543\n",
      "Epoch 94/2000, Train Loss: 8.160426594521445, Val Loss: 8.868673320036574, Val MAE: 1.6515506505966187\n",
      "Epoch 95/2000, Train Loss: 8.13373108366524, Val Loss: 8.838100530013488, Val MAE: 1.647691249847412\n",
      "Epoch 96/2000, Train Loss: 8.107081466623798, Val Loss: 8.807043812294642, Val MAE: 1.6436434984207153\n",
      "Epoch 97/2000, Train Loss: 8.08014893733138, Val Loss: 8.776339851445105, Val MAE: 1.639707088470459\n",
      "Epoch 98/2000, Train Loss: 8.053520144919624, Val Loss: 8.74551570199897, Val MAE: 1.6357595920562744\n",
      "Epoch 99/2000, Train Loss: 8.026794280576336, Val Loss: 8.715400165097861, Val MAE: 1.6319180727005005\n",
      "Epoch 100/2000, Train Loss: 8.000301124846574, Val Loss: 8.68518507025498, Val MAE: 1.628085732460022\n",
      "Epoch 101/2000, Train Loss: 7.973787856722784, Val Loss: 8.654509536010591, Val MAE: 1.6241490840911865\n",
      "Epoch 102/2000, Train Loss: 7.947552793047782, Val Loss: 8.624738958003483, Val MAE: 1.6203967332839966\n",
      "Epoch 103/2000, Train Loss: 7.921633333156513, Val Loss: 8.594071264629358, Val MAE: 1.61652410030365\n",
      "Epoch 104/2000, Train Loss: 7.895718742977306, Val Loss: 8.56393169836406, Val MAE: 1.612757921218872\n",
      "Epoch 105/2000, Train Loss: 7.869823932647705, Val Loss: 8.534233796865955, Val MAE: 1.6089953184127808\n",
      "Epoch 106/2000, Train Loss: 7.843978699959977, Val Loss: 8.505577761605837, Val MAE: 1.605512261390686\n",
      "Epoch 107/2000, Train Loss: 7.818288524917613, Val Loss: 8.475944118189771, Val MAE: 1.6018919944763184\n",
      "Epoch 108/2000, Train Loss: 7.792856521123232, Val Loss: 8.446125095519337, Val MAE: 1.5981963872909546\n",
      "Epoch 109/2000, Train Loss: 7.766889654692422, Val Loss: 8.415949166966005, Val MAE: 1.5944424867630005\n",
      "Epoch 110/2000, Train Loss: 7.74106970561549, Val Loss: 8.386342541904215, Val MAE: 1.5907707214355469\n",
      "Epoch 111/2000, Train Loss: 7.7159875266406, Val Loss: 8.357200433799159, Val MAE: 1.5872159004211426\n",
      "Epoch 112/2000, Train Loss: 7.690796732231398, Val Loss: 8.328978074076572, Val MAE: 1.5837867259979248\n",
      "Epoch 113/2000, Train Loss: 7.666048665016491, Val Loss: 8.300247084412051, Val MAE: 1.5803031921386719\n",
      "Epoch 114/2000, Train Loss: 7.64121282081802, Val Loss: 8.270936575612845, Val MAE: 1.5766512155532837\n",
      "Epoch 115/2000, Train Loss: 7.616479299170798, Val Loss: 8.243051370765318, Val MAE: 1.5732492208480835\n",
      "Epoch 116/2000, Train Loss: 7.591727073892248, Val Loss: 8.21447763186438, Val MAE: 1.5698074102401733\n",
      "Epoch 117/2000, Train Loss: 7.566970843315795, Val Loss: 8.186247859144475, Val MAE: 1.566347599029541\n",
      "Epoch 118/2000, Train Loss: 7.54273481657604, Val Loss: 8.15753194282475, Val MAE: 1.5630321502685547\n",
      "Epoch 119/2000, Train Loss: 7.518147267228193, Val Loss: 8.130389745663338, Val MAE: 1.5597432851791382\n",
      "Epoch 120/2000, Train Loss: 7.49414975973689, Val Loss: 8.103200746557764, Val MAE: 1.5565907955169678\n",
      "Epoch 121/2000, Train Loss: 7.4701771353601485, Val Loss: 8.075093017002448, Val MAE: 1.5532516241073608\n",
      "Epoch 122/2000, Train Loss: 7.446462529885106, Val Loss: 8.047409662492687, Val MAE: 1.5499626398086548\n",
      "Epoch 123/2000, Train Loss: 7.4228674182583125, Val Loss: 8.020582364575898, Val MAE: 1.5468432903289795\n",
      "Epoch 124/2000, Train Loss: 7.39915410135096, Val Loss: 7.99386619373199, Val MAE: 1.54377019405365\n",
      "Epoch 125/2000, Train Loss: 7.375860781001843, Val Loss: 7.968186956679374, Val MAE: 1.5408633947372437\n",
      "Epoch 126/2000, Train Loss: 7.353028076035636, Val Loss: 7.939874305609123, Val MAE: 1.5375384092330933\n",
      "Epoch 127/2000, Train Loss: 7.329523585867832, Val Loss: 7.91436531538833, Val MAE: 1.534752368927002\n",
      "Epoch 128/2000, Train Loss: 7.306338258564765, Val Loss: 7.887901416435747, Val MAE: 1.5316033363342285\n",
      "Epoch 129/2000, Train Loss: 7.2833201001010925, Val Loss: 7.861586686947158, Val MAE: 1.5286657810211182\n",
      "Epoch 130/2000, Train Loss: 7.260216909928023, Val Loss: 7.834654977812862, Val MAE: 1.525662899017334\n",
      "Epoch 131/2000, Train Loss: 7.2373396727839125, Val Loss: 7.80822376038207, Val MAE: 1.5227293968200684\n",
      "Epoch 132/2000, Train Loss: 7.2147029907580915, Val Loss: 7.783283646620058, Val MAE: 1.5201141834259033\n",
      "Epoch 133/2000, Train Loss: 7.192376124028334, Val Loss: 7.756687486965768, Val MAE: 1.5172712802886963\n",
      "Epoch 134/2000, Train Loss: 7.170125419702604, Val Loss: 7.731593756420988, Val MAE: 1.5145831108093262\n",
      "Epoch 135/2000, Train Loss: 7.148538172035297, Val Loss: 7.706099703896574, Val MAE: 1.511963129043579\n",
      "Epoch 136/2000, Train Loss: 7.126440399889372, Val Loss: 7.681615887830655, Val MAE: 1.5094013214111328\n",
      "Epoch 137/2000, Train Loss: 7.104545022802064, Val Loss: 7.656934488325491, Val MAE: 1.5068461894989014\n",
      "Epoch 138/2000, Train Loss: 7.082766978527609, Val Loss: 7.631376574499481, Val MAE: 1.5041574239730835\n",
      "Epoch 139/2000, Train Loss: 7.061042336659227, Val Loss: 7.606985456472553, Val MAE: 1.5016018152236938\n",
      "Epoch 140/2000, Train Loss: 7.0397255122032405, Val Loss: 7.583045999112251, Val MAE: 1.499171257019043\n",
      "Epoch 141/2000, Train Loss: 7.018959376612321, Val Loss: 7.558139318457893, Val MAE: 1.4965468645095825\n",
      "Epoch 142/2000, Train Loss: 6.998102362436111, Val Loss: 7.534502213980308, Val MAE: 1.4941133260726929\n",
      "Epoch 143/2000, Train Loss: 6.977290558865337, Val Loss: 7.51196097635553, Val MAE: 1.4918804168701172\n",
      "Epoch 144/2000, Train Loss: 6.956949009180573, Val Loss: 7.486999312654958, Val MAE: 1.4893460273742676\n",
      "Epoch 145/2000, Train Loss: 6.935913473850065, Val Loss: 7.464166323281255, Val MAE: 1.4871515035629272\n",
      "Epoch 146/2000, Train Loss: 6.915609257058473, Val Loss: 7.439579018291986, Val MAE: 1.4848966598510742\n",
      "Epoch 147/2000, Train Loss: 6.895243019557687, Val Loss: 7.417019492470936, Val MAE: 1.482812523841858\n",
      "Epoch 148/2000, Train Loss: 6.875469432591888, Val Loss: 7.3947056934112165, Val MAE: 1.4807865619659424\n",
      "Epoch 149/2000, Train Loss: 6.855873914607221, Val Loss: 7.371954450833779, Val MAE: 1.4787921905517578\n",
      "Epoch 150/2000, Train Loss: 6.835817614212412, Val Loss: 7.3490168804285725, Val MAE: 1.4766645431518555\n",
      "Epoch 151/2000, Train Loss: 6.816109284811, Val Loss: 7.325705787468849, Val MAE: 1.4744837284088135\n",
      "Epoch 152/2000, Train Loss: 6.796715482031267, Val Loss: 7.303462709075221, Val MAE: 1.4724948406219482\n",
      "Epoch 153/2000, Train Loss: 6.777249035241317, Val Loss: 7.28202531806874, Val MAE: 1.4706265926361084\n",
      "Epoch 154/2000, Train Loss: 6.7580929057519254, Val Loss: 7.260472579770907, Val MAE: 1.4687031507492065\n",
      "Epoch 155/2000, Train Loss: 6.739165348540218, Val Loss: 7.238496929794459, Val MAE: 1.4668020009994507\n",
      "Epoch 156/2000, Train Loss: 6.7209048435605805, Val Loss: 7.216839420697466, Val MAE: 1.4649258852005005\n",
      "Epoch 157/2000, Train Loss: 6.70191039391423, Val Loss: 7.195808918913114, Val MAE: 1.4631445407867432\n",
      "Epoch 158/2000, Train Loss: 6.683590266840463, Val Loss: 7.175211053147957, Val MAE: 1.4613631963729858\n",
      "Epoch 159/2000, Train Loss: 6.665454087334565, Val Loss: 7.153716663360049, Val MAE: 1.459442377090454\n",
      "Epoch 160/2000, Train Loss: 6.647267436242959, Val Loss: 7.13336825926734, Val MAE: 1.4576891660690308\n",
      "Epoch 161/2000, Train Loss: 6.629583770166729, Val Loss: 7.112725292004941, Val MAE: 1.4559005498886108\n",
      "Epoch 162/2000, Train Loss: 6.612042138141952, Val Loss: 7.092930775910434, Val MAE: 1.4543137550354004\n",
      "Epoch 163/2000, Train Loss: 6.594561093127031, Val Loss: 7.073404600314044, Val MAE: 1.4527232646942139\n",
      "Epoch 164/2000, Train Loss: 6.577493333715859, Val Loss: 7.054409043048865, Val MAE: 1.4511712789535522\n",
      "Epoch 165/2000, Train Loss: 6.560541318051837, Val Loss: 7.035243359932659, Val MAE: 1.4497541189193726\n",
      "Epoch 166/2000, Train Loss: 6.543707899691604, Val Loss: 7.01446082615023, Val MAE: 1.4480705261230469\n",
      "Epoch 167/2000, Train Loss: 6.526513456711041, Val Loss: 6.996715508617848, Val MAE: 1.4467681646347046\n",
      "Epoch 168/2000, Train Loss: 6.509997283967761, Val Loss: 6.976279735793032, Val MAE: 1.4452518224716187\n",
      "Epoch 169/2000, Train Loss: 6.493349617589619, Val Loss: 6.957945191953616, Val MAE: 1.4439398050308228\n",
      "Epoch 170/2000, Train Loss: 6.477154372025342, Val Loss: 6.939185251894803, Val MAE: 1.4426374435424805\n",
      "Epoch 171/2000, Train Loss: 6.460983988770627, Val Loss: 6.920870196176779, Val MAE: 1.441410779953003\n",
      "Epoch 172/2000, Train Loss: 6.445222173418317, Val Loss: 6.903348965758304, Val MAE: 1.4403858184814453\n",
      "Epoch 173/2000, Train Loss: 6.42944467545898, Val Loss: 6.885637779324244, Val MAE: 1.4392880201339722\n",
      "Epoch 174/2000, Train Loss: 6.41389667207636, Val Loss: 6.867266266638077, Val MAE: 1.438132405281067\n",
      "Epoch 175/2000, Train Loss: 6.3982419732756215, Val Loss: 6.849376878857795, Val MAE: 1.4370157718658447\n",
      "Epoch 176/2000, Train Loss: 6.382665406978105, Val Loss: 6.832164695161745, Val MAE: 1.435986876487732\n",
      "Epoch 177/2000, Train Loss: 6.367448433279404, Val Loss: 6.81455133258385, Val MAE: 1.4349547624588013\n",
      "Epoch 178/2000, Train Loss: 6.352360266015028, Val Loss: 6.7973226908970314, Val MAE: 1.433942198753357\n",
      "Epoch 179/2000, Train Loss: 6.337701290782952, Val Loss: 6.780853782988379, Val MAE: 1.4330503940582275\n",
      "Epoch 180/2000, Train Loss: 6.32294131634019, Val Loss: 6.764084526534201, Val MAE: 1.4321221113204956\n",
      "Epoch 181/2000, Train Loss: 6.308245028684376, Val Loss: 6.748253560052552, Val MAE: 1.4312529563903809\n",
      "Epoch 182/2000, Train Loss: 6.293951134544121, Val Loss: 6.730422385082514, Val MAE: 1.4301915168762207\n",
      "Epoch 183/2000, Train Loss: 6.279286845983711, Val Loss: 6.714965022006713, Val MAE: 1.4294581413269043\n",
      "Epoch 184/2000, Train Loss: 6.265479377820238, Val Loss: 6.698528098472215, Val MAE: 1.428494930267334\n",
      "Epoch 185/2000, Train Loss: 6.251626571746547, Val Loss: 6.6830920718769775, Val MAE: 1.4277307987213135\n",
      "Epoch 186/2000, Train Loss: 6.237901880236095, Val Loss: 6.668006611082259, Val MAE: 1.4270013570785522\n",
      "Epoch 187/2000, Train Loss: 6.224351768171511, Val Loss: 6.652242328857701, Val MAE: 1.426185965538025\n",
      "Epoch 188/2000, Train Loss: 6.21079010178219, Val Loss: 6.637370446941812, Val MAE: 1.4255083799362183\n",
      "Epoch 189/2000, Train Loss: 6.197798858135204, Val Loss: 6.6220282084488, Val MAE: 1.4247474670410156\n",
      "Epoch 190/2000, Train Loss: 6.184443891761506, Val Loss: 6.607370127744358, Val MAE: 1.4241091012954712\n",
      "Epoch 191/2000, Train Loss: 6.171734077002615, Val Loss: 6.592860747128725, Val MAE: 1.4235423803329468\n",
      "Epoch 192/2000, Train Loss: 6.158993541145727, Val Loss: 6.578593140234458, Val MAE: 1.4230955839157104\n",
      "Epoch 193/2000, Train Loss: 6.146461715000268, Val Loss: 6.564272051090403, Val MAE: 1.4225915670394897\n",
      "Epoch 194/2000, Train Loss: 6.133834116396813, Val Loss: 6.550320887567891, Val MAE: 1.422029733657837\n",
      "Epoch 195/2000, Train Loss: 6.121658218234128, Val Loss: 6.536535483856175, Val MAE: 1.4215799570083618\n",
      "Epoch 196/2000, Train Loss: 6.109595940366419, Val Loss: 6.522879420503382, Val MAE: 1.4211270809173584\n",
      "Epoch 197/2000, Train Loss: 6.097538022572184, Val Loss: 6.50891036203057, Val MAE: 1.4206695556640625\n",
      "Epoch 198/2000, Train Loss: 6.0855790345623495, Val Loss: 6.495622836064764, Val MAE: 1.4202659130096436\n",
      "Epoch 199/2000, Train Loss: 6.073961106762762, Val Loss: 6.482822873188177, Val MAE: 1.4200600385665894\n",
      "Epoch 200/2000, Train Loss: 6.062707993952344, Val Loss: 6.469793497946466, Val MAE: 1.419754981994629\n",
      "Epoch 201/2000, Train Loss: 6.051563488271016, Val Loss: 6.456287642536451, Val MAE: 1.419469952583313\n",
      "Epoch 202/2000, Train Loss: 6.04002490167799, Val Loss: 6.44421374184914, Val MAE: 1.4193155765533447\n",
      "Epoch 203/2000, Train Loss: 6.028776458918755, Val Loss: 6.432100817310518, Val MAE: 1.41922128200531\n",
      "Epoch 204/2000, Train Loss: 6.017808290705725, Val Loss: 6.420084848977193, Val MAE: 1.4191724061965942\n",
      "Epoch 205/2000, Train Loss: 6.006951972013792, Val Loss: 6.407221745038889, Val MAE: 1.4190574884414673\n",
      "Epoch 206/2000, Train Loss: 5.99611045742102, Val Loss: 6.39529157236264, Val MAE: 1.4190579652786255\n",
      "Epoch 207/2000, Train Loss: 5.985767849057095, Val Loss: 6.384093666876401, Val MAE: 1.419105052947998\n",
      "Epoch 208/2000, Train Loss: 5.975541521259633, Val Loss: 6.372803039096911, Val MAE: 1.4191495180130005\n",
      "Epoch 209/2000, Train Loss: 5.965487030842705, Val Loss: 6.361117978480613, Val MAE: 1.4192049503326416\n",
      "Epoch 210/2000, Train Loss: 5.955190817291346, Val Loss: 6.350060960773481, Val MAE: 1.4192646741867065\n",
      "Epoch 211/2000, Train Loss: 5.9453722905475095, Val Loss: 6.338353614832648, Val MAE: 1.419315218925476\n",
      "Epoch 212/2000, Train Loss: 5.935355972690032, Val Loss: 6.327560499059862, Val MAE: 1.419417142868042\n",
      "Epoch 213/2000, Train Loss: 5.925684440312128, Val Loss: 6.31704435618281, Val MAE: 1.4195365905761719\n",
      "Epoch 214/2000, Train Loss: 5.916172205437077, Val Loss: 6.306291188326997, Val MAE: 1.4197407960891724\n",
      "Epoch 215/2000, Train Loss: 5.9066799680252124, Val Loss: 6.295399755923026, Val MAE: 1.4198803901672363\n",
      "Epoch 216/2000, Train Loss: 5.89756928221765, Val Loss: 6.285953792831675, Val MAE: 1.4201353788375854\n",
      "Epoch 217/2000, Train Loss: 5.888688891308145, Val Loss: 6.2752897073529, Val MAE: 1.420324683189392\n",
      "Epoch 218/2000, Train Loss: 5.879696617404983, Val Loss: 6.265563690903901, Val MAE: 1.4205658435821533\n",
      "Epoch 219/2000, Train Loss: 5.870940009109744, Val Loss: 6.256102602849131, Val MAE: 1.420751690864563\n",
      "Epoch 220/2000, Train Loss: 5.862580485531849, Val Loss: 6.246089779296235, Val MAE: 1.4209924936294556\n",
      "Epoch 221/2000, Train Loss: 5.853957808412019, Val Loss: 6.237174103258673, Val MAE: 1.4212125539779663\n",
      "Epoch 222/2000, Train Loss: 5.845635741382816, Val Loss: 6.2277763173075265, Val MAE: 1.4214915037155151\n",
      "Epoch 223/2000, Train Loss: 5.837403155816096, Val Loss: 6.21882531642686, Val MAE: 1.421705722808838\n",
      "Epoch 224/2000, Train Loss: 5.829107519776615, Val Loss: 6.209628524241644, Val MAE: 1.421936273574829\n",
      "Epoch 225/2000, Train Loss: 5.821075552202798, Val Loss: 6.200394534577105, Val MAE: 1.4222239255905151\n",
      "Epoch 226/2000, Train Loss: 5.813077518421525, Val Loss: 6.1912048231890076, Val MAE: 1.4224350452423096\n",
      "Epoch 227/2000, Train Loss: 5.805123663384842, Val Loss: 6.1838984354100095, Val MAE: 1.4227651357650757\n",
      "Epoch 228/2000, Train Loss: 5.797576057835417, Val Loss: 6.174916572527054, Val MAE: 1.4229401350021362\n",
      "Epoch 229/2000, Train Loss: 5.790124260313824, Val Loss: 6.166354390596031, Val MAE: 1.4232312440872192\n",
      "Epoch 230/2000, Train Loss: 5.782785228404421, Val Loss: 6.159035152798399, Val MAE: 1.4235222339630127\n",
      "Epoch 231/2000, Train Loss: 5.775793241330388, Val Loss: 6.151005127128293, Val MAE: 1.423913836479187\n",
      "Epoch 232/2000, Train Loss: 5.7687236435559335, Val Loss: 6.143254711768313, Val MAE: 1.4242182970046997\n",
      "Epoch 233/2000, Train Loss: 5.761886190562748, Val Loss: 6.135752247606578, Val MAE: 1.424598217010498\n",
      "Epoch 234/2000, Train Loss: 5.755039076164187, Val Loss: 6.128570253923763, Val MAE: 1.425009846687317\n",
      "Epoch 235/2000, Train Loss: 5.748285218622381, Val Loss: 6.120900112873553, Val MAE: 1.4253520965576172\n",
      "Epoch 236/2000, Train Loss: 5.74163219242512, Val Loss: 6.114235200147381, Val MAE: 1.4258166551589966\n",
      "Epoch 237/2000, Train Loss: 5.7350600452007, Val Loss: 6.106271651040888, Val MAE: 1.4262100458145142\n",
      "Epoch 238/2000, Train Loss: 5.728644478245573, Val Loss: 6.100134376857988, Val MAE: 1.4266897439956665\n",
      "Epoch 239/2000, Train Loss: 5.722432237037213, Val Loss: 6.093497478223722, Val MAE: 1.4271211624145508\n",
      "Epoch 240/2000, Train Loss: 5.716282351934767, Val Loss: 6.086204665744341, Val MAE: 1.427539348602295\n",
      "Epoch 241/2000, Train Loss: 5.709961472893164, Val Loss: 6.079989592621633, Val MAE: 1.4280041456222534\n",
      "Epoch 242/2000, Train Loss: 5.704123058762037, Val Loss: 6.072950905570561, Val MAE: 1.4284451007843018\n",
      "Epoch 243/2000, Train Loss: 5.698053059319557, Val Loss: 6.066678284322815, Val MAE: 1.4289554357528687\n",
      "Epoch 244/2000, Train Loss: 5.692226583063393, Val Loss: 6.060478792710954, Val MAE: 1.4293673038482666\n",
      "Epoch 245/2000, Train Loss: 5.686449631765474, Val Loss: 6.054633622185899, Val MAE: 1.4299383163452148\n",
      "Epoch 246/2000, Train Loss: 5.680843011331256, Val Loss: 6.047938668035221, Val MAE: 1.4303711652755737\n",
      "Epoch 247/2000, Train Loss: 5.675288247693013, Val Loss: 6.0427020223133425, Val MAE: 1.4309160709381104\n",
      "Epoch 248/2000, Train Loss: 5.669927748888836, Val Loss: 6.036727331897716, Val MAE: 1.4313806295394897\n",
      "Epoch 249/2000, Train Loss: 5.664626190982848, Val Loss: 6.031397221102992, Val MAE: 1.4319061040878296\n",
      "Epoch 250/2000, Train Loss: 5.659365160879663, Val Loss: 6.025438406006276, Val MAE: 1.4323883056640625\n",
      "Epoch 251/2000, Train Loss: 5.654083466882189, Val Loss: 6.019994730317811, Val MAE: 1.4328863620758057\n",
      "Epoch 252/2000, Train Loss: 5.648991264733188, Val Loss: 6.0142719175791886, Val MAE: 1.4333945512771606\n",
      "Epoch 253/2000, Train Loss: 5.643977002137819, Val Loss: 6.009446709846867, Val MAE: 1.4338911771774292\n",
      "Epoch 254/2000, Train Loss: 5.639018692658201, Val Loss: 6.004283701972494, Val MAE: 1.4344669580459595\n",
      "Epoch 255/2000, Train Loss: 5.634197015816019, Val Loss: 5.998779735821285, Val MAE: 1.4351203441619873\n",
      "Epoch 256/2000, Train Loss: 5.629424978024015, Val Loss: 5.994049504308161, Val MAE: 1.435684084892273\n",
      "Epoch 257/2000, Train Loss: 5.624800929378909, Val Loss: 5.989356863526029, Val MAE: 1.4362479448318481\n",
      "Epoch 258/2000, Train Loss: 5.620318021512552, Val Loss: 5.98413018497214, Val MAE: 1.436811089515686\n",
      "Epoch 259/2000, Train Loss: 5.615622556016614, Val Loss: 5.979651186330421, Val MAE: 1.4374544620513916\n",
      "Epoch 260/2000, Train Loss: 5.611233295813822, Val Loss: 5.975225570164507, Val MAE: 1.4379935264587402\n",
      "Epoch 261/2000, Train Loss: 5.606901845656844, Val Loss: 5.970971341435698, Val MAE: 1.4386024475097656\n",
      "Epoch 262/2000, Train Loss: 5.602607792150295, Val Loss: 5.966702235697424, Val MAE: 1.4391833543777466\n",
      "Epoch 263/2000, Train Loss: 5.598317667768507, Val Loss: 5.961990648134404, Val MAE: 1.4398469924926758\n",
      "Epoch 264/2000, Train Loss: 5.594118608881436, Val Loss: 5.957631838203206, Val MAE: 1.4404642581939697\n",
      "Epoch 265/2000, Train Loss: 5.5899799012701585, Val Loss: 5.9535102464190315, Val MAE: 1.4410662651062012\n",
      "Epoch 266/2000, Train Loss: 5.586002157239491, Val Loss: 5.9491455176736965, Val MAE: 1.4417463541030884\n",
      "Epoch 267/2000, Train Loss: 5.581897568736254, Val Loss: 5.945253799062804, Val MAE: 1.4423843622207642\n",
      "Epoch 268/2000, Train Loss: 5.578152463605587, Val Loss: 5.941198291923475, Val MAE: 1.443053126335144\n",
      "Epoch 269/2000, Train Loss: 5.574400001139309, Val Loss: 5.938125747196171, Val MAE: 1.4436326026916504\n",
      "Epoch 270/2000, Train Loss: 5.570749871584834, Val Loss: 5.933644586587353, Val MAE: 1.444331169128418\n",
      "Epoch 271/2000, Train Loss: 5.566870375303715, Val Loss: 5.9304082938427225, Val MAE: 1.4449907541275024\n",
      "Epoch 272/2000, Train Loss: 5.563159648672449, Val Loss: 5.926908068222191, Val MAE: 1.4456533193588257\n",
      "Epoch 273/2000, Train Loss: 5.559748276449441, Val Loss: 5.923064569882843, Val MAE: 1.4463988542556763\n",
      "Epoch 274/2000, Train Loss: 5.5561318144170775, Val Loss: 5.9193574481099756, Val MAE: 1.447095513343811\n",
      "Epoch 275/2000, Train Loss: 5.552743758725751, Val Loss: 5.916689589596852, Val MAE: 1.447736144065857\n",
      "Epoch 276/2000, Train Loss: 5.549308443304353, Val Loss: 5.913161241816818, Val MAE: 1.4484405517578125\n",
      "Epoch 277/2000, Train Loss: 5.5459441964178335, Val Loss: 5.909677191999163, Val MAE: 1.4492108821868896\n",
      "Epoch 278/2000, Train Loss: 5.542976356072798, Val Loss: 5.906450969158509, Val MAE: 1.4498432874679565\n",
      "Epoch 279/2000, Train Loss: 5.539797413944779, Val Loss: 5.903645416305881, Val MAE: 1.45054030418396\n",
      "Epoch 280/2000, Train Loss: 5.536689555619821, Val Loss: 5.9010480868114605, Val MAE: 1.4511609077453613\n",
      "Epoch 281/2000, Train Loss: 5.533885109852436, Val Loss: 5.897802927351873, Val MAE: 1.4519250392913818\n",
      "Epoch 282/2000, Train Loss: 5.530615291823643, Val Loss: 5.895458635979471, Val MAE: 1.4525777101516724\n",
      "Epoch 283/2000, Train Loss: 5.527639433668165, Val Loss: 5.892243854769873, Val MAE: 1.4533318281173706\n",
      "Epoch 284/2000, Train Loss: 5.524908615450554, Val Loss: 5.889429162105471, Val MAE: 1.4540157318115234\n",
      "Epoch 285/2000, Train Loss: 5.521969454293516, Val Loss: 5.8867235442777295, Val MAE: 1.4547204971313477\n",
      "Epoch 286/2000, Train Loss: 5.519165454491354, Val Loss: 5.884233778860955, Val MAE: 1.4554094076156616\n",
      "Epoch 287/2000, Train Loss: 5.516412010501591, Val Loss: 5.881553341626027, Val MAE: 1.4561094045639038\n",
      "Epoch 288/2000, Train Loss: 5.513594837779314, Val Loss: 5.87898502184437, Val MAE: 1.4568144083023071\n",
      "Epoch 289/2000, Train Loss: 5.510956733349257, Val Loss: 5.876455618040095, Val MAE: 1.45759117603302\n",
      "Epoch 290/2000, Train Loss: 5.508198351360055, Val Loss: 5.874262353969277, Val MAE: 1.4582531452178955\n",
      "Epoch 291/2000, Train Loss: 5.505785203667949, Val Loss: 5.871489098039242, Val MAE: 1.45900559425354\n",
      "Epoch 292/2000, Train Loss: 5.50321407103354, Val Loss: 5.869626471186085, Val MAE: 1.4596415758132935\n",
      "Epoch 293/2000, Train Loss: 5.500655921557518, Val Loss: 5.867171771300313, Val MAE: 1.4603304862976074\n",
      "Epoch 294/2000, Train Loss: 5.498112400177077, Val Loss: 5.86483645341174, Val MAE: 1.461084008216858\n",
      "Epoch 295/2000, Train Loss: 5.495647929068773, Val Loss: 5.862947497687755, Val MAE: 1.4617100954055786\n",
      "Epoch 296/2000, Train Loss: 5.493155861639121, Val Loss: 5.86055870130886, Val MAE: 1.4624382257461548\n",
      "Epoch 297/2000, Train Loss: 5.490925343543354, Val Loss: 5.858233339697214, Val MAE: 1.4631881713867188\n",
      "Epoch 298/2000, Train Loss: 5.488480881684938, Val Loss: 5.856284907195911, Val MAE: 1.4638738632202148\n",
      "Epoch 299/2000, Train Loss: 5.486166309253336, Val Loss: 5.854173184443686, Val MAE: 1.4645869731903076\n",
      "Epoch 300/2000, Train Loss: 5.483845136267966, Val Loss: 5.852179066053042, Val MAE: 1.465270757675171\n",
      "Epoch 301/2000, Train Loss: 5.481623388527985, Val Loss: 5.849858411385561, Val MAE: 1.466065526008606\n",
      "Epoch 302/2000, Train Loss: 5.479360642188205, Val Loss: 5.847950688150315, Val MAE: 1.46681809425354\n",
      "Epoch 303/2000, Train Loss: 5.477079952878905, Val Loss: 5.846250258203858, Val MAE: 1.4674079418182373\n",
      "Epoch 304/2000, Train Loss: 5.47494921761445, Val Loss: 5.844191045525971, Val MAE: 1.4682023525238037\n",
      "Epoch 305/2000, Train Loss: 5.4728108322763, Val Loss: 5.842172120803175, Val MAE: 1.468916416168213\n",
      "Epoch 306/2000, Train Loss: 5.470708816572447, Val Loss: 5.840621298150549, Val MAE: 1.4695688486099243\n",
      "Epoch 307/2000, Train Loss: 5.468766837620048, Val Loss: 5.838789931632328, Val MAE: 1.4702786207199097\n",
      "Epoch 308/2000, Train Loss: 5.466751809693993, Val Loss: 5.837252366684081, Val MAE: 1.4709302186965942\n",
      "Epoch 309/2000, Train Loss: 5.46476870234125, Val Loss: 5.8355872621047755, Val MAE: 1.4715925455093384\n",
      "Epoch 310/2000, Train Loss: 5.462727824539303, Val Loss: 5.833795406402798, Val MAE: 1.472325086593628\n",
      "Epoch 311/2000, Train Loss: 5.460815480693304, Val Loss: 5.832181525339774, Val MAE: 1.473013162612915\n",
      "Epoch 312/2000, Train Loss: 5.458816904823684, Val Loss: 5.83068618155583, Val MAE: 1.4737201929092407\n",
      "Epoch 313/2000, Train Loss: 5.4567737180023945, Val Loss: 5.828997063094505, Val MAE: 1.4743753671646118\n",
      "Epoch 314/2000, Train Loss: 5.454972952373593, Val Loss: 5.827394507119051, Val MAE: 1.4750573635101318\n",
      "Epoch 315/2000, Train Loss: 5.453122770509444, Val Loss: 5.8257290179120655, Val MAE: 1.4757888317108154\n",
      "Epoch 316/2000, Train Loss: 5.451343016923103, Val Loss: 5.824426609851899, Val MAE: 1.476452350616455\n",
      "Epoch 317/2000, Train Loss: 5.44945373197914, Val Loss: 5.823175491315145, Val MAE: 1.4771206378936768\n",
      "Epoch 318/2000, Train Loss: 5.447802553707072, Val Loss: 5.821762208089187, Val MAE: 1.4777313470840454\n",
      "Epoch 319/2000, Train Loss: 5.445970251390751, Val Loss: 5.820429264633298, Val MAE: 1.4783753156661987\n",
      "Epoch 320/2000, Train Loss: 5.444249950186792, Val Loss: 5.818828571371347, Val MAE: 1.47905433177948\n",
      "Epoch 321/2000, Train Loss: 5.442502005011326, Val Loss: 5.817382612952034, Val MAE: 1.4797708988189697\n",
      "Epoch 322/2000, Train Loss: 5.440764881874959, Val Loss: 5.816012897030293, Val MAE: 1.480405330657959\n",
      "Epoch 323/2000, Train Loss: 5.439109095211687, Val Loss: 5.814975159613, Val MAE: 1.480947494506836\n",
      "Epoch 324/2000, Train Loss: 5.437505853251619, Val Loss: 5.813381459370301, Val MAE: 1.4816497564315796\n",
      "Epoch 325/2000, Train Loss: 5.4359710594032284, Val Loss: 5.812337491581564, Val MAE: 1.4822365045547485\n",
      "Epoch 326/2000, Train Loss: 5.434366282235561, Val Loss: 5.811493271397888, Val MAE: 1.4827123880386353\n",
      "Epoch 327/2000, Train Loss: 5.432852071578195, Val Loss: 5.809883906753785, Val MAE: 1.4834542274475098\n",
      "Epoch 328/2000, Train Loss: 5.43146402277131, Val Loss: 5.8087759074450265, Val MAE: 1.4840995073318481\n",
      "Epoch 329/2000, Train Loss: 5.429725341608959, Val Loss: 5.8079364037295, Val MAE: 1.484552264213562\n",
      "Epoch 330/2000, Train Loss: 5.428300183898206, Val Loss: 5.806454345447208, Val MAE: 1.4852561950683594\n",
      "Epoch 331/2000, Train Loss: 5.426634610328433, Val Loss: 5.805600485670457, Val MAE: 1.485746145248413\n",
      "Epoch 332/2000, Train Loss: 5.4251892075917825, Val Loss: 5.80458591368038, Val MAE: 1.4863379001617432\n",
      "Epoch 333/2000, Train Loss: 5.423804298578057, Val Loss: 5.803614937338625, Val MAE: 1.4869447946548462\n",
      "Epoch 334/2000, Train Loss: 5.422405797943609, Val Loss: 5.80238214846051, Val MAE: 1.4875293970108032\n",
      "Epoch 335/2000, Train Loss: 5.420797792980991, Val Loss: 5.801493176535364, Val MAE: 1.4880785942077637\n",
      "Epoch 336/2000, Train Loss: 5.419378692041729, Val Loss: 5.800589001005578, Val MAE: 1.4886317253112793\n",
      "Epoch 337/2000, Train Loss: 5.418009437195941, Val Loss: 5.799538659210963, Val MAE: 1.4891878366470337\n",
      "Epoch 338/2000, Train Loss: 5.416604975929234, Val Loss: 5.798703335931906, Val MAE: 1.4897621870040894\n",
      "Epoch 339/2000, Train Loss: 5.415288098801366, Val Loss: 5.797765613057927, Val MAE: 1.4903669357299805\n",
      "Epoch 340/2000, Train Loss: 5.413947373104297, Val Loss: 5.7967411591828055, Val MAE: 1.4909743070602417\n",
      "Epoch 341/2000, Train Loss: 5.412637234740824, Val Loss: 5.795856270950504, Val MAE: 1.4915262460708618\n",
      "Epoch 342/2000, Train Loss: 5.41130017798019, Val Loss: 5.795053595432083, Val MAE: 1.4920607805252075\n",
      "Epoch 343/2000, Train Loss: 5.410064005499403, Val Loss: 5.793967757749995, Val MAE: 1.492736577987671\n",
      "Epoch 344/2000, Train Loss: 5.408715705817892, Val Loss: 5.793266976417387, Val MAE: 1.4932096004486084\n",
      "Epoch 345/2000, Train Loss: 5.407283598901855, Val Loss: 5.792350812516081, Val MAE: 1.493835210800171\n",
      "Epoch 346/2000, Train Loss: 5.406084608310214, Val Loss: 5.79151993693744, Val MAE: 1.4943464994430542\n",
      "Epoch 347/2000, Train Loss: 5.404934605270268, Val Loss: 5.790767719530549, Val MAE: 1.4948203563690186\n",
      "Epoch 348/2000, Train Loss: 5.4035261346788825, Val Loss: 5.789883583875242, Val MAE: 1.4954904317855835\n",
      "Epoch 349/2000, Train Loss: 5.402209065146718, Val Loss: 5.789023619771733, Val MAE: 1.4960829019546509\n",
      "Epoch 350/2000, Train Loss: 5.4009890546268515, Val Loss: 5.78840439696013, Val MAE: 1.4965542554855347\n",
      "Epoch 351/2000, Train Loss: 5.399738181904116, Val Loss: 5.787432489592001, Val MAE: 1.4971836805343628\n",
      "Epoch 352/2000, Train Loss: 5.398484571646168, Val Loss: 5.7866380347297826, Val MAE: 1.497722864151001\n",
      "Epoch 353/2000, Train Loss: 5.397257610243194, Val Loss: 5.785852031104426, Val MAE: 1.4983359575271606\n",
      "Epoch 354/2000, Train Loss: 5.39618944215741, Val Loss: 5.785230678748283, Val MAE: 1.498795747756958\n",
      "Epoch 355/2000, Train Loss: 5.39492409385316, Val Loss: 5.784530592074088, Val MAE: 1.499306082725525\n",
      "Epoch 356/2000, Train Loss: 5.393776665767425, Val Loss: 5.783968577007635, Val MAE: 1.4997248649597168\n",
      "Epoch 357/2000, Train Loss: 5.392641734607114, Val Loss: 5.7831168456817625, Val MAE: 1.5003559589385986\n",
      "Epoch 358/2000, Train Loss: 5.391396146018601, Val Loss: 5.782246360538203, Val MAE: 1.5009574890136719\n",
      "Epoch 359/2000, Train Loss: 5.390287235033168, Val Loss: 5.781765638916135, Val MAE: 1.5013236999511719\n",
      "Epoch 360/2000, Train Loss: 5.3891785149839375, Val Loss: 5.781029181378332, Val MAE: 1.501921534538269\n",
      "Epoch 361/2000, Train Loss: 5.388058040277963, Val Loss: 5.780572850496397, Val MAE: 1.5022701025009155\n",
      "Epoch 362/2000, Train Loss: 5.386962727540651, Val Loss: 5.779773305543337, Val MAE: 1.5028910636901855\n",
      "Epoch 363/2000, Train Loss: 5.385765230378829, Val Loss: 5.779201488155837, Val MAE: 1.5033175945281982\n",
      "Epoch 364/2000, Train Loss: 5.3846175514586285, Val Loss: 5.778398133563703, Val MAE: 1.5038684606552124\n",
      "Epoch 365/2000, Train Loss: 5.383516096884226, Val Loss: 5.777881596248814, Val MAE: 1.5042155981063843\n",
      "Epoch 366/2000, Train Loss: 5.382570347259113, Val Loss: 5.777267961372658, Val MAE: 1.5047550201416016\n",
      "Epoch 367/2000, Train Loss: 5.381247536721656, Val Loss: 5.776647205974348, Val MAE: 1.5052114725112915\n",
      "Epoch 368/2000, Train Loss: 5.3802539743561715, Val Loss: 5.7761098614891, Val MAE: 1.5056647062301636\n",
      "Epoch 369/2000, Train Loss: 5.379204032959693, Val Loss: 5.775334969436357, Val MAE: 1.506279468536377\n",
      "Epoch 370/2000, Train Loss: 5.378189091737802, Val Loss: 5.774860128942615, Val MAE: 1.5066670179367065\n",
      "Epoch 371/2000, Train Loss: 5.377101339814027, Val Loss: 5.774251101623982, Val MAE: 1.507097840309143\n",
      "Epoch 372/2000, Train Loss: 5.376112767963826, Val Loss: 5.773734257582131, Val MAE: 1.5075751543045044\n",
      "Epoch 373/2000, Train Loss: 5.375048026830524, Val Loss: 5.773198787407773, Val MAE: 1.5079795122146606\n",
      "Epoch 374/2000, Train Loss: 5.374039631842896, Val Loss: 5.772776806163131, Val MAE: 1.5083625316619873\n",
      "Epoch 375/2000, Train Loss: 5.372995262914439, Val Loss: 5.771954617642481, Val MAE: 1.508910059928894\n",
      "Epoch 376/2000, Train Loss: 5.371912930734891, Val Loss: 5.7715753252170865, Val MAE: 1.5093247890472412\n",
      "Epoch 377/2000, Train Loss: 5.370985136830748, Val Loss: 5.770996900782308, Val MAE: 1.509830355644226\n",
      "Epoch 378/2000, Train Loss: 5.369942692951952, Val Loss: 5.770518902427195, Val MAE: 1.5101463794708252\n",
      "Epoch 379/2000, Train Loss: 5.368935100466831, Val Loss: 5.770029928614969, Val MAE: 1.5105830430984497\n",
      "Epoch 380/2000, Train Loss: 5.367951781878918, Val Loss: 5.769419736362743, Val MAE: 1.5110372304916382\n",
      "Epoch 381/2000, Train Loss: 5.366954541474811, Val Loss: 5.768864093114841, Val MAE: 1.5114516019821167\n",
      "Epoch 382/2000, Train Loss: 5.365942457039368, Val Loss: 5.768533931458397, Val MAE: 1.5117416381835938\n",
      "Epoch 383/2000, Train Loss: 5.364930073028715, Val Loss: 5.767874061970171, Val MAE: 1.5123370885849\n",
      "Epoch 384/2000, Train Loss: 5.363857063250505, Val Loss: 5.76739729567223, Val MAE: 1.5127369165420532\n",
      "Epoch 385/2000, Train Loss: 5.362940652029855, Val Loss: 5.766924767455923, Val MAE: 1.5131639242172241\n",
      "Epoch 386/2000, Train Loss: 5.362055838737247, Val Loss: 5.766354616401028, Val MAE: 1.5136768817901611\n",
      "Epoch 387/2000, Train Loss: 5.360949158584627, Val Loss: 5.765891351876638, Val MAE: 1.5139648914337158\n",
      "Epoch 388/2000, Train Loss: 5.360169049638162, Val Loss: 5.7653388631635485, Val MAE: 1.5145549774169922\n",
      "Epoch 389/2000, Train Loss: 5.359074770103954, Val Loss: 5.7648969186008525, Val MAE: 1.5149303674697876\n",
      "Epoch 390/2000, Train Loss: 5.358059258464354, Val Loss: 5.7643532856433035, Val MAE: 1.5153663158416748\n",
      "Epoch 391/2000, Train Loss: 5.357128557313252, Val Loss: 5.763910292758855, Val MAE: 1.515762209892273\n",
      "Epoch 392/2000, Train Loss: 5.356234151825445, Val Loss: 5.763489256120239, Val MAE: 1.516183614730835\n",
      "Epoch 393/2000, Train Loss: 5.35525481957604, Val Loss: 5.763026236530838, Val MAE: 1.5165479183197021\n",
      "Epoch 394/2000, Train Loss: 5.354367001806658, Val Loss: 5.762597568082517, Val MAE: 1.5169670581817627\n",
      "Epoch 395/2000, Train Loss: 5.353392129209493, Val Loss: 5.762154224314457, Val MAE: 1.5174144506454468\n",
      "Epoch 396/2000, Train Loss: 5.352526712719611, Val Loss: 5.7617859680444825, Val MAE: 1.5176514387130737\n",
      "Epoch 397/2000, Train Loss: 5.351628766606845, Val Loss: 5.761191195395379, Val MAE: 1.5182586908340454\n",
      "Epoch 398/2000, Train Loss: 5.350676207371282, Val Loss: 5.760883160960784, Val MAE: 1.5185885429382324\n",
      "Epoch 399/2000, Train Loss: 5.349702219322146, Val Loss: 5.760474158581973, Val MAE: 1.5189615488052368\n",
      "Epoch 400/2000, Train Loss: 5.34880663189227, Val Loss: 5.760063173346198, Val MAE: 1.5193525552749634\n",
      "Epoch 401/2000, Train Loss: 5.347911386402622, Val Loss: 5.759699329369294, Val MAE: 1.5196479558944702\n",
      "Epoch 402/2000, Train Loss: 5.347006021965397, Val Loss: 5.75935679450859, Val MAE: 1.5199779272079468\n",
      "Epoch 403/2000, Train Loss: 5.346189180786219, Val Loss: 5.758864526992909, Val MAE: 1.5204421281814575\n",
      "Epoch 404/2000, Train Loss: 5.3453053166378055, Val Loss: 5.758618294509179, Val MAE: 1.5207544565200806\n",
      "Epoch 405/2000, Train Loss: 5.3444369691178295, Val Loss: 5.758238137770865, Val MAE: 1.5210835933685303\n",
      "Epoch 406/2000, Train Loss: 5.343532199175067, Val Loss: 5.757699977230589, Val MAE: 1.5215167999267578\n",
      "Epoch 407/2000, Train Loss: 5.342634282256414, Val Loss: 5.757412551831033, Val MAE: 1.521880865097046\n",
      "Epoch 408/2000, Train Loss: 5.341894347679439, Val Loss: 5.756988397249753, Val MAE: 1.5222312211990356\n",
      "Epoch 409/2000, Train Loss: 5.340934612480877, Val Loss: 5.756669808190533, Val MAE: 1.522605061531067\n",
      "Epoch 410/2000, Train Loss: 5.340096998852294, Val Loss: 5.756328557336002, Val MAE: 1.5229551792144775\n",
      "Epoch 411/2000, Train Loss: 5.339249565935235, Val Loss: 5.75593004614935, Val MAE: 1.5232141017913818\n",
      "Epoch 412/2000, Train Loss: 5.338438236067447, Val Loss: 5.755707677670211, Val MAE: 1.5235893726348877\n",
      "Epoch 413/2000, Train Loss: 5.337520981908767, Val Loss: 5.755279210774906, Val MAE: 1.5239224433898926\n",
      "Epoch 414/2000, Train Loss: 5.336621698907024, Val Loss: 5.754815631776775, Val MAE: 1.524370551109314\n",
      "Epoch 415/2000, Train Loss: 5.335769918751834, Val Loss: 5.754423739576558, Val MAE: 1.5247553586959839\n",
      "Epoch 416/2000, Train Loss: 5.334934852728955, Val Loss: 5.754142205922975, Val MAE: 1.525090217590332\n",
      "Epoch 417/2000, Train Loss: 5.3340671151394075, Val Loss: 5.753671698203874, Val MAE: 1.5254496335983276\n",
      "Epoch 418/2000, Train Loss: 5.333330971015163, Val Loss: 5.753518114259483, Val MAE: 1.5257527828216553\n",
      "Epoch 419/2000, Train Loss: 5.332437604183382, Val Loss: 5.753162196606671, Val MAE: 1.526158094406128\n",
      "Epoch 420/2000, Train Loss: 5.3316588133342835, Val Loss: 5.752666313850551, Val MAE: 1.5265991687774658\n",
      "Epoch 421/2000, Train Loss: 5.3308318174699085, Val Loss: 5.752508477971459, Val MAE: 1.5268046855926514\n",
      "Epoch 422/2000, Train Loss: 5.329975647963242, Val Loss: 5.752118660678192, Val MAE: 1.5272244215011597\n",
      "Epoch 423/2000, Train Loss: 5.329144447643433, Val Loss: 5.751758656188254, Val MAE: 1.527475118637085\n",
      "Epoch 424/2000, Train Loss: 5.3284130437033514, Val Loss: 5.751392434769814, Val MAE: 1.5280036926269531\n",
      "Epoch 425/2000, Train Loss: 5.327522977819584, Val Loss: 5.75106611551677, Val MAE: 1.5282238721847534\n",
      "Epoch 426/2000, Train Loss: 5.326740048621256, Val Loss: 5.750752324677024, Val MAE: 1.5286785364151\n",
      "Epoch 427/2000, Train Loss: 5.325875902075234, Val Loss: 5.750495757167128, Val MAE: 1.5288875102996826\n",
      "Epoch 428/2000, Train Loss: 5.325259132170492, Val Loss: 5.750409911076228, Val MAE: 1.5290801525115967\n",
      "Epoch 429/2000, Train Loss: 5.324399413565865, Val Loss: 5.750035846087546, Val MAE: 1.5295627117156982\n",
      "Epoch 430/2000, Train Loss: 5.323672100660417, Val Loss: 5.749845524674527, Val MAE: 1.5298537015914917\n",
      "Epoch 431/2000, Train Loss: 5.322859280550673, Val Loss: 5.749257686260279, Val MAE: 1.5302999019622803\n",
      "Epoch 432/2000, Train Loss: 5.322003307517023, Val Loss: 5.749087042126816, Val MAE: 1.530540943145752\n",
      "Epoch 433/2000, Train Loss: 5.321236286257288, Val Loss: 5.748858259221829, Val MAE: 1.5308942794799805\n",
      "Epoch 434/2000, Train Loss: 5.320457114542479, Val Loss: 5.7484784538228215, Val MAE: 1.5311737060546875\n",
      "Epoch 435/2000, Train Loss: 5.319678106583146, Val Loss: 5.748239628401007, Val MAE: 1.5314425230026245\n",
      "Epoch 436/2000, Train Loss: 5.318954899309388, Val Loss: 5.747991700055767, Val MAE: 1.5317517518997192\n",
      "Epoch 437/2000, Train Loss: 5.318257897609896, Val Loss: 5.747687863647391, Val MAE: 1.5320603847503662\n",
      "Epoch 438/2000, Train Loss: 5.317543007637899, Val Loss: 5.747239540658595, Val MAE: 1.5325623750686646\n",
      "Epoch 439/2000, Train Loss: 5.316742416169759, Val Loss: 5.747023135332522, Val MAE: 1.5329339504241943\n",
      "Epoch 440/2000, Train Loss: 5.31597623449241, Val Loss: 5.746923930920228, Val MAE: 1.5331294536590576\n",
      "Epoch 441/2000, Train Loss: 5.315200166917032, Val Loss: 5.746694827252936, Val MAE: 1.5334362983703613\n",
      "Epoch 442/2000, Train Loss: 5.31458884557312, Val Loss: 5.746540513151647, Val MAE: 1.5335729122161865\n",
      "Epoch 443/2000, Train Loss: 5.31385362307678, Val Loss: 5.7462904638288945, Val MAE: 1.5339593887329102\n",
      "Epoch 444/2000, Train Loss: 5.313045882742477, Val Loss: 5.746039905770474, Val MAE: 1.534171462059021\n",
      "Epoch 445/2000, Train Loss: 5.312404148013218, Val Loss: 5.745956720060894, Val MAE: 1.5343353748321533\n",
      "Epoch 446/2000, Train Loss: 5.311635154640901, Val Loss: 5.745639726793001, Val MAE: 1.5347541570663452\n",
      "Epoch 447/2000, Train Loss: 5.310877686231427, Val Loss: 5.745335684730373, Val MAE: 1.535070776939392\n",
      "Epoch 448/2000, Train Loss: 5.3100999931815975, Val Loss: 5.7450694717945305, Val MAE: 1.5353803634643555\n",
      "Epoch 449/2000, Train Loss: 5.309405025170774, Val Loss: 5.744678785676985, Val MAE: 1.535813331604004\n",
      "Epoch 450/2000, Train Loss: 5.308819853026291, Val Loss: 5.7445974964249755, Val MAE: 1.5359792709350586\n",
      "Epoch 451/2000, Train Loss: 5.308000126999084, Val Loss: 5.7442615419535095, Val MAE: 1.5363585948944092\n",
      "Epoch 452/2000, Train Loss: 5.307235099915297, Val Loss: 5.744188806879411, Val MAE: 1.536502480506897\n",
      "Epoch 453/2000, Train Loss: 5.306544244163561, Val Loss: 5.743874071205792, Val MAE: 1.5368332862854004\n",
      "Epoch 454/2000, Train Loss: 5.305908882643118, Val Loss: 5.743753095240039, Val MAE: 1.5370979309082031\n",
      "Epoch 455/2000, Train Loss: 5.305211512997149, Val Loss: 5.743469808945598, Val MAE: 1.5374549627304077\n",
      "Epoch 456/2000, Train Loss: 5.304501198614927, Val Loss: 5.743185743416121, Val MAE: 1.5377042293548584\n",
      "Epoch 457/2000, Train Loss: 5.303713085280599, Val Loss: 5.743018001135701, Val MAE: 1.537928581237793\n",
      "Epoch 458/2000, Train Loss: 5.303162712349512, Val Loss: 5.742922909990727, Val MAE: 1.538092017173767\n",
      "Epoch 459/2000, Train Loss: 5.302464711171149, Val Loss: 5.74248643543013, Val MAE: 1.5385829210281372\n",
      "Epoch 460/2000, Train Loss: 5.3017076023190395, Val Loss: 5.74223384406953, Val MAE: 1.5389143228530884\n",
      "Epoch 461/2000, Train Loss: 5.3010312161925475, Val Loss: 5.7420742841671, Val MAE: 1.5390926599502563\n",
      "Epoch 462/2000, Train Loss: 5.300365268471038, Val Loss: 5.741823754407215, Val MAE: 1.5392894744873047\n",
      "Epoch 463/2000, Train Loss: 5.299621591427056, Val Loss: 5.741705932110457, Val MAE: 1.539477825164795\n",
      "Epoch 464/2000, Train Loss: 5.2989264836536165, Val Loss: 5.741292311055216, Val MAE: 1.5397744178771973\n",
      "Epoch 465/2000, Train Loss: 5.298294835154395, Val Loss: 5.741090033989434, Val MAE: 1.5401562452316284\n",
      "Epoch 466/2000, Train Loss: 5.297538288876173, Val Loss: 5.74083916551294, Val MAE: 1.5404181480407715\n",
      "Epoch 467/2000, Train Loss: 5.2970007187375545, Val Loss: 5.740701732650081, Val MAE: 1.540658712387085\n",
      "Epoch 468/2000, Train Loss: 5.296264082340184, Val Loss: 5.7403954522689915, Val MAE: 1.540931224822998\n",
      "Epoch 469/2000, Train Loss: 5.295591477438398, Val Loss: 5.740185848343263, Val MAE: 1.541141390800476\n",
      "Epoch 470/2000, Train Loss: 5.2949526611975095, Val Loss: 5.7400653144452916, Val MAE: 1.5413928031921387\n",
      "Epoch 471/2000, Train Loss: 5.294385324138558, Val Loss: 5.739728752580622, Val MAE: 1.5416946411132812\n",
      "Epoch 472/2000, Train Loss: 5.293820065062789, Val Loss: 5.739665872186696, Val MAE: 1.5418423414230347\n",
      "Epoch 473/2000, Train Loss: 5.293128139615311, Val Loss: 5.739259833817453, Val MAE: 1.5422828197479248\n",
      "Epoch 474/2000, Train Loss: 5.292332029778885, Val Loss: 5.739119217027582, Val MAE: 1.542466640472412\n",
      "Epoch 475/2000, Train Loss: 5.2918415315800535, Val Loss: 5.739028530383329, Val MAE: 1.5427625179290771\n",
      "Epoch 476/2000, Train Loss: 5.291116278243686, Val Loss: 5.738847385670431, Val MAE: 1.5429519414901733\n",
      "Epoch 477/2000, Train Loss: 5.29036408398539, Val Loss: 5.738479867349707, Val MAE: 1.5433543920516968\n",
      "Epoch 478/2000, Train Loss: 5.289792128300515, Val Loss: 5.738319103597501, Val MAE: 1.543574571609497\n",
      "Epoch 479/2000, Train Loss: 5.289329139050424, Val Loss: 5.738349034698731, Val MAE: 1.5436949729919434\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 480/2000, Train Loss: 5.28847543397141, Val Loss: 5.738056544863849, Val MAE: 1.5439997911453247\n",
      "Epoch 481/2000, Train Loss: 5.287917851898387, Val Loss: 5.737839545496378, Val MAE: 1.5442516803741455\n",
      "Epoch 482/2000, Train Loss: 5.287273608992252, Val Loss: 5.737408131087592, Val MAE: 1.5446183681488037\n",
      "Epoch 483/2000, Train Loss: 5.286583803473519, Val Loss: 5.737432016418614, Val MAE: 1.5448508262634277\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 484/2000, Train Loss: 5.2860233397822745, Val Loss: 5.737187235180391, Val MAE: 1.5451809167861938\n",
      "Epoch 485/2000, Train Loss: 5.285436123378842, Val Loss: 5.737105351249013, Val MAE: 1.5451903343200684\n",
      "Epoch 486/2000, Train Loss: 5.284783041535928, Val Loss: 5.736770917971929, Val MAE: 1.5455459356307983\n",
      "Epoch 487/2000, Train Loss: 5.284101155339455, Val Loss: 5.736627428812354, Val MAE: 1.5458084344863892\n",
      "Epoch 488/2000, Train Loss: 5.283469638381518, Val Loss: 5.736566795577332, Val MAE: 1.5459376573562622\n",
      "Epoch 489/2000, Train Loss: 5.282890884914841, Val Loss: 5.736257327259134, Val MAE: 1.5461854934692383\n",
      "Epoch 490/2000, Train Loss: 5.282223521362804, Val Loss: 5.7360367167068915, Val MAE: 1.5465971231460571\n",
      "Epoch 491/2000, Train Loss: 5.281599987064928, Val Loss: 5.735951116416797, Val MAE: 1.5466365814208984\n",
      "Epoch 492/2000, Train Loss: 5.2810142360716785, Val Loss: 5.735811608191295, Val MAE: 1.5468688011169434\n",
      "Epoch 493/2000, Train Loss: 5.280466706546405, Val Loss: 5.735682596216144, Val MAE: 1.5471662282943726\n",
      "Epoch 494/2000, Train Loss: 5.279808712206954, Val Loss: 5.735575188190565, Val MAE: 1.5472761392593384\n",
      "Epoch 495/2000, Train Loss: 5.279214072781159, Val Loss: 5.73541088616447, Val MAE: 1.547504186630249\n",
      "Epoch 496/2000, Train Loss: 5.278561391937825, Val Loss: 5.735166605277164, Val MAE: 1.5479518175125122\n",
      "Epoch 497/2000, Train Loss: 5.278002600700062, Val Loss: 5.7351616172615545, Val MAE: 1.547979712486267\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 498/2000, Train Loss: 5.277320616686537, Val Loss: 5.734919838949081, Val MAE: 1.548209309577942\n",
      "Epoch 499/2000, Train Loss: 5.276750740410995, Val Loss: 5.734807143791007, Val MAE: 1.548393964767456\n",
      "Epoch 500/2000, Train Loss: 5.276257984743917, Val Loss: 5.734589305948409, Val MAE: 1.5487192869186401\n",
      "Epoch 501/2000, Train Loss: 5.275542750147176, Val Loss: 5.734582770763916, Val MAE: 1.5487453937530518\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 502/2000, Train Loss: 5.274974457736086, Val Loss: 5.734304305063475, Val MAE: 1.5490561723709106\n",
      "Epoch 503/2000, Train Loss: 5.274392371274987, Val Loss: 5.734186012172553, Val MAE: 1.5492265224456787\n",
      "Epoch 504/2000, Train Loss: 5.273820476075615, Val Loss: 5.7340596795811205, Val MAE: 1.5494415760040283\n",
      "Epoch 505/2000, Train Loss: 5.273272794875186, Val Loss: 5.734014673699662, Val MAE: 1.5495790243148804\n",
      "Epoch 506/2000, Train Loss: 5.272590722272633, Val Loss: 5.733915302367021, Val MAE: 1.5497682094573975\n",
      "Epoch 507/2000, Train Loss: 5.272034116398692, Val Loss: 5.7337198171776, Val MAE: 1.5499974489212036\n",
      "Epoch 508/2000, Train Loss: 5.271524252814361, Val Loss: 5.733363858512417, Val MAE: 1.5502514839172363\n",
      "Epoch 509/2000, Train Loss: 5.271049169316248, Val Loss: 5.733404030767056, Val MAE: 1.5503076314926147\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 510/2000, Train Loss: 5.270324832532374, Val Loss: 5.733127695340264, Val MAE: 1.5506771802902222\n",
      "Epoch 511/2000, Train Loss: 5.269696333977137, Val Loss: 5.733051502376521, Val MAE: 1.5507756471633911\n",
      "Epoch 512/2000, Train Loss: 5.269140255275031, Val Loss: 5.732946877176973, Val MAE: 1.5509974956512451\n",
      "Epoch 513/2000, Train Loss: 5.268544594390891, Val Loss: 5.732720655312232, Val MAE: 1.5512422323226929\n",
      "Epoch 514/2000, Train Loss: 5.268024345252986, Val Loss: 5.7324172485312195, Val MAE: 1.5514479875564575\n",
      "Epoch 515/2000, Train Loss: 5.267610221576892, Val Loss: 5.732155779268399, Val MAE: 1.551881194114685\n",
      "Epoch 516/2000, Train Loss: 5.266824800476568, Val Loss: 5.732185926094696, Val MAE: 1.5519098043441772\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 517/2000, Train Loss: 5.266292174713113, Val Loss: 5.731983128399659, Val MAE: 1.5522500276565552\n",
      "Epoch 518/2000, Train Loss: 5.265796834528237, Val Loss: 5.731879079742899, Val MAE: 1.5523639917373657\n",
      "Epoch 519/2000, Train Loss: 5.26514265035593, Val Loss: 5.731738950498972, Val MAE: 1.552518367767334\n",
      "Epoch 520/2000, Train Loss: 5.2645685013375765, Val Loss: 5.731565097876645, Val MAE: 1.5526821613311768\n",
      "Epoch 521/2000, Train Loss: 5.264105934395411, Val Loss: 5.731600569293404, Val MAE: 1.5528393983840942\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 522/2000, Train Loss: 5.263407486580695, Val Loss: 5.731425387141172, Val MAE: 1.5529701709747314\n",
      "Epoch 523/2000, Train Loss: 5.263046380753756, Val Loss: 5.731440772157197, Val MAE: 1.5530731678009033\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 524/2000, Train Loss: 5.262383433482246, Val Loss: 5.731078949269898, Val MAE: 1.5536037683486938\n",
      "Epoch 525/2000, Train Loss: 5.261829003408541, Val Loss: 5.730981905161423, Val MAE: 1.5536773204803467\n",
      "Epoch 526/2000, Train Loss: 5.261320019842117, Val Loss: 5.730859648379346, Val MAE: 1.5538350343704224\n",
      "Epoch 527/2000, Train Loss: 5.260801595168412, Val Loss: 5.730626204145064, Val MAE: 1.55400812625885\n",
      "Epoch 528/2000, Train Loss: 5.26022758264092, Val Loss: 5.730447297978474, Val MAE: 1.5543403625488281\n",
      "Epoch 529/2000, Train Loss: 5.259629512655324, Val Loss: 5.730454661886262, Val MAE: 1.5544443130493164\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 530/2000, Train Loss: 5.259116986366664, Val Loss: 5.730262473180025, Val MAE: 1.5546468496322632\n",
      "Epoch 531/2000, Train Loss: 5.2586547356940425, Val Loss: 5.7302173210211125, Val MAE: 1.5547329187393188\n",
      "Epoch 532/2000, Train Loss: 5.258043136167157, Val Loss: 5.72991625868946, Val MAE: 1.5550997257232666\n",
      "Epoch 533/2000, Train Loss: 5.257636506149754, Val Loss: 5.730087031738473, Val MAE: 1.5549718141555786\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 534/2000, Train Loss: 5.2570363893045835, Val Loss: 5.729845548715066, Val MAE: 1.555246353149414\n",
      "Epoch 535/2000, Train Loss: 5.256389274134089, Val Loss: 5.729719409884298, Val MAE: 1.5554308891296387\n",
      "Epoch 536/2000, Train Loss: 5.255946072787151, Val Loss: 5.72955450761938, Val MAE: 1.5556522607803345\n",
      "Epoch 537/2000, Train Loss: 5.255456594243006, Val Loss: 5.729557687991985, Val MAE: 1.5556528568267822\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 538/2000, Train Loss: 5.255081653594971, Val Loss: 5.729345806512629, Val MAE: 1.5560081005096436\n",
      "Epoch 539/2000, Train Loss: 5.254421056449456, Val Loss: 5.7292716339093825, Val MAE: 1.5561965703964233\n",
      "Epoch 540/2000, Train Loss: 5.253882039692266, Val Loss: 5.7290476258012495, Val MAE: 1.5563606023788452\n",
      "Epoch 541/2000, Train Loss: 5.253561430292848, Val Loss: 5.7287354571374545, Val MAE: 1.5567224025726318\n",
      "Epoch 542/2000, Train Loss: 5.25286699415176, Val Loss: 5.7288353557069005, Val MAE: 1.5567610263824463\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 543/2000, Train Loss: 5.252322217429884, Val Loss: 5.728756693525052, Val MAE: 1.5568076372146606\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 544/2000, Train Loss: 5.251801982050123, Val Loss: 5.7285620745168915, Val MAE: 1.5569508075714111\n",
      "Epoch 545/2000, Train Loss: 5.251364040844546, Val Loss: 5.728408823501809, Val MAE: 1.5572686195373535\n",
      "Epoch 546/2000, Train Loss: 5.250835683461564, Val Loss: 5.728426729320386, Val MAE: 1.5572763681411743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 547/2000, Train Loss: 5.250369820772637, Val Loss: 5.728200548012322, Val MAE: 1.55753493309021\n",
      "Epoch 548/2000, Train Loss: 5.249824716288132, Val Loss: 5.728247794171721, Val MAE: 1.5575567483901978\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 549/2000, Train Loss: 5.2492971463911475, Val Loss: 5.72806487635735, Val MAE: 1.5577820539474487\n",
      "Epoch 550/2000, Train Loss: 5.24874963750309, Val Loss: 5.727890019569922, Val MAE: 1.5579805374145508\n",
      "Epoch 551/2000, Train Loss: 5.24817628545043, Val Loss: 5.727819328096664, Val MAE: 1.5580840110778809\n",
      "Epoch 552/2000, Train Loss: 5.2477925070400895, Val Loss: 5.727714160622442, Val MAE: 1.5581740140914917\n",
      "Epoch 553/2000, Train Loss: 5.247318706069506, Val Loss: 5.727594731871141, Val MAE: 1.5584050416946411\n",
      "Epoch 554/2000, Train Loss: 5.246737094935526, Val Loss: 5.727370572381792, Val MAE: 1.5586333274841309\n",
      "Epoch 555/2000, Train Loss: 5.246194541286533, Val Loss: 5.727311342863497, Val MAE: 1.5588196516036987\n",
      "Epoch 556/2000, Train Loss: 5.245808674113, Val Loss: 5.727143024450413, Val MAE: 1.5589457750320435\n",
      "Epoch 557/2000, Train Loss: 5.245140192795605, Val Loss: 5.727066712003965, Val MAE: 1.5589991807937622\n",
      "Epoch 558/2000, Train Loss: 5.244681567058523, Val Loss: 5.726987526048578, Val MAE: 1.5592602491378784\n",
      "Epoch 559/2000, Train Loss: 5.244232580999016, Val Loss: 5.726833864818655, Val MAE: 1.5593410730361938\n",
      "Epoch 560/2000, Train Loss: 5.243688300115638, Val Loss: 5.726784971511328, Val MAE: 1.559536099433899\n",
      "Epoch 561/2000, Train Loss: 5.2433103932871274, Val Loss: 5.726704975880614, Val MAE: 1.559583306312561\n",
      "Epoch 562/2000, Train Loss: 5.2427017955525, Val Loss: 5.726567305920685, Val MAE: 1.5597403049468994\n",
      "Epoch 563/2000, Train Loss: 5.242202946034055, Val Loss: 5.7264523416848725, Val MAE: 1.559877872467041\n",
      "Epoch 564/2000, Train Loss: 5.241738270404555, Val Loss: 5.726293170215158, Val MAE: 1.5600385665893555\n",
      "Epoch 565/2000, Train Loss: 5.241202382618571, Val Loss: 5.726136763343753, Val MAE: 1.5601766109466553\n",
      "Epoch 566/2000, Train Loss: 5.240787656771977, Val Loss: 5.725962645872653, Val MAE: 1.5603535175323486\n",
      "Epoch 567/2000, Train Loss: 5.240278695017246, Val Loss: 5.725976410444359, Val MAE: 1.5603731870651245\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 568/2000, Train Loss: 5.23968975968465, Val Loss: 5.725641379115778, Val MAE: 1.5607154369354248\n",
      "Epoch 569/2000, Train Loss: 5.239232900862455, Val Loss: 5.725650621845817, Val MAE: 1.560778260231018\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 570/2000, Train Loss: 5.2387776797628165, Val Loss: 5.725498581698181, Val MAE: 1.560926914215088\n",
      "Epoch 571/2000, Train Loss: 5.238194996835815, Val Loss: 5.7253484502662575, Val MAE: 1.561130166053772\n",
      "Epoch 572/2000, Train Loss: 5.2377861693742656, Val Loss: 5.725245567819029, Val MAE: 1.56123685836792\n",
      "Epoch 573/2000, Train Loss: 5.23731521885635, Val Loss: 5.725170411648007, Val MAE: 1.5614149570465088\n",
      "Epoch 574/2000, Train Loss: 5.2368147163471646, Val Loss: 5.725052445944661, Val MAE: 1.5615533590316772\n",
      "Epoch 575/2000, Train Loss: 5.236226915893716, Val Loss: 5.724952838471905, Val MAE: 1.5617095232009888\n",
      "Epoch 576/2000, Train Loss: 5.235875448821549, Val Loss: 5.724984987066426, Val MAE: 1.5616284608840942\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 577/2000, Train Loss: 5.23537082390245, Val Loss: 5.724853176680544, Val MAE: 1.561903715133667\n",
      "Epoch 578/2000, Train Loss: 5.2349157487733695, Val Loss: 5.724666963203238, Val MAE: 1.5620981454849243\n",
      "Epoch 579/2000, Train Loss: 5.234451604678042, Val Loss: 5.724411773754552, Val MAE: 1.562341332435608\n",
      "Epoch 580/2000, Train Loss: 5.2338821547372, Val Loss: 5.724480739882962, Val MAE: 1.56234872341156\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 581/2000, Train Loss: 5.233491746243417, Val Loss: 5.724311675044739, Val MAE: 1.5625336170196533\n",
      "Epoch 582/2000, Train Loss: 5.233023305328524, Val Loss: 5.724280292652433, Val MAE: 1.5627354383468628\n",
      "Epoch 583/2000, Train Loss: 5.2324753651561915, Val Loss: 5.7240134040150075, Val MAE: 1.5629130601882935\n",
      "Epoch 584/2000, Train Loss: 5.232002683125441, Val Loss: 5.723803619361434, Val MAE: 1.5630638599395752\n",
      "Epoch 585/2000, Train Loss: 5.231561098360495, Val Loss: 5.7239549575595685, Val MAE: 1.5631003379821777\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 586/2000, Train Loss: 5.231179030657989, Val Loss: 5.723842059709246, Val MAE: 1.5631487369537354\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 587/2000, Train Loss: 5.230806596005659, Val Loss: 5.723627661528573, Val MAE: 1.5635501146316528\n",
      "Epoch 588/2000, Train Loss: 5.230147118471106, Val Loss: 5.72352883146808, Val MAE: 1.5635299682617188\n",
      "Epoch 589/2000, Train Loss: 5.229642317045079, Val Loss: 5.723563860315795, Val MAE: 1.563722014427185\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 590/2000, Train Loss: 5.229289658215581, Val Loss: 5.723337072662622, Val MAE: 1.5638432502746582\n",
      "Epoch 591/2000, Train Loss: 5.228810343584655, Val Loss: 5.723205136687748, Val MAE: 1.5640965700149536\n",
      "Epoch 592/2000, Train Loss: 5.228213756626379, Val Loss: 5.723142861955392, Val MAE: 1.5640772581100464\n",
      "Epoch 593/2000, Train Loss: 5.227873245315767, Val Loss: 5.723021118283636, Val MAE: 1.5643270015716553\n",
      "Epoch 594/2000, Train Loss: 5.227364783048798, Val Loss: 5.723092559098468, Val MAE: 1.564240574836731\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 595/2000, Train Loss: 5.226976423847432, Val Loss: 5.72267684923764, Val MAE: 1.5644408464431763\n",
      "Epoch 596/2000, Train Loss: 5.226425444970745, Val Loss: 5.722662149219338, Val MAE: 1.56459641456604\n",
      "Epoch 597/2000, Train Loss: 5.226112436191202, Val Loss: 5.722595776439807, Val MAE: 1.5647324323654175\n",
      "Epoch 598/2000, Train Loss: 5.225544895444598, Val Loss: 5.722554148610579, Val MAE: 1.564816951751709\n",
      "Epoch 599/2000, Train Loss: 5.225112207669091, Val Loss: 5.722608069031975, Val MAE: 1.5647692680358887\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 600/2000, Train Loss: 5.224668099794649, Val Loss: 5.722477022660982, Val MAE: 1.5649603605270386\n",
      "Epoch 601/2000, Train Loss: 5.224155614612641, Val Loss: 5.722417412912445, Val MAE: 1.5650031566619873\n",
      "Epoch 602/2000, Train Loss: 5.22371015038648, Val Loss: 5.722304678473633, Val MAE: 1.5650471448898315\n",
      "Epoch 603/2000, Train Loss: 5.223285811959145, Val Loss: 5.722218092155019, Val MAE: 1.5651882886886597\n",
      "Epoch 604/2000, Train Loss: 5.222876826614498, Val Loss: 5.72214758897411, Val MAE: 1.5653082132339478\n",
      "Epoch 605/2000, Train Loss: 5.222464164154031, Val Loss: 5.722029794611333, Val MAE: 1.5653380155563354\n",
      "Epoch 606/2000, Train Loss: 5.222059612119811, Val Loss: 5.721825408370488, Val MAE: 1.5657315254211426\n",
      "Epoch 607/2000, Train Loss: 5.221405094396388, Val Loss: 5.7218510708130825, Val MAE: 1.5656695365905762\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 608/2000, Train Loss: 5.221074697587123, Val Loss: 5.72173613328088, Val MAE: 1.5658202171325684\n",
      "Epoch 609/2000, Train Loss: 5.220542548996391, Val Loss: 5.721611385680849, Val MAE: 1.5657638311386108\n",
      "Epoch 610/2000, Train Loss: 5.220145288107682, Val Loss: 5.721547432995718, Val MAE: 1.565826177597046\n",
      "Epoch 611/2000, Train Loss: 5.21984419178409, Val Loss: 5.721484038078821, Val MAE: 1.5659782886505127\n",
      "Epoch 612/2000, Train Loss: 5.21926150231358, Val Loss: 5.721462728324651, Val MAE: 1.5660146474838257\n",
      "Epoch 613/2000, Train Loss: 5.21881170292961, Val Loss: 5.721437337748501, Val MAE: 1.5661098957061768\n",
      "Epoch 614/2000, Train Loss: 5.218343813934434, Val Loss: 5.721346126601601, Val MAE: 1.566167950630188\n",
      "Epoch 615/2000, Train Loss: 5.21808437492376, Val Loss: 5.721198385279478, Val MAE: 1.5663715600967407\n",
      "Epoch 616/2000, Train Loss: 5.217433170732019, Val Loss: 5.72102873268842, Val MAE: 1.5665583610534668\n",
      "Epoch 617/2000, Train Loss: 5.217031535592237, Val Loss: 5.721117182973693, Val MAE: 1.5666395425796509\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 618/2000, Train Loss: 5.2166435971887575, Val Loss: 5.721157037336892, Val MAE: 1.5666033029556274\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 619/2000, Train Loss: 5.2162237216015255, Val Loss: 5.720888918966328, Val MAE: 1.5668787956237793\n",
      "Epoch 620/2000, Train Loss: 5.21572371019099, Val Loss: 5.720874877880108, Val MAE: 1.5669450759887695\n",
      "Epoch 621/2000, Train Loss: 5.215242414555023, Val Loss: 5.720767593894165, Val MAE: 1.5670132637023926\n",
      "Epoch 622/2000, Train Loss: 5.214913516313907, Val Loss: 5.720720337800659, Val MAE: 1.5671073198318481\n",
      "Epoch 623/2000, Train Loss: 5.214476389418514, Val Loss: 5.720601734491663, Val MAE: 1.5672085285186768\n",
      "Epoch 624/2000, Train Loss: 5.214080975309046, Val Loss: 5.720556648317098, Val MAE: 1.5672072172164917\n",
      "Epoch 625/2000, Train Loss: 5.213793591157724, Val Loss: 5.720416741633634, Val MAE: 1.5674742460250854\n",
      "Epoch 626/2000, Train Loss: 5.2131123106551795, Val Loss: 5.720331321980246, Val MAE: 1.5675625801086426\n",
      "Epoch 627/2000, Train Loss: 5.21275123318344, Val Loss: 5.7201325077711624, Val MAE: 1.567644476890564\n",
      "Epoch 628/2000, Train Loss: 5.212304363613142, Val Loss: 5.72010601870146, Val MAE: 1.5676685571670532\n",
      "Epoch 629/2000, Train Loss: 5.21189610677903, Val Loss: 5.720094975437228, Val MAE: 1.5677400827407837\n",
      "Epoch 630/2000, Train Loss: 5.211393067108586, Val Loss: 5.719940313961163, Val MAE: 1.5678837299346924\n",
      "Epoch 631/2000, Train Loss: 5.211045727437856, Val Loss: 5.7199276084199955, Val MAE: 1.5679271221160889\n",
      "Epoch 632/2000, Train Loss: 5.210574657458977, Val Loss: 5.719887248146424, Val MAE: 1.56797194480896\n",
      "Epoch 633/2000, Train Loss: 5.210226281439226, Val Loss: 5.719878781188154, Val MAE: 1.5679845809936523\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 634/2000, Train Loss: 5.209845245597566, Val Loss: 5.719768561445609, Val MAE: 1.568084478378296\n",
      "Epoch 635/2000, Train Loss: 5.20939994420408, Val Loss: 5.719768612665503, Val MAE: 1.568123459815979\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 636/2000, Train Loss: 5.2089559387607025, Val Loss: 5.719602283443515, Val MAE: 1.568389654159546\n",
      "Epoch 637/2000, Train Loss: 5.208576944157911, Val Loss: 5.719621960176241, Val MAE: 1.568275809288025\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 638/2000, Train Loss: 5.208134393218871, Val Loss: 5.719507898030296, Val MAE: 1.5684361457824707\n",
      "Epoch 639/2000, Train Loss: 5.207745236296791, Val Loss: 5.719557105492379, Val MAE: 1.5683302879333496\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 640/2000, Train Loss: 5.207277753670227, Val Loss: 5.719403281306637, Val MAE: 1.5684715509414673\n",
      "Epoch 641/2000, Train Loss: 5.2067880362041565, Val Loss: 5.719418636155785, Val MAE: 1.5684857368469238\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 642/2000, Train Loss: 5.206490785449764, Val Loss: 5.71921734726028, Val MAE: 1.5685635805130005\n",
      "Epoch 643/2000, Train Loss: 5.206169090163615, Val Loss: 5.71932610914977, Val MAE: 1.5685672760009766\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 644/2000, Train Loss: 5.205620788839986, Val Loss: 5.719108902077427, Val MAE: 1.5687859058380127\n",
      "Epoch 645/2000, Train Loss: 5.20518476974788, Val Loss: 5.718933466559886, Val MAE: 1.5689630508422852\n",
      "Epoch 646/2000, Train Loss: 5.204804236749626, Val Loss: 5.719110130899178, Val MAE: 1.5689454078674316\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 647/2000, Train Loss: 5.20441666226585, Val Loss: 5.718948915281792, Val MAE: 1.569049596786499\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 648/2000, Train Loss: 5.203986263543598, Val Loss: 5.718965761614138, Val MAE: 1.5689846277236938\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 649/2000, Train Loss: 5.203624336758103, Val Loss: 5.718774589558989, Val MAE: 1.5691204071044922\n",
      "Epoch 650/2000, Train Loss: 5.203370224162779, Val Loss: 5.718604741293356, Val MAE: 1.5693587064743042\n",
      "Epoch 651/2000, Train Loss: 5.2029299069927415, Val Loss: 5.718592823554252, Val MAE: 1.5693275928497314\n",
      "Epoch 652/2000, Train Loss: 5.202450467699602, Val Loss: 5.718533298655752, Val MAE: 1.5693233013153076\n",
      "Epoch 653/2000, Train Loss: 5.201918528165556, Val Loss: 5.718354730795648, Val MAE: 1.5695871114730835\n",
      "Epoch 654/2000, Train Loss: 5.20152601182335, Val Loss: 5.718179686262703, Val MAE: 1.5697178840637207\n",
      "Epoch 655/2000, Train Loss: 5.201180571992996, Val Loss: 5.718110858391549, Val MAE: 1.5697091817855835\n",
      "Epoch 656/2000, Train Loss: 5.200726087289659, Val Loss: 5.717965244335502, Val MAE: 1.5698051452636719\n",
      "Epoch 657/2000, Train Loss: 5.2003111322525095, Val Loss: 5.717958139990448, Val MAE: 1.5699082612991333\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 658/2000, Train Loss: 5.200047604052807, Val Loss: 5.718052830717979, Val MAE: 1.5699330568313599\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 659/2000, Train Loss: 5.199567120315154, Val Loss: 5.717865575435329, Val MAE: 1.5700228214263916\n",
      "Epoch 660/2000, Train Loss: 5.19924682869532, Val Loss: 5.717625654527536, Val MAE: 1.5701713562011719\n",
      "Epoch 661/2000, Train Loss: 5.198730193419325, Val Loss: 5.717600868747139, Val MAE: 1.5701781511306763\n",
      "Epoch 662/2000, Train Loss: 5.198460478920235, Val Loss: 5.7177185195666205, Val MAE: 1.5700900554656982\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 663/2000, Train Loss: 5.197916786239484, Val Loss: 5.717474401817409, Val MAE: 1.5702452659606934\n",
      "Epoch 664/2000, Train Loss: 5.19752966057323, Val Loss: 5.717478043533611, Val MAE: 1.5702550411224365\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 665/2000, Train Loss: 5.197153182704208, Val Loss: 5.717431765539566, Val MAE: 1.570410132408142\n",
      "Epoch 666/2000, Train Loss: 5.196846906942519, Val Loss: 5.7173286581622715, Val MAE: 1.5704435110092163\n",
      "Epoch 667/2000, Train Loss: 5.196404645679536, Val Loss: 5.717275371427565, Val MAE: 1.5705002546310425\n",
      "Epoch 668/2000, Train Loss: 5.196064036300197, Val Loss: 5.717192211282363, Val MAE: 1.5705783367156982\n",
      "Epoch 669/2000, Train Loss: 5.195576554029278, Val Loss: 5.7171458570964475, Val MAE: 1.5705071687698364\n",
      "Epoch 670/2000, Train Loss: 5.195186710458335, Val Loss: 5.717014921094299, Val MAE: 1.570620059967041\n",
      "Epoch 671/2000, Train Loss: 5.194823739983346, Val Loss: 5.716901569282608, Val MAE: 1.5707839727401733\n",
      "Epoch 672/2000, Train Loss: 5.194403664483561, Val Loss: 5.716868466740355, Val MAE: 1.5706813335418701\n",
      "Epoch 673/2000, Train Loss: 5.194012797944232, Val Loss: 5.716863989556601, Val MAE: 1.570794939994812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 674/2000, Train Loss: 5.193533843038453, Val Loss: 5.716861309261497, Val MAE: 1.5708931684494019\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 675/2000, Train Loss: 5.193197392981125, Val Loss: 5.71672095991785, Val MAE: 1.570863127708435\n",
      "Epoch 676/2000, Train Loss: 5.192768438192927, Val Loss: 5.7166933715343475, Val MAE: 1.5709267854690552\n",
      "Epoch 677/2000, Train Loss: 5.192386812764435, Val Loss: 5.716558013851854, Val MAE: 1.5710529088974\n",
      "Epoch 678/2000, Train Loss: 5.192078408517106, Val Loss: 5.716546035778997, Val MAE: 1.5710258483886719\n",
      "Epoch 679/2000, Train Loss: 5.191612998001346, Val Loss: 5.716379564745346, Val MAE: 1.5712248086929321\n",
      "Epoch 680/2000, Train Loss: 5.191187282081727, Val Loss: 5.716436270635792, Val MAE: 1.5712178945541382\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 681/2000, Train Loss: 5.1908881920982965, Val Loss: 5.716296860293146, Val MAE: 1.5712625980377197\n",
      "Epoch 682/2000, Train Loss: 5.190441531249119, Val Loss: 5.716210943114138, Val MAE: 1.5713666677474976\n",
      "Epoch 683/2000, Train Loss: 5.190053843428433, Val Loss: 5.716272737728346, Val MAE: 1.57131028175354\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 684/2000, Train Loss: 5.18975512667662, Val Loss: 5.716249830679062, Val MAE: 1.5713138580322266\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 685/2000, Train Loss: 5.18928753331377, Val Loss: 5.716076239533381, Val MAE: 1.5713331699371338\n",
      "Epoch 686/2000, Train Loss: 5.188975382488769, Val Loss: 5.715930811673494, Val MAE: 1.5716007947921753\n",
      "Epoch 687/2000, Train Loss: 5.188584279544248, Val Loss: 5.7159464877862085, Val MAE: 1.5715882778167725\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 688/2000, Train Loss: 5.18819731310335, Val Loss: 5.716054811845861, Val MAE: 1.571431279182434\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 689/2000, Train Loss: 5.187785946592992, Val Loss: 5.715923631227709, Val MAE: 1.571593165397644\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 690/2000, Train Loss: 5.187346559943321, Val Loss: 5.715878438876674, Val MAE: 1.5715382099151611\n",
      "Epoch 691/2000, Train Loss: 5.187039120779501, Val Loss: 5.715901740282683, Val MAE: 1.5716756582260132\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 692/2000, Train Loss: 5.186575190942108, Val Loss: 5.715728212204913, Val MAE: 1.5716043710708618\n",
      "Epoch 693/2000, Train Loss: 5.186279583279303, Val Loss: 5.715776186379453, Val MAE: 1.5714973211288452\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 694/2000, Train Loss: 5.185899182577355, Val Loss: 5.715787964220805, Val MAE: 1.5716136693954468\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 695/2000, Train Loss: 5.185600566411002, Val Loss: 5.715517884547557, Val MAE: 1.5718199014663696\n",
      "Epoch 696/2000, Train Loss: 5.18518349756916, Val Loss: 5.715691644664204, Val MAE: 1.5716724395751953\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 697/2000, Train Loss: 5.184803515735602, Val Loss: 5.715575294848247, Val MAE: 1.5717142820358276\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 698/2000, Train Loss: 5.184439139916476, Val Loss: 5.7153804582922465, Val MAE: 1.5718859434127808\n",
      "Epoch 699/2000, Train Loss: 5.184016038799353, Val Loss: 5.715368752997221, Val MAE: 1.5718485116958618\n",
      "Epoch 700/2000, Train Loss: 5.183671045102006, Val Loss: 5.715363029401966, Val MAE: 1.5717859268188477\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 701/2000, Train Loss: 5.183256996592362, Val Loss: 5.7151961929025275, Val MAE: 1.5718740224838257\n",
      "Epoch 702/2000, Train Loss: 5.182996032776588, Val Loss: 5.71520149653111, Val MAE: 1.5719152688980103\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 703/2000, Train Loss: 5.18255949339172, Val Loss: 5.715111672969404, Val MAE: 1.572026014328003\n",
      "Epoch 704/2000, Train Loss: 5.1823262748879335, Val Loss: 5.715039455471418, Val MAE: 1.5720793008804321\n",
      "Epoch 705/2000, Train Loss: 5.181815116527968, Val Loss: 5.71493949192015, Val MAE: 1.5720630884170532\n",
      "Epoch 706/2000, Train Loss: 5.181525693318946, Val Loss: 5.7149797175820085, Val MAE: 1.5722190141677856\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 707/2000, Train Loss: 5.18111309126009, Val Loss: 5.7149867115582165, Val MAE: 1.5721944570541382\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 708/2000, Train Loss: 5.1807325794024, Val Loss: 5.715015394516312, Val MAE: 1.572160005569458\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 709/2000, Train Loss: 5.180433117277936, Val Loss: 5.7148416564552065, Val MAE: 1.5722919702529907\n",
      "Epoch 710/2000, Train Loss: 5.180016403043883, Val Loss: 5.7147745255847955, Val MAE: 1.572426199913025\n",
      "Epoch 711/2000, Train Loss: 5.179696059914897, Val Loss: 5.714823702971141, Val MAE: 1.5723402500152588\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 712/2000, Train Loss: 5.179357086076944, Val Loss: 5.714941009560127, Val MAE: 1.572354793548584\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 713/2000, Train Loss: 5.17903530236955, Val Loss: 5.714655155162199, Val MAE: 1.5725005865097046\n",
      "Epoch 714/2000, Train Loss: 5.178637141832409, Val Loss: 5.7147000199611035, Val MAE: 1.5724776983261108\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 715/2000, Train Loss: 5.178213228229735, Val Loss: 5.714669192056044, Val MAE: 1.5726572275161743\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 716/2000, Train Loss: 5.177933592262778, Val Loss: 5.714690130055862, Val MAE: 1.5724939107894897\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 717/2000, Train Loss: 5.177559423413099, Val Loss: 5.714647944822953, Val MAE: 1.5726553201675415\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 718/2000, Train Loss: 5.1773469335675495, Val Loss: 5.714759078503384, Val MAE: 1.5726734399795532\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 719/2000, Train Loss: 5.176954580141909, Val Loss: 5.714499946125422, Val MAE: 1.5729268789291382\n",
      "Epoch 720/2000, Train Loss: 5.176552421185939, Val Loss: 5.714559251926725, Val MAE: 1.572892427444458\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 721/2000, Train Loss: 5.17617431515795, Val Loss: 5.714522646928052, Val MAE: 1.572812557220459\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 722/2000, Train Loss: 5.1758578493090095, Val Loss: 5.714371719309314, Val MAE: 1.5728602409362793\n",
      "Epoch 723/2000, Train Loss: 5.17551778538977, Val Loss: 5.714281598088938, Val MAE: 1.5729591846466064\n",
      "Epoch 724/2000, Train Loss: 5.175222890465969, Val Loss: 5.714254062201269, Val MAE: 1.5729928016662598\n",
      "Epoch 725/2000, Train Loss: 5.174860764690724, Val Loss: 5.714187712388665, Val MAE: 1.5730960369110107\n",
      "Epoch 726/2000, Train Loss: 5.174521385155288, Val Loss: 5.714203845379185, Val MAE: 1.5729823112487793\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 727/2000, Train Loss: 5.174284874260383, Val Loss: 5.714184493737847, Val MAE: 1.573045253753662\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 728/2000, Train Loss: 5.17377645041555, Val Loss: 5.7142553347695495, Val MAE: 1.5730133056640625\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 729/2000, Train Loss: 5.173521818030812, Val Loss: 5.714095886603043, Val MAE: 1.5731309652328491\n",
      "Epoch 730/2000, Train Loss: 5.173132472102027, Val Loss: 5.714032360628839, Val MAE: 1.573272943496704\n",
      "Epoch 731/2000, Train Loss: 5.172855240622513, Val Loss: 5.7140522617448, Val MAE: 1.5730504989624023\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 732/2000, Train Loss: 5.172388616537729, Val Loss: 5.713920254590679, Val MAE: 1.5731929540634155\n",
      "Epoch 733/2000, Train Loss: 5.1721133390503145, Val Loss: 5.713853101573588, Val MAE: 1.5732923746109009\n",
      "Epoch 734/2000, Train Loss: 5.171744616076948, Val Loss: 5.713779459123582, Val MAE: 1.5732566118240356\n",
      "Epoch 735/2000, Train Loss: 5.171463785835926, Val Loss: 5.713736602108049, Val MAE: 1.5733187198638916\n",
      "Epoch 736/2000, Train Loss: 5.171118430903393, Val Loss: 5.713762613155791, Val MAE: 1.5732629299163818\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 737/2000, Train Loss: 5.170718336340242, Val Loss: 5.713659237010764, Val MAE: 1.5734857320785522\n",
      "Epoch 738/2000, Train Loss: 5.170423545777001, Val Loss: 5.713704890523117, Val MAE: 1.573417067527771\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 739/2000, Train Loss: 5.170047396425627, Val Loss: 5.713786762241924, Val MAE: 1.5735173225402832\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 740/2000, Train Loss: 5.169690079625268, Val Loss: 5.713776485817877, Val MAE: 1.5736136436462402\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 741/2000, Train Loss: 5.169310808685109, Val Loss: 5.713535062216837, Val MAE: 1.5735710859298706\n",
      "Epoch 742/2000, Train Loss: 5.169211730581199, Val Loss: 5.713507929070645, Val MAE: 1.5734437704086304\n",
      "Epoch 743/2000, Train Loss: 5.168725745095744, Val Loss: 5.713565300364013, Val MAE: 1.5735085010528564\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 744/2000, Train Loss: 5.168433972431857, Val Loss: 5.71343528039594, Val MAE: 1.5736173391342163\n",
      "Epoch 745/2000, Train Loss: 5.168029003962088, Val Loss: 5.713581087939236, Val MAE: 1.5735143423080444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 746/2000, Train Loss: 5.167667442606336, Val Loss: 5.713360503999465, Val MAE: 1.5737351179122925\n",
      "Epoch 747/2000, Train Loss: 5.167348705824624, Val Loss: 5.71335603346883, Val MAE: 1.573677659034729\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 748/2000, Train Loss: 5.167001624244942, Val Loss: 5.713262688038181, Val MAE: 1.5734912157058716\n",
      "Epoch 749/2000, Train Loss: 5.166884085395486, Val Loss: 5.713011973130229, Val MAE: 1.573810338973999\n",
      "Epoch 750/2000, Train Loss: 5.166372852707983, Val Loss: 5.713119203891229, Val MAE: 1.5737560987472534\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 751/2000, Train Loss: 5.166098094674418, Val Loss: 5.713179855536248, Val MAE: 1.573704481124878\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 752/2000, Train Loss: 5.165726286185451, Val Loss: 5.713076539269281, Val MAE: 1.5738168954849243\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 753/2000, Train Loss: 5.165413005346987, Val Loss: 5.713290616824358, Val MAE: 1.5736628770828247\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 754/2000, Train Loss: 5.165148573304297, Val Loss: 5.7129665090950255, Val MAE: 1.5737954378128052\n",
      "Epoch 755/2000, Train Loss: 5.164743371449416, Val Loss: 5.713107384399536, Val MAE: 1.5736364126205444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 756/2000, Train Loss: 5.164427159119458, Val Loss: 5.713000138874083, Val MAE: 1.573826551437378\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 757/2000, Train Loss: 5.164074265898154, Val Loss: 5.713054994104105, Val MAE: 1.573823094367981\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 758/2000, Train Loss: 5.163782133173557, Val Loss: 5.71296272110137, Val MAE: 1.5736972093582153\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 759/2000, Train Loss: 5.163409368241865, Val Loss: 5.712918448065399, Val MAE: 1.5737754106521606\n",
      "Epoch 760/2000, Train Loss: 5.163063763733232, Val Loss: 5.712892042934348, Val MAE: 1.5738139152526855\n",
      "Epoch 761/2000, Train Loss: 5.162745005643175, Val Loss: 5.712982716910336, Val MAE: 1.5737965106964111\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 762/2000, Train Loss: 5.162384764733741, Val Loss: 5.712896370741935, Val MAE: 1.5738884210586548\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 763/2000, Train Loss: 5.1621212539833925, Val Loss: 5.712880244039979, Val MAE: 1.573884129524231\n",
      "Epoch 764/2000, Train Loss: 5.161787399853056, Val Loss: 5.712760474977143, Val MAE: 1.5739712715148926\n",
      "Epoch 765/2000, Train Loss: 5.1614104260197, Val Loss: 5.712731067831728, Val MAE: 1.5739527940750122\n",
      "Epoch 766/2000, Train Loss: 5.161133907446972, Val Loss: 5.7127110760328605, Val MAE: 1.5740036964416504\n",
      "Epoch 767/2000, Train Loss: 5.160871225205381, Val Loss: 5.712780783847202, Val MAE: 1.5740002393722534\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 768/2000, Train Loss: 5.160556355636779, Val Loss: 5.71267625598368, Val MAE: 1.574015498161316\n",
      "Epoch 769/2000, Train Loss: 5.160171662393713, Val Loss: 5.712650736719826, Val MAE: 1.574020504951477\n",
      "Epoch 770/2000, Train Loss: 5.159987630515263, Val Loss: 5.712564621223223, Val MAE: 1.5740565061569214\n",
      "Epoch 771/2000, Train Loss: 5.1595454315833855, Val Loss: 5.712548907652542, Val MAE: 1.5740547180175781\n",
      "Epoch 772/2000, Train Loss: 5.159231293629292, Val Loss: 5.71238582269132, Val MAE: 1.5740082263946533\n",
      "Epoch 773/2000, Train Loss: 5.158951544660988, Val Loss: 5.712448169423171, Val MAE: 1.5739941596984863\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 774/2000, Train Loss: 5.158553566419265, Val Loss: 5.7123767914939725, Val MAE: 1.5739052295684814\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 775/2000, Train Loss: 5.158343314453383, Val Loss: 5.712345983912821, Val MAE: 1.5741026401519775\n",
      "Epoch 776/2000, Train Loss: 5.157992508694959, Val Loss: 5.712380129080665, Val MAE: 1.574097990989685\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 777/2000, Train Loss: 5.157653968322453, Val Loss: 5.712268318878401, Val MAE: 1.5742374658584595\n",
      "Epoch 778/2000, Train Loss: 5.15734425805808, Val Loss: 5.712047298293595, Val MAE: 1.5741320848464966\n",
      "Epoch 779/2000, Train Loss: 5.1570238104007515, Val Loss: 5.712128118546366, Val MAE: 1.5741758346557617\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 780/2000, Train Loss: 5.156757154823776, Val Loss: 5.7119778341838705, Val MAE: 1.5742236375808716\n",
      "Epoch 781/2000, Train Loss: 5.156400692286749, Val Loss: 5.7120953782800505, Val MAE: 1.5741926431655884\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 782/2000, Train Loss: 5.156190327571195, Val Loss: 5.711868583700343, Val MAE: 1.5743863582611084\n",
      "Epoch 783/2000, Train Loss: 5.155788936890153, Val Loss: 5.711885201548218, Val MAE: 1.5741798877716064\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 784/2000, Train Loss: 5.155444729252653, Val Loss: 5.7117676147991725, Val MAE: 1.5742748975753784\n",
      "Epoch 785/2000, Train Loss: 5.155110039063359, Val Loss: 5.7118877920718, Val MAE: 1.5742541551589966\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 786/2000, Train Loss: 5.15477776695187, Val Loss: 5.711899729132288, Val MAE: 1.5742284059524536\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 787/2000, Train Loss: 5.154506022623104, Val Loss: 5.71169229776852, Val MAE: 1.574280023574829\n",
      "Epoch 788/2000, Train Loss: 5.154228656666116, Val Loss: 5.711601409525682, Val MAE: 1.574156403541565\n",
      "Epoch 789/2000, Train Loss: 5.153899028047888, Val Loss: 5.711687469610016, Val MAE: 1.5741808414459229\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 790/2000, Train Loss: 5.153595562051669, Val Loss: 5.7114443294134345, Val MAE: 1.574305772781372\n",
      "Epoch 791/2000, Train Loss: 5.15338008593372, Val Loss: 5.711589584565673, Val MAE: 1.5741907358169556\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 792/2000, Train Loss: 5.152962047897704, Val Loss: 5.711460252238341, Val MAE: 1.5742825269699097\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 793/2000, Train Loss: 5.15271658632304, Val Loss: 5.711437586159517, Val MAE: 1.574240803718567\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 794/2000, Train Loss: 5.152417093326641, Val Loss: 5.711441608926207, Val MAE: 1.574401617050171\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 795/2000, Train Loss: 5.1520686679789085, Val Loss: 5.711362386150827, Val MAE: 1.5743083953857422\n",
      "Epoch 796/2000, Train Loss: 5.1519580732173775, Val Loss: 5.71142937812601, Val MAE: 1.574396014213562\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 797/2000, Train Loss: 5.151450468066039, Val Loss: 5.711216335814298, Val MAE: 1.574427843093872\n",
      "Epoch 798/2000, Train Loss: 5.151127745449165, Val Loss: 5.711266206583116, Val MAE: 1.5742727518081665\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 799/2000, Train Loss: 5.150844657530171, Val Loss: 5.711122117549272, Val MAE: 1.5744214057922363\n",
      "Epoch 800/2000, Train Loss: 5.150633135033191, Val Loss: 5.711088083935076, Val MAE: 1.5744214057922363\n",
      "Epoch 801/2000, Train Loss: 5.15032844288428, Val Loss: 5.711165188740518, Val MAE: 1.5742928981781006\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 802/2000, Train Loss: 5.149994299161778, Val Loss: 5.711203936225413, Val MAE: 1.5744860172271729\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 803/2000, Train Loss: 5.149632190164758, Val Loss: 5.711051116022495, Val MAE: 1.5744404792785645\n",
      "Epoch 804/2000, Train Loss: 5.149404244637674, Val Loss: 5.710972128749258, Val MAE: 1.57453453540802\n",
      "Epoch 805/2000, Train Loss: 5.149149710153879, Val Loss: 5.71104567441007, Val MAE: 1.5743128061294556\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 806/2000, Train Loss: 5.148824028025874, Val Loss: 5.710853528447837, Val MAE: 1.5744457244873047\n",
      "Epoch 807/2000, Train Loss: 5.148595051057401, Val Loss: 5.710782501766806, Val MAE: 1.5745278596878052\n",
      "Epoch 808/2000, Train Loss: 5.148211440513203, Val Loss: 5.710813658591075, Val MAE: 1.5745737552642822\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 809/2000, Train Loss: 5.147995153168068, Val Loss: 5.710767674701294, Val MAE: 1.574666142463684\n",
      "Epoch 810/2000, Train Loss: 5.147569665096747, Val Loss: 5.710796192333968, Val MAE: 1.5745394229888916\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 811/2000, Train Loss: 5.1473723837055845, Val Loss: 5.7107637931082955, Val MAE: 1.5745091438293457\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 812/2000, Train Loss: 5.147051787393182, Val Loss: 5.7106485533604925, Val MAE: 1.5746694803237915\n",
      "Epoch 813/2000, Train Loss: 5.146729581713425, Val Loss: 5.710634002900634, Val MAE: 1.574449062347412\n",
      "Epoch 814/2000, Train Loss: 5.14653416706759, Val Loss: 5.710670053867754, Val MAE: 1.5745004415512085\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 815/2000, Train Loss: 5.146161369334468, Val Loss: 5.7105169604313115, Val MAE: 1.5743952989578247\n",
      "Epoch 816/2000, Train Loss: 5.145763345707646, Val Loss: 5.710657320529314, Val MAE: 1.5745961666107178\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 817/2000, Train Loss: 5.145596494983402, Val Loss: 5.710642044423917, Val MAE: 1.5746973752975464\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 818/2000, Train Loss: 5.145246044000549, Val Loss: 5.710464510075543, Val MAE: 1.574545979499817\n",
      "Epoch 819/2000, Train Loss: 5.1449267755168835, Val Loss: 5.710511150163248, Val MAE: 1.5746238231658936\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 820/2000, Train Loss: 5.144673435978282, Val Loss: 5.710461912807704, Val MAE: 1.5746040344238281\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 821/2000, Train Loss: 5.144342858719205, Val Loss: 5.7104993106210635, Val MAE: 1.5744774341583252\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 822/2000, Train Loss: 5.144118701967028, Val Loss: 5.710406334301747, Val MAE: 1.5746268033981323\n",
      "Epoch 823/2000, Train Loss: 5.143786611899283, Val Loss: 5.710387190820974, Val MAE: 1.5745964050292969\n",
      "Epoch 824/2000, Train Loss: 5.143618381837151, Val Loss: 5.7103182983325524, Val MAE: 1.5745991468429565\n",
      "Epoch 825/2000, Train Loss: 5.1432478137623665, Val Loss: 5.710374369533784, Val MAE: 1.5744863748550415\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 826/2000, Train Loss: 5.142948238552665, Val Loss: 5.710268229701833, Val MAE: 1.5744025707244873\n",
      "Epoch 827/2000, Train Loss: 5.142637508507768, Val Loss: 5.710189065619711, Val MAE: 1.5746607780456543\n",
      "Epoch 828/2000, Train Loss: 5.142357875225327, Val Loss: 5.710190842640145, Val MAE: 1.5744975805282593\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 829/2000, Train Loss: 5.142225125908096, Val Loss: 5.710257844367159, Val MAE: 1.5744051933288574\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 830/2000, Train Loss: 5.141815071790509, Val Loss: 5.7101822624877325, Val MAE: 1.5744701623916626\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 831/2000, Train Loss: 5.141572142721145, Val Loss: 5.710006172959593, Val MAE: 1.5745338201522827\n",
      "Epoch 832/2000, Train Loss: 5.14120830288242, Val Loss: 5.710126181443532, Val MAE: 1.5744140148162842\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 833/2000, Train Loss: 5.140873589539176, Val Loss: 5.709980047624046, Val MAE: 1.5743892192840576\n",
      "Epoch 834/2000, Train Loss: 5.1407138423798875, Val Loss: 5.709878336581251, Val MAE: 1.5745482444763184\n",
      "Epoch 835/2000, Train Loss: 5.140475234206842, Val Loss: 5.70983215464729, Val MAE: 1.57464599609375\n",
      "Epoch 836/2000, Train Loss: 5.140154100320106, Val Loss: 5.709889494042149, Val MAE: 1.5744632482528687\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 837/2000, Train Loss: 5.139845243731827, Val Loss: 5.709656154799534, Val MAE: 1.5745348930358887\n",
      "Epoch 838/2000, Train Loss: 5.139487347076008, Val Loss: 5.709607806956732, Val MAE: 1.57460618019104\n",
      "Epoch 839/2000, Train Loss: 5.1391998011825955, Val Loss: 5.709480093433223, Val MAE: 1.5747249126434326\n",
      "Epoch 840/2000, Train Loss: 5.138946471459256, Val Loss: 5.709614770036954, Val MAE: 1.5745562314987183\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 841/2000, Train Loss: 5.138589767399679, Val Loss: 5.709713689686691, Val MAE: 1.5745670795440674\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 842/2000, Train Loss: 5.138438304153485, Val Loss: 5.709691807308693, Val MAE: 1.5745664834976196\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 843/2000, Train Loss: 5.138005941539982, Val Loss: 5.709614045211664, Val MAE: 1.5745787620544434\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 844/2000, Train Loss: 5.137726379229433, Val Loss: 5.709549553715125, Val MAE: 1.5744454860687256\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 845/2000, Train Loss: 5.137474237572215, Val Loss: 5.709516050345307, Val MAE: 1.5745404958724976\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 846/2000, Train Loss: 5.137187255133549, Val Loss: 5.709458596571506, Val MAE: 1.5746045112609863\n",
      "Epoch 847/2000, Train Loss: 5.136894066513298, Val Loss: 5.709411285885977, Val MAE: 1.574550986289978\n",
      "Epoch 848/2000, Train Loss: 5.136672330859008, Val Loss: 5.709470356731969, Val MAE: 1.5744235515594482\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 849/2000, Train Loss: 5.136410423603635, Val Loss: 5.709503368409037, Val MAE: 1.5744860172271729\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 850/2000, Train Loss: 5.136092517344068, Val Loss: 5.70965159303916, Val MAE: 1.5742971897125244\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 851/2000, Train Loss: 5.135855819539735, Val Loss: 5.709584262484075, Val MAE: 1.5743401050567627\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 852/2000, Train Loss: 5.135635810197029, Val Loss: 5.709526755120776, Val MAE: 1.574133276939392\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 853/2000, Train Loss: 5.1353921121816075, Val Loss: 5.70970452916367, Val MAE: 1.5742886066436768\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 854/2000, Train Loss: 5.134957863863382, Val Loss: 5.709426342347347, Val MAE: 1.5741708278656006\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 855/2000, Train Loss: 5.134730895416238, Val Loss: 5.709395466321105, Val MAE: 1.5742615461349487\n",
      "Epoch 856/2000, Train Loss: 5.134452451206613, Val Loss: 5.709497498718606, Val MAE: 1.5742329359054565\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 857/2000, Train Loss: 5.134191363064191, Val Loss: 5.709592048636999, Val MAE: 1.5741933584213257\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 858/2000, Train Loss: 5.1339146475151, Val Loss: 5.709200357989798, Val MAE: 1.5742592811584473\n",
      "Epoch 859/2000, Train Loss: 5.133569593080578, Val Loss: 5.7093337421570345, Val MAE: 1.5742658376693726\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 860/2000, Train Loss: 5.133332892088383, Val Loss: 5.709181007168708, Val MAE: 1.5742210149765015\n",
      "Epoch 861/2000, Train Loss: 5.133171529521579, Val Loss: 5.709324640691827, Val MAE: 1.574138879776001\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 862/2000, Train Loss: 5.132704704601441, Val Loss: 5.709210061334324, Val MAE: 1.574140191078186\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 863/2000, Train Loss: 5.132628368375001, Val Loss: 5.709061686234372, Val MAE: 1.574463963508606\n",
      "Epoch 864/2000, Train Loss: 5.132221108670809, Val Loss: 5.709218828320868, Val MAE: 1.5743385553359985\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 865/2000, Train Loss: 5.131916850658473, Val Loss: 5.709229252662863, Val MAE: 1.5741996765136719\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 866/2000, Train Loss: 5.131650685760691, Val Loss: 5.709197264654557, Val MAE: 1.5742706060409546\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 867/2000, Train Loss: 5.131465106678211, Val Loss: 5.708893831078795, Val MAE: 1.574383020401001\n",
      "Epoch 868/2000, Train Loss: 5.131091708452411, Val Loss: 5.7089258966460505, Val MAE: 1.5743846893310547\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 869/2000, Train Loss: 5.130792353540806, Val Loss: 5.708841910238295, Val MAE: 1.5744074583053589\n",
      "Epoch 870/2000, Train Loss: 5.130568612385266, Val Loss: 5.708982827557701, Val MAE: 1.5742682218551636\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 871/2000, Train Loss: 5.130249039544596, Val Loss: 5.708829700946808, Val MAE: 1.5742616653442383\n",
      "Epoch 872/2000, Train Loss: 5.130080900588227, Val Loss: 5.708690261713226, Val MAE: 1.5745129585266113\n",
      "Epoch 873/2000, Train Loss: 5.1296841049597015, Val Loss: 5.708829849411588, Val MAE: 1.5743087530136108\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 874/2000, Train Loss: 5.129512933339139, Val Loss: 5.708806328634968, Val MAE: 1.5743865966796875\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 875/2000, Train Loss: 5.129232301248957, Val Loss: 5.7088304238581875, Val MAE: 1.5743111371994019\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 876/2000, Train Loss: 5.128984768066836, Val Loss: 5.708874189069876, Val MAE: 1.5742629766464233\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 877/2000, Train Loss: 5.128756807691023, Val Loss: 5.708657682579957, Val MAE: 1.574399471282959\n",
      "Epoch 878/2000, Train Loss: 5.128412312474744, Val Loss: 5.708791252396522, Val MAE: 1.5741844177246094\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 879/2000, Train Loss: 5.128198125679504, Val Loss: 5.7085042370751, Val MAE: 1.5743387937545776\n",
      "Epoch 880/2000, Train Loss: 5.127861316232594, Val Loss: 5.708643334994623, Val MAE: 1.5744091272354126\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 881/2000, Train Loss: 5.127510475156007, Val Loss: 5.708392273668849, Val MAE: 1.5744401216506958\n",
      "Epoch 882/2000, Train Loss: 5.127322140921177, Val Loss: 5.708320814899713, Val MAE: 1.5743731260299683\n",
      "Epoch 883/2000, Train Loss: 5.127021227312457, Val Loss: 5.708530120164247, Val MAE: 1.5743517875671387\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 884/2000, Train Loss: 5.126748497095182, Val Loss: 5.708395686444886, Val MAE: 1.5743569135665894\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 885/2000, Train Loss: 5.126515370032722, Val Loss: 5.708403451909348, Val MAE: 1.5744233131408691\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 886/2000, Train Loss: 5.126321982532048, Val Loss: 5.708221397268662, Val MAE: 1.5745137929916382\n",
      "Epoch 887/2000, Train Loss: 5.1260165136352045, Val Loss: 5.708252492574377, Val MAE: 1.5744621753692627\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 888/2000, Train Loss: 5.1257289024744965, Val Loss: 5.70821634044341, Val MAE: 1.5745593309402466\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 889/2000, Train Loss: 5.125430650060064, Val Loss: 5.708304906019013, Val MAE: 1.5742765665054321\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 890/2000, Train Loss: 5.12522763890502, Val Loss: 5.7082652955427085, Val MAE: 1.5744470357894897\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 891/2000, Train Loss: 5.125058308862449, Val Loss: 5.708120314991073, Val MAE: 1.5743699073791504\n",
      "Epoch 892/2000, Train Loss: 5.124800353214659, Val Loss: 5.708009284479538, Val MAE: 1.5744727849960327\n",
      "Epoch 893/2000, Train Loss: 5.124394323689932, Val Loss: 5.7079437705901785, Val MAE: 1.5742928981781006\n",
      "Epoch 894/2000, Train Loss: 5.124114932517281, Val Loss: 5.707953725203827, Val MAE: 1.5743612051010132\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 895/2000, Train Loss: 5.123851717734824, Val Loss: 5.707946114492708, Val MAE: 1.5743223428726196\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 896/2000, Train Loss: 5.123786133330611, Val Loss: 5.708001595298085, Val MAE: 1.5743281841278076\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 897/2000, Train Loss: 5.123402157943908, Val Loss: 5.707930270138137, Val MAE: 1.5743608474731445\n",
      "Epoch 898/2000, Train Loss: 5.123007910592215, Val Loss: 5.707824235844685, Val MAE: 1.5744177103042603\n",
      "Epoch 899/2000, Train Loss: 5.122831466078842, Val Loss: 5.707914842013555, Val MAE: 1.574316143989563\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 900/2000, Train Loss: 5.122559506485447, Val Loss: 5.707782523529974, Val MAE: 1.5742206573486328\n",
      "Epoch 901/2000, Train Loss: 5.1223551185931395, Val Loss: 5.7075728489718305, Val MAE: 1.5744158029556274\n",
      "Epoch 902/2000, Train Loss: 5.122115080328075, Val Loss: 5.707680857053955, Val MAE: 1.5743381977081299\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 903/2000, Train Loss: 5.121828780385661, Val Loss: 5.707780865354276, Val MAE: 1.5741009712219238\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 904/2000, Train Loss: 5.121545421833894, Val Loss: 5.707597282136981, Val MAE: 1.5742998123168945\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 905/2000, Train Loss: 5.121272405715663, Val Loss: 5.707589193495042, Val MAE: 1.5742473602294922\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 906/2000, Train Loss: 5.121075066859079, Val Loss: 5.707601414269993, Val MAE: 1.5741223096847534\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 907/2000, Train Loss: 5.120790022515143, Val Loss: 5.707596708237213, Val MAE: 1.5741748809814453\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 908/2000, Train Loss: 5.120577023282679, Val Loss: 5.707452670878956, Val MAE: 1.5742038488388062\n",
      "Epoch 909/2000, Train Loss: 5.120302254678832, Val Loss: 5.70753857985549, Val MAE: 1.5741500854492188\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 910/2000, Train Loss: 5.119979750011103, Val Loss: 5.707508865481852, Val MAE: 1.57401442527771\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 911/2000, Train Loss: 5.1198363990367595, Val Loss: 5.7074082255910294, Val MAE: 1.5739550590515137\n",
      "Epoch 912/2000, Train Loss: 5.119595286042484, Val Loss: 5.707428530268713, Val MAE: 1.5739039182662964\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 913/2000, Train Loss: 5.119436544234445, Val Loss: 5.7070338036124495, Val MAE: 1.5740264654159546\n",
      "Epoch 914/2000, Train Loss: 5.119009660885923, Val Loss: 5.7073170143529905, Val MAE: 1.5739834308624268\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 915/2000, Train Loss: 5.118867574821971, Val Loss: 5.7071697163290205, Val MAE: 1.5740642547607422\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 916/2000, Train Loss: 5.118528797121471, Val Loss: 5.707127524078439, Val MAE: 1.5739630460739136\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 917/2000, Train Loss: 5.118243402699867, Val Loss: 5.707089709670536, Val MAE: 1.5740251541137695\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 918/2000, Train Loss: 5.117989149503017, Val Loss: 5.706984065997126, Val MAE: 1.5739821195602417\n",
      "Epoch 919/2000, Train Loss: 5.117683386903343, Val Loss: 5.707130802971872, Val MAE: 1.5739167928695679\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 920/2000, Train Loss: 5.117509538233071, Val Loss: 5.707096778744951, Val MAE: 1.57387375831604\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 921/2000, Train Loss: 5.11743396583533, Val Loss: 5.707004898731862, Val MAE: 1.5737793445587158\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 922/2000, Train Loss: 5.1169753525980255, Val Loss: 5.707044666695667, Val MAE: 1.5737367868423462\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 923/2000, Train Loss: 5.116711454774023, Val Loss: 5.707044300409632, Val MAE: 1.573740005493164\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 924/2000, Train Loss: 5.116449826288861, Val Loss: 5.7071069618430705, Val MAE: 1.5735796689987183\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 925/2000, Train Loss: 5.116302985863145, Val Loss: 5.70694847910776, Val MAE: 1.5736143589019775\n",
      "Epoch 926/2000, Train Loss: 5.115970073662368, Val Loss: 5.706959632649699, Val MAE: 1.5736790895462036\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 927/2000, Train Loss: 5.115721723045119, Val Loss: 5.7069058800144665, Val MAE: 1.573564887046814\n",
      "Epoch 928/2000, Train Loss: 5.115433988228891, Val Loss: 5.7069162405594405, Val MAE: 1.573521375656128\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 929/2000, Train Loss: 5.115194698356559, Val Loss: 5.707107111948345, Val MAE: 1.5735180377960205\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 930/2000, Train Loss: 5.114974144224882, Val Loss: 5.706948933251407, Val MAE: 1.5735820531845093\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 931/2000, Train Loss: 5.114713907493495, Val Loss: 5.706901833825155, Val MAE: 1.5735068321228027\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 932/2000, Train Loss: 5.114483007992094, Val Loss: 5.706825326888933, Val MAE: 1.5733275413513184\n",
      "Epoch 933/2000, Train Loss: 5.114240329589414, Val Loss: 5.706894648731302, Val MAE: 1.57331120967865\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 934/2000, Train Loss: 5.11402314569982, Val Loss: 5.706872304918569, Val MAE: 1.5736838579177856\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 935/2000, Train Loss: 5.1137672990749286, Val Loss: 5.706948325630357, Val MAE: 1.5734668970108032\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 936/2000, Train Loss: 5.113447539324831, Val Loss: 5.7068360984689965, Val MAE: 1.573669195175171\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 937/2000, Train Loss: 5.113214407228903, Val Loss: 5.706588788954854, Val MAE: 1.573641300201416\n",
      "Epoch 938/2000, Train Loss: 5.113005878051178, Val Loss: 5.706724421602506, Val MAE: 1.573646068572998\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 939/2000, Train Loss: 5.112837863253675, Val Loss: 5.7066840284279, Val MAE: 1.5736950635910034\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 940/2000, Train Loss: 5.112533858089192, Val Loss: 5.706674846760723, Val MAE: 1.5737403631210327\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 941/2000, Train Loss: 5.112198256338926, Val Loss: 5.706474657725851, Val MAE: 1.5737937688827515\n",
      "Epoch 942/2000, Train Loss: 5.11198173183358, Val Loss: 5.706457887494236, Val MAE: 1.573792815208435\n",
      "Epoch 943/2000, Train Loss: 5.111696235903714, Val Loss: 5.706464973702708, Val MAE: 1.5737371444702148\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 944/2000, Train Loss: 5.111456612144701, Val Loss: 5.7063250466041975, Val MAE: 1.5737700462341309\n",
      "Epoch 945/2000, Train Loss: 5.1112201356451585, Val Loss: 5.706233015665585, Val MAE: 1.573809027671814\n",
      "Epoch 946/2000, Train Loss: 5.110976120605174, Val Loss: 5.706234007526975, Val MAE: 1.573823094367981\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 947/2000, Train Loss: 5.110749635958151, Val Loss: 5.706302848886642, Val MAE: 1.5736232995986938\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 948/2000, Train Loss: 5.110596582639562, Val Loss: 5.706048233064307, Val MAE: 1.5737189054489136\n",
      "Epoch 949/2000, Train Loss: 5.110285300926622, Val Loss: 5.706094868412805, Val MAE: 1.5736770629882812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 950/2000, Train Loss: 5.11009809259459, Val Loss: 5.706124445258295, Val MAE: 1.57347571849823\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 951/2000, Train Loss: 5.109744642755668, Val Loss: 5.706057641666599, Val MAE: 1.5735341310501099\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 952/2000, Train Loss: 5.109618315471888, Val Loss: 5.705951068106048, Val MAE: 1.5735790729522705\n",
      "Epoch 953/2000, Train Loss: 5.109420563283728, Val Loss: 5.706103308576327, Val MAE: 1.573578119277954\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 954/2000, Train Loss: 5.109005455900296, Val Loss: 5.705905807947894, Val MAE: 1.5734448432922363\n",
      "Epoch 955/2000, Train Loss: 5.108812235510744, Val Loss: 5.705894877695527, Val MAE: 1.573486566543579\n",
      "Epoch 956/2000, Train Loss: 5.108564547968951, Val Loss: 5.705778649458463, Val MAE: 1.5735766887664795\n",
      "Epoch 957/2000, Train Loss: 5.108370316020205, Val Loss: 5.705855887052845, Val MAE: 1.5735987424850464\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 958/2000, Train Loss: 5.108068525078093, Val Loss: 5.705609165382677, Val MAE: 1.5735974311828613\n",
      "Epoch 959/2000, Train Loss: 5.107842546462342, Val Loss: 5.705659054926047, Val MAE: 1.5735315084457397\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 960/2000, Train Loss: 5.107645311295189, Val Loss: 5.705704138457593, Val MAE: 1.573384404182434\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 961/2000, Train Loss: 5.107491998585239, Val Loss: 5.705515066144663, Val MAE: 1.573547124862671\n",
      "Epoch 962/2000, Train Loss: 5.107128668804558, Val Loss: 5.705490416981029, Val MAE: 1.5734219551086426\n",
      "Epoch 963/2000, Train Loss: 5.106888305964728, Val Loss: 5.705344286533671, Val MAE: 1.5736489295959473\n",
      "Epoch 964/2000, Train Loss: 5.1066891298086015, Val Loss: 5.705260396641694, Val MAE: 1.5737351179122925\n",
      "Epoch 965/2000, Train Loss: 5.106428068343558, Val Loss: 5.705264940083209, Val MAE: 1.573734164237976\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 966/2000, Train Loss: 5.1061707202352995, Val Loss: 5.7051909678937465, Val MAE: 1.573762059211731\n",
      "Epoch 967/2000, Train Loss: 5.105931542441511, Val Loss: 5.705326282467682, Val MAE: 1.57359778881073\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 968/2000, Train Loss: 5.105690494882314, Val Loss: 5.705261323430123, Val MAE: 1.5735212564468384\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 969/2000, Train Loss: 5.105490466056787, Val Loss: 5.705186696409815, Val MAE: 1.573641300201416\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 970/2000, Train Loss: 5.105313482291257, Val Loss: 5.705060506905984, Val MAE: 1.5736185312271118\n",
      "Epoch 971/2000, Train Loss: 5.104992501925281, Val Loss: 5.705187697293926, Val MAE: 1.573521614074707\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 972/2000, Train Loss: 5.104732092629848, Val Loss: 5.705088477342501, Val MAE: 1.5736242532730103\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 973/2000, Train Loss: 5.10458858779918, Val Loss: 5.705123142820615, Val MAE: 1.5733593702316284\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 974/2000, Train Loss: 5.1042765903271565, Val Loss: 5.705060548191771, Val MAE: 1.5734796524047852\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 975/2000, Train Loss: 5.1040778393312545, Val Loss: 5.704978434864534, Val MAE: 1.573477029800415\n",
      "Epoch 976/2000, Train Loss: 5.1037575064369864, Val Loss: 5.7050736058924905, Val MAE: 1.573352336883545\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 977/2000, Train Loss: 5.103623603113431, Val Loss: 5.705034614156146, Val MAE: 1.5734484195709229\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 978/2000, Train Loss: 5.10339969417564, Val Loss: 5.705032041860283, Val MAE: 1.5731066465377808\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 979/2000, Train Loss: 5.103191752571358, Val Loss: 5.704893513557014, Val MAE: 1.5734306573867798\n",
      "Epoch 980/2000, Train Loss: 5.1028784356597106, Val Loss: 5.7048155686359525, Val MAE: 1.5733318328857422\n",
      "Epoch 981/2000, Train Loss: 5.1026747821503164, Val Loss: 5.705034776929686, Val MAE: 1.5731663703918457\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 982/2000, Train Loss: 5.102405923089035, Val Loss: 5.704944187894873, Val MAE: 1.5731985569000244\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 983/2000, Train Loss: 5.102202267566254, Val Loss: 5.705053848228688, Val MAE: 1.5731877088546753\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 984/2000, Train Loss: 5.101952331993967, Val Loss: 5.704938171471295, Val MAE: 1.5731544494628906\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 985/2000, Train Loss: 5.101718472729763, Val Loss: 5.704902119045958, Val MAE: 1.573120355606079\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 986/2000, Train Loss: 5.101481261511742, Val Loss: 5.704855214747449, Val MAE: 1.5730884075164795\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 987/2000, Train Loss: 5.101501419878106, Val Loss: 5.704811330873302, Val MAE: 1.573060393333435\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 988/2000, Train Loss: 5.10100200459791, Val Loss: 5.7047028994523785, Val MAE: 1.5730273723602295\n",
      "Epoch 989/2000, Train Loss: 5.100839930634361, Val Loss: 5.704676176247611, Val MAE: 1.5730139017105103\n",
      "Epoch 990/2000, Train Loss: 5.100587536800419, Val Loss: 5.704618321646244, Val MAE: 1.5730501413345337\n",
      "Epoch 991/2000, Train Loss: 5.100247363822717, Val Loss: 5.704812071921264, Val MAE: 1.5730525255203247\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 992/2000, Train Loss: 5.1000601971845745, Val Loss: 5.704811074409281, Val MAE: 1.5730125904083252\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 993/2000, Train Loss: 5.099911125338807, Val Loss: 5.704684641838803, Val MAE: 1.5731122493743896\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 994/2000, Train Loss: 5.099581944783081, Val Loss: 5.704737291514691, Val MAE: 1.5731396675109863\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 995/2000, Train Loss: 5.099392919788723, Val Loss: 5.704843525310539, Val MAE: 1.5729866027832031\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 996/2000, Train Loss: 5.099193319479065, Val Loss: 5.704640044713968, Val MAE: 1.5731768608093262\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 997/2000, Train Loss: 5.098946646180981, Val Loss: 5.704626972977904, Val MAE: 1.573016881942749\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 998/2000, Train Loss: 5.09861940955042, Val Loss: 5.704578936737977, Val MAE: 1.5730026960372925\n",
      "Epoch 999/2000, Train Loss: 5.0985366054859735, Val Loss: 5.70458017321537, Val MAE: 1.5729674100875854\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1000/2000, Train Loss: 5.0981440032392555, Val Loss: 5.7044595949329, Val MAE: 1.5730019807815552\n",
      "Epoch 1001/2000, Train Loss: 5.097966727029262, Val Loss: 5.70448462861029, Val MAE: 1.5728812217712402\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1002/2000, Train Loss: 5.097812862651269, Val Loss: 5.704551331220417, Val MAE: 1.57282292842865\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1003/2000, Train Loss: 5.097499306314349, Val Loss: 5.704408403201206, Val MAE: 1.5730165243148804\n",
      "Epoch 1004/2000, Train Loss: 5.097282273093216, Val Loss: 5.704510072867076, Val MAE: 1.5730419158935547\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1005/2000, Train Loss: 5.097121379561696, Val Loss: 5.704318116199715, Val MAE: 1.572998046875\n",
      "Epoch 1006/2000, Train Loss: 5.096863586243234, Val Loss: 5.7043770563529534, Val MAE: 1.572853922843933\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1007/2000, Train Loss: 5.0966174310567425, Val Loss: 5.704311002467386, Val MAE: 1.5729527473449707\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1008/2000, Train Loss: 5.096405853778858, Val Loss: 5.704350611576611, Val MAE: 1.572872281074524\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1009/2000, Train Loss: 5.0961177501772426, Val Loss: 5.704305969429308, Val MAE: 1.5728472471237183\n",
      "Epoch 1010/2000, Train Loss: 5.095864999470453, Val Loss: 5.704326623988079, Val MAE: 1.5727628469467163\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1011/2000, Train Loss: 5.0956194576958715, Val Loss: 5.7041582075098605, Val MAE: 1.572882890701294\n",
      "Epoch 1012/2000, Train Loss: 5.095442227625998, Val Loss: 5.7043002923147395, Val MAE: 1.572914958000183\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1013/2000, Train Loss: 5.0951209298160025, Val Loss: 5.704262566766972, Val MAE: 1.5728232860565186\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1014/2000, Train Loss: 5.094988655490325, Val Loss: 5.704113575205525, Val MAE: 1.5727617740631104\n",
      "Epoch 1015/2000, Train Loss: 5.09484724961562, Val Loss: 5.704267894547285, Val MAE: 1.5726051330566406\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1016/2000, Train Loss: 5.094463241343596, Val Loss: 5.704076783919553, Val MAE: 1.5728106498718262\n",
      "Epoch 1017/2000, Train Loss: 5.094241702330104, Val Loss: 5.704155380663886, Val MAE: 1.572587251663208\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1018/2000, Train Loss: 5.094055107493873, Val Loss: 5.704067702140283, Val MAE: 1.5726549625396729\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1019/2000, Train Loss: 5.093815235752694, Val Loss: 5.704232289247921, Val MAE: 1.5725502967834473\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1020/2000, Train Loss: 5.093515033708501, Val Loss: 5.704067974462422, Val MAE: 1.5725767612457275\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1021/2000, Train Loss: 5.093381136760671, Val Loss: 5.70413347477213, Val MAE: 1.5724467039108276\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1022/2000, Train Loss: 5.093166236890864, Val Loss: 5.704119997013599, Val MAE: 1.5725008249282837\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1023/2000, Train Loss: 5.092974209433119, Val Loss: 5.703902036133892, Val MAE: 1.5723589658737183\n",
      "Epoch 1024/2000, Train Loss: 5.092601913201146, Val Loss: 5.704046100560314, Val MAE: 1.5722936391830444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1025/2000, Train Loss: 5.092448785722801, Val Loss: 5.703926248652491, Val MAE: 1.5721144676208496\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1026/2000, Train Loss: 5.092266339470516, Val Loss: 5.70391459576216, Val MAE: 1.5722368955612183\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1027/2000, Train Loss: 5.092040656906547, Val Loss: 5.703874543719335, Val MAE: 1.5720263719558716\n",
      "Epoch 1028/2000, Train Loss: 5.0918901285430564, Val Loss: 5.703799778077216, Val MAE: 1.5720921754837036\n",
      "Epoch 1029/2000, Train Loss: 5.091561006291663, Val Loss: 5.703833924429861, Val MAE: 1.5720419883728027\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1030/2000, Train Loss: 5.091333780886001, Val Loss: 5.7038330289931105, Val MAE: 1.5719987154006958\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1031/2000, Train Loss: 5.091091252900109, Val Loss: 5.703774849482633, Val MAE: 1.5718940496444702\n",
      "Epoch 1032/2000, Train Loss: 5.090842318484516, Val Loss: 5.703833577282932, Val MAE: 1.5719125270843506\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1033/2000, Train Loss: 5.090774175065408, Val Loss: 5.70363859754819, Val MAE: 1.5719667673110962\n",
      "Epoch 1034/2000, Train Loss: 5.09045371700558, Val Loss: 5.703665683666865, Val MAE: 1.5720494985580444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1035/2000, Train Loss: 5.090193669952027, Val Loss: 5.703829244644999, Val MAE: 1.571860909461975\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1036/2000, Train Loss: 5.0899291718031305, Val Loss: 5.703721920922626, Val MAE: 1.5719307661056519\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1037/2000, Train Loss: 5.089789728477419, Val Loss: 5.703753557318212, Val MAE: 1.5718393325805664\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1038/2000, Train Loss: 5.089577123197681, Val Loss: 5.703630563042579, Val MAE: 1.5719128847122192\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1039/2000, Train Loss: 5.089298436710484, Val Loss: 5.70357331360152, Val MAE: 1.5718889236450195\n",
      "Epoch 1040/2000, Train Loss: 5.089107915769573, Val Loss: 5.703548332328825, Val MAE: 1.5718122720718384\n",
      "Epoch 1041/2000, Train Loss: 5.088929312905319, Val Loss: 5.703503718707904, Val MAE: 1.571811318397522\n",
      "Epoch 1042/2000, Train Loss: 5.088809168229717, Val Loss: 5.703506904822242, Val MAE: 1.571855902671814\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1043/2000, Train Loss: 5.088493210257652, Val Loss: 5.70348044956496, Val MAE: 1.5718324184417725\n",
      "Epoch 1044/2000, Train Loss: 5.088297111731361, Val Loss: 5.703285080121562, Val MAE: 1.5718587636947632\n",
      "Epoch 1045/2000, Train Loss: 5.088101066224262, Val Loss: 5.703324500300469, Val MAE: 1.5718914270401\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1046/2000, Train Loss: 5.087886497892183, Val Loss: 5.7034595701672615, Val MAE: 1.571815848350525\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1047/2000, Train Loss: 5.0876425072645155, Val Loss: 5.703328557152996, Val MAE: 1.5718456506729126\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1048/2000, Train Loss: 5.087422722666134, Val Loss: 5.70319689422937, Val MAE: 1.5718284845352173\n",
      "Epoch 1049/2000, Train Loss: 5.087156545100792, Val Loss: 5.7032098881330695, Val MAE: 1.571744441986084\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1050/2000, Train Loss: 5.086889278376966, Val Loss: 5.7031445174224515, Val MAE: 1.5718538761138916\n",
      "Epoch 1051/2000, Train Loss: 5.086720616506239, Val Loss: 5.7030025557458215, Val MAE: 1.5718262195587158\n",
      "Epoch 1052/2000, Train Loss: 5.086528003257736, Val Loss: 5.703187930201172, Val MAE: 1.5717437267303467\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1053/2000, Train Loss: 5.086236769174875, Val Loss: 5.7031353347527505, Val MAE: 1.571748971939087\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1054/2000, Train Loss: 5.0861267700403365, Val Loss: 5.703056512987213, Val MAE: 1.5718777179718018\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1055/2000, Train Loss: 5.0857907923739365, Val Loss: 5.702985029428377, Val MAE: 1.5717542171478271\n",
      "Epoch 1056/2000, Train Loss: 5.085581764966816, Val Loss: 5.7030354155859815, Val MAE: 1.5718251466751099\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1057/2000, Train Loss: 5.085427889599085, Val Loss: 5.703084059446959, Val MAE: 1.5716418027877808\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1058/2000, Train Loss: 5.085180091488789, Val Loss: 5.702988759731299, Val MAE: 1.5718321800231934\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1059/2000, Train Loss: 5.084998923401695, Val Loss: 5.702936536948616, Val MAE: 1.5716056823730469\n",
      "Epoch 1060/2000, Train Loss: 5.084762156135511, Val Loss: 5.702982563308984, Val MAE: 1.571516752243042\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1061/2000, Train Loss: 5.084534552464763, Val Loss: 5.702925507264035, Val MAE: 1.571576476097107\n",
      "Epoch 1062/2000, Train Loss: 5.084377803238742, Val Loss: 5.702796592351493, Val MAE: 1.571579098701477\n",
      "Epoch 1063/2000, Train Loss: 5.084105803042042, Val Loss: 5.702929255521261, Val MAE: 1.5715041160583496\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1064/2000, Train Loss: 5.083867691168896, Val Loss: 5.702809016365523, Val MAE: 1.5716304779052734\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1065/2000, Train Loss: 5.083667538352285, Val Loss: 5.702565251413835, Val MAE: 1.5716444253921509\n",
      "Epoch 1066/2000, Train Loss: 5.083531976585066, Val Loss: 5.702887851163881, Val MAE: 1.5714749097824097\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1067/2000, Train Loss: 5.083310948720203, Val Loss: 5.702805304928293, Val MAE: 1.5714582204818726\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1068/2000, Train Loss: 5.082949964199831, Val Loss: 5.702830863564022, Val MAE: 1.571414828300476\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1069/2000, Train Loss: 5.082756737434553, Val Loss: 5.702761216630265, Val MAE: 1.5712815523147583\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1070/2000, Train Loss: 5.082571551130323, Val Loss: 5.702885337196723, Val MAE: 1.5712536573410034\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1071/2000, Train Loss: 5.082537288484566, Val Loss: 5.7028938741494395, Val MAE: 1.57131028175354\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1072/2000, Train Loss: 5.082125413426206, Val Loss: 5.702877252200328, Val MAE: 1.5712640285491943\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1073/2000, Train Loss: 5.081923920364971, Val Loss: 5.702856690511791, Val MAE: 1.571309208869934\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1074/2000, Train Loss: 5.081591362184743, Val Loss: 5.702652773087907, Val MAE: 1.5712698698043823\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1075/2000, Train Loss: 5.081414785794521, Val Loss: 5.702793361123549, Val MAE: 1.5711792707443237\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1076/2000, Train Loss: 5.081236746846413, Val Loss: 5.702727187755276, Val MAE: 1.5711979866027832\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1077/2000, Train Loss: 5.08099401005551, Val Loss: 5.7025868194548, Val MAE: 1.5713075399398804\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1078/2000, Train Loss: 5.080881418433515, Val Loss: 5.702685472250714, Val MAE: 1.5710996389389038\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1079/2000, Train Loss: 5.0805437128616004, Val Loss: 5.702641171599747, Val MAE: 1.5712662935256958\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1080/2000, Train Loss: 5.080291868178967, Val Loss: 5.702620865099291, Val MAE: 1.5713107585906982\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1081/2000, Train Loss: 5.080158044719763, Val Loss: 5.702540139174972, Val MAE: 1.5711126327514648\n",
      "Epoch 1082/2000, Train Loss: 5.07994011789036, Val Loss: 5.702567926149485, Val MAE: 1.5712543725967407\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1083/2000, Train Loss: 5.0797046482521075, Val Loss: 5.702564277597888, Val MAE: 1.571028470993042\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1084/2000, Train Loss: 5.07956886744516, Val Loss: 5.702437926231903, Val MAE: 1.571191668510437\n",
      "Epoch 1085/2000, Train Loss: 5.079253612145163, Val Loss: 5.702574513191841, Val MAE: 1.5711233615875244\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1086/2000, Train Loss: 5.079122380250612, Val Loss: 5.702540975007078, Val MAE: 1.571051836013794\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1087/2000, Train Loss: 5.078814277675771, Val Loss: 5.702491563303391, Val MAE: 1.5711164474487305\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1088/2000, Train Loss: 5.078687689322808, Val Loss: 5.702622907059637, Val MAE: 1.571229338645935\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1089/2000, Train Loss: 5.078455440097087, Val Loss: 5.702290986656049, Val MAE: 1.5712814331054688\n",
      "Epoch 1090/2000, Train Loss: 5.078253306107652, Val Loss: 5.702487307130744, Val MAE: 1.571286678314209\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1091/2000, Train Loss: 5.077981306078734, Val Loss: 5.702242958071764, Val MAE: 1.5714234113693237\n",
      "Epoch 1092/2000, Train Loss: 5.0777527797733875, Val Loss: 5.702359256303274, Val MAE: 1.5713244676589966\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1093/2000, Train Loss: 5.0775013904517845, Val Loss: 5.702287867528583, Val MAE: 1.5712471008300781\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1094/2000, Train Loss: 5.077362170779814, Val Loss: 5.70237900658485, Val MAE: 1.5712999105453491\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1095/2000, Train Loss: 5.077284588937941, Val Loss: 5.702464200852479, Val MAE: 1.5710923671722412\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1096/2000, Train Loss: 5.0770579303846155, Val Loss: 5.702457409112825, Val MAE: 1.570993423461914\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1097/2000, Train Loss: 5.076689987263153, Val Loss: 5.702185291215914, Val MAE: 1.5711475610733032\n",
      "Epoch 1098/2000, Train Loss: 5.076471187127131, Val Loss: 5.702292447243262, Val MAE: 1.5710798501968384\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1099/2000, Train Loss: 5.076317455129335, Val Loss: 5.70218498079784, Val MAE: 1.571160078048706\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1100/2000, Train Loss: 5.076073923386125, Val Loss: 5.702243025787745, Val MAE: 1.5710337162017822\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1101/2000, Train Loss: 5.075843508523757, Val Loss: 5.702280159937132, Val MAE: 1.5709106922149658\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1102/2000, Train Loss: 5.075851221641631, Val Loss: 5.702090861691612, Val MAE: 1.571157693862915\n",
      "Epoch 1103/2000, Train Loss: 5.075369570307292, Val Loss: 5.702211503315409, Val MAE: 1.5711395740509033\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1104/2000, Train Loss: 5.0751896968280485, Val Loss: 5.70226138502086, Val MAE: 1.5709336996078491\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1105/2000, Train Loss: 5.075158088161944, Val Loss: 5.7020805930143466, Val MAE: 1.5711461305618286\n",
      "Epoch 1106/2000, Train Loss: 5.074767608360704, Val Loss: 5.702294729627236, Val MAE: 1.5708743333816528\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1107/2000, Train Loss: 5.074536058619189, Val Loss: 5.702210261643115, Val MAE: 1.5709545612335205\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1108/2000, Train Loss: 5.07433054230735, Val Loss: 5.702210420953389, Val MAE: 1.5709983110427856\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1109/2000, Train Loss: 5.074172748133467, Val Loss: 5.702162697774555, Val MAE: 1.5708476305007935\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1110/2000, Train Loss: 5.0739357785218875, Val Loss: 5.701994737171616, Val MAE: 1.5707753896713257\n",
      "Epoch 1111/2000, Train Loss: 5.073696347544682, Val Loss: 5.701917160997333, Val MAE: 1.5709407329559326\n",
      "Epoch 1112/2000, Train Loss: 5.073544532794671, Val Loss: 5.701930957558687, Val MAE: 1.570963740348816\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1113/2000, Train Loss: 5.073306640380709, Val Loss: 5.701993601675792, Val MAE: 1.5706888437271118\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1114/2000, Train Loss: 5.073107437505259, Val Loss: 5.701949343313135, Val MAE: 1.5707677602767944\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1115/2000, Train Loss: 5.07291961941058, Val Loss: 5.701967328148879, Val MAE: 1.5708017349243164\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1116/2000, Train Loss: 5.0726643101922395, Val Loss: 5.701845490695504, Val MAE: 1.570669412612915\n",
      "Epoch 1117/2000, Train Loss: 5.072486258157117, Val Loss: 5.7020122654029715, Val MAE: 1.570755958557129\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1118/2000, Train Loss: 5.072280393529324, Val Loss: 5.701856570415176, Val MAE: 1.5705708265304565\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1119/2000, Train Loss: 5.072053638836769, Val Loss: 5.701757850085559, Val MAE: 1.5707664489746094\n",
      "Epoch 1120/2000, Train Loss: 5.0718887370711565, Val Loss: 5.701873889301166, Val MAE: 1.570770025253296\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1121/2000, Train Loss: 5.071699699660911, Val Loss: 5.701673776275886, Val MAE: 1.5707820653915405\n",
      "Epoch 1122/2000, Train Loss: 5.0714066179599, Val Loss: 5.701763394958018, Val MAE: 1.5707511901855469\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1123/2000, Train Loss: 5.071213448911717, Val Loss: 5.701685054859984, Val MAE: 1.57062828540802\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1124/2000, Train Loss: 5.071053507963928, Val Loss: 5.7017449634519926, Val MAE: 1.5707734823226929\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1125/2000, Train Loss: 5.070760141704218, Val Loss: 5.701581369937378, Val MAE: 1.5704834461212158\n",
      "Epoch 1126/2000, Train Loss: 5.070623186085947, Val Loss: 5.70176836848259, Val MAE: 1.5706552267074585\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1127/2000, Train Loss: 5.070386070168245, Val Loss: 5.701511374763028, Val MAE: 1.5705592632293701\n",
      "Epoch 1128/2000, Train Loss: 5.070133077016101, Val Loss: 5.701529748031488, Val MAE: 1.5705044269561768\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1129/2000, Train Loss: 5.069935988910093, Val Loss: 5.7015138834441474, Val MAE: 1.570570468902588\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1130/2000, Train Loss: 5.069840213096453, Val Loss: 5.701299224607077, Val MAE: 1.5707476139068604\n",
      "Epoch 1131/2000, Train Loss: 5.0695296770078375, Val Loss: 5.701362163010723, Val MAE: 1.5704610347747803\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1132/2000, Train Loss: 5.069333303654219, Val Loss: 5.701330184571969, Val MAE: 1.5706249475479126\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1133/2000, Train Loss: 5.06918541735784, Val Loss: 5.70154426641056, Val MAE: 1.5706350803375244\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1134/2000, Train Loss: 5.068833016913009, Val Loss: 5.701439178044643, Val MAE: 1.5704976320266724\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1135/2000, Train Loss: 5.068845238843323, Val Loss: 5.7013128998629545, Val MAE: 1.5706944465637207\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1136/2000, Train Loss: 5.0684275798274125, Val Loss: 5.70140292274478, Val MAE: 1.5706592798233032\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1137/2000, Train Loss: 5.068266933103584, Val Loss: 5.7013811799911185, Val MAE: 1.57085120677948\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1138/2000, Train Loss: 5.068047564772969, Val Loss: 5.701363458454791, Val MAE: 1.5706862211227417\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1139/2000, Train Loss: 5.067915908053759, Val Loss: 5.701171258447367, Val MAE: 1.5707305669784546\n",
      "Epoch 1140/2000, Train Loss: 5.067606135626061, Val Loss: 5.701206453258473, Val MAE: 1.570761799812317\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1141/2000, Train Loss: 5.067368515354911, Val Loss: 5.701094293813093, Val MAE: 1.5706391334533691\n",
      "Epoch 1142/2000, Train Loss: 5.0672928396033035, Val Loss: 5.701135991272212, Val MAE: 1.5707272291183472\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1143/2000, Train Loss: 5.0670265409830675, Val Loss: 5.7011142596374595, Val MAE: 1.5707876682281494\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1144/2000, Train Loss: 5.066780541330723, Val Loss: 5.701000419140591, Val MAE: 1.5706846714019775\n",
      "Epoch 1145/2000, Train Loss: 5.066524859300221, Val Loss: 5.700844049909429, Val MAE: 1.5708802938461304\n",
      "Epoch 1146/2000, Train Loss: 5.066412425057977, Val Loss: 5.701012592158915, Val MAE: 1.570764422416687\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1147/2000, Train Loss: 5.066119904588596, Val Loss: 5.700720193189219, Val MAE: 1.5708400011062622\n",
      "Epoch 1148/2000, Train Loss: 5.065949033969393, Val Loss: 5.700827416567992, Val MAE: 1.5708143711090088\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1149/2000, Train Loss: 5.065722035824791, Val Loss: 5.7006430799990255, Val MAE: 1.570894479751587\n",
      "Epoch 1150/2000, Train Loss: 5.065635478471384, Val Loss: 5.700700376103049, Val MAE: 1.5708599090576172\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1151/2000, Train Loss: 5.065296506646819, Val Loss: 5.700577563740063, Val MAE: 1.570738434791565\n",
      "Epoch 1152/2000, Train Loss: 5.0650892064740844, Val Loss: 5.700625774236994, Val MAE: 1.5706961154937744\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1153/2000, Train Loss: 5.064829707397364, Val Loss: 5.70057429459846, Val MAE: 1.5707932710647583\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1154/2000, Train Loss: 5.064701518737288, Val Loss: 5.700670165662007, Val MAE: 1.5706582069396973\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1155/2000, Train Loss: 5.06444185506618, Val Loss: 5.7005944998439295, Val MAE: 1.570703387260437\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1156/2000, Train Loss: 5.064208502504374, Val Loss: 5.700602902093794, Val MAE: 1.5705965757369995\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1157/2000, Train Loss: 5.064021111541361, Val Loss: 5.700532562324396, Val MAE: 1.5706843137741089\n",
      "Epoch 1158/2000, Train Loss: 5.063867289108932, Val Loss: 5.700466991929104, Val MAE: 1.5706273317337036\n",
      "Epoch 1159/2000, Train Loss: 5.063730336259067, Val Loss: 5.700419452303411, Val MAE: 1.5707809925079346\n",
      "Epoch 1160/2000, Train Loss: 5.06339492368329, Val Loss: 5.700474464109549, Val MAE: 1.5704345703125\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1161/2000, Train Loss: 5.063264123995819, Val Loss: 5.700560074516028, Val MAE: 1.570581078529358\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1162/2000, Train Loss: 5.063046791618932, Val Loss: 5.70042401442834, Val MAE: 1.5706931352615356\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1163/2000, Train Loss: 5.062718566918692, Val Loss: 5.700449489489972, Val MAE: 1.5705289840698242\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1164/2000, Train Loss: 5.06258227747985, Val Loss: 5.700403640849875, Val MAE: 1.5705697536468506\n",
      "Epoch 1165/2000, Train Loss: 5.062431706583559, Val Loss: 5.700455357722186, Val MAE: 1.5704097747802734\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1166/2000, Train Loss: 5.0622691873259145, Val Loss: 5.700160981591688, Val MAE: 1.5705517530441284\n",
      "Epoch 1167/2000, Train Loss: 5.061974907995193, Val Loss: 5.700265697200728, Val MAE: 1.5705962181091309\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1168/2000, Train Loss: 5.061715264625737, Val Loss: 5.700207550226731, Val MAE: 1.570652961730957\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1169/2000, Train Loss: 5.061503856304579, Val Loss: 5.700108195025622, Val MAE: 1.5706843137741089\n",
      "Epoch 1170/2000, Train Loss: 5.0612945214364835, Val Loss: 5.700017260666652, Val MAE: 1.570723533630371\n",
      "Epoch 1171/2000, Train Loss: 5.061062380598097, Val Loss: 5.700005219890437, Val MAE: 1.570595622062683\n",
      "Epoch 1172/2000, Train Loss: 5.060893883762185, Val Loss: 5.700015032054452, Val MAE: 1.5705006122589111\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1173/2000, Train Loss: 5.06066507960943, Val Loss: 5.6999144959704955, Val MAE: 1.5704933404922485\n",
      "Epoch 1174/2000, Train Loss: 5.060485857628668, Val Loss: 5.699869177483637, Val MAE: 1.5705379247665405\n",
      "Epoch 1175/2000, Train Loss: 5.0602989612877325, Val Loss: 5.699861585273656, Val MAE: 1.5702461004257202\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1176/2000, Train Loss: 5.060090393687871, Val Loss: 5.699778430323353, Val MAE: 1.5704857110977173\n",
      "Epoch 1177/2000, Train Loss: 5.060020758852331, Val Loss: 5.6998351613680525, Val MAE: 1.5704034566879272\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1178/2000, Train Loss: 5.0596858057482486, Val Loss: 5.699692347818923, Val MAE: 1.5703896284103394\n",
      "Epoch 1179/2000, Train Loss: 5.059464318747255, Val Loss: 5.6997906919283965, Val MAE: 1.5706533193588257\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1180/2000, Train Loss: 5.059304988526862, Val Loss: 5.699772413535221, Val MAE: 1.5705890655517578\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1181/2000, Train Loss: 5.0590452077437424, Val Loss: 5.699809845525555, Val MAE: 1.570610761642456\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1182/2000, Train Loss: 5.058999161918259, Val Loss: 5.6997856303639365, Val MAE: 1.5702518224716187\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1183/2000, Train Loss: 5.058695389467759, Val Loss: 5.699681203117429, Val MAE: 1.5703356266021729\n",
      "Epoch 1184/2000, Train Loss: 5.058419319964227, Val Loss: 5.699651523649875, Val MAE: 1.5707110166549683\n",
      "Epoch 1185/2000, Train Loss: 5.058178385574158, Val Loss: 5.6996607905316425, Val MAE: 1.5704269409179688\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1186/2000, Train Loss: 5.057933368790242, Val Loss: 5.699750060790905, Val MAE: 1.5704894065856934\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1187/2000, Train Loss: 5.057769742505304, Val Loss: 5.699632023908312, Val MAE: 1.5705302953720093\n",
      "Epoch 1188/2000, Train Loss: 5.057535312781445, Val Loss: 5.699550619365972, Val MAE: 1.5705585479736328\n",
      "Epoch 1189/2000, Train Loss: 5.057424247558481, Val Loss: 5.699681199380747, Val MAE: 1.5705448389053345\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1190/2000, Train Loss: 5.057254949646211, Val Loss: 5.699475455995, Val MAE: 1.5705620050430298\n",
      "Epoch 1191/2000, Train Loss: 5.057048603315575, Val Loss: 5.699596922240855, Val MAE: 1.5704724788665771\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1192/2000, Train Loss: 5.056794866216929, Val Loss: 5.699513133695001, Val MAE: 1.5706487894058228\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1193/2000, Train Loss: 5.056538932177485, Val Loss: 5.699318424459626, Val MAE: 1.570479393005371\n",
      "Epoch 1194/2000, Train Loss: 5.056275934169697, Val Loss: 5.699428625245342, Val MAE: 1.5707134008407593\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1195/2000, Train Loss: 5.05605259569156, Val Loss: 5.6993608899437325, Val MAE: 1.570503830909729\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1196/2000, Train Loss: 5.055865079059641, Val Loss: 5.699309792813905, Val MAE: 1.570467472076416\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1197/2000, Train Loss: 5.055755368426683, Val Loss: 5.699337327425633, Val MAE: 1.570593237876892\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1198/2000, Train Loss: 5.055497217815917, Val Loss: 5.69920269329249, Val MAE: 1.5706359148025513\n",
      "Epoch 1199/2000, Train Loss: 5.055298328064094, Val Loss: 5.6992098608877315, Val MAE: 1.5705174207687378\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1200/2000, Train Loss: 5.055118776223426, Val Loss: 5.699278314726068, Val MAE: 1.5705676078796387\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1201/2000, Train Loss: 5.054881079108005, Val Loss: 5.69924205650977, Val MAE: 1.5704524517059326\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1202/2000, Train Loss: 5.054686267471582, Val Loss: 5.699148557966273, Val MAE: 1.5703811645507812\n",
      "Epoch 1203/2000, Train Loss: 5.054481548125437, Val Loss: 5.6990649178852, Val MAE: 1.5706298351287842\n",
      "Epoch 1204/2000, Train Loss: 5.05433651194616, Val Loss: 5.698926411728611, Val MAE: 1.5704975128173828\n",
      "Epoch 1205/2000, Train Loss: 5.054084125494638, Val Loss: 5.698984478318363, Val MAE: 1.5703879594802856\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1206/2000, Train Loss: 5.053856980205001, Val Loss: 5.698998746521976, Val MAE: 1.5704675912857056\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1207/2000, Train Loss: 5.053651274215328, Val Loss: 5.698962715969902, Val MAE: 1.570586085319519\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1208/2000, Train Loss: 5.053484451594611, Val Loss: 5.698763050252873, Val MAE: 1.5703728199005127\n",
      "Epoch 1209/2000, Train Loss: 5.053368555668959, Val Loss: 5.698880083426059, Val MAE: 1.5705405473709106\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1210/2000, Train Loss: 5.053146550891938, Val Loss: 5.698929939157008, Val MAE: 1.5705372095108032\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1211/2000, Train Loss: 5.05280853207056, Val Loss: 5.698947497008408, Val MAE: 1.5704292058944702\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1212/2000, Train Loss: 5.052703313136252, Val Loss: 5.698909688433376, Val MAE: 1.5704903602600098\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1213/2000, Train Loss: 5.052420604572927, Val Loss: 5.69893235387423, Val MAE: 1.5704679489135742\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1214/2000, Train Loss: 5.0522200650182265, Val Loss: 5.698826411813042, Val MAE: 1.5704823732376099\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1215/2000, Train Loss: 5.052191677268, Val Loss: 5.698783640558931, Val MAE: 1.570521593093872\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1216/2000, Train Loss: 5.051867486724209, Val Loss: 5.698841188023215, Val MAE: 1.5703883171081543\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1217/2000, Train Loss: 5.051735767254437, Val Loss: 5.698869455389291, Val MAE: 1.5704196691513062\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1218/2000, Train Loss: 5.051429395185735, Val Loss: 5.698800622596654, Val MAE: 1.5704460144042969\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1219/2000, Train Loss: 5.051266887672848, Val Loss: 5.698765846658555, Val MAE: 1.570344090461731\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1220/2000, Train Loss: 5.051078233980612, Val Loss: 5.698911184200089, Val MAE: 1.5703647136688232\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1221/2000, Train Loss: 5.050916143574403, Val Loss: 5.698704721092813, Val MAE: 1.5704267024993896\n",
      "Epoch 1222/2000, Train Loss: 5.0506557878015075, Val Loss: 5.698768027878683, Val MAE: 1.5702494382858276\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1223/2000, Train Loss: 5.050416655141144, Val Loss: 5.69881606867554, Val MAE: 1.570198655128479\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1224/2000, Train Loss: 5.050281307296297, Val Loss: 5.698685823595123, Val MAE: 1.5703705549240112\n",
      "Epoch 1225/2000, Train Loss: 5.050070831089436, Val Loss: 5.698701686177414, Val MAE: 1.5703415870666504\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1226/2000, Train Loss: 5.049964085084968, Val Loss: 5.698608634668752, Val MAE: 1.5702040195465088\n",
      "Epoch 1227/2000, Train Loss: 5.04964806360732, Val Loss: 5.698509549237902, Val MAE: 1.5702126026153564\n",
      "Epoch 1228/2000, Train Loss: 5.049451615422817, Val Loss: 5.698612812917896, Val MAE: 1.5701179504394531\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1229/2000, Train Loss: 5.049286686644262, Val Loss: 5.6986202882881924, Val MAE: 1.570043921470642\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1230/2000, Train Loss: 5.04909465046184, Val Loss: 5.698360108180878, Val MAE: 1.57020103931427\n",
      "Epoch 1231/2000, Train Loss: 5.049057022271905, Val Loss: 5.698385032583085, Val MAE: 1.5703520774841309\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1232/2000, Train Loss: 5.048741165975883, Val Loss: 5.698372618047469, Val MAE: 1.5704269409179688\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1233/2000, Train Loss: 5.048553050109654, Val Loss: 5.698535859858224, Val MAE: 1.5701721906661987\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1234/2000, Train Loss: 5.048337592447752, Val Loss: 5.698440981476314, Val MAE: 1.5701568126678467\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1235/2000, Train Loss: 5.048080504066419, Val Loss: 5.6982939418302765, Val MAE: 1.570103406906128\n",
      "Epoch 1236/2000, Train Loss: 5.047835039975021, Val Loss: 5.698203038367292, Val MAE: 1.5701355934143066\n",
      "Epoch 1237/2000, Train Loss: 5.047810748796241, Val Loss: 5.698056811404156, Val MAE: 1.5703669786453247\n",
      "Epoch 1238/2000, Train Loss: 5.047537840897562, Val Loss: 5.698238831138757, Val MAE: 1.5700933933258057\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1239/2000, Train Loss: 5.047246002127468, Val Loss: 5.698116106359966, Val MAE: 1.5702069997787476\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1240/2000, Train Loss: 5.047098059120688, Val Loss: 5.698112764854314, Val MAE: 1.5702893733978271\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1241/2000, Train Loss: 5.046935149349855, Val Loss: 5.698072757195989, Val MAE: 1.5703797340393066\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1242/2000, Train Loss: 5.046697022963543, Val Loss: 5.698041191159403, Val MAE: 1.5702919960021973\n",
      "Epoch 1243/2000, Train Loss: 5.04655052103852, Val Loss: 5.697892908838546, Val MAE: 1.570440411567688\n",
      "Epoch 1244/2000, Train Loss: 5.046234880216855, Val Loss: 5.698012057065235, Val MAE: 1.5703610181808472\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1245/2000, Train Loss: 5.0460413899914975, Val Loss: 5.698028127534674, Val MAE: 1.570286512374878\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1246/2000, Train Loss: 5.0458680857746305, Val Loss: 5.697939282710399, Val MAE: 1.5703774690628052\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1247/2000, Train Loss: 5.045703712103654, Val Loss: 5.697961724679405, Val MAE: 1.5702019929885864\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1248/2000, Train Loss: 5.045555406016082, Val Loss: 5.697908567452649, Val MAE: 1.5703009366989136\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1249/2000, Train Loss: 5.045303229334319, Val Loss: 5.697819357891695, Val MAE: 1.5704141855239868\n",
      "Epoch 1250/2000, Train Loss: 5.045033338454137, Val Loss: 5.697747870049345, Val MAE: 1.5702906847000122\n",
      "Epoch 1251/2000, Train Loss: 5.044857108869828, Val Loss: 5.697699793435018, Val MAE: 1.5704517364501953\n",
      "Epoch 1252/2000, Train Loss: 5.044675807060279, Val Loss: 5.697821725490261, Val MAE: 1.570284366607666\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1253/2000, Train Loss: 5.044491413191621, Val Loss: 5.697720671374499, Val MAE: 1.5702577829360962\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1254/2000, Train Loss: 5.044375888400309, Val Loss: 5.6978277158482, Val MAE: 1.5703046321868896\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1255/2000, Train Loss: 5.0441254196999195, Val Loss: 5.69781914982227, Val MAE: 1.570357084274292\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1256/2000, Train Loss: 5.043932745273813, Val Loss: 5.697605608708997, Val MAE: 1.5703226327896118\n",
      "Epoch 1257/2000, Train Loss: 5.043700287662536, Val Loss: 5.697578524959926, Val MAE: 1.5704576969146729\n",
      "Epoch 1258/2000, Train Loss: 5.043597088490968, Val Loss: 5.697468633042927, Val MAE: 1.5701911449432373\n",
      "Epoch 1259/2000, Train Loss: 5.043412871297021, Val Loss: 5.697372708298745, Val MAE: 1.5702910423278809\n",
      "Epoch 1260/2000, Train Loss: 5.04317518525523, Val Loss: 5.697374246444906, Val MAE: 1.5702019929885864\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1261/2000, Train Loss: 5.043001468210133, Val Loss: 5.697297931082023, Val MAE: 1.5702251195907593\n",
      "Epoch 1262/2000, Train Loss: 5.042830617221454, Val Loss: 5.697422388041056, Val MAE: 1.5702508687973022\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1263/2000, Train Loss: 5.042564167261627, Val Loss: 5.6971605135760175, Val MAE: 1.5703412294387817\n",
      "Epoch 1264/2000, Train Loss: 5.042382106237391, Val Loss: 5.69709764498454, Val MAE: 1.570461392402649\n",
      "Epoch 1265/2000, Train Loss: 5.042330428800644, Val Loss: 5.697228990654698, Val MAE: 1.5703860521316528\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1266/2000, Train Loss: 5.0419904895436165, Val Loss: 5.697171279778903, Val MAE: 1.5704524517059326\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1267/2000, Train Loss: 5.041771910376821, Val Loss: 5.697013369758559, Val MAE: 1.5703606605529785\n",
      "Epoch 1268/2000, Train Loss: 5.041635528070502, Val Loss: 5.697156837956256, Val MAE: 1.5702930688858032\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1269/2000, Train Loss: 5.04153709223705, Val Loss: 5.69719445887691, Val MAE: 1.570326328277588\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1270/2000, Train Loss: 5.041162004108416, Val Loss: 5.697166264968545, Val MAE: 1.5702977180480957\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1271/2000, Train Loss: 5.041140211375141, Val Loss: 5.697138568859217, Val MAE: 1.5702134370803833\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1272/2000, Train Loss: 5.040909157765742, Val Loss: 5.697096975024687, Val MAE: 1.5699774026870728\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1273/2000, Train Loss: 5.040667456359111, Val Loss: 5.697159492823691, Val MAE: 1.5701079368591309\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1274/2000, Train Loss: 5.0404497771763115, Val Loss: 5.697138480545913, Val MAE: 1.5700709819793701\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1275/2000, Train Loss: 5.040297271540935, Val Loss: 5.697069500108742, Val MAE: 1.5699809789657593\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1276/2000, Train Loss: 5.040057903104564, Val Loss: 5.697093355090611, Val MAE: 1.5699490308761597\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1277/2000, Train Loss: 5.039935320217285, Val Loss: 5.697150010307994, Val MAE: 1.570022463798523\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1278/2000, Train Loss: 5.03964268834049, Val Loss: 5.697031985091142, Val MAE: 1.5700453519821167\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1279/2000, Train Loss: 5.039504253721673, Val Loss: 5.6970403455083884, Val MAE: 1.5700123310089111\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1280/2000, Train Loss: 5.039319932670513, Val Loss: 5.696975861576354, Val MAE: 1.570147156715393\n",
      "Epoch 1281/2000, Train Loss: 5.039112805984626, Val Loss: 5.697148166118412, Val MAE: 1.5701755285263062\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1282/2000, Train Loss: 5.038971923422763, Val Loss: 5.696982979227643, Val MAE: 1.570095181465149\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1283/2000, Train Loss: 5.0387824120276585, Val Loss: 5.69683361363338, Val MAE: 1.5701050758361816\n",
      "Epoch 1284/2000, Train Loss: 5.0385819182775125, Val Loss: 5.697014470530577, Val MAE: 1.5700922012329102\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1285/2000, Train Loss: 5.038526853652675, Val Loss: 5.696971086733932, Val MAE: 1.5701217651367188\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1286/2000, Train Loss: 5.038097380920668, Val Loss: 5.696960360814307, Val MAE: 1.5699955224990845\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1287/2000, Train Loss: 5.038230858412869, Val Loss: 5.697034394522325, Val MAE: 1.5698602199554443\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1288/2000, Train Loss: 5.0377926574803675, Val Loss: 5.6968586271691395, Val MAE: 1.5700631141662598\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1289/2000, Train Loss: 5.037549336136101, Val Loss: 5.6968792181131676, Val MAE: 1.5700771808624268\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1290/2000, Train Loss: 5.037424839860029, Val Loss: 5.696862235437475, Val MAE: 1.5699492692947388\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1291/2000, Train Loss: 5.0372388935022, Val Loss: 5.6968410238395775, Val MAE: 1.5702345371246338\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1292/2000, Train Loss: 5.037036981656466, Val Loss: 5.696870336291987, Val MAE: 1.5700265169143677\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1293/2000, Train Loss: 5.036864536568617, Val Loss: 5.696751873792129, Val MAE: 1.570149540901184\n",
      "Epoch 1294/2000, Train Loss: 5.036549943337383, Val Loss: 5.696864640129451, Val MAE: 1.570027470588684\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1295/2000, Train Loss: 5.036477354695308, Val Loss: 5.696733518113419, Val MAE: 1.5701398849487305\n",
      "Epoch 1296/2000, Train Loss: 5.03622086059871, Val Loss: 5.6967488663094485, Val MAE: 1.5702946186065674\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1297/2000, Train Loss: 5.036049391248543, Val Loss: 5.696891268003242, Val MAE: 1.570220708847046\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1298/2000, Train Loss: 5.035821152270302, Val Loss: 5.696889843598053, Val MAE: 1.57010817527771\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1299/2000, Train Loss: 5.03567487030781, Val Loss: 5.696842812434613, Val MAE: 1.5700780153274536\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1300/2000, Train Loss: 5.035414162779425, Val Loss: 5.696850276868278, Val MAE: 1.5701172351837158\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1301/2000, Train Loss: 5.035309713378412, Val Loss: 5.696756359269495, Val MAE: 1.5701848268508911\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1302/2000, Train Loss: 5.035216583490875, Val Loss: 5.696904044723657, Val MAE: 1.5699033737182617\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1303/2000, Train Loss: 5.034893269944912, Val Loss: 5.696766602154536, Val MAE: 1.5700265169143677\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1304/2000, Train Loss: 5.034763487856796, Val Loss: 5.6968671380562155, Val MAE: 1.5698003768920898\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1305/2000, Train Loss: 5.0345524543611875, Val Loss: 5.696820896517611, Val MAE: 1.5698750019073486\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1306/2000, Train Loss: 5.034298115930953, Val Loss: 5.6966683982162305, Val MAE: 1.569913387298584\n",
      "Epoch 1307/2000, Train Loss: 5.034217833633745, Val Loss: 5.696636860250333, Val MAE: 1.5698221921920776\n",
      "Epoch 1308/2000, Train Loss: 5.034045205327677, Val Loss: 5.696722151216017, Val MAE: 1.5698869228363037\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1309/2000, Train Loss: 5.033753532997577, Val Loss: 5.696838584332656, Val MAE: 1.569886565208435\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1310/2000, Train Loss: 5.0337193701822605, Val Loss: 5.696608674362166, Val MAE: 1.5699396133422852\n",
      "Epoch 1311/2000, Train Loss: 5.033423870739007, Val Loss: 5.696794373270933, Val MAE: 1.5699821710586548\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1312/2000, Train Loss: 5.033192104390605, Val Loss: 5.696688042321337, Val MAE: 1.5700089931488037\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1313/2000, Train Loss: 5.033013206732936, Val Loss: 5.696647305463068, Val MAE: 1.5699403285980225\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1314/2000, Train Loss: 5.03289592509367, Val Loss: 5.696678455998773, Val MAE: 1.5698572397232056\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1315/2000, Train Loss: 5.032699784789599, Val Loss: 5.6964008015230165, Val MAE: 1.5699232816696167\n",
      "Epoch 1316/2000, Train Loss: 5.032478868080477, Val Loss: 5.696476441640008, Val MAE: 1.5698637962341309\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1317/2000, Train Loss: 5.032320284994449, Val Loss: 5.69633776286691, Val MAE: 1.56996750831604\n",
      "Epoch 1318/2000, Train Loss: 5.032078773517327, Val Loss: 5.696508295310018, Val MAE: 1.5698970556259155\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1319/2000, Train Loss: 5.031891281549399, Val Loss: 5.696500155995016, Val MAE: 1.5700616836547852\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1320/2000, Train Loss: 5.031764573399908, Val Loss: 5.696388995337559, Val MAE: 1.57001531124115\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1321/2000, Train Loss: 5.031556441194989, Val Loss: 5.696506016115895, Val MAE: 1.5697991847991943\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1322/2000, Train Loss: 5.0313038822633125, Val Loss: 5.696470818661768, Val MAE: 1.5698044300079346\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1323/2000, Train Loss: 5.031187319906558, Val Loss: 5.69636154220374, Val MAE: 1.569851279258728\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1324/2000, Train Loss: 5.030951596665433, Val Loss: 5.696413007715791, Val MAE: 1.569836139678955\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1325/2000, Train Loss: 5.0308343084826594, Val Loss: 5.696287427473505, Val MAE: 1.5698922872543335\n",
      "Epoch 1326/2000, Train Loss: 5.030677607410143, Val Loss: 5.696285483760572, Val MAE: 1.5698968172073364\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1327/2000, Train Loss: 5.03035560843812, Val Loss: 5.696305818878547, Val MAE: 1.5698658227920532\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1328/2000, Train Loss: 5.03024733376956, Val Loss: 5.696182822415588, Val MAE: 1.5696836709976196\n",
      "Epoch 1329/2000, Train Loss: 5.030077666484663, Val Loss: 5.696365168882073, Val MAE: 1.5696502923965454\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1330/2000, Train Loss: 5.029894016730291, Val Loss: 5.696263748206859, Val MAE: 1.5698330402374268\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1331/2000, Train Loss: 5.02974026510197, Val Loss: 5.696112546260933, Val MAE: 1.5699554681777954\n",
      "Epoch 1332/2000, Train Loss: 5.029495124494753, Val Loss: 5.696170780636849, Val MAE: 1.5698119401931763\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1333/2000, Train Loss: 5.029357455885805, Val Loss: 5.6961812880972476, Val MAE: 1.5699139833450317\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1334/2000, Train Loss: 5.029166157861397, Val Loss: 5.696335299299397, Val MAE: 1.5699650049209595\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1335/2000, Train Loss: 5.028975241793284, Val Loss: 5.696104180375371, Val MAE: 1.5697239637374878\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1336/2000, Train Loss: 5.02891296471281, Val Loss: 5.69612979041327, Val MAE: 1.5698518753051758\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1337/2000, Train Loss: 5.028652696186686, Val Loss: 5.696339572332685, Val MAE: 1.5699056386947632\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1338/2000, Train Loss: 5.028428290613431, Val Loss: 5.696174448600967, Val MAE: 1.5699816942214966\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1339/2000, Train Loss: 5.0282155338263195, Val Loss: 5.696207329220728, Val MAE: 1.5699928998947144\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1340/2000, Train Loss: 5.028197883133482, Val Loss: 5.696058923953899, Val MAE: 1.5701347589492798\n",
      "Epoch 1341/2000, Train Loss: 5.027791734000984, Val Loss: 5.696123948246696, Val MAE: 1.5699496269226074\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1342/2000, Train Loss: 5.027666761034563, Val Loss: 5.696182943994481, Val MAE: 1.5699182748794556\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1343/2000, Train Loss: 5.0274577993141945, Val Loss: 5.696230335337671, Val MAE: 1.5698436498641968\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1344/2000, Train Loss: 5.027308025803053, Val Loss: 5.696043493733129, Val MAE: 1.5697535276412964\n",
      "Epoch 1345/2000, Train Loss: 5.027156290497937, Val Loss: 5.696070002944462, Val MAE: 1.5698350667953491\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1346/2000, Train Loss: 5.027108800654509, Val Loss: 5.6960220077169055, Val MAE: 1.5697126388549805\n",
      "Epoch 1347/2000, Train Loss: 5.026827182340253, Val Loss: 5.695940054155636, Val MAE: 1.5697932243347168\n",
      "Epoch 1348/2000, Train Loss: 5.026682093141785, Val Loss: 5.6960432344438106, Val MAE: 1.5695991516113281\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1349/2000, Train Loss: 5.026401049985087, Val Loss: 5.695975990743812, Val MAE: 1.5695945024490356\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1350/2000, Train Loss: 5.026247900527267, Val Loss: 5.696105475363746, Val MAE: 1.5695897340774536\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1351/2000, Train Loss: 5.025975458224335, Val Loss: 5.695865988731384, Val MAE: 1.5696043968200684\n",
      "Epoch 1352/2000, Train Loss: 5.025850738294523, Val Loss: 5.6957070893103925, Val MAE: 1.5695477724075317\n",
      "Epoch 1353/2000, Train Loss: 5.025674978668299, Val Loss: 5.695799901671366, Val MAE: 1.5694811344146729\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1354/2000, Train Loss: 5.025586267997479, Val Loss: 5.695750332389038, Val MAE: 1.5698447227478027\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1355/2000, Train Loss: 5.025357529615366, Val Loss: 5.695617782413413, Val MAE: 1.5697321891784668\n",
      "Epoch 1356/2000, Train Loss: 5.025171339973273, Val Loss: 5.695601305192399, Val MAE: 1.569675326347351\n",
      "Epoch 1357/2000, Train Loss: 5.024881268285849, Val Loss: 5.695616876951416, Val MAE: 1.5696715116500854\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1358/2000, Train Loss: 5.024791685482887, Val Loss: 5.69552960044018, Val MAE: 1.5698802471160889\n",
      "Epoch 1359/2000, Train Loss: 5.024714750357701, Val Loss: 5.695484325517572, Val MAE: 1.569707989692688\n",
      "Epoch 1360/2000, Train Loss: 5.0244953274978545, Val Loss: 5.69576469966031, Val MAE: 1.569623589515686\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1361/2000, Train Loss: 5.024252095917092, Val Loss: 5.695687774853604, Val MAE: 1.5695751905441284\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1362/2000, Train Loss: 5.024025597260252, Val Loss: 5.695604559660687, Val MAE: 1.5694934129714966\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1363/2000, Train Loss: 5.023942727425164, Val Loss: 5.6954292792610435, Val MAE: 1.5696489810943604\n",
      "Epoch 1364/2000, Train Loss: 5.023777761231167, Val Loss: 5.695450341391636, Val MAE: 1.5694807767868042\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1365/2000, Train Loss: 5.023596401201177, Val Loss: 5.695426128143929, Val MAE: 1.569309115409851\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1366/2000, Train Loss: 5.023439755664587, Val Loss: 5.695436910387208, Val MAE: 1.5694326162338257\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1367/2000, Train Loss: 5.023230696127165, Val Loss: 5.695326573258146, Val MAE: 1.569439172744751\n",
      "Epoch 1368/2000, Train Loss: 5.022910068439481, Val Loss: 5.6952950280923, Val MAE: 1.5693731307983398\n",
      "Epoch 1369/2000, Train Loss: 5.022789207501448, Val Loss: 5.695457757521842, Val MAE: 1.5693747997283936\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1370/2000, Train Loss: 5.022744237915216, Val Loss: 5.695502988697921, Val MAE: 1.5692459344863892\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1371/2000, Train Loss: 5.02235803698084, Val Loss: 5.695423383869527, Val MAE: 1.56926691532135\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1372/2000, Train Loss: 5.022238572662268, Val Loss: 5.695399587307501, Val MAE: 1.5691571235656738\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1373/2000, Train Loss: 5.0221521621183305, Val Loss: 5.695334263259847, Val MAE: 1.5690628290176392\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1374/2000, Train Loss: 5.021838758798824, Val Loss: 5.695268376887027, Val MAE: 1.5691723823547363\n",
      "Epoch 1375/2000, Train Loss: 5.021707279761017, Val Loss: 5.6952198958907285, Val MAE: 1.569214940071106\n",
      "Epoch 1376/2000, Train Loss: 5.021577195497066, Val Loss: 5.695203337374084, Val MAE: 1.5692036151885986\n",
      "Epoch 1377/2000, Train Loss: 5.021400570785555, Val Loss: 5.695149384780762, Val MAE: 1.5692205429077148\n",
      "Epoch 1378/2000, Train Loss: 5.021147879977028, Val Loss: 5.695039013930417, Val MAE: 1.5691989660263062\n",
      "Epoch 1379/2000, Train Loss: 5.020998095224476, Val Loss: 5.695120255334662, Val MAE: 1.5692200660705566\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1380/2000, Train Loss: 5.020750353824581, Val Loss: 5.6950104751535875, Val MAE: 1.5692033767700195\n",
      "Epoch 1381/2000, Train Loss: 5.020678011654633, Val Loss: 5.695074677831901, Val MAE: 1.569067120552063\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1382/2000, Train Loss: 5.020624909075442, Val Loss: 5.695089512097361, Val MAE: 1.5690208673477173\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1383/2000, Train Loss: 5.020259557183069, Val Loss: 5.6949141871309426, Val MAE: 1.5689760446548462\n",
      "Epoch 1384/2000, Train Loss: 5.020126559380324, Val Loss: 5.695107676293872, Val MAE: 1.569125771522522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1385/2000, Train Loss: 5.019943863635832, Val Loss: 5.694899869621347, Val MAE: 1.5690977573394775\n",
      "Epoch 1386/2000, Train Loss: 5.019797561371687, Val Loss: 5.694957470310573, Val MAE: 1.5689960718154907\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1387/2000, Train Loss: 5.019602533799507, Val Loss: 5.695125922606261, Val MAE: 1.5690275430679321\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1388/2000, Train Loss: 5.019343058883431, Val Loss: 5.695061811339965, Val MAE: 1.568829894065857\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1389/2000, Train Loss: 5.019229341610312, Val Loss: 5.69500053548667, Val MAE: 1.5688997507095337\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1390/2000, Train Loss: 5.0190071911982965, Val Loss: 5.694868834467109, Val MAE: 1.568879246711731\n",
      "Epoch 1391/2000, Train Loss: 5.019002012431328, Val Loss: 5.695003977882752, Val MAE: 1.5688951015472412\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1392/2000, Train Loss: 5.018615255886027, Val Loss: 5.69488599695926, Val MAE: 1.5687103271484375\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1393/2000, Train Loss: 5.018568296318403, Val Loss: 5.694748000723142, Val MAE: 1.5687668323516846\n",
      "Epoch 1394/2000, Train Loss: 5.018294975675386, Val Loss: 5.69476636433091, Val MAE: 1.568808674812317\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1395/2000, Train Loss: 5.018137973098164, Val Loss: 5.6947704066924, Val MAE: 1.568764090538025\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1396/2000, Train Loss: 5.018029867973569, Val Loss: 5.694712415109716, Val MAE: 1.5686331987380981\n",
      "Epoch 1397/2000, Train Loss: 5.017765574686801, Val Loss: 5.694730011877299, Val MAE: 1.568784236907959\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1398/2000, Train Loss: 5.017582149425079, Val Loss: 5.6947178371273415, Val MAE: 1.568730115890503\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1399/2000, Train Loss: 5.017502202860157, Val Loss: 5.694566426109466, Val MAE: 1.5686380863189697\n",
      "Epoch 1400/2000, Train Loss: 5.017259022956328, Val Loss: 5.694663276548415, Val MAE: 1.568655252456665\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1401/2000, Train Loss: 5.017190178878537, Val Loss: 5.694465085362805, Val MAE: 1.568700909614563\n",
      "Epoch 1402/2000, Train Loss: 5.016894765507579, Val Loss: 5.694569055640369, Val MAE: 1.5687084197998047\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1403/2000, Train Loss: 5.01672897567051, Val Loss: 5.694795134052953, Val MAE: 1.5686856508255005\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1404/2000, Train Loss: 5.01657885719906, Val Loss: 5.694606374163146, Val MAE: 1.5686397552490234\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1405/2000, Train Loss: 5.016375501083707, Val Loss: 5.694568282967314, Val MAE: 1.5684168338775635\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1406/2000, Train Loss: 5.016199050530172, Val Loss: 5.69472310065494, Val MAE: 1.5684945583343506\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1407/2000, Train Loss: 5.015970112151616, Val Loss: 5.694813393398163, Val MAE: 1.5685008764266968\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1408/2000, Train Loss: 5.015866700735502, Val Loss: 5.694666976775598, Val MAE: 1.5685620307922363\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1409/2000, Train Loss: 5.015775787587068, Val Loss: 5.694658277687312, Val MAE: 1.5685560703277588\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1410/2000, Train Loss: 5.0155956716624726, Val Loss: 5.694824066457398, Val MAE: 1.5683646202087402\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1411/2000, Train Loss: 5.015465085058461, Val Loss: 5.694459418638037, Val MAE: 1.5687447786331177\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1412/2000, Train Loss: 5.015132170844296, Val Loss: 5.694566941953945, Val MAE: 1.5686925649642944\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1413/2000, Train Loss: 5.014974378516018, Val Loss: 5.694403383163137, Val MAE: 1.5685275793075562\n",
      "Epoch 1414/2000, Train Loss: 5.014804311909364, Val Loss: 5.694529126908072, Val MAE: 1.5684577226638794\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1415/2000, Train Loss: 5.014658499066046, Val Loss: 5.694522010623862, Val MAE: 1.568619966506958\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1416/2000, Train Loss: 5.014550026833885, Val Loss: 5.69427718563911, Val MAE: 1.5685452222824097\n",
      "Epoch 1417/2000, Train Loss: 5.014366007958559, Val Loss: 5.694349403045957, Val MAE: 1.568514347076416\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1418/2000, Train Loss: 5.014025638340058, Val Loss: 5.6942123362412875, Val MAE: 1.5686441659927368\n",
      "Epoch 1419/2000, Train Loss: 5.013926397655146, Val Loss: 5.694327674965611, Val MAE: 1.5684068202972412\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1420/2000, Train Loss: 5.013776978500119, Val Loss: 5.694282430118742, Val MAE: 1.5684610605239868\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1421/2000, Train Loss: 5.013553039193741, Val Loss: 5.694170382500424, Val MAE: 1.568302869796753\n",
      "Epoch 1422/2000, Train Loss: 5.013458094070026, Val Loss: 5.694328479810593, Val MAE: 1.5684289932250977\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1423/2000, Train Loss: 5.0132477030126585, Val Loss: 5.694258152982875, Val MAE: 1.5682835578918457\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1424/2000, Train Loss: 5.013073954424499, Val Loss: 5.694238372625561, Val MAE: 1.5683494806289673\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1425/2000, Train Loss: 5.012879317113835, Val Loss: 5.694204381937645, Val MAE: 1.5683497190475464\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1426/2000, Train Loss: 5.012624183311838, Val Loss: 5.694200785881883, Val MAE: 1.5683162212371826\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1427/2000, Train Loss: 5.012558898482836, Val Loss: 5.694255634367648, Val MAE: 1.5682616233825684\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1428/2000, Train Loss: 5.012322946973958, Val Loss: 5.694161358503027, Val MAE: 1.5682111978530884\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1429/2000, Train Loss: 5.012184956894216, Val Loss: 5.693954642850689, Val MAE: 1.5682785511016846\n",
      "Epoch 1430/2000, Train Loss: 5.011924147354223, Val Loss: 5.694070112450043, Val MAE: 1.5681719779968262\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1431/2000, Train Loss: 5.01181758213513, Val Loss: 5.6938674094480115, Val MAE: 1.5683388710021973\n",
      "Epoch 1432/2000, Train Loss: 5.011522988044905, Val Loss: 5.69401438329198, Val MAE: 1.5682462453842163\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1433/2000, Train Loss: 5.011484000147606, Val Loss: 5.694001571938896, Val MAE: 1.5681066513061523\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1434/2000, Train Loss: 5.0112785972565685, Val Loss: 5.69392792629904, Val MAE: 1.5682405233383179\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1435/2000, Train Loss: 5.011094607743137, Val Loss: 5.694007567309458, Val MAE: 1.56801438331604\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1436/2000, Train Loss: 5.0109146443326065, Val Loss: 5.6939404085506355, Val MAE: 1.5678908824920654\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1437/2000, Train Loss: 5.010756357046687, Val Loss: 5.694059066998484, Val MAE: 1.5679155588150024\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1438/2000, Train Loss: 5.010722210002902, Val Loss: 5.69374259094215, Val MAE: 1.5679035186767578\n",
      "Epoch 1439/2000, Train Loss: 5.0103976983238825, Val Loss: 5.693929122766588, Val MAE: 1.568127155303955\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1440/2000, Train Loss: 5.010255238441076, Val Loss: 5.693935258308318, Val MAE: 1.5679855346679688\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1441/2000, Train Loss: 5.010090889014634, Val Loss: 5.693731632801371, Val MAE: 1.568054437637329\n",
      "Epoch 1442/2000, Train Loss: 5.009967117054542, Val Loss: 5.693705175721317, Val MAE: 1.5681456327438354\n",
      "Epoch 1443/2000, Train Loss: 5.009626243753722, Val Loss: 5.693704717020741, Val MAE: 1.5681759119033813\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1444/2000, Train Loss: 5.009542352460288, Val Loss: 5.693742129963108, Val MAE: 1.568132996559143\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1445/2000, Train Loss: 5.00937190180006, Val Loss: 5.693712191206235, Val MAE: 1.568001389503479\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1446/2000, Train Loss: 5.009244578560836, Val Loss: 5.693636439809012, Val MAE: 1.5680928230285645\n",
      "Epoch 1447/2000, Train Loss: 5.009115539915875, Val Loss: 5.693720175312199, Val MAE: 1.5678322315216064\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1448/2000, Train Loss: 5.00878401477097, Val Loss: 5.693666389137962, Val MAE: 1.567922830581665\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1449/2000, Train Loss: 5.008582743685653, Val Loss: 5.6935385955036235, Val MAE: 1.567872166633606\n",
      "Epoch 1450/2000, Train Loss: 5.008510362086876, Val Loss: 5.693421947482165, Val MAE: 1.5678819417953491\n",
      "Epoch 1451/2000, Train Loss: 5.00829675900609, Val Loss: 5.693542650897933, Val MAE: 1.567784070968628\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1452/2000, Train Loss: 5.008099229800542, Val Loss: 5.693677601373159, Val MAE: 1.567819595336914\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1453/2000, Train Loss: 5.007989795504952, Val Loss: 5.693553419561561, Val MAE: 1.567783236503601\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1454/2000, Train Loss: 5.007839284423033, Val Loss: 5.693647032757418, Val MAE: 1.5676968097686768\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1455/2000, Train Loss: 5.007607034358401, Val Loss: 5.693649199395369, Val MAE: 1.5677605867385864\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1456/2000, Train Loss: 5.007420407643543, Val Loss: 5.693505968340311, Val MAE: 1.5675910711288452\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1457/2000, Train Loss: 5.007333701802843, Val Loss: 5.693549966502262, Val MAE: 1.567594289779663\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1458/2000, Train Loss: 5.0072133079705985, Val Loss: 5.693621883607421, Val MAE: 1.5677238702774048\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1459/2000, Train Loss: 5.007037414789703, Val Loss: 5.693783424954167, Val MAE: 1.5675626993179321\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1460/2000, Train Loss: 5.006753346686796, Val Loss: 5.693615029984658, Val MAE: 1.5674996376037598\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1461/2000, Train Loss: 5.00660097859092, Val Loss: 5.693587306351472, Val MAE: 1.5675501823425293\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1462/2000, Train Loss: 5.006404840719691, Val Loss: 5.693517645291232, Val MAE: 1.567549705505371\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1463/2000, Train Loss: 5.006274741103664, Val Loss: 5.693533405616014, Val MAE: 1.5674220323562622\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1464/2000, Train Loss: 5.00607217490715, Val Loss: 5.6935208872735315, Val MAE: 1.5673460960388184\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1465/2000, Train Loss: 5.005997115503643, Val Loss: 5.693566235289296, Val MAE: 1.5675616264343262\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1466/2000, Train Loss: 5.005740314868871, Val Loss: 5.693540021002475, Val MAE: 1.567628026008606\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 1467/2000, Train Loss: 5.005577717592479, Val Loss: 5.693548240428306, Val MAE: 1.567548394203186\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 1468/2000, Train Loss: 5.005392404444867, Val Loss: 5.693460552790843, Val MAE: 1.5674570798873901\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 1469/2000, Train Loss: 5.005267938872947, Val Loss: 5.693234826173257, Val MAE: 1.5678563117980957\n",
      "Epoch 1470/2000, Train Loss: 5.005067792767626, Val Loss: 5.693249720134502, Val MAE: 1.5676549673080444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1471/2000, Train Loss: 5.004835390272818, Val Loss: 5.693169192892332, Val MAE: 1.567731499671936\n",
      "Epoch 1472/2000, Train Loss: 5.004704286144452, Val Loss: 5.693103783174393, Val MAE: 1.5675045251846313\n",
      "Epoch 1473/2000, Train Loss: 5.004566480541296, Val Loss: 5.693179347464069, Val MAE: 1.56769859790802\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1474/2000, Train Loss: 5.004404581071289, Val Loss: 5.693036142201234, Val MAE: 1.5676155090332031\n",
      "Epoch 1475/2000, Train Loss: 5.004213668023256, Val Loss: 5.692915962650142, Val MAE: 1.567758560180664\n",
      "Epoch 1476/2000, Train Loss: 5.003980969141102, Val Loss: 5.693014924132496, Val MAE: 1.5676383972167969\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1477/2000, Train Loss: 5.003836797543767, Val Loss: 5.692975713207817, Val MAE: 1.5676060914993286\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1478/2000, Train Loss: 5.003785826302514, Val Loss: 5.6931008233572005, Val MAE: 1.5676395893096924\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1479/2000, Train Loss: 5.003545150716568, Val Loss: 5.693079280379351, Val MAE: 1.5675171613693237\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1480/2000, Train Loss: 5.003434750162993, Val Loss: 5.693023404214725, Val MAE: 1.5674227476119995\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1481/2000, Train Loss: 5.003141487424933, Val Loss: 5.692975311377727, Val MAE: 1.5677363872528076\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1482/2000, Train Loss: 5.002940002081681, Val Loss: 5.692794967135158, Val MAE: 1.5676778554916382\n",
      "Epoch 1483/2000, Train Loss: 5.002967498908154, Val Loss: 5.692866803673794, Val MAE: 1.56739342212677\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1484/2000, Train Loss: 5.002730315374204, Val Loss: 5.692721611862883, Val MAE: 1.5676125288009644\n",
      "Epoch 1485/2000, Train Loss: 5.002535473033628, Val Loss: 5.692522251642443, Val MAE: 1.5675480365753174\n",
      "Epoch 1486/2000, Train Loss: 5.0023652792480275, Val Loss: 5.692701070042561, Val MAE: 1.5675045251846313\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1487/2000, Train Loss: 5.002119630763935, Val Loss: 5.692757951465951, Val MAE: 1.5674487352371216\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1488/2000, Train Loss: 5.001990984729442, Val Loss: 5.692756099256171, Val MAE: 1.5676031112670898\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1489/2000, Train Loss: 5.001789858440208, Val Loss: 5.692777055119156, Val MAE: 1.567455768585205\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1490/2000, Train Loss: 5.001568047955034, Val Loss: 5.692783300847453, Val MAE: 1.5674182176589966\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1491/2000, Train Loss: 5.0013882273272, Val Loss: 5.692858587073259, Val MAE: 1.5672399997711182\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1492/2000, Train Loss: 5.001207538426216, Val Loss: 5.692686670599363, Val MAE: 1.5676791667938232\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1493/2000, Train Loss: 5.001133389576986, Val Loss: 5.692777634396101, Val MAE: 1.567665457725525\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1494/2000, Train Loss: 5.000854470291916, Val Loss: 5.69260743433547, Val MAE: 1.5674623250961304\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1495/2000, Train Loss: 5.000741528424472, Val Loss: 5.692731564015788, Val MAE: 1.5674062967300415\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1496/2000, Train Loss: 5.000601478230357, Val Loss: 5.692641380365471, Val MAE: 1.567381739616394\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1497/2000, Train Loss: 5.0004403386461656, Val Loss: 5.692615835947364, Val MAE: 1.5673514604568481\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1498/2000, Train Loss: 5.00030211123843, Val Loss: 5.692656457333025, Val MAE: 1.5674470663070679\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1499/2000, Train Loss: 5.000159634912962, Val Loss: 5.692663786334729, Val MAE: 1.567421317100525\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1500/2000, Train Loss: 4.999901912818066, Val Loss: 5.6926147364512865, Val MAE: 1.567450761795044\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1501/2000, Train Loss: 4.999799727051297, Val Loss: 5.692636924052457, Val MAE: 1.567260503768921\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 1502/2000, Train Loss: 4.999577018931078, Val Loss: 5.692684553905365, Val MAE: 1.5673977136611938\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 1503/2000, Train Loss: 4.9993718953974895, Val Loss: 5.692648616496941, Val MAE: 1.5673177242279053\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 1504/2000, Train Loss: 4.999241924218776, Val Loss: 5.692641185875697, Val MAE: 1.5674335956573486\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 1505/2000, Train Loss: 4.999104333376901, Val Loss: 5.692704757145785, Val MAE: 1.5674018859863281\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 3.9587652683258057\n",
      "Test Mean Absolute Error (MAE): 1.243308655036487\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(CNNModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (conv1d): Conv1d(6, 64, kernel_size=(2,), stride=(1,))\n",
       "   (dense1): Linear(in_features=577, out_features=64, bias=True)\n",
       "   (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=577, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'test_mae': 1.243308655036487})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 444\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_cnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=6,\n",
    "                kernel_size=2,\n",
    "                num_filters=64,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = ['total_points', 'ict_index', 'clean_sheets', 'goals_conceded', 'bps', 'matchup_difficulty', 'goals_scored', 'assists', 'yellow_cards', 'red_cards'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpremier.cnn.experiment import gridsearch_cnn\n",
    "\n",
    "#gridsearch_cnn(epochs=100, verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized) — {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
