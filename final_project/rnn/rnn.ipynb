{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.rnn.model import build_train_rnn, full_rnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2502, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2502, 7)\n",
      "Shape of a given window (prior to preprocessing): (6, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[ 1.91043017e+00  1.20188568e+00  1.49675899e-01  7.44843842e-01\n",
      "  9.65822039e+00 -5.24454920e-02  0.00000000e+00  1.76782557e-03\n",
      "  2.06246317e-02  1.17855038e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.71102698  1.51262882  0.35675345  1.18885945 10.44252919  1.38395817\n",
      "  1.          0.04200834  0.14212409  0.03430979]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building rnn Architecture ======\n",
      "====== Done Building rnn Architecture ======\n",
      "Epoch 1/2000, Train Loss: 10.865554429711295, Val Loss: 11.979912449080292, Val MAE: 2.0610673427581787\n",
      "Epoch 2/2000, Train Loss: 10.85216928249845, Val Loss: 11.965742425226951, Val MAE: 2.0598785877227783\n",
      "Epoch 3/2000, Train Loss: 10.838874015063153, Val Loss: 11.951202344941118, Val MAE: 2.0585989952087402\n",
      "Epoch 4/2000, Train Loss: 10.82549961178677, Val Loss: 11.937231810065699, Val MAE: 2.057384729385376\n",
      "Epoch 5/2000, Train Loss: 10.812195685948392, Val Loss: 11.922926683770635, Val MAE: 2.0561418533325195\n",
      "Epoch 6/2000, Train Loss: 10.798655128076327, Val Loss: 11.90845801682989, Val MAE: 2.054840564727783\n",
      "Epoch 7/2000, Train Loss: 10.785251373811812, Val Loss: 11.893862017831944, Val MAE: 2.0535919666290283\n",
      "Epoch 8/2000, Train Loss: 10.771755941982928, Val Loss: 11.879715315678615, Val MAE: 2.0523269176483154\n",
      "Epoch 9/2000, Train Loss: 10.7583172487424, Val Loss: 11.865164279920656, Val MAE: 2.051039457321167\n",
      "Epoch 10/2000, Train Loss: 10.74484464100429, Val Loss: 11.850690254850445, Val MAE: 2.049778699874878\n",
      "Epoch 11/2000, Train Loss: 10.731290630686878, Val Loss: 11.836410045091188, Val MAE: 2.04852294921875\n",
      "Epoch 12/2000, Train Loss: 10.717803132945093, Val Loss: 11.821392734532191, Val MAE: 2.047163486480713\n",
      "Epoch 13/2000, Train Loss: 10.704118134017397, Val Loss: 11.806898068767667, Val MAE: 2.04585599899292\n",
      "Epoch 14/2000, Train Loss: 10.690445922278418, Val Loss: 11.791943049351671, Val MAE: 2.044508934020996\n",
      "Epoch 15/2000, Train Loss: 10.676721973539992, Val Loss: 11.777269695301623, Val MAE: 2.043196678161621\n",
      "Epoch 16/2000, Train Loss: 10.662950143941602, Val Loss: 11.76289004575894, Val MAE: 2.0419163703918457\n",
      "Epoch 17/2000, Train Loss: 10.649050231721517, Val Loss: 11.748012846654754, Val MAE: 2.0405871868133545\n",
      "Epoch 18/2000, Train Loss: 10.63526824706042, Val Loss: 11.732753225026897, Val MAE: 2.0392048358917236\n",
      "Epoch 19/2000, Train Loss: 10.62128453318457, Val Loss: 11.718217524281199, Val MAE: 2.0378780364990234\n",
      "Epoch 20/2000, Train Loss: 10.607375255694782, Val Loss: 11.703123096664314, Val MAE: 2.0364935398101807\n",
      "Epoch 21/2000, Train Loss: 10.59317742452749, Val Loss: 11.687672873172875, Val MAE: 2.035092353820801\n",
      "Epoch 22/2000, Train Loss: 10.57901938409557, Val Loss: 11.67249206598814, Val MAE: 2.033689260482788\n",
      "Epoch 23/2000, Train Loss: 10.564816777593732, Val Loss: 11.657172533509746, Val MAE: 2.0322799682617188\n",
      "Epoch 24/2000, Train Loss: 10.55048232914108, Val Loss: 11.641557920630166, Val MAE: 2.0308094024658203\n",
      "Epoch 25/2000, Train Loss: 10.53599309350805, Val Loss: 11.626221706748966, Val MAE: 2.029413938522339\n",
      "Epoch 26/2000, Train Loss: 10.521594865652643, Val Loss: 11.610628397522968, Val MAE: 2.027991533279419\n",
      "Epoch 27/2000, Train Loss: 10.507027843483396, Val Loss: 11.594588683271605, Val MAE: 2.0264878273010254\n",
      "Epoch 28/2000, Train Loss: 10.49225040425053, Val Loss: 11.578604391568115, Val MAE: 2.024979591369629\n",
      "Epoch 29/2000, Train Loss: 10.47746991876311, Val Loss: 11.562616442800115, Val MAE: 2.023484468460083\n",
      "Epoch 30/2000, Train Loss: 10.462587751191238, Val Loss: 11.546045679411073, Val MAE: 2.0219171047210693\n",
      "Epoch 31/2000, Train Loss: 10.44741462637722, Val Loss: 11.529880710572733, Val MAE: 2.020355463027954\n",
      "Epoch 32/2000, Train Loss: 10.43225416845205, Val Loss: 11.513203269992855, Val MAE: 2.0187551975250244\n",
      "Epoch 33/2000, Train Loss: 10.416863569652925, Val Loss: 11.496199192006472, Val MAE: 2.0171022415161133\n",
      "Epoch 34/2000, Train Loss: 10.40143322256734, Val Loss: 11.479794870515208, Val MAE: 2.0155446529388428\n",
      "Epoch 35/2000, Train Loss: 10.386019947661387, Val Loss: 11.462369571881375, Val MAE: 2.013824224472046\n",
      "Epoch 36/2000, Train Loss: 10.370465305806214, Val Loss: 11.445208414454891, Val MAE: 2.0121378898620605\n",
      "Epoch 37/2000, Train Loss: 10.354927721701399, Val Loss: 11.428435368298208, Val MAE: 2.0105273723602295\n",
      "Epoch 38/2000, Train Loss: 10.339302437813831, Val Loss: 11.411185062675528, Val MAE: 2.008814811706543\n",
      "Epoch 39/2000, Train Loss: 10.323612602389588, Val Loss: 11.393858917103083, Val MAE: 2.0071210861206055\n",
      "Epoch 40/2000, Train Loss: 10.307789070349525, Val Loss: 11.376161088808141, Val MAE: 2.0054068565368652\n",
      "Epoch 41/2000, Train Loss: 10.2917257694188, Val Loss: 11.358660698335743, Val MAE: 2.0036566257476807\n",
      "Epoch 42/2000, Train Loss: 10.275676699107503, Val Loss: 11.341074019157832, Val MAE: 2.001919746398926\n",
      "Epoch 43/2000, Train Loss: 10.25940176838305, Val Loss: 11.32273400498594, Val MAE: 2.0001018047332764\n",
      "Epoch 44/2000, Train Loss: 10.242779432426952, Val Loss: 11.304241473510269, Val MAE: 1.998254656791687\n",
      "Epoch 45/2000, Train Loss: 10.226179968547351, Val Loss: 11.285731531081533, Val MAE: 1.9963732957839966\n",
      "Epoch 46/2000, Train Loss: 10.209423289342634, Val Loss: 11.267562219343327, Val MAE: 1.9945546388626099\n",
      "Epoch 47/2000, Train Loss: 10.192562663664203, Val Loss: 11.249157741563309, Val MAE: 1.992693543434143\n",
      "Epoch 48/2000, Train Loss: 10.175675546174986, Val Loss: 11.230176997607408, Val MAE: 1.9907398223876953\n",
      "Epoch 49/2000, Train Loss: 10.158562969607756, Val Loss: 11.211610895419202, Val MAE: 1.988858699798584\n",
      "Epoch 50/2000, Train Loss: 10.141306497948342, Val Loss: 11.192048513646656, Val MAE: 1.9868197441101074\n",
      "Epoch 51/2000, Train Loss: 10.124025983763445, Val Loss: 11.172896080829819, Val MAE: 1.9848382472991943\n",
      "Epoch 52/2000, Train Loss: 10.106512825728974, Val Loss: 11.153121061981954, Val MAE: 1.9827550649642944\n",
      "Epoch 53/2000, Train Loss: 10.088875789024895, Val Loss: 11.134131902173754, Val MAE: 1.9807718992233276\n",
      "Epoch 54/2000, Train Loss: 10.071439457141036, Val Loss: 11.114529686356219, Val MAE: 1.9786944389343262\n",
      "Epoch 55/2000, Train Loss: 10.053751347855226, Val Loss: 11.094632493075702, Val MAE: 1.976617693901062\n",
      "Epoch 56/2000, Train Loss: 10.035798817097008, Val Loss: 11.07505483192816, Val MAE: 1.9745382070541382\n",
      "Epoch 57/2000, Train Loss: 10.01771886947707, Val Loss: 11.054431175488398, Val MAE: 1.972345232963562\n",
      "Epoch 58/2000, Train Loss: 9.999369839393783, Val Loss: 11.034109062848424, Val MAE: 1.9701778888702393\n",
      "Epoch 59/2000, Train Loss: 9.980874789759444, Val Loss: 11.01431517167991, Val MAE: 1.968061089515686\n",
      "Epoch 60/2000, Train Loss: 9.962635902683974, Val Loss: 10.993354444953097, Val MAE: 1.9658057689666748\n",
      "Epoch 61/2000, Train Loss: 9.943818723375239, Val Loss: 10.972670348167465, Val MAE: 1.963584542274475\n",
      "Epoch 62/2000, Train Loss: 9.925171699765531, Val Loss: 10.952064780786133, Val MAE: 1.9613473415374756\n",
      "Epoch 63/2000, Train Loss: 9.90634418758685, Val Loss: 10.931196793560678, Val MAE: 1.9590829610824585\n",
      "Epoch 64/2000, Train Loss: 9.887245087074948, Val Loss: 10.909836367581507, Val MAE: 1.9567326307296753\n",
      "Epoch 65/2000, Train Loss: 9.868168173836287, Val Loss: 10.8890098132626, Val MAE: 1.9544377326965332\n",
      "Epoch 66/2000, Train Loss: 9.849186722448726, Val Loss: 10.867823613460093, Val MAE: 1.95212721824646\n",
      "Epoch 67/2000, Train Loss: 9.830032055349438, Val Loss: 10.846263859090637, Val MAE: 1.949766755104065\n",
      "Epoch 68/2000, Train Loss: 9.810458548382753, Val Loss: 10.825011417076038, Val MAE: 1.9473810195922852\n",
      "Epoch 69/2000, Train Loss: 9.790935770044186, Val Loss: 10.803238862721017, Val MAE: 1.9449918270111084\n",
      "Epoch 70/2000, Train Loss: 9.771248840933362, Val Loss: 10.78080904296961, Val MAE: 1.9424725770950317\n",
      "Epoch 71/2000, Train Loss: 9.7513404526734, Val Loss: 10.758674055155629, Val MAE: 1.939975380897522\n",
      "Epoch 72/2000, Train Loss: 9.731217951442389, Val Loss: 10.73638233820415, Val MAE: 1.9374666213989258\n",
      "Epoch 73/2000, Train Loss: 9.710935545672337, Val Loss: 10.714470731384983, Val MAE: 1.9350160360336304\n",
      "Epoch 74/2000, Train Loss: 9.690808104260718, Val Loss: 10.691615988195672, Val MAE: 1.9324270486831665\n",
      "Epoch 75/2000, Train Loss: 9.670364558403127, Val Loss: 10.66853396509082, Val MAE: 1.9297969341278076\n",
      "Epoch 76/2000, Train Loss: 9.649712637308028, Val Loss: 10.645917941762038, Val MAE: 1.9272141456604004\n",
      "Epoch 77/2000, Train Loss: 9.629176813309499, Val Loss: 10.623321845090809, Val MAE: 1.9246456623077393\n",
      "Epoch 78/2000, Train Loss: 9.608564889909179, Val Loss: 10.599840352397173, Val MAE: 1.9219603538513184\n",
      "Epoch 79/2000, Train Loss: 9.587572797765873, Val Loss: 10.576925107463055, Val MAE: 1.919339895248413\n",
      "Epoch 80/2000, Train Loss: 9.566751286817386, Val Loss: 10.553587647639102, Val MAE: 1.9166446924209595\n",
      "Epoch 81/2000, Train Loss: 9.545504679401352, Val Loss: 10.530069538782314, Val MAE: 1.9139277935028076\n",
      "Epoch 82/2000, Train Loss: 9.524193148307612, Val Loss: 10.50599584672657, Val MAE: 1.9111061096191406\n",
      "Epoch 83/2000, Train Loss: 9.502608818709222, Val Loss: 10.481752870131615, Val MAE: 1.908281683921814\n",
      "Epoch 84/2000, Train Loss: 9.481067595726834, Val Loss: 10.457582750463066, Val MAE: 1.905462384223938\n",
      "Epoch 85/2000, Train Loss: 9.45930552633609, Val Loss: 10.433904764985819, Val MAE: 1.9026628732681274\n",
      "Epoch 86/2000, Train Loss: 9.437647691333403, Val Loss: 10.409008756758423, Val MAE: 1.899779200553894\n",
      "Epoch 87/2000, Train Loss: 9.415477679532485, Val Loss: 10.384433885552102, Val MAE: 1.896868109703064\n",
      "Epoch 88/2000, Train Loss: 9.393344794588137, Val Loss: 10.359826839321068, Val MAE: 1.8939272165298462\n",
      "Epoch 89/2000, Train Loss: 9.371290209593695, Val Loss: 10.334903849385656, Val MAE: 1.890936017036438\n",
      "Epoch 90/2000, Train Loss: 9.349143941665853, Val Loss: 10.310432073423804, Val MAE: 1.8880325555801392\n",
      "Epoch 91/2000, Train Loss: 9.326797926954448, Val Loss: 10.285215634602016, Val MAE: 1.8850116729736328\n",
      "Epoch 92/2000, Train Loss: 9.304367315760807, Val Loss: 10.259969983249903, Val MAE: 1.881953477859497\n",
      "Epoch 93/2000, Train Loss: 9.281870327224034, Val Loss: 10.235302840738305, Val MAE: 1.878986120223999\n",
      "Epoch 94/2000, Train Loss: 9.259299099403398, Val Loss: 10.209876240621284, Val MAE: 1.8759269714355469\n",
      "Epoch 95/2000, Train Loss: 9.236739884393472, Val Loss: 10.18387730582546, Val MAE: 1.8727302551269531\n",
      "Epoch 96/2000, Train Loss: 9.21396692666598, Val Loss: 10.158471550678533, Val MAE: 1.8696088790893555\n",
      "Epoch 97/2000, Train Loss: 9.190851699458639, Val Loss: 10.133032260468338, Val MAE: 1.8664746284484863\n",
      "Epoch 98/2000, Train Loss: 9.168029540529728, Val Loss: 10.106911510377211, Val MAE: 1.8632501363754272\n",
      "Epoch 99/2000, Train Loss: 9.144860067307151, Val Loss: 10.081167758263032, Val MAE: 1.860132098197937\n",
      "Epoch 100/2000, Train Loss: 9.121576314237569, Val Loss: 10.055650722531642, Val MAE: 1.8569616079330444\n",
      "Epoch 101/2000, Train Loss: 9.098397305278523, Val Loss: 10.029210578673021, Val MAE: 1.8536690473556519\n",
      "Epoch 102/2000, Train Loss: 9.07472217124923, Val Loss: 10.003193543113152, Val MAE: 1.8504186868667603\n",
      "Epoch 103/2000, Train Loss: 9.05104907074082, Val Loss: 9.976053082523725, Val MAE: 1.847000002861023\n",
      "Epoch 104/2000, Train Loss: 9.027129378644284, Val Loss: 9.949146946681385, Val MAE: 1.8435790538787842\n",
      "Epoch 105/2000, Train Loss: 9.003162924963181, Val Loss: 9.921746374703146, Val MAE: 1.8401166200637817\n",
      "Epoch 106/2000, Train Loss: 8.978973480280985, Val Loss: 9.895847476790987, Val MAE: 1.8368916511535645\n",
      "Epoch 107/2000, Train Loss: 8.955076818982956, Val Loss: 9.86891207472629, Val MAE: 1.8334404230117798\n",
      "Epoch 108/2000, Train Loss: 8.930837566126073, Val Loss: 9.842226834494952, Val MAE: 1.830041766166687\n",
      "Epoch 109/2000, Train Loss: 8.906590933534648, Val Loss: 9.814611784339133, Val MAE: 1.8264585733413696\n",
      "Epoch 110/2000, Train Loss: 8.882165704790259, Val Loss: 9.78746011014743, Val MAE: 1.8229954242706299\n",
      "Epoch 111/2000, Train Loss: 8.85768950044228, Val Loss: 9.759469537377722, Val MAE: 1.8193540573120117\n",
      "Epoch 112/2000, Train Loss: 8.832864917054469, Val Loss: 9.732041113625426, Val MAE: 1.8157943487167358\n",
      "Epoch 113/2000, Train Loss: 8.808055291118796, Val Loss: 9.704693643962935, Val MAE: 1.8121838569641113\n",
      "Epoch 114/2000, Train Loss: 8.783090751830496, Val Loss: 9.67583941791309, Val MAE: 1.8084208965301514\n",
      "Epoch 115/2000, Train Loss: 8.75804652436194, Val Loss: 9.647724458771197, Val MAE: 1.8047586679458618\n",
      "Epoch 116/2000, Train Loss: 8.733054187916938, Val Loss: 9.619374940086395, Val MAE: 1.8010890483856201\n",
      "Epoch 117/2000, Train Loss: 8.707786335565942, Val Loss: 9.591615543407402, Val MAE: 1.797539472579956\n",
      "Epoch 118/2000, Train Loss: 8.682623250479432, Val Loss: 9.563366036189988, Val MAE: 1.7938522100448608\n",
      "Epoch 119/2000, Train Loss: 8.657040524868627, Val Loss: 9.533734210951977, Val MAE: 1.7900594472885132\n",
      "Epoch 120/2000, Train Loss: 8.631043308641942, Val Loss: 9.505689918744272, Val MAE: 1.7864829301834106\n",
      "Epoch 121/2000, Train Loss: 8.605786286635269, Val Loss: 9.47587374745888, Val MAE: 1.7826011180877686\n",
      "Epoch 122/2000, Train Loss: 8.580048259759268, Val Loss: 9.44817411425646, Val MAE: 1.7790883779525757\n",
      "Epoch 123/2000, Train Loss: 8.554737352133635, Val Loss: 9.419147443880728, Val MAE: 1.775364875793457\n",
      "Epoch 124/2000, Train Loss: 8.52904403251632, Val Loss: 9.389892052938814, Val MAE: 1.771780252456665\n",
      "Epoch 125/2000, Train Loss: 8.502854758631084, Val Loss: 9.361543219616289, Val MAE: 1.7682521343231201\n",
      "Epoch 126/2000, Train Loss: 8.477123380084846, Val Loss: 9.331736244215696, Val MAE: 1.7645936012268066\n",
      "Epoch 127/2000, Train Loss: 8.451263230422448, Val Loss: 9.303189107470374, Val MAE: 1.7610304355621338\n",
      "Epoch 128/2000, Train Loss: 8.425596814689126, Val Loss: 9.273548581822567, Val MAE: 1.7573325634002686\n",
      "Epoch 129/2000, Train Loss: 8.399493278541001, Val Loss: 9.24460623925979, Val MAE: 1.753699779510498\n",
      "Epoch 130/2000, Train Loss: 8.373628384122371, Val Loss: 9.215468607842922, Val MAE: 1.7500441074371338\n",
      "Epoch 131/2000, Train Loss: 8.34736841887676, Val Loss: 9.185158500726981, Val MAE: 1.7462847232818604\n",
      "Epoch 132/2000, Train Loss: 8.320669507074323, Val Loss: 9.155858521204475, Val MAE: 1.7426780462265015\n",
      "Epoch 133/2000, Train Loss: 8.294707528476057, Val Loss: 9.125233649501196, Val MAE: 1.738937497138977\n",
      "Epoch 134/2000, Train Loss: 8.267912248924196, Val Loss: 9.09567902581225, Val MAE: 1.7353463172912598\n",
      "Epoch 135/2000, Train Loss: 8.241804959822675, Val Loss: 9.066127501177496, Val MAE: 1.7318048477172852\n",
      "Epoch 136/2000, Train Loss: 8.215726571213938, Val Loss: 9.035950785337603, Val MAE: 1.7281579971313477\n",
      "Epoch 137/2000, Train Loss: 8.1892088227168, Val Loss: 9.006760905368612, Val MAE: 1.7247885465621948\n",
      "Epoch 138/2000, Train Loss: 8.163070021675642, Val Loss: 8.977280081822238, Val MAE: 1.7213784456253052\n",
      "Epoch 139/2000, Train Loss: 8.137058661740737, Val Loss: 8.946698830055716, Val MAE: 1.717835545539856\n",
      "Epoch 140/2000, Train Loss: 8.110406813866483, Val Loss: 8.917702910253944, Val MAE: 1.714479923248291\n",
      "Epoch 141/2000, Train Loss: 8.084184839556034, Val Loss: 8.887610273486978, Val MAE: 1.7111001014709473\n",
      "Epoch 142/2000, Train Loss: 8.057588463849706, Val Loss: 8.85825990541448, Val MAE: 1.7077950239181519\n",
      "Epoch 143/2000, Train Loss: 8.031276325706358, Val Loss: 8.828066403393716, Val MAE: 1.7044453620910645\n",
      "Epoch 144/2000, Train Loss: 8.004527197347235, Val Loss: 8.798405425719894, Val MAE: 1.7010986804962158\n",
      "Epoch 145/2000, Train Loss: 7.977652770545095, Val Loss: 8.767666839063168, Val MAE: 1.697718858718872\n",
      "Epoch 146/2000, Train Loss: 7.950949147207313, Val Loss: 8.737861440223655, Val MAE: 1.6945210695266724\n",
      "Epoch 147/2000, Train Loss: 7.9247551165694174, Val Loss: 8.707491645230613, Val MAE: 1.6911888122558594\n",
      "Epoch 148/2000, Train Loss: 7.898271987171428, Val Loss: 8.678123998031339, Val MAE: 1.688090205192566\n",
      "Epoch 149/2000, Train Loss: 7.87206465890926, Val Loss: 8.647810123860836, Val MAE: 1.6847903728485107\n",
      "Epoch 150/2000, Train Loss: 7.8452073851578, Val Loss: 8.618253269759705, Val MAE: 1.6816977262496948\n",
      "Epoch 151/2000, Train Loss: 7.818837032277847, Val Loss: 8.588340757226725, Val MAE: 1.6785321235656738\n",
      "Epoch 152/2000, Train Loss: 7.792370517685076, Val Loss: 8.558225794191207, Val MAE: 1.675336241722107\n",
      "Epoch 153/2000, Train Loss: 7.765645738892955, Val Loss: 8.52803578701679, Val MAE: 1.6721653938293457\n",
      "Epoch 154/2000, Train Loss: 7.739312685181942, Val Loss: 8.497625310554964, Val MAE: 1.668965458869934\n",
      "Epoch 155/2000, Train Loss: 7.712832348183961, Val Loss: 8.468543987664242, Val MAE: 1.6659331321716309\n",
      "Epoch 156/2000, Train Loss: 7.686523405956937, Val Loss: 8.438272698733966, Val MAE: 1.6628494262695312\n",
      "Epoch 157/2000, Train Loss: 7.659895976440408, Val Loss: 8.408512322044153, Val MAE: 1.6597881317138672\n",
      "Epoch 158/2000, Train Loss: 7.633606967751532, Val Loss: 8.379187427347224, Val MAE: 1.656755805015564\n",
      "Epoch 159/2000, Train Loss: 7.607435933139272, Val Loss: 8.348192521715566, Val MAE: 1.6535117626190186\n",
      "Epoch 160/2000, Train Loss: 7.580973293309812, Val Loss: 8.31892328900755, Val MAE: 1.6504579782485962\n",
      "Epoch 161/2000, Train Loss: 7.554937342537028, Val Loss: 8.28926901125124, Val MAE: 1.6473900079727173\n",
      "Epoch 162/2000, Train Loss: 7.528603079283766, Val Loss: 8.258778568573684, Val MAE: 1.644234299659729\n",
      "Epoch 163/2000, Train Loss: 7.502378351666573, Val Loss: 8.229229977120864, Val MAE: 1.6411209106445312\n",
      "Epoch 164/2000, Train Loss: 7.476214507530475, Val Loss: 8.200068498787894, Val MAE: 1.6380720138549805\n",
      "Epoch 165/2000, Train Loss: 7.450459450755968, Val Loss: 8.170371402704388, Val MAE: 1.634968876838684\n",
      "Epoch 166/2000, Train Loss: 7.424034664280225, Val Loss: 8.141197090666592, Val MAE: 1.6318942308425903\n",
      "Epoch 167/2000, Train Loss: 7.3983003145200446, Val Loss: 8.112061779160019, Val MAE: 1.6287785768508911\n",
      "Epoch 168/2000, Train Loss: 7.372603806368823, Val Loss: 8.083032546513671, Val MAE: 1.6257025003433228\n",
      "Epoch 169/2000, Train Loss: 7.346978850972056, Val Loss: 8.053295018293806, Val MAE: 1.6225024461746216\n",
      "Epoch 170/2000, Train Loss: 7.321133452515465, Val Loss: 8.024398654836034, Val MAE: 1.6194450855255127\n",
      "Epoch 171/2000, Train Loss: 7.295656770656513, Val Loss: 7.994911068486511, Val MAE: 1.6162008047103882\n",
      "Epoch 172/2000, Train Loss: 7.269780846232683, Val Loss: 7.965494539850713, Val MAE: 1.6130132675170898\n",
      "Epoch 173/2000, Train Loss: 7.244215341121021, Val Loss: 7.9363495485133715, Val MAE: 1.6097853183746338\n",
      "Epoch 174/2000, Train Loss: 7.218603897900081, Val Loss: 7.9069057856131035, Val MAE: 1.6064811944961548\n",
      "Epoch 175/2000, Train Loss: 7.192826611655099, Val Loss: 7.87796383549314, Val MAE: 1.6032730340957642\n",
      "Epoch 176/2000, Train Loss: 7.167516139298573, Val Loss: 7.848853621369837, Val MAE: 1.6000831127166748\n",
      "Epoch 177/2000, Train Loss: 7.142607918010473, Val Loss: 7.820288576117349, Val MAE: 1.5969525575637817\n",
      "Epoch 178/2000, Train Loss: 7.117660162735791, Val Loss: 7.792780107587849, Val MAE: 1.5939252376556396\n",
      "Epoch 179/2000, Train Loss: 7.093038556275445, Val Loss: 7.763767559867387, Val MAE: 1.590761661529541\n",
      "Epoch 180/2000, Train Loss: 7.068393558284751, Val Loss: 7.734976867561311, Val MAE: 1.5875656604766846\n",
      "Epoch 181/2000, Train Loss: 7.043460586555905, Val Loss: 7.706953234036399, Val MAE: 1.5845439434051514\n",
      "Epoch 182/2000, Train Loss: 7.018933277747566, Val Loss: 7.679218929933116, Val MAE: 1.5814406871795654\n",
      "Epoch 183/2000, Train Loss: 6.994334132297202, Val Loss: 7.651090300547967, Val MAE: 1.5782349109649658\n",
      "Epoch 184/2000, Train Loss: 6.970029637647464, Val Loss: 7.623485065882724, Val MAE: 1.5751374959945679\n",
      "Epoch 185/2000, Train Loss: 6.945853463534651, Val Loss: 7.5953972735171655, Val MAE: 1.5721592903137207\n",
      "Epoch 186/2000, Train Loss: 6.9215688973895935, Val Loss: 7.5684064151405925, Val MAE: 1.569183588027954\n",
      "Epoch 187/2000, Train Loss: 6.897616458727724, Val Loss: 7.540586282126036, Val MAE: 1.5661094188690186\n",
      "Epoch 188/2000, Train Loss: 6.873908654306239, Val Loss: 7.5125959691469095, Val MAE: 1.5631492137908936\n",
      "Epoch 189/2000, Train Loss: 6.849894975961555, Val Loss: 7.486004947704642, Val MAE: 1.5602905750274658\n",
      "Epoch 190/2000, Train Loss: 6.82622625121426, Val Loss: 7.45885814639771, Val MAE: 1.557375431060791\n",
      "Epoch 191/2000, Train Loss: 6.802485405937372, Val Loss: 7.431531894279912, Val MAE: 1.5546826124191284\n",
      "Epoch 192/2000, Train Loss: 6.779136847644352, Val Loss: 7.404518220266071, Val MAE: 1.5519788265228271\n",
      "Epoch 193/2000, Train Loss: 6.756208395303596, Val Loss: 7.37794885526921, Val MAE: 1.5493804216384888\n",
      "Epoch 194/2000, Train Loss: 6.733183563300877, Val Loss: 7.3515961695610565, Val MAE: 1.5468266010284424\n",
      "Epoch 195/2000, Train Loss: 6.710713791478108, Val Loss: 7.325565335127192, Val MAE: 1.544286847114563\n",
      "Epoch 196/2000, Train Loss: 6.6883714492685105, Val Loss: 7.300671649726524, Val MAE: 1.541899561882019\n",
      "Epoch 197/2000, Train Loss: 6.66616021135506, Val Loss: 7.274098331365017, Val MAE: 1.5394216775894165\n",
      "Epoch 198/2000, Train Loss: 6.643927591942633, Val Loss: 7.247897373518083, Val MAE: 1.5372084379196167\n",
      "Epoch 199/2000, Train Loss: 6.621595870601216, Val Loss: 7.222802445155765, Val MAE: 1.5351474285125732\n",
      "Epoch 200/2000, Train Loss: 6.599695436053508, Val Loss: 7.19727924279299, Val MAE: 1.5332238674163818\n",
      "Epoch 201/2000, Train Loss: 6.578233992021575, Val Loss: 7.1732493811244264, Val MAE: 1.5314581394195557\n",
      "Epoch 202/2000, Train Loss: 6.55683486248556, Val Loss: 7.148076155180231, Val MAE: 1.5295863151550293\n",
      "Epoch 203/2000, Train Loss: 6.535498975113527, Val Loss: 7.122385859671718, Val MAE: 1.5278902053833008\n",
      "Epoch 204/2000, Train Loss: 6.514407093469565, Val Loss: 7.097918122687836, Val MAE: 1.526424527168274\n",
      "Epoch 205/2000, Train Loss: 6.493286501388161, Val Loss: 7.074602790473069, Val MAE: 1.5251874923706055\n",
      "Epoch 206/2000, Train Loss: 6.473110820104504, Val Loss: 7.050885999011337, Val MAE: 1.523774266242981\n",
      "Epoch 207/2000, Train Loss: 6.452540789080035, Val Loss: 7.027470818626771, Val MAE: 1.5225034952163696\n",
      "Epoch 208/2000, Train Loss: 6.432484240199712, Val Loss: 7.004010475693493, Val MAE: 1.521337628364563\n",
      "Epoch 209/2000, Train Loss: 6.412413548114516, Val Loss: 6.980357969162661, Val MAE: 1.5201648473739624\n",
      "Epoch 210/2000, Train Loss: 6.392382079828465, Val Loss: 6.956995698107857, Val MAE: 1.5191293954849243\n",
      "Epoch 211/2000, Train Loss: 6.3729932018772635, Val Loss: 6.933900393522843, Val MAE: 1.5181164741516113\n",
      "Epoch 212/2000, Train Loss: 6.3535882760570725, Val Loss: 6.912386714227338, Val MAE: 1.5172020196914673\n",
      "Epoch 213/2000, Train Loss: 6.334401535702624, Val Loss: 6.889651997328169, Val MAE: 1.5161852836608887\n",
      "Epoch 214/2000, Train Loss: 6.31555468527722, Val Loss: 6.866891707256664, Val MAE: 1.5152866840362549\n",
      "Epoch 215/2000, Train Loss: 6.296703130237491, Val Loss: 6.845092462427026, Val MAE: 1.5146167278289795\n",
      "Epoch 216/2000, Train Loss: 6.27838238656395, Val Loss: 6.823709646222788, Val MAE: 1.514000415802002\n",
      "Epoch 217/2000, Train Loss: 6.260330356400589, Val Loss: 6.802868321948095, Val MAE: 1.5135297775268555\n",
      "Epoch 218/2000, Train Loss: 6.242596645241132, Val Loss: 6.781678700574677, Val MAE: 1.5130915641784668\n",
      "Epoch 219/2000, Train Loss: 6.2253754402784125, Val Loss: 6.7604594292170415, Val MAE: 1.5127763748168945\n",
      "Epoch 220/2000, Train Loss: 6.207791857950961, Val Loss: 6.742041075493947, Val MAE: 1.5124708414077759\n",
      "Epoch 221/2000, Train Loss: 6.191211851925853, Val Loss: 6.719280746961952, Val MAE: 1.5121577978134155\n",
      "Epoch 222/2000, Train Loss: 6.173627089648747, Val Loss: 6.700870119377014, Val MAE: 1.511890172958374\n",
      "Epoch 223/2000, Train Loss: 6.156999575932373, Val Loss: 6.680818587040318, Val MAE: 1.5117461681365967\n",
      "Epoch 224/2000, Train Loss: 6.140498856605567, Val Loss: 6.661265284278707, Val MAE: 1.5117696523666382\n",
      "Epoch 225/2000, Train Loss: 6.124025563972253, Val Loss: 6.642060029761871, Val MAE: 1.5117233991622925\n",
      "Epoch 226/2000, Train Loss: 6.108483816350874, Val Loss: 6.622708784604291, Val MAE: 1.5119036436080933\n",
      "Epoch 227/2000, Train Loss: 6.0927056729332145, Val Loss: 6.604595936766458, Val MAE: 1.5120795965194702\n",
      "Epoch 228/2000, Train Loss: 6.076984705512244, Val Loss: 6.585831880934013, Val MAE: 1.512305736541748\n",
      "Epoch 229/2000, Train Loss: 6.06162178004652, Val Loss: 6.567439015259801, Val MAE: 1.5126229524612427\n",
      "Epoch 230/2000, Train Loss: 6.046430471998801, Val Loss: 6.549593805856661, Val MAE: 1.5129177570343018\n",
      "Epoch 231/2000, Train Loss: 6.031708302169011, Val Loss: 6.532022197494449, Val MAE: 1.513296365737915\n",
      "Epoch 232/2000, Train Loss: 6.017412772990716, Val Loss: 6.514697938018253, Val MAE: 1.513600468635559\n",
      "Epoch 233/2000, Train Loss: 6.003049394599491, Val Loss: 6.497725818864431, Val MAE: 1.514001727104187\n",
      "Epoch 234/2000, Train Loss: 5.989393891204335, Val Loss: 6.480534808715913, Val MAE: 1.5142472982406616\n",
      "Epoch 235/2000, Train Loss: 5.975651078623504, Val Loss: 6.464683686784648, Val MAE: 1.5147124528884888\n",
      "Epoch 236/2000, Train Loss: 5.961638594411948, Val Loss: 6.4474706993828494, Val MAE: 1.5150734186172485\n",
      "Epoch 237/2000, Train Loss: 5.948147467707849, Val Loss: 6.43106893529768, Val MAE: 1.5156714916229248\n",
      "Epoch 238/2000, Train Loss: 5.934532817864737, Val Loss: 6.4147288247441665, Val MAE: 1.5160640478134155\n",
      "Epoch 239/2000, Train Loss: 5.921818168795839, Val Loss: 6.3987429458704925, Val MAE: 1.5166538953781128\n",
      "Epoch 240/2000, Train Loss: 5.908934171060539, Val Loss: 6.383734079858214, Val MAE: 1.517277479171753\n",
      "Epoch 241/2000, Train Loss: 5.896618476474394, Val Loss: 6.368447796922941, Val MAE: 1.517990231513977\n",
      "Epoch 242/2000, Train Loss: 5.884503591740157, Val Loss: 6.354088527073554, Val MAE: 1.5185810327529907\n",
      "Epoch 243/2000, Train Loss: 5.872394110768886, Val Loss: 6.339496587850268, Val MAE: 1.5192172527313232\n",
      "Epoch 244/2000, Train Loss: 5.860300736558178, Val Loss: 6.324454247586953, Val MAE: 1.5199036598205566\n",
      "Epoch 245/2000, Train Loss: 5.848306728030157, Val Loss: 6.310357840053896, Val MAE: 1.5205824375152588\n",
      "Epoch 246/2000, Train Loss: 5.837247773344294, Val Loss: 6.296447381149374, Val MAE: 1.5212478637695312\n",
      "Epoch 247/2000, Train Loss: 5.826310340574642, Val Loss: 6.283166130748364, Val MAE: 1.521995186805725\n",
      "Epoch 248/2000, Train Loss: 5.815766750382001, Val Loss: 6.269657055810321, Val MAE: 1.5227611064910889\n",
      "Epoch 249/2000, Train Loss: 5.8053334985796115, Val Loss: 6.2568516753135475, Val MAE: 1.5233759880065918\n",
      "Epoch 250/2000, Train Loss: 5.795202421353452, Val Loss: 6.244081354195918, Val MAE: 1.5241434574127197\n",
      "Epoch 251/2000, Train Loss: 5.785221705715225, Val Loss: 6.231739606605757, Val MAE: 1.5249006748199463\n",
      "Epoch 252/2000, Train Loss: 5.775408002543332, Val Loss: 6.219714936131002, Val MAE: 1.5255768299102783\n",
      "Epoch 253/2000, Train Loss: 5.76595824105399, Val Loss: 6.207405922609731, Val MAE: 1.5264356136322021\n",
      "Epoch 254/2000, Train Loss: 5.756492437568037, Val Loss: 6.1960356525324904, Val MAE: 1.527129888534546\n",
      "Epoch 255/2000, Train Loss: 5.747416073717256, Val Loss: 6.185145202670987, Val MAE: 1.5278656482696533\n",
      "Epoch 256/2000, Train Loss: 5.73882988000906, Val Loss: 6.173607924480321, Val MAE: 1.5286158323287964\n",
      "Epoch 257/2000, Train Loss: 5.730318397640091, Val Loss: 6.162906069821174, Val MAE: 1.5295382738113403\n",
      "Epoch 258/2000, Train Loss: 5.721818261210303, Val Loss: 6.152712493952626, Val MAE: 1.5303624868392944\n",
      "Epoch 259/2000, Train Loss: 5.713774866239023, Val Loss: 6.141914933646491, Val MAE: 1.5313163995742798\n",
      "Epoch 260/2000, Train Loss: 5.705811897186559, Val Loss: 6.131999093275916, Val MAE: 1.5321779251098633\n",
      "Epoch 261/2000, Train Loss: 5.698257274480037, Val Loss: 6.122223958054085, Val MAE: 1.5332510471343994\n",
      "Epoch 262/2000, Train Loss: 5.690389016662828, Val Loss: 6.1129273579937236, Val MAE: 1.5343340635299683\n",
      "Epoch 263/2000, Train Loss: 5.683264686052938, Val Loss: 6.103099406130088, Val MAE: 1.5355160236358643\n",
      "Epoch 264/2000, Train Loss: 5.676008916421309, Val Loss: 6.093868269012608, Val MAE: 1.536651611328125\n",
      "Epoch 265/2000, Train Loss: 5.669019410846772, Val Loss: 6.0852622630399305, Val MAE: 1.537874460220337\n",
      "Epoch 266/2000, Train Loss: 5.662341789659021, Val Loss: 6.076678063709072, Val MAE: 1.5390902757644653\n",
      "Epoch 267/2000, Train Loss: 5.655865032013498, Val Loss: 6.067151230956436, Val MAE: 1.540289044380188\n",
      "Epoch 268/2000, Train Loss: 5.6493497891463, Val Loss: 6.060022442621558, Val MAE: 1.5416433811187744\n",
      "Epoch 269/2000, Train Loss: 5.643381690240762, Val Loss: 6.051450527498117, Val MAE: 1.5428420305252075\n",
      "Epoch 270/2000, Train Loss: 5.637308988497343, Val Loss: 6.044339336386514, Val MAE: 1.5439389944076538\n",
      "Epoch 271/2000, Train Loss: 5.6317378426839735, Val Loss: 6.036392682462657, Val MAE: 1.5452862977981567\n",
      "Epoch 272/2000, Train Loss: 5.626179791100842, Val Loss: 6.029448763310727, Val MAE: 1.5463743209838867\n",
      "Epoch 273/2000, Train Loss: 5.62079706953748, Val Loss: 6.022588357830631, Val MAE: 1.5475854873657227\n",
      "Epoch 274/2000, Train Loss: 5.615608061979054, Val Loss: 6.014775772251485, Val MAE: 1.5488262176513672\n",
      "Epoch 275/2000, Train Loss: 5.61051708707333, Val Loss: 6.0083131268848335, Val MAE: 1.5499542951583862\n",
      "Epoch 276/2000, Train Loss: 5.6055798560779415, Val Loss: 6.001998173169769, Val MAE: 1.5511815547943115\n",
      "Epoch 277/2000, Train Loss: 5.601056534081257, Val Loss: 5.994477281512106, Val MAE: 1.5530942678451538\n",
      "Epoch 278/2000, Train Loss: 5.596088737941476, Val Loss: 5.989061507701145, Val MAE: 1.5540980100631714\n",
      "Epoch 279/2000, Train Loss: 5.591775304913773, Val Loss: 5.983396390709308, Val MAE: 1.5554084777832031\n",
      "Epoch 280/2000, Train Loss: 5.587516585174872, Val Loss: 5.977894431133882, Val MAE: 1.55644690990448\n",
      "Epoch 281/2000, Train Loss: 5.583390882533675, Val Loss: 5.971852383117793, Val MAE: 1.55787193775177\n",
      "Epoch 282/2000, Train Loss: 5.579362927315354, Val Loss: 5.965824904882944, Val MAE: 1.5594812631607056\n",
      "Epoch 283/2000, Train Loss: 5.575360216950129, Val Loss: 5.960327730630881, Val MAE: 1.560613751411438\n",
      "Epoch 284/2000, Train Loss: 5.571672820273124, Val Loss: 5.954849466271357, Val MAE: 1.5619521141052246\n",
      "Epoch 285/2000, Train Loss: 5.568077658197563, Val Loss: 5.950502864446844, Val MAE: 1.5629850625991821\n",
      "Epoch 286/2000, Train Loss: 5.5646222968239085, Val Loss: 5.945001468928218, Val MAE: 1.5641485452651978\n",
      "Epoch 287/2000, Train Loss: 5.561137158462986, Val Loss: 5.940613823199491, Val MAE: 1.565406084060669\n",
      "Epoch 288/2000, Train Loss: 5.557900119381501, Val Loss: 5.936020113003728, Val MAE: 1.5667266845703125\n",
      "Epoch 289/2000, Train Loss: 5.554760473534559, Val Loss: 5.931501134546525, Val MAE: 1.5677123069763184\n",
      "Epoch 290/2000, Train Loss: 5.551630500669969, Val Loss: 5.927405620386841, Val MAE: 1.5688796043395996\n",
      "Epoch 291/2000, Train Loss: 5.548799218802617, Val Loss: 5.922554856137762, Val MAE: 1.5701258182525635\n",
      "Epoch 292/2000, Train Loss: 5.545838320532121, Val Loss: 5.918479117231631, Val MAE: 1.5713670253753662\n",
      "Epoch 293/2000, Train Loss: 5.543227997738907, Val Loss: 5.915289234495309, Val MAE: 1.5721540451049805\n",
      "Epoch 294/2000, Train Loss: 5.540799985210464, Val Loss: 5.911032592849264, Val MAE: 1.5737203359603882\n",
      "Epoch 295/2000, Train Loss: 5.5381588450961345, Val Loss: 5.907657745781295, Val MAE: 1.574505090713501\n",
      "Epoch 296/2000, Train Loss: 5.53586536658473, Val Loss: 5.904263731536515, Val MAE: 1.5756301879882812\n",
      "Epoch 297/2000, Train Loss: 5.533598954538323, Val Loss: 5.900878106417641, Val MAE: 1.576617956161499\n",
      "Epoch 298/2000, Train Loss: 5.531435604753165, Val Loss: 5.897617181689003, Val MAE: 1.5775898694992065\n",
      "Epoch 299/2000, Train Loss: 5.529564063117841, Val Loss: 5.8942368188217875, Val MAE: 1.5789378881454468\n",
      "Epoch 300/2000, Train Loss: 5.527460088870796, Val Loss: 5.891745576526776, Val MAE: 1.5796350240707397\n",
      "Epoch 301/2000, Train Loss: 5.525644702361051, Val Loss: 5.889180496198322, Val MAE: 1.5803446769714355\n",
      "Epoch 302/2000, Train Loss: 5.523689201564372, Val Loss: 5.885927650359792, Val MAE: 1.5815376043319702\n",
      "Epoch 303/2000, Train Loss: 5.521762659648704, Val Loss: 5.883321364720662, Val MAE: 1.5824694633483887\n",
      "Epoch 304/2000, Train Loss: 5.520094665148827, Val Loss: 5.880309708439246, Val MAE: 1.583473563194275\n",
      "Epoch 305/2000, Train Loss: 5.5183962047477575, Val Loss: 5.878470592998219, Val MAE: 1.5842159986495972\n",
      "Epoch 306/2000, Train Loss: 5.516806253658727, Val Loss: 5.875510141845872, Val MAE: 1.5851863622665405\n",
      "Epoch 307/2000, Train Loss: 5.515296614564363, Val Loss: 5.8733595594171355, Val MAE: 1.5858590602874756\n",
      "Epoch 308/2000, Train Loss: 5.513766580782333, Val Loss: 5.871305811569231, Val MAE: 1.5866752862930298\n",
      "Epoch 309/2000, Train Loss: 5.512334584350908, Val Loss: 5.868502951452127, Val MAE: 1.5875422954559326\n",
      "Epoch 310/2000, Train Loss: 5.511096736899906, Val Loss: 5.866912145406828, Val MAE: 1.5881990194320679\n",
      "Epoch 311/2000, Train Loss: 5.509719699306609, Val Loss: 5.864551995416664, Val MAE: 1.5890460014343262\n",
      "Epoch 312/2000, Train Loss: 5.50854343733764, Val Loss: 5.862466127136067, Val MAE: 1.5897510051727295\n",
      "Epoch 313/2000, Train Loss: 5.5071806577122775, Val Loss: 5.860701215832241, Val MAE: 1.5903042554855347\n",
      "Epoch 314/2000, Train Loss: 5.506000480880039, Val Loss: 5.858975939612141, Val MAE: 1.5908035039901733\n",
      "Epoch 315/2000, Train Loss: 5.504776742574113, Val Loss: 5.856508465990743, Val MAE: 1.592005968093872\n",
      "Epoch 316/2000, Train Loss: 5.503804123628148, Val Loss: 5.854422958072172, Val MAE: 1.592761516571045\n",
      "Epoch 317/2000, Train Loss: 5.502475460677982, Val Loss: 5.852840042168941, Val MAE: 1.5934494733810425\n",
      "Epoch 318/2000, Train Loss: 5.501296767544193, Val Loss: 5.85105514735986, Val MAE: 1.5941135883331299\n",
      "Epoch 319/2000, Train Loss: 5.500372469047691, Val Loss: 5.849153151388197, Val MAE: 1.5949571132659912\n",
      "Epoch 320/2000, Train Loss: 5.499399259675983, Val Loss: 5.848284330663331, Val MAE: 1.595131278038025\n",
      "Epoch 321/2000, Train Loss: 5.498460422008495, Val Loss: 5.846556062545251, Val MAE: 1.5959006547927856\n",
      "Epoch 322/2000, Train Loss: 5.497545316637107, Val Loss: 5.8448583811794945, Val MAE: 1.5963534116744995\n",
      "Epoch 323/2000, Train Loss: 5.496859227663022, Val Loss: 5.843390368996046, Val MAE: 1.5971940755844116\n",
      "Epoch 324/2000, Train Loss: 5.49597794064999, Val Loss: 5.8426056933512385, Val MAE: 1.597859501838684\n",
      "Epoch 325/2000, Train Loss: 5.4951260430472235, Val Loss: 5.841245505605634, Val MAE: 1.5983392000198364\n",
      "Epoch 326/2000, Train Loss: 5.494427405135217, Val Loss: 5.840145148815365, Val MAE: 1.5988019704818726\n",
      "Epoch 327/2000, Train Loss: 5.493742609258943, Val Loss: 5.83892694097411, Val MAE: 1.599328637123108\n",
      "Epoch 328/2000, Train Loss: 5.492918666528196, Val Loss: 5.837770105228511, Val MAE: 1.5999785661697388\n",
      "Epoch 329/2000, Train Loss: 5.492094571702838, Val Loss: 5.8363952320467805, Val MAE: 1.6003985404968262\n",
      "Epoch 330/2000, Train Loss: 5.491428980602503, Val Loss: 5.835556348438292, Val MAE: 1.6005381345748901\n",
      "Epoch 331/2000, Train Loss: 5.490823725929234, Val Loss: 5.834162777534683, Val MAE: 1.6013928651809692\n",
      "Epoch 332/2000, Train Loss: 5.490129684915348, Val Loss: 5.833018358238611, Val MAE: 1.60185706615448\n",
      "Epoch 333/2000, Train Loss: 5.489448975543587, Val Loss: 5.832192434265709, Val MAE: 1.6020832061767578\n",
      "Epoch 334/2000, Train Loss: 5.48887124236078, Val Loss: 5.8312123050565745, Val MAE: 1.6022435426712036\n",
      "Epoch 335/2000, Train Loss: 5.488353119191781, Val Loss: 5.83024406332853, Val MAE: 1.6028525829315186\n",
      "Epoch 336/2000, Train Loss: 5.487714937922822, Val Loss: 5.829449324432863, Val MAE: 1.602897047996521\n",
      "Epoch 337/2000, Train Loss: 5.487175366980857, Val Loss: 5.8288583250585315, Val MAE: 1.6033087968826294\n",
      "Epoch 338/2000, Train Loss: 5.486686970967797, Val Loss: 5.827804478574601, Val MAE: 1.603511929512024\n",
      "Epoch 339/2000, Train Loss: 5.486129330045149, Val Loss: 5.827347608243289, Val MAE: 1.6036498546600342\n",
      "Epoch 340/2000, Train Loss: 5.485496553881415, Val Loss: 5.826350385351648, Val MAE: 1.6040714979171753\n",
      "Epoch 341/2000, Train Loss: 5.484889168709118, Val Loss: 5.8253910605331445, Val MAE: 1.6046148538589478\n",
      "Epoch 342/2000, Train Loss: 5.484287320686678, Val Loss: 5.8242659866809845, Val MAE: 1.6050142049789429\n",
      "Epoch 343/2000, Train Loss: 5.483741699684513, Val Loss: 5.82326508434176, Val MAE: 1.6052687168121338\n",
      "Epoch 344/2000, Train Loss: 5.483333353422462, Val Loss: 5.822448928604068, Val MAE: 1.6054234504699707\n",
      "Epoch 345/2000, Train Loss: 5.482840445409099, Val Loss: 5.822200399199757, Val MAE: 1.6056241989135742\n",
      "Epoch 346/2000, Train Loss: 5.4823112215985725, Val Loss: 5.820945801173511, Val MAE: 1.606386661529541\n",
      "Epoch 347/2000, Train Loss: 5.481948587443106, Val Loss: 5.820374842357198, Val MAE: 1.606540560722351\n",
      "Epoch 348/2000, Train Loss: 5.481318551331318, Val Loss: 5.819702640312527, Val MAE: 1.6067514419555664\n",
      "Epoch 349/2000, Train Loss: 5.48095677487872, Val Loss: 5.8193024545452285, Val MAE: 1.6069002151489258\n",
      "Epoch 350/2000, Train Loss: 5.480450419285698, Val Loss: 5.818767247670287, Val MAE: 1.6069186925888062\n",
      "Epoch 351/2000, Train Loss: 5.480028405145891, Val Loss: 5.818027488135416, Val MAE: 1.6072404384613037\n",
      "Epoch 352/2000, Train Loss: 5.479643777086229, Val Loss: 5.817480984904351, Val MAE: 1.6073763370513916\n",
      "Epoch 353/2000, Train Loss: 5.479292463031476, Val Loss: 5.817238622575725, Val MAE: 1.6074461936950684\n",
      "Epoch 354/2000, Train Loss: 5.47880660715781, Val Loss: 5.816162897906172, Val MAE: 1.6077234745025635\n",
      "Epoch 355/2000, Train Loss: 5.478409243670255, Val Loss: 5.81577083222363, Val MAE: 1.607846736907959\n",
      "Epoch 356/2000, Train Loss: 5.4780023579527, Val Loss: 5.815371988927917, Val MAE: 1.607703685760498\n",
      "Epoch 357/2000, Train Loss: 5.477489722437795, Val Loss: 5.8147884819668, Val MAE: 1.607933521270752\n",
      "Epoch 358/2000, Train Loss: 5.477134919351125, Val Loss: 5.814221614271129, Val MAE: 1.6082043647766113\n",
      "Epoch 359/2000, Train Loss: 5.476832087152362, Val Loss: 5.813331666980679, Val MAE: 1.6090441942214966\n",
      "Epoch 360/2000, Train Loss: 5.476239553393821, Val Loss: 5.8128804772272025, Val MAE: 1.6090108156204224\n",
      "Epoch 361/2000, Train Loss: 5.475858715870781, Val Loss: 5.8124095747412525, Val MAE: 1.6090450286865234\n",
      "Epoch 362/2000, Train Loss: 5.475377575433397, Val Loss: 5.811648959504719, Val MAE: 1.6093829870224\n",
      "Epoch 363/2000, Train Loss: 5.475049985117849, Val Loss: 5.811272662622849, Val MAE: 1.6095695495605469\n",
      "Epoch 364/2000, Train Loss: 5.474686429334476, Val Loss: 5.810610059569006, Val MAE: 1.60942542552948\n",
      "Epoch 365/2000, Train Loss: 5.474199572555789, Val Loss: 5.810089099753522, Val MAE: 1.609744906425476\n",
      "Epoch 366/2000, Train Loss: 5.473857301461034, Val Loss: 5.809921458135687, Val MAE: 1.6098111867904663\n",
      "Epoch 367/2000, Train Loss: 5.473525945943984, Val Loss: 5.809468770063616, Val MAE: 1.609889030456543\n",
      "Epoch 368/2000, Train Loss: 5.473058901900225, Val Loss: 5.808757347251297, Val MAE: 1.6098909378051758\n",
      "Epoch 369/2000, Train Loss: 5.472698379284391, Val Loss: 5.808292774432296, Val MAE: 1.609966516494751\n",
      "Epoch 370/2000, Train Loss: 5.472413572939578, Val Loss: 5.808270010652892, Val MAE: 1.6097315549850464\n",
      "Epoch 371/2000, Train Loss: 5.4719634294342105, Val Loss: 5.807818718003935, Val MAE: 1.6096601486206055\n",
      "Epoch 372/2000, Train Loss: 5.471602129147642, Val Loss: 5.807032938761814, Val MAE: 1.610080599784851\n",
      "Epoch 373/2000, Train Loss: 5.471341759952167, Val Loss: 5.806805135004382, Val MAE: 1.610271692276001\n",
      "Epoch 374/2000, Train Loss: 5.470792488847125, Val Loss: 5.806439958489999, Val MAE: 1.6100249290466309\n",
      "Epoch 375/2000, Train Loss: 5.470417515313768, Val Loss: 5.805954741137472, Val MAE: 1.610234022140503\n",
      "Epoch 376/2000, Train Loss: 5.470126868468787, Val Loss: 5.805676097899037, Val MAE: 1.6103031635284424\n",
      "Epoch 377/2000, Train Loss: 5.469707801759788, Val Loss: 5.805157136844203, Val MAE: 1.610193133354187\n",
      "Epoch 378/2000, Train Loss: 5.469396071061545, Val Loss: 5.804438793148834, Val MAE: 1.6105583906173706\n",
      "Epoch 379/2000, Train Loss: 5.468974729896347, Val Loss: 5.804312614490497, Val MAE: 1.6101070642471313\n",
      "Epoch 380/2000, Train Loss: 5.468564956142901, Val Loss: 5.803858129712055, Val MAE: 1.6103601455688477\n",
      "Epoch 381/2000, Train Loss: 5.468356831655965, Val Loss: 5.803253350819287, Val MAE: 1.6106016635894775\n",
      "Epoch 382/2000, Train Loss: 5.467885540540079, Val Loss: 5.803251537799106, Val MAE: 1.610213041305542\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 383/2000, Train Loss: 5.467526932487514, Val Loss: 5.803103454524953, Val MAE: 1.609970211982727\n",
      "Epoch 384/2000, Train Loss: 5.4672114921236945, Val Loss: 5.802638600816785, Val MAE: 1.609875202178955\n",
      "Epoch 385/2000, Train Loss: 5.467014157042211, Val Loss: 5.802415437745756, Val MAE: 1.6095792055130005\n",
      "Epoch 386/2000, Train Loss: 5.466530690471695, Val Loss: 5.801985078813104, Val MAE: 1.609969139099121\n",
      "Epoch 387/2000, Train Loss: 5.466182008752682, Val Loss: 5.801617175796346, Val MAE: 1.6100764274597168\n",
      "Epoch 388/2000, Train Loss: 5.4657676919592175, Val Loss: 5.801130321776831, Val MAE: 1.610062837600708\n",
      "Epoch 389/2000, Train Loss: 5.465441420412164, Val Loss: 5.800689613600389, Val MAE: 1.6102166175842285\n",
      "Epoch 390/2000, Train Loss: 5.464970953778261, Val Loss: 5.800264211696223, Val MAE: 1.610266089439392\n",
      "Epoch 391/2000, Train Loss: 5.46467469948266, Val Loss: 5.8000507651848165, Val MAE: 1.610347867012024\n",
      "Epoch 392/2000, Train Loss: 5.464339938153691, Val Loss: 5.799864299800418, Val MAE: 1.609960675239563\n",
      "Epoch 393/2000, Train Loss: 5.463926625537, Val Loss: 5.7994722318394105, Val MAE: 1.6102499961853027\n",
      "Epoch 394/2000, Train Loss: 5.463674156033263, Val Loss: 5.798988112980437, Val MAE: 1.6104506254196167\n",
      "Epoch 395/2000, Train Loss: 5.463467488231833, Val Loss: 5.798763740044486, Val MAE: 1.6103405952453613\n",
      "Epoch 396/2000, Train Loss: 5.462906898536119, Val Loss: 5.798440967131098, Val MAE: 1.6105225086212158\n",
      "Epoch 397/2000, Train Loss: 5.462659954437481, Val Loss: 5.7979565302895475, Val MAE: 1.610708236694336\n",
      "Epoch 398/2000, Train Loss: 5.4624035417152745, Val Loss: 5.797930079498058, Val MAE: 1.6102770566940308\n",
      "Epoch 399/2000, Train Loss: 5.461981050570526, Val Loss: 5.797561800261156, Val MAE: 1.6103758811950684\n",
      "Epoch 400/2000, Train Loss: 5.461555360489375, Val Loss: 5.797341908245641, Val MAE: 1.6104063987731934\n",
      "Epoch 401/2000, Train Loss: 5.4611990037390585, Val Loss: 5.79668757018693, Val MAE: 1.6107416152954102\n",
      "Epoch 402/2000, Train Loss: 5.460895088002515, Val Loss: 5.796397244620396, Val MAE: 1.610514521598816\n",
      "Epoch 403/2000, Train Loss: 5.460487657328209, Val Loss: 5.796183808134236, Val MAE: 1.6104696989059448\n",
      "Epoch 404/2000, Train Loss: 5.4602179060848055, Val Loss: 5.795818521342146, Val MAE: 1.6107128858566284\n",
      "Epoch 405/2000, Train Loss: 5.459904109567592, Val Loss: 5.795580285222523, Val MAE: 1.6104230880737305\n",
      "Epoch 406/2000, Train Loss: 5.459482480907507, Val Loss: 5.7954543275752926, Val MAE: 1.610317587852478\n",
      "Epoch 407/2000, Train Loss: 5.459149873734863, Val Loss: 5.795130070925488, Val MAE: 1.610207438468933\n",
      "Epoch 408/2000, Train Loss: 5.458763672884378, Val Loss: 5.7948822552275585, Val MAE: 1.6099467277526855\n",
      "Epoch 409/2000, Train Loss: 5.458521546317523, Val Loss: 5.794482454794262, Val MAE: 1.6100473403930664\n",
      "Epoch 410/2000, Train Loss: 5.458104940303022, Val Loss: 5.793876483626322, Val MAE: 1.6102455854415894\n",
      "Epoch 411/2000, Train Loss: 5.457866454275455, Val Loss: 5.793909760152164, Val MAE: 1.610013723373413\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 412/2000, Train Loss: 5.457568558901653, Val Loss: 5.793776814999566, Val MAE: 1.609755039215088\n",
      "Epoch 413/2000, Train Loss: 5.457056223381413, Val Loss: 5.7933037986449145, Val MAE: 1.6100140810012817\n",
      "Epoch 414/2000, Train Loss: 5.456728713433563, Val Loss: 5.792946780675777, Val MAE: 1.6100351810455322\n",
      "Epoch 415/2000, Train Loss: 5.456368624534848, Val Loss: 5.792718980017058, Val MAE: 1.609838604927063\n",
      "Epoch 416/2000, Train Loss: 5.455973971150108, Val Loss: 5.792587548858894, Val MAE: 1.6098028421401978\n",
      "Epoch 417/2000, Train Loss: 5.4557003474923444, Val Loss: 5.792622253253191, Val MAE: 1.6094400882720947\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 418/2000, Train Loss: 5.455476491070398, Val Loss: 5.792144957485549, Val MAE: 1.6096359491348267\n",
      "Epoch 419/2000, Train Loss: 5.455290769205221, Val Loss: 5.791944417749341, Val MAE: 1.6095372438430786\n",
      "Epoch 420/2000, Train Loss: 5.454836545235501, Val Loss: 5.7913833930404905, Val MAE: 1.6098028421401978\n",
      "Epoch 421/2000, Train Loss: 5.454355265371066, Val Loss: 5.7911981618550215, Val MAE: 1.6096006631851196\n",
      "Epoch 422/2000, Train Loss: 5.4539508504149445, Val Loss: 5.791048146624813, Val MAE: 1.6095036268234253\n",
      "Epoch 423/2000, Train Loss: 5.453598878821548, Val Loss: 5.790696632880318, Val MAE: 1.6096363067626953\n",
      "Epoch 424/2000, Train Loss: 5.453290873895977, Val Loss: 5.790292875755817, Val MAE: 1.6094495058059692\n",
      "Epoch 425/2000, Train Loss: 5.4530315714600555, Val Loss: 5.790054885892693, Val MAE: 1.6094231605529785\n",
      "Epoch 426/2000, Train Loss: 5.452598460705493, Val Loss: 5.78993205967664, Val MAE: 1.6094526052474976\n",
      "Epoch 427/2000, Train Loss: 5.452321433584762, Val Loss: 5.789683536834309, Val MAE: 1.6093809604644775\n",
      "Epoch 428/2000, Train Loss: 5.452058865435101, Val Loss: 5.78937415509778, Val MAE: 1.6091073751449585\n",
      "Epoch 429/2000, Train Loss: 5.451632382582813, Val Loss: 5.788977325782863, Val MAE: 1.6091715097427368\n",
      "Epoch 430/2000, Train Loss: 5.451271162328378, Val Loss: 5.788737885175495, Val MAE: 1.6090888977050781\n",
      "Epoch 431/2000, Train Loss: 5.450952134612242, Val Loss: 5.788702625200289, Val MAE: 1.6088478565216064\n",
      "Epoch 432/2000, Train Loss: 5.450746978193333, Val Loss: 5.78819579478433, Val MAE: 1.6089024543762207\n",
      "Epoch 433/2000, Train Loss: 5.450283241137747, Val Loss: 5.78809899583869, Val MAE: 1.6085551977157593\n",
      "Epoch 434/2000, Train Loss: 5.449881731163524, Val Loss: 5.787614025654049, Val MAE: 1.609290599822998\n",
      "Epoch 435/2000, Train Loss: 5.449614384910911, Val Loss: 5.787031903908522, Val MAE: 1.6095032691955566\n",
      "Epoch 436/2000, Train Loss: 5.449359972488704, Val Loss: 5.786980052242221, Val MAE: 1.6092716455459595\n",
      "Epoch 437/2000, Train Loss: 5.44895277083717, Val Loss: 5.786881895422571, Val MAE: 1.6088216304779053\n",
      "Epoch 438/2000, Train Loss: 5.448611878576957, Val Loss: 5.786455721209903, Val MAE: 1.6088972091674805\n",
      "Epoch 439/2000, Train Loss: 5.448375765325988, Val Loss: 5.785984326848926, Val MAE: 1.6091761589050293\n",
      "Epoch 440/2000, Train Loss: 5.44803193721194, Val Loss: 5.786190703374531, Val MAE: 1.608310341835022\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 441/2000, Train Loss: 5.447574124883213, Val Loss: 5.7860000420601, Val MAE: 1.6084578037261963\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 442/2000, Train Loss: 5.447217931888374, Val Loss: 5.785439262422947, Val MAE: 1.608572006225586\n",
      "Epoch 443/2000, Train Loss: 5.446930259151579, Val Loss: 5.785133627758842, Val MAE: 1.6085458993911743\n",
      "Epoch 444/2000, Train Loss: 5.446508824363885, Val Loss: 5.784898361846213, Val MAE: 1.6083749532699585\n",
      "Epoch 445/2000, Train Loss: 5.44619639367809, Val Loss: 5.784480685305523, Val MAE: 1.6083297729492188\n",
      "Epoch 446/2000, Train Loss: 5.4459055176425535, Val Loss: 5.784288692000445, Val MAE: 1.6082961559295654\n",
      "Epoch 447/2000, Train Loss: 5.445584902454647, Val Loss: 5.784002502941575, Val MAE: 1.608109474182129\n",
      "Epoch 448/2000, Train Loss: 5.445143958366228, Val Loss: 5.783841694075762, Val MAE: 1.6080150604248047\n",
      "Epoch 449/2000, Train Loss: 5.445093212624321, Val Loss: 5.7834127835177505, Val MAE: 1.608248233795166\n",
      "Epoch 450/2000, Train Loss: 5.44463257400357, Val Loss: 5.783466029786918, Val MAE: 1.608170747756958\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 451/2000, Train Loss: 5.444303542522374, Val Loss: 5.783396373830439, Val MAE: 1.6080474853515625\n",
      "Epoch 452/2000, Train Loss: 5.443900850102735, Val Loss: 5.782948965508639, Val MAE: 1.6077063083648682\n",
      "Epoch 453/2000, Train Loss: 5.443830256894976, Val Loss: 5.782606233515142, Val MAE: 1.608039140701294\n",
      "Epoch 454/2000, Train Loss: 5.443165673075387, Val Loss: 5.782688930493976, Val MAE: 1.607704520225525\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 455/2000, Train Loss: 5.442859437581437, Val Loss: 5.782446577096933, Val MAE: 1.607313632965088\n",
      "Epoch 456/2000, Train Loss: 5.442573090156646, Val Loss: 5.782100760699777, Val MAE: 1.6073483228683472\n",
      "Epoch 457/2000, Train Loss: 5.442141562427626, Val Loss: 5.781852146627706, Val MAE: 1.6074031591415405\n",
      "Epoch 458/2000, Train Loss: 5.441942964616919, Val Loss: 5.7814589175609274, Val MAE: 1.6074868440628052\n",
      "Epoch 459/2000, Train Loss: 5.44157429432718, Val Loss: 5.78146477804636, Val MAE: 1.60721755027771\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 460/2000, Train Loss: 5.44121564567131, Val Loss: 5.7811781628233945, Val MAE: 1.6071298122406006\n",
      "Epoch 461/2000, Train Loss: 5.4408343487604665, Val Loss: 5.780732487410945, Val MAE: 1.60719895362854\n",
      "Epoch 462/2000, Train Loss: 5.440570229631675, Val Loss: 5.780272620351307, Val MAE: 1.6074113845825195\n",
      "Epoch 463/2000, Train Loss: 5.44012611034804, Val Loss: 5.779898145876893, Val MAE: 1.6073576211929321\n",
      "Epoch 464/2000, Train Loss: 5.439848413943909, Val Loss: 5.779771869426838, Val MAE: 1.6073442697525024\n",
      "Epoch 465/2000, Train Loss: 5.439501785208523, Val Loss: 5.7796058248307, Val MAE: 1.6070470809936523\n",
      "Epoch 466/2000, Train Loss: 5.4391271363338225, Val Loss: 5.779313528391929, Val MAE: 1.6071114540100098\n",
      "Epoch 467/2000, Train Loss: 5.438838259759375, Val Loss: 5.779176404443356, Val MAE: 1.6069358587265015\n",
      "Epoch 468/2000, Train Loss: 5.438477067860141, Val Loss: 5.778909855661771, Val MAE: 1.6070786714553833\n",
      "Epoch 469/2000, Train Loss: 5.438210101969892, Val Loss: 5.778731819595401, Val MAE: 1.6068108081817627\n",
      "Epoch 470/2000, Train Loss: 5.437919282812539, Val Loss: 5.778263510914024, Val MAE: 1.6069316864013672\n",
      "Epoch 471/2000, Train Loss: 5.43757956326637, Val Loss: 5.778121847989727, Val MAE: 1.6067484617233276\n",
      "Epoch 472/2000, Train Loss: 5.437208146465068, Val Loss: 5.777918335312368, Val MAE: 1.6067208051681519\n",
      "Epoch 473/2000, Train Loss: 5.437185407272113, Val Loss: 5.777934694235478, Val MAE: 1.6061619520187378\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 474/2000, Train Loss: 5.436687437939359, Val Loss: 5.777529857300837, Val MAE: 1.6062642335891724\n",
      "Epoch 475/2000, Train Loss: 5.436309137955088, Val Loss: 5.777042176107384, Val MAE: 1.6067609786987305\n",
      "Epoch 476/2000, Train Loss: 5.435975924249605, Val Loss: 5.777224793528927, Val MAE: 1.6066066026687622\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 477/2000, Train Loss: 5.435629172529828, Val Loss: 5.7770839404622345, Val MAE: 1.6063109636306763\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 478/2000, Train Loss: 5.435317073381761, Val Loss: 5.776845802590024, Val MAE: 1.6063116788864136\n",
      "Epoch 479/2000, Train Loss: 5.435186651204355, Val Loss: 5.776605749895813, Val MAE: 1.6061725616455078\n",
      "Epoch 480/2000, Train Loss: 5.4347052013765, Val Loss: 5.776344838856922, Val MAE: 1.6062265634536743\n",
      "Epoch 481/2000, Train Loss: 5.4342459900793605, Val Loss: 5.775958475625479, Val MAE: 1.606168270111084\n",
      "Epoch 482/2000, Train Loss: 5.433906376655466, Val Loss: 5.775519414505827, Val MAE: 1.6062829494476318\n",
      "Epoch 483/2000, Train Loss: 5.433657052901158, Val Loss: 5.775247153247897, Val MAE: 1.6064518690109253\n",
      "Epoch 484/2000, Train Loss: 5.433355746514188, Val Loss: 5.775159961951253, Val MAE: 1.6060497760772705\n",
      "Epoch 485/2000, Train Loss: 5.433069902436151, Val Loss: 5.775135176079718, Val MAE: 1.6059752702713013\n",
      "Epoch 486/2000, Train Loss: 5.432667742809051, Val Loss: 5.774821658564635, Val MAE: 1.6057053804397583\n",
      "Epoch 487/2000, Train Loss: 5.4323235232590115, Val Loss: 5.774412799227857, Val MAE: 1.6059049367904663\n",
      "Epoch 488/2000, Train Loss: 5.431895462917996, Val Loss: 5.774138032934352, Val MAE: 1.6056565046310425\n",
      "Epoch 489/2000, Train Loss: 5.43163329157336, Val Loss: 5.774117392502064, Val MAE: 1.6053625345230103\n",
      "Epoch 490/2000, Train Loss: 5.4312658091819594, Val Loss: 5.773805867459067, Val MAE: 1.605388879776001\n",
      "Epoch 491/2000, Train Loss: 5.43094081596788, Val Loss: 5.773598456692623, Val MAE: 1.6052318811416626\n",
      "Epoch 492/2000, Train Loss: 5.430700353465392, Val Loss: 5.772872934738795, Val MAE: 1.6054930686950684\n",
      "Epoch 493/2000, Train Loss: 5.430386551327474, Val Loss: 5.772916608629606, Val MAE: 1.6053175926208496\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 494/2000, Train Loss: 5.429872965493897, Val Loss: 5.772690079263226, Val MAE: 1.6052802801132202\n",
      "Epoch 495/2000, Train Loss: 5.429619838787081, Val Loss: 5.772387639553904, Val MAE: 1.6051946878433228\n",
      "Epoch 496/2000, Train Loss: 5.429312099460814, Val Loss: 5.772488284183934, Val MAE: 1.6047194004058838\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 497/2000, Train Loss: 5.429156803396871, Val Loss: 5.77201325239027, Val MAE: 1.6049418449401855\n",
      "Epoch 498/2000, Train Loss: 5.428592605376059, Val Loss: 5.771893541838415, Val MAE: 1.6045044660568237\n",
      "Epoch 499/2000, Train Loss: 5.428275736178587, Val Loss: 5.771633662032789, Val MAE: 1.6046112775802612\n",
      "Epoch 500/2000, Train Loss: 5.428002102453888, Val Loss: 5.771299778321468, Val MAE: 1.6046913862228394\n",
      "Epoch 501/2000, Train Loss: 5.427615112020129, Val Loss: 5.771269221098051, Val MAE: 1.6046644449234009\n",
      "Epoch 502/2000, Train Loss: 5.427297350892209, Val Loss: 5.770825963501537, Val MAE: 1.6049761772155762\n",
      "Epoch 503/2000, Train Loss: 5.426972619250658, Val Loss: 5.770629114058404, Val MAE: 1.6046205759048462\n",
      "Epoch 504/2000, Train Loss: 5.426673947212815, Val Loss: 5.7705786264635375, Val MAE: 1.6045169830322266\n",
      "Epoch 505/2000, Train Loss: 5.426291958367967, Val Loss: 5.770189232235655, Val MAE: 1.6048680543899536\n",
      "Epoch 506/2000, Train Loss: 5.426030896567359, Val Loss: 5.770216338131406, Val MAE: 1.6043410301208496\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 507/2000, Train Loss: 5.425693301899512, Val Loss: 5.769718207077148, Val MAE: 1.6046441793441772\n",
      "Epoch 508/2000, Train Loss: 5.425213237953052, Val Loss: 5.76957711869059, Val MAE: 1.604333758354187\n",
      "Epoch 509/2000, Train Loss: 5.424917030133134, Val Loss: 5.769054386958435, Val MAE: 1.604812502861023\n",
      "Epoch 510/2000, Train Loss: 5.424621016269499, Val Loss: 5.768886340958628, Val MAE: 1.6049424409866333\n",
      "Epoch 511/2000, Train Loss: 5.4241749530607, Val Loss: 5.76867560181049, Val MAE: 1.6047818660736084\n",
      "Epoch 512/2000, Train Loss: 5.423857851988157, Val Loss: 5.768384379075572, Val MAE: 1.604644536972046\n",
      "Epoch 513/2000, Train Loss: 5.4236434900282475, Val Loss: 5.767906173835836, Val MAE: 1.604687213897705\n",
      "Epoch 514/2000, Train Loss: 5.423144476083196, Val Loss: 5.767720157217907, Val MAE: 1.6048994064331055\n",
      "Epoch 515/2000, Train Loss: 5.422860430127546, Val Loss: 5.767308925452218, Val MAE: 1.6049871444702148\n",
      "Epoch 516/2000, Train Loss: 5.422517671793133, Val Loss: 5.767429839215876, Val MAE: 1.604440689086914\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 517/2000, Train Loss: 5.422157914432147, Val Loss: 5.766931422806661, Val MAE: 1.604722499847412\n",
      "Epoch 518/2000, Train Loss: 5.421908176423462, Val Loss: 5.766816622438052, Val MAE: 1.604390263557434\n",
      "Epoch 519/2000, Train Loss: 5.421534110424302, Val Loss: 5.766820155517041, Val MAE: 1.6042723655700684\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 520/2000, Train Loss: 5.421202462127224, Val Loss: 5.7662261893078455, Val MAE: 1.604590892791748\n",
      "Epoch 521/2000, Train Loss: 5.420827764931235, Val Loss: 5.766098559267295, Val MAE: 1.6041666269302368\n",
      "Epoch 522/2000, Train Loss: 5.420417997050168, Val Loss: 5.76587417092892, Val MAE: 1.604193925857544\n",
      "Epoch 523/2000, Train Loss: 5.420237245566404, Val Loss: 5.76574394170662, Val MAE: 1.6041468381881714\n",
      "Epoch 524/2000, Train Loss: 5.419777932928785, Val Loss: 5.765197360369773, Val MAE: 1.6042715311050415\n",
      "Epoch 525/2000, Train Loss: 5.4194895558421, Val Loss: 5.764943952133896, Val MAE: 1.6043424606323242\n",
      "Epoch 526/2000, Train Loss: 5.41937635349271, Val Loss: 5.764690644059342, Val MAE: 1.6042126417160034\n",
      "Epoch 527/2000, Train Loss: 5.4187921702568165, Val Loss: 5.7645380781331195, Val MAE: 1.6040375232696533\n",
      "Epoch 528/2000, Train Loss: 5.418515922149078, Val Loss: 5.764114563253677, Val MAE: 1.6041072607040405\n",
      "Epoch 529/2000, Train Loss: 5.418174304062846, Val Loss: 5.763891194665104, Val MAE: 1.6039477586746216\n",
      "Epoch 530/2000, Train Loss: 5.417789433725614, Val Loss: 5.763588645224907, Val MAE: 1.604234218597412\n",
      "Epoch 531/2000, Train Loss: 5.417542443486857, Val Loss: 5.763505031300612, Val MAE: 1.6039254665374756\n",
      "Epoch 532/2000, Train Loss: 5.417088478069587, Val Loss: 5.763055053052552, Val MAE: 1.6040685176849365\n",
      "Epoch 533/2000, Train Loss: 5.416791584355826, Val Loss: 5.762826892305222, Val MAE: 1.6040319204330444\n",
      "Epoch 534/2000, Train Loss: 5.416472163861441, Val Loss: 5.762774174366522, Val MAE: 1.6039042472839355\n",
      "Epoch 535/2000, Train Loss: 5.416256599573918, Val Loss: 5.762208767713757, Val MAE: 1.6041240692138672\n",
      "Epoch 536/2000, Train Loss: 5.4158791938690465, Val Loss: 5.762162080325118, Val MAE: 1.6039336919784546\n",
      "Epoch 537/2000, Train Loss: 5.415486799840101, Val Loss: 5.762007200389827, Val MAE: 1.6036676168441772\n",
      "Epoch 538/2000, Train Loss: 5.415143550574822, Val Loss: 5.761774546874044, Val MAE: 1.603655457496643\n",
      "Epoch 539/2000, Train Loss: 5.414833811116336, Val Loss: 5.761204146555075, Val MAE: 1.6037627458572388\n",
      "Epoch 540/2000, Train Loss: 5.4145912323763135, Val Loss: 5.76132227639904, Val MAE: 1.6034793853759766\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 541/2000, Train Loss: 5.414145431746738, Val Loss: 5.760937289907298, Val MAE: 1.603698968887329\n",
      "Epoch 542/2000, Train Loss: 5.413860282790568, Val Loss: 5.760635426649625, Val MAE: 1.6037306785583496\n",
      "Epoch 543/2000, Train Loss: 5.413515967436863, Val Loss: 5.760565966823415, Val MAE: 1.603453278541565\n",
      "Epoch 544/2000, Train Loss: 5.413204230698459, Val Loss: 5.760222442015231, Val MAE: 1.6036343574523926\n",
      "Epoch 545/2000, Train Loss: 5.412911707712344, Val Loss: 5.759774191755038, Val MAE: 1.604028582572937\n",
      "Epoch 546/2000, Train Loss: 5.412554050146233, Val Loss: 5.759877577196203, Val MAE: 1.6035720109939575\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 547/2000, Train Loss: 5.412167454839675, Val Loss: 5.759501557922509, Val MAE: 1.6037168502807617\n",
      "Epoch 548/2000, Train Loss: 5.41184979727535, Val Loss: 5.759045972736604, Val MAE: 1.603861689567566\n",
      "Epoch 549/2000, Train Loss: 5.411724719088485, Val Loss: 5.75904683326727, Val MAE: 1.603234887123108\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 550/2000, Train Loss: 5.411210896010805, Val Loss: 5.758650008691561, Val MAE: 1.6035232543945312\n",
      "Epoch 551/2000, Train Loss: 5.410858906296926, Val Loss: 5.758212686860962, Val MAE: 1.6035346984863281\n",
      "Epoch 552/2000, Train Loss: 5.410557756283013, Val Loss: 5.758127368371421, Val MAE: 1.6033506393432617\n",
      "Epoch 553/2000, Train Loss: 5.410276754065184, Val Loss: 5.758032356380323, Val MAE: 1.6030837297439575\n",
      "Epoch 554/2000, Train Loss: 5.409941465228817, Val Loss: 5.757932009408963, Val MAE: 1.6026426553726196\n",
      "Epoch 555/2000, Train Loss: 5.409677936861332, Val Loss: 5.7577132976383245, Val MAE: 1.6028105020523071\n",
      "Epoch 556/2000, Train Loss: 5.409323357763968, Val Loss: 5.75723908445157, Val MAE: 1.603061318397522\n",
      "Epoch 557/2000, Train Loss: 5.408954123725865, Val Loss: 5.757194273515579, Val MAE: 1.6025925874710083\n",
      "Epoch 558/2000, Train Loss: 5.408648732596094, Val Loss: 5.756774085468473, Val MAE: 1.6027911901474\n",
      "Epoch 559/2000, Train Loss: 5.408314428389869, Val Loss: 5.756629688112743, Val MAE: 1.602685570716858\n",
      "Epoch 560/2000, Train Loss: 5.40811613963407, Val Loss: 5.756615699431218, Val MAE: 1.6023420095443726\n",
      "Epoch 561/2000, Train Loss: 5.407675801155686, Val Loss: 5.756151398205247, Val MAE: 1.6025640964508057\n",
      "Epoch 562/2000, Train Loss: 5.407434938408639, Val Loss: 5.755929907162984, Val MAE: 1.6026129722595215\n",
      "Epoch 563/2000, Train Loss: 5.407045641430325, Val Loss: 5.755813185683813, Val MAE: 1.6024430990219116\n",
      "Epoch 564/2000, Train Loss: 5.406849351758105, Val Loss: 5.755411607318697, Val MAE: 1.602372169494629\n",
      "Epoch 565/2000, Train Loss: 5.40646152174196, Val Loss: 5.755518996205169, Val MAE: 1.6020677089691162\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 566/2000, Train Loss: 5.406086070196969, Val Loss: 5.755065222672366, Val MAE: 1.6024572849273682\n",
      "Epoch 567/2000, Train Loss: 5.405846120940389, Val Loss: 5.754774820184854, Val MAE: 1.602342128753662\n",
      "Epoch 568/2000, Train Loss: 5.4055726717761665, Val Loss: 5.754719656632945, Val MAE: 1.6018176078796387\n",
      "Epoch 569/2000, Train Loss: 5.4052257077279515, Val Loss: 5.754523979505632, Val MAE: 1.6019434928894043\n",
      "Epoch 570/2000, Train Loss: 5.404875172434518, Val Loss: 5.754350956426848, Val MAE: 1.6016954183578491\n",
      "Epoch 571/2000, Train Loss: 5.404582634180218, Val Loss: 5.754023922875023, Val MAE: 1.601768970489502\n",
      "Epoch 572/2000, Train Loss: 5.404214671092667, Val Loss: 5.753678278638683, Val MAE: 1.601589322090149\n",
      "Epoch 573/2000, Train Loss: 5.403894162782056, Val Loss: 5.753359103512691, Val MAE: 1.6018606424331665\n",
      "Epoch 574/2000, Train Loss: 5.403692433063956, Val Loss: 5.753291981847279, Val MAE: 1.601701021194458\n",
      "Epoch 575/2000, Train Loss: 5.403346649951116, Val Loss: 5.75303769786059, Val MAE: 1.601486325263977\n",
      "Epoch 576/2000, Train Loss: 5.40291229358112, Val Loss: 5.752521485363672, Val MAE: 1.6017531156539917\n",
      "Epoch 577/2000, Train Loss: 5.402592095332109, Val Loss: 5.752490801366462, Val MAE: 1.6015535593032837\n",
      "Epoch 578/2000, Train Loss: 5.402278706861331, Val Loss: 5.7522756919401505, Val MAE: 1.6013168096542358\n",
      "Epoch 579/2000, Train Loss: 5.402119316742674, Val Loss: 5.752301694603141, Val MAE: 1.601082444190979\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 580/2000, Train Loss: 5.401613600567812, Val Loss: 5.751698632214777, Val MAE: 1.6016218662261963\n",
      "Epoch 581/2000, Train Loss: 5.40132974823288, Val Loss: 5.751429615764443, Val MAE: 1.601381778717041\n",
      "Epoch 582/2000, Train Loss: 5.401060350842244, Val Loss: 5.751196615375146, Val MAE: 1.6012791395187378\n",
      "Epoch 583/2000, Train Loss: 5.400708968332332, Val Loss: 5.751185500658251, Val MAE: 1.6011016368865967\n",
      "Epoch 584/2000, Train Loss: 5.400435688963748, Val Loss: 5.751031233447772, Val MAE: 1.6010791063308716\n",
      "Epoch 585/2000, Train Loss: 5.400206045731955, Val Loss: 5.750770653242, Val MAE: 1.6009875535964966\n",
      "Epoch 586/2000, Train Loss: 5.399870075531865, Val Loss: 5.750484277440139, Val MAE: 1.6009725332260132\n",
      "Epoch 587/2000, Train Loss: 5.399578041127666, Val Loss: 5.750283481422185, Val MAE: 1.601074457168579\n",
      "Epoch 588/2000, Train Loss: 5.39927046807361, Val Loss: 5.749998011811429, Val MAE: 1.6010046005249023\n",
      "Epoch 589/2000, Train Loss: 5.399033485375014, Val Loss: 5.749665196700199, Val MAE: 1.601151466369629\n",
      "Epoch 590/2000, Train Loss: 5.398710535580973, Val Loss: 5.7495932996455315, Val MAE: 1.6011725664138794\n",
      "Epoch 591/2000, Train Loss: 5.398454251967208, Val Loss: 5.749339413752249, Val MAE: 1.6011765003204346\n",
      "Epoch 592/2000, Train Loss: 5.3982461064236, Val Loss: 5.748958207233966, Val MAE: 1.601360559463501\n",
      "Epoch 593/2000, Train Loss: 5.397802078497401, Val Loss: 5.748916635454977, Val MAE: 1.6010174751281738\n",
      "Epoch 594/2000, Train Loss: 5.397508353435347, Val Loss: 5.748842890382907, Val MAE: 1.6008778810501099\n",
      "Epoch 595/2000, Train Loss: 5.39724636530893, Val Loss: 5.748672684489405, Val MAE: 1.6005345582962036\n",
      "Epoch 596/2000, Train Loss: 5.396964005564905, Val Loss: 5.748208123336145, Val MAE: 1.600769281387329\n",
      "Epoch 597/2000, Train Loss: 5.3966164525170965, Val Loss: 5.748182950034419, Val MAE: 1.6006720066070557\n",
      "Epoch 598/2000, Train Loss: 5.396503672643416, Val Loss: 5.747693952617295, Val MAE: 1.6011178493499756\n",
      "Epoch 599/2000, Train Loss: 5.396039542306233, Val Loss: 5.747685239220249, Val MAE: 1.600794792175293\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 600/2000, Train Loss: 5.395885991513268, Val Loss: 5.7472494083260175, Val MAE: 1.601273775100708\n",
      "Epoch 601/2000, Train Loss: 5.39541002229265, Val Loss: 5.747322667082515, Val MAE: 1.6005724668502808\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 602/2000, Train Loss: 5.395209168817358, Val Loss: 5.747092282917886, Val MAE: 1.600462794303894\n",
      "Epoch 603/2000, Train Loss: 5.394923606110156, Val Loss: 5.746805495442965, Val MAE: 1.6007895469665527\n",
      "Epoch 604/2000, Train Loss: 5.394654896794533, Val Loss: 5.7465074838848285, Val MAE: 1.6008362770080566\n",
      "Epoch 605/2000, Train Loss: 5.394367863643681, Val Loss: 5.746378910012931, Val MAE: 1.6009505987167358\n",
      "Epoch 606/2000, Train Loss: 5.394148741365737, Val Loss: 5.746039911512206, Val MAE: 1.6009584665298462\n",
      "Epoch 607/2000, Train Loss: 5.393788816054718, Val Loss: 5.745744349395099, Val MAE: 1.6010751724243164\n",
      "Epoch 608/2000, Train Loss: 5.3935568468575745, Val Loss: 5.745680816220945, Val MAE: 1.601041555404663\n",
      "Epoch 609/2000, Train Loss: 5.3932103120802495, Val Loss: 5.745381074578755, Val MAE: 1.600759744644165\n",
      "Epoch 610/2000, Train Loss: 5.392918453176285, Val Loss: 5.74513506971368, Val MAE: 1.600751280784607\n",
      "Epoch 611/2000, Train Loss: 5.392650983435935, Val Loss: 5.7449920808138835, Val MAE: 1.6008819341659546\n",
      "Epoch 612/2000, Train Loss: 5.392398233568056, Val Loss: 5.744796202725956, Val MAE: 1.6009517908096313\n",
      "Epoch 613/2000, Train Loss: 5.39214087673512, Val Loss: 5.744595331700935, Val MAE: 1.6008700132369995\n",
      "Epoch 614/2000, Train Loss: 5.3918542415637685, Val Loss: 5.744161839970026, Val MAE: 1.6008307933807373\n",
      "Epoch 615/2000, Train Loss: 5.391580945752525, Val Loss: 5.744003596291265, Val MAE: 1.6010066270828247\n",
      "Epoch 616/2000, Train Loss: 5.391220315588267, Val Loss: 5.74388236645894, Val MAE: 1.6008857488632202\n",
      "Epoch 617/2000, Train Loss: 5.39123106254481, Val Loss: 5.743670470579684, Val MAE: 1.6009122133255005\n",
      "Epoch 618/2000, Train Loss: 5.390870459477386, Val Loss: 5.743638999236104, Val MAE: 1.6005616188049316\n",
      "Epoch 619/2000, Train Loss: 5.390448042576285, Val Loss: 5.743519603112422, Val MAE: 1.6005347967147827\n",
      "Epoch 620/2000, Train Loss: 5.390140950889507, Val Loss: 5.743143421065188, Val MAE: 1.6005440950393677\n",
      "Epoch 621/2000, Train Loss: 5.389982262436225, Val Loss: 5.742726096410634, Val MAE: 1.6007115840911865\n",
      "Epoch 622/2000, Train Loss: 5.389574213987669, Val Loss: 5.742717038053985, Val MAE: 1.6006845235824585\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 623/2000, Train Loss: 5.389503156190854, Val Loss: 5.742542761728304, Val MAE: 1.600311517715454\n",
      "Epoch 624/2000, Train Loss: 5.389029053883348, Val Loss: 5.742179989632481, Val MAE: 1.6007568836212158\n",
      "Epoch 625/2000, Train Loss: 5.388754671849808, Val Loss: 5.741771683142455, Val MAE: 1.6008470058441162\n",
      "Epoch 626/2000, Train Loss: 5.388530710396172, Val Loss: 5.741658255776863, Val MAE: 1.6005582809448242\n",
      "Epoch 627/2000, Train Loss: 5.38845140408832, Val Loss: 5.74118275762698, Val MAE: 1.6010167598724365\n",
      "Epoch 628/2000, Train Loss: 5.387988841508493, Val Loss: 5.740991220926291, Val MAE: 1.6007508039474487\n",
      "Epoch 629/2000, Train Loss: 5.387721475793139, Val Loss: 5.741007532275051, Val MAE: 1.6008703708648682\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 630/2000, Train Loss: 5.387688056756542, Val Loss: 5.74083136664618, Val MAE: 1.6008825302124023\n",
      "Epoch 631/2000, Train Loss: 5.38716259244292, Val Loss: 5.740770184118813, Val MAE: 1.6007294654846191\n",
      "Epoch 632/2000, Train Loss: 5.386928239563332, Val Loss: 5.740361000602034, Val MAE: 1.600894808769226\n",
      "Epoch 633/2000, Train Loss: 5.386726010609143, Val Loss: 5.7402480557241935, Val MAE: 1.6006598472595215\n",
      "Epoch 634/2000, Train Loss: 5.386427990069782, Val Loss: 5.739986797356095, Val MAE: 1.6005160808563232\n",
      "Epoch 635/2000, Train Loss: 5.386252675737653, Val Loss: 5.739635264381357, Val MAE: 1.6005902290344238\n",
      "Epoch 636/2000, Train Loss: 5.385821834741387, Val Loss: 5.739754492718874, Val MAE: 1.60036301612854\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 637/2000, Train Loss: 5.385593035789881, Val Loss: 5.739497158961194, Val MAE: 1.6005698442459106\n",
      "Epoch 638/2000, Train Loss: 5.385625204680924, Val Loss: 5.739636716492679, Val MAE: 1.600012183189392\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 639/2000, Train Loss: 5.3851322553931285, Val Loss: 5.739400760934258, Val MAE: 1.6000235080718994\n",
      "Epoch 640/2000, Train Loss: 5.384855431456703, Val Loss: 5.739281701658844, Val MAE: 1.5996143817901611\n",
      "Epoch 641/2000, Train Loss: 5.38461715951929, Val Loss: 5.738873854051672, Val MAE: 1.6001322269439697\n",
      "Epoch 642/2000, Train Loss: 5.384403944183285, Val Loss: 5.738735501919318, Val MAE: 1.6002922058105469\n",
      "Epoch 643/2000, Train Loss: 5.3842001117341205, Val Loss: 5.738826158396694, Val MAE: 1.5995628833770752\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 644/2000, Train Loss: 5.383770813915111, Val Loss: 5.738464497189274, Val MAE: 1.5998743772506714\n",
      "Epoch 645/2000, Train Loss: 5.383461342153207, Val Loss: 5.738254956422596, Val MAE: 1.599826693534851\n",
      "Epoch 646/2000, Train Loss: 5.383210733857983, Val Loss: 5.738099452370168, Val MAE: 1.5996437072753906\n",
      "Epoch 647/2000, Train Loss: 5.38301188135382, Val Loss: 5.738114457064812, Val MAE: 1.599200963973999\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 648/2000, Train Loss: 5.382795677480355, Val Loss: 5.737708884459388, Val MAE: 1.5997755527496338\n",
      "Epoch 649/2000, Train Loss: 5.382427288561451, Val Loss: 5.737473367915605, Val MAE: 1.5996648073196411\n",
      "Epoch 650/2000, Train Loss: 5.382171859504638, Val Loss: 5.7373943071846565, Val MAE: 1.5994346141815186\n",
      "Epoch 651/2000, Train Loss: 5.382011776655011, Val Loss: 5.737284320301237, Val MAE: 1.5994040966033936\n",
      "Epoch 652/2000, Train Loss: 5.3816327874212515, Val Loss: 5.736979450745685, Val MAE: 1.599397897720337\n",
      "Epoch 653/2000, Train Loss: 5.381445956515393, Val Loss: 5.736924604720661, Val MAE: 1.5994021892547607\n",
      "Epoch 654/2000, Train Loss: 5.381144967740225, Val Loss: 5.736824613463259, Val MAE: 1.5992352962493896\n",
      "Epoch 655/2000, Train Loss: 5.3808446074773695, Val Loss: 5.736651099511972, Val MAE: 1.599285364151001\n",
      "Epoch 656/2000, Train Loss: 5.380681488482068, Val Loss: 5.736269374870743, Val MAE: 1.5995503664016724\n",
      "Epoch 657/2000, Train Loss: 5.380310773010576, Val Loss: 5.736186991955527, Val MAE: 1.5992528200149536\n",
      "Epoch 658/2000, Train Loss: 5.380171396965547, Val Loss: 5.736021031844871, Val MAE: 1.5993585586547852\n",
      "Epoch 659/2000, Train Loss: 5.379858488100335, Val Loss: 5.736160540617205, Val MAE: 1.598574161529541\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 660/2000, Train Loss: 5.379614166438286, Val Loss: 5.7358967095886895, Val MAE: 1.5990290641784668\n",
      "Epoch 661/2000, Train Loss: 5.3794190388678835, Val Loss: 5.735553979964796, Val MAE: 1.5991475582122803\n",
      "Epoch 662/2000, Train Loss: 5.379008672675308, Val Loss: 5.735510749098722, Val MAE: 1.5989627838134766\n",
      "Epoch 663/2000, Train Loss: 5.378831759210542, Val Loss: 5.735206356836022, Val MAE: 1.5990550518035889\n",
      "Epoch 664/2000, Train Loss: 5.378522018242016, Val Loss: 5.735149873323762, Val MAE: 1.5987823009490967\n",
      "Epoch 665/2000, Train Loss: 5.3783484548854625, Val Loss: 5.735029315912031, Val MAE: 1.5985084772109985\n",
      "Epoch 666/2000, Train Loss: 5.377937468234122, Val Loss: 5.734906213090325, Val MAE: 1.5984408855438232\n",
      "Epoch 667/2000, Train Loss: 5.377859364925011, Val Loss: 5.7348489805099065, Val MAE: 1.597951889038086\n",
      "Epoch 668/2000, Train Loss: 5.377457548580398, Val Loss: 5.734563172045833, Val MAE: 1.5981873273849487\n",
      "Epoch 669/2000, Train Loss: 5.377237964397916, Val Loss: 5.734580771456436, Val MAE: 1.5981669425964355\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 670/2000, Train Loss: 5.376968703917598, Val Loss: 5.7343115364557375, Val MAE: 1.5983612537384033\n",
      "Epoch 671/2000, Train Loss: 5.376716409075184, Val Loss: 5.733972040155977, Val MAE: 1.5982778072357178\n",
      "Epoch 672/2000, Train Loss: 5.3764711651141, Val Loss: 5.734042264088214, Val MAE: 1.5981606245040894\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 673/2000, Train Loss: 5.376143498960974, Val Loss: 5.733709382628082, Val MAE: 1.598323941230774\n",
      "Epoch 674/2000, Train Loss: 5.375939036657238, Val Loss: 5.733595934118335, Val MAE: 1.5981286764144897\n",
      "Epoch 675/2000, Train Loss: 5.375654357826936, Val Loss: 5.7335387024493025, Val MAE: 1.5978736877441406\n",
      "Epoch 676/2000, Train Loss: 5.375474298277177, Val Loss: 5.733012017215793, Val MAE: 1.598137617111206\n",
      "Epoch 677/2000, Train Loss: 5.3751369734031895, Val Loss: 5.733043090739382, Val MAE: 1.5979444980621338\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 678/2000, Train Loss: 5.374867449504737, Val Loss: 5.73279155002457, Val MAE: 1.5979173183441162\n",
      "Epoch 679/2000, Train Loss: 5.374713759116939, Val Loss: 5.732758615724173, Val MAE: 1.5976364612579346\n",
      "Epoch 680/2000, Train Loss: 5.374555294485179, Val Loss: 5.73272788570197, Val MAE: 1.597299337387085\n",
      "Epoch 681/2000, Train Loss: 5.374169037847096, Val Loss: 5.732389974302473, Val MAE: 1.5979384183883667\n",
      "Epoch 682/2000, Train Loss: 5.37381990420659, Val Loss: 5.732404922946878, Val MAE: 1.597548007965088\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 683/2000, Train Loss: 5.373669770634736, Val Loss: 5.732186530799312, Val MAE: 1.5975744724273682\n",
      "Epoch 684/2000, Train Loss: 5.373378199309551, Val Loss: 5.73198472068215, Val MAE: 1.5974582433700562\n",
      "Epoch 685/2000, Train Loss: 5.373043126073377, Val Loss: 5.7316291040601355, Val MAE: 1.597683072090149\n",
      "Epoch 686/2000, Train Loss: 5.3727576009494, Val Loss: 5.731495617270834, Val MAE: 1.5976077318191528\n",
      "Epoch 687/2000, Train Loss: 5.372593272327958, Val Loss: 5.731388681160929, Val MAE: 1.5974669456481934\n",
      "Epoch 688/2000, Train Loss: 5.372367626172736, Val Loss: 5.731129040502991, Val MAE: 1.597649335861206\n",
      "Epoch 689/2000, Train Loss: 5.372157661617851, Val Loss: 5.730904538697059, Val MAE: 1.5976624488830566\n",
      "Epoch 690/2000, Train Loss: 5.371835026079966, Val Loss: 5.730872360938186, Val MAE: 1.5974875688552856\n",
      "Epoch 691/2000, Train Loss: 5.3715685509528015, Val Loss: 5.730437086353973, Val MAE: 1.5975311994552612\n",
      "Epoch 692/2000, Train Loss: 5.371247912023707, Val Loss: 5.730285615458037, Val MAE: 1.597585678100586\n",
      "Epoch 693/2000, Train Loss: 5.370975541196014, Val Loss: 5.730179177147169, Val MAE: 1.5974111557006836\n",
      "Epoch 694/2000, Train Loss: 5.370745016268489, Val Loss: 5.730165528048069, Val MAE: 1.5970604419708252\n",
      "Epoch 695/2000, Train Loss: 5.370514986466389, Val Loss: 5.7299658460172305, Val MAE: 1.5971709489822388\n",
      "Epoch 696/2000, Train Loss: 5.370526323597, Val Loss: 5.729685055097673, Val MAE: 1.5972756147384644\n",
      "Epoch 697/2000, Train Loss: 5.370181305655118, Val Loss: 5.729745305732849, Val MAE: 1.5970065593719482\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 698/2000, Train Loss: 5.36982420126709, Val Loss: 5.729739802693008, Val MAE: 1.5965090990066528\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 699/2000, Train Loss: 5.369497988611607, Val Loss: 5.729468221329038, Val MAE: 1.5964465141296387\n",
      "Epoch 700/2000, Train Loss: 5.369232721684433, Val Loss: 5.729363060325658, Val MAE: 1.5965733528137207\n",
      "Epoch 701/2000, Train Loss: 5.369046878344907, Val Loss: 5.729352448511561, Val MAE: 1.5962960720062256\n",
      "Epoch 702/2000, Train Loss: 5.3687461622158965, Val Loss: 5.729051630157943, Val MAE: 1.5965852737426758\n",
      "Epoch 703/2000, Train Loss: 5.3685792255871405, Val Loss: 5.728645108161716, Val MAE: 1.5968965291976929\n",
      "Epoch 704/2000, Train Loss: 5.368398089043109, Val Loss: 5.728719745024994, Val MAE: 1.5963441133499146\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 705/2000, Train Loss: 5.367932979193143, Val Loss: 5.728438150445256, Val MAE: 1.596431851387024\n",
      "Epoch 706/2000, Train Loss: 5.3676915269431555, Val Loss: 5.72823608003625, Val MAE: 1.5966523885726929\n",
      "Epoch 707/2000, Train Loss: 5.367466100414902, Val Loss: 5.728064681229606, Val MAE: 1.5963679552078247\n",
      "Epoch 708/2000, Train Loss: 5.367223020173729, Val Loss: 5.727952282545399, Val MAE: 1.5963127613067627\n",
      "Epoch 709/2000, Train Loss: 5.3669987999998625, Val Loss: 5.727773569957196, Val MAE: 1.5962724685668945\n",
      "Epoch 710/2000, Train Loss: 5.366701257975483, Val Loss: 5.727809621744564, Val MAE: 1.5960062742233276\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 711/2000, Train Loss: 5.3664781184870955, Val Loss: 5.727759225106021, Val MAE: 1.5957614183425903\n",
      "Epoch 712/2000, Train Loss: 5.366685192908811, Val Loss: 5.727506424855749, Val MAE: 1.5956648588180542\n",
      "Epoch 713/2000, Train Loss: 5.3661220572013235, Val Loss: 5.727178894051718, Val MAE: 1.5956710577011108\n",
      "Epoch 714/2000, Train Loss: 5.365817714626734, Val Loss: 5.727233929371615, Val MAE: 1.5958150625228882\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 715/2000, Train Loss: 5.365501793734546, Val Loss: 5.7269820697628395, Val MAE: 1.595587968826294\n",
      "Epoch 716/2000, Train Loss: 5.365255526301728, Val Loss: 5.727065076704054, Val MAE: 1.5958136320114136\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 717/2000, Train Loss: 5.365185326878912, Val Loss: 5.726670073897831, Val MAE: 1.596086859703064\n",
      "Epoch 718/2000, Train Loss: 5.364768469862163, Val Loss: 5.726562247819492, Val MAE: 1.5958524942398071\n",
      "Epoch 719/2000, Train Loss: 5.364505441318005, Val Loss: 5.726155887776558, Val MAE: 1.5962718725204468\n",
      "Epoch 720/2000, Train Loss: 5.364307437120903, Val Loss: 5.72617160444595, Val MAE: 1.5955666303634644\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 721/2000, Train Loss: 5.364037919430394, Val Loss: 5.725800417978829, Val MAE: 1.595971941947937\n",
      "Epoch 722/2000, Train Loss: 5.363742502200444, Val Loss: 5.725711307212118, Val MAE: 1.5959311723709106\n",
      "Epoch 723/2000, Train Loss: 5.363575949121914, Val Loss: 5.725486030636942, Val MAE: 1.5956639051437378\n",
      "Epoch 724/2000, Train Loss: 5.3632534340516855, Val Loss: 5.725265090254104, Val MAE: 1.5958430767059326\n",
      "Epoch 725/2000, Train Loss: 5.363103691412478, Val Loss: 5.725326499443171, Val MAE: 1.5956499576568604\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 726/2000, Train Loss: 5.3628221418889455, Val Loss: 5.724753256784666, Val MAE: 1.5960596799850464\n",
      "Epoch 727/2000, Train Loss: 5.362648880037097, Val Loss: 5.724885440383118, Val MAE: 1.5956836938858032\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 728/2000, Train Loss: 5.362250317493683, Val Loss: 5.724697756657907, Val MAE: 1.5957001447677612\n",
      "Epoch 729/2000, Train Loss: 5.3621312721273915, Val Loss: 5.724763696255669, Val MAE: 1.5953952074050903\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 730/2000, Train Loss: 5.361844544004673, Val Loss: 5.724358696912042, Val MAE: 1.5955777168273926\n",
      "Epoch 731/2000, Train Loss: 5.361624135843555, Val Loss: 5.7243703975589995, Val MAE: 1.5955919027328491\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 732/2000, Train Loss: 5.361301362724224, Val Loss: 5.7240759101482706, Val MAE: 1.5960286855697632\n",
      "Epoch 733/2000, Train Loss: 5.361065962799496, Val Loss: 5.723950180165264, Val MAE: 1.5960566997528076\n",
      "Epoch 734/2000, Train Loss: 5.360867781340446, Val Loss: 5.723846194211861, Val MAE: 1.5958225727081299\n",
      "Epoch 735/2000, Train Loss: 5.360605816703544, Val Loss: 5.723634303527505, Val MAE: 1.595905065536499\n",
      "Epoch 736/2000, Train Loss: 5.36032930802997, Val Loss: 5.723407949238378, Val MAE: 1.5955463647842407\n",
      "Epoch 737/2000, Train Loss: 5.3600633058809715, Val Loss: 5.723225945908724, Val MAE: 1.5956997871398926\n",
      "Epoch 738/2000, Train Loss: 5.359834151566658, Val Loss: 5.722885750211343, Val MAE: 1.5956774950027466\n",
      "Epoch 739/2000, Train Loss: 5.359600555552805, Val Loss: 5.722684136042172, Val MAE: 1.5956658124923706\n",
      "Epoch 740/2000, Train Loss: 5.359326590625943, Val Loss: 5.7226322758088415, Val MAE: 1.5954484939575195\n",
      "Epoch 741/2000, Train Loss: 5.359317946987702, Val Loss: 5.722765218956391, Val MAE: 1.5949370861053467\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 742/2000, Train Loss: 5.35890638786332, Val Loss: 5.722331174743285, Val MAE: 1.5954140424728394\n",
      "Epoch 743/2000, Train Loss: 5.358799865932048, Val Loss: 5.72242555195403, Val MAE: 1.5947952270507812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 744/2000, Train Loss: 5.358523467826977, Val Loss: 5.722345335221072, Val MAE: 1.594847559928894\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 745/2000, Train Loss: 5.358198911853444, Val Loss: 5.722130908696294, Val MAE: 1.5947495698928833\n",
      "Epoch 746/2000, Train Loss: 5.357922457373537, Val Loss: 5.721886322768092, Val MAE: 1.5948694944381714\n",
      "Epoch 747/2000, Train Loss: 5.35765179373025, Val Loss: 5.721494292173911, Val MAE: 1.5951647758483887\n",
      "Epoch 748/2000, Train Loss: 5.357485651298781, Val Loss: 5.721263364367529, Val MAE: 1.5952601432800293\n",
      "Epoch 749/2000, Train Loss: 5.35729103068245, Val Loss: 5.721003381575284, Val MAE: 1.5953830480575562\n",
      "Epoch 750/2000, Train Loss: 5.356962292568521, Val Loss: 5.720843686514309, Val MAE: 1.5952363014221191\n",
      "Epoch 751/2000, Train Loss: 5.356801595258344, Val Loss: 5.720714692128908, Val MAE: 1.595243215560913\n",
      "Epoch 752/2000, Train Loss: 5.356486005736102, Val Loss: 5.720590987336745, Val MAE: 1.5949628353118896\n",
      "Epoch 753/2000, Train Loss: 5.3562101448698, Val Loss: 5.720720475693361, Val MAE: 1.5946025848388672\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 754/2000, Train Loss: 5.356084495174642, Val Loss: 5.720498000809906, Val MAE: 1.5946941375732422\n",
      "Epoch 755/2000, Train Loss: 5.355799223989101, Val Loss: 5.720316284020012, Val MAE: 1.594470500946045\n",
      "Epoch 756/2000, Train Loss: 5.355734710958456, Val Loss: 5.720406897935663, Val MAE: 1.5940155982971191\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 757/2000, Train Loss: 5.355310310871143, Val Loss: 5.719963969167219, Val MAE: 1.5942118167877197\n",
      "Epoch 758/2000, Train Loss: 5.3550663088342825, Val Loss: 5.719712787538493, Val MAE: 1.5944626331329346\n",
      "Epoch 759/2000, Train Loss: 5.35487067003136, Val Loss: 5.719641248658527, Val MAE: 1.5941177606582642\n",
      "Epoch 760/2000, Train Loss: 5.354596733291916, Val Loss: 5.719174405485847, Val MAE: 1.5946464538574219\n",
      "Epoch 761/2000, Train Loss: 5.3543510252451245, Val Loss: 5.718986987520795, Val MAE: 1.5947325229644775\n",
      "Epoch 762/2000, Train Loss: 5.354254465888371, Val Loss: 5.719175459412624, Val MAE: 1.5941673517227173\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 763/2000, Train Loss: 5.353946677401903, Val Loss: 5.718778753089248, Val MAE: 1.5944010019302368\n",
      "Epoch 764/2000, Train Loss: 5.353605231124695, Val Loss: 5.718644734825198, Val MAE: 1.594561219215393\n",
      "Epoch 765/2000, Train Loss: 5.353396390207547, Val Loss: 5.718649496908217, Val MAE: 1.5941874980926514\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 766/2000, Train Loss: 5.35317162079848, Val Loss: 5.7184688136300545, Val MAE: 1.5941572189331055\n",
      "Epoch 767/2000, Train Loss: 5.352912327506692, Val Loss: 5.718321284250746, Val MAE: 1.5940992832183838\n",
      "Epoch 768/2000, Train Loss: 5.352805537627164, Val Loss: 5.718335800214645, Val MAE: 1.5939041376113892\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 769/2000, Train Loss: 5.3524821057947145, Val Loss: 5.718038626220248, Val MAE: 1.593885898590088\n",
      "Epoch 770/2000, Train Loss: 5.352124429269209, Val Loss: 5.717784757780008, Val MAE: 1.5939172506332397\n",
      "Epoch 771/2000, Train Loss: 5.351988301320784, Val Loss: 5.7175257705402664, Val MAE: 1.5939475297927856\n",
      "Epoch 772/2000, Train Loss: 5.351812195224212, Val Loss: 5.717658554195264, Val MAE: 1.5938503742218018\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 773/2000, Train Loss: 5.3515122577726295, Val Loss: 5.717448315808168, Val MAE: 1.593575119972229\n",
      "Epoch 774/2000, Train Loss: 5.351221960418078, Val Loss: 5.717211562559145, Val MAE: 1.5938279628753662\n",
      "Epoch 775/2000, Train Loss: 5.351071475677974, Val Loss: 5.716632921260067, Val MAE: 1.5942323207855225\n",
      "Epoch 776/2000, Train Loss: 5.35070941785454, Val Loss: 5.71658994982002, Val MAE: 1.5939106941223145\n",
      "Epoch 777/2000, Train Loss: 5.350516849131252, Val Loss: 5.716564105282501, Val MAE: 1.5939792394638062\n",
      "Epoch 778/2000, Train Loss: 5.350250701850607, Val Loss: 5.716431170337426, Val MAE: 1.5940760374069214\n",
      "Epoch 779/2000, Train Loss: 5.350090749325843, Val Loss: 5.715918991817247, Val MAE: 1.5943896770477295\n",
      "Epoch 780/2000, Train Loss: 5.349774524496107, Val Loss: 5.716061727260596, Val MAE: 1.5940179824829102\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 781/2000, Train Loss: 5.34965929223315, Val Loss: 5.716089601363611, Val MAE: 1.5935497283935547\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 782/2000, Train Loss: 5.349317620121032, Val Loss: 5.715652701960426, Val MAE: 1.5939488410949707\n",
      "Epoch 783/2000, Train Loss: 5.349032869815491, Val Loss: 5.715382829445217, Val MAE: 1.5940042734146118\n",
      "Epoch 784/2000, Train Loss: 5.348927559607638, Val Loss: 5.715124775692593, Val MAE: 1.5942367315292358\n",
      "Epoch 785/2000, Train Loss: 5.348553925647776, Val Loss: 5.714861787012593, Val MAE: 1.594125747680664\n",
      "Epoch 786/2000, Train Loss: 5.348423034015632, Val Loss: 5.714623975598848, Val MAE: 1.594196081161499\n",
      "Epoch 787/2000, Train Loss: 5.348155888430591, Val Loss: 5.714702641335102, Val MAE: 1.5939284563064575\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 788/2000, Train Loss: 5.3480088501057095, Val Loss: 5.714264560183254, Val MAE: 1.5942597389221191\n",
      "Epoch 789/2000, Train Loss: 5.34765684998933, Val Loss: 5.71420282781671, Val MAE: 1.5940650701522827\n",
      "Epoch 790/2000, Train Loss: 5.347555524274382, Val Loss: 5.714229643709434, Val MAE: 1.5939749479293823\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 791/2000, Train Loss: 5.347265506109497, Val Loss: 5.714274028937022, Val MAE: 1.593568205833435\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 792/2000, Train Loss: 5.347042825337785, Val Loss: 5.7137601402010025, Val MAE: 1.5940406322479248\n",
      "Epoch 793/2000, Train Loss: 5.346862291635383, Val Loss: 5.713770786556629, Val MAE: 1.593942642211914\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 794/2000, Train Loss: 5.346609656195, Val Loss: 5.7137067609606165, Val MAE: 1.593682050704956\n",
      "Epoch 795/2000, Train Loss: 5.346380302555372, Val Loss: 5.713556876468731, Val MAE: 1.5936846733093262\n",
      "Epoch 796/2000, Train Loss: 5.346086867169374, Val Loss: 5.713151605121951, Val MAE: 1.5938185453414917\n",
      "Epoch 797/2000, Train Loss: 5.34582825163399, Val Loss: 5.7132083819182276, Val MAE: 1.5937100648880005\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 798/2000, Train Loss: 5.345709533946435, Val Loss: 5.712831946414545, Val MAE: 1.5939888954162598\n",
      "Epoch 799/2000, Train Loss: 5.345423429973691, Val Loss: 5.712779006598922, Val MAE: 1.5934089422225952\n",
      "Epoch 800/2000, Train Loss: 5.3450855396735175, Val Loss: 5.71255514558849, Val MAE: 1.5935823917388916\n",
      "Epoch 801/2000, Train Loss: 5.344955996675109, Val Loss: 5.712337459218247, Val MAE: 1.593672752380371\n",
      "Epoch 802/2000, Train Loss: 5.34465386614172, Val Loss: 5.712326332334349, Val MAE: 1.5936431884765625\n",
      "Epoch 803/2000, Train Loss: 5.344493989790099, Val Loss: 5.712285959301374, Val MAE: 1.5933676958084106\n",
      "Epoch 804/2000, Train Loss: 5.344312215841295, Val Loss: 5.71186937068216, Val MAE: 1.5938832759857178\n",
      "Epoch 805/2000, Train Loss: 5.343971913167912, Val Loss: 5.711776561602176, Val MAE: 1.5934360027313232\n",
      "Epoch 806/2000, Train Loss: 5.343815245484064, Val Loss: 5.711774282362483, Val MAE: 1.593097448348999\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 807/2000, Train Loss: 5.343526562698117, Val Loss: 5.711361955156384, Val MAE: 1.5936849117279053\n",
      "Epoch 808/2000, Train Loss: 5.343352111102325, Val Loss: 5.711163055094739, Val MAE: 1.5932542085647583\n",
      "Epoch 809/2000, Train Loss: 5.343053657349191, Val Loss: 5.710977042122354, Val MAE: 1.5938196182250977\n",
      "Epoch 810/2000, Train Loss: 5.342860872186799, Val Loss: 5.711099532902788, Val MAE: 1.5933758020401\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 811/2000, Train Loss: 5.342657909353043, Val Loss: 5.710766903121901, Val MAE: 1.5935850143432617\n",
      "Epoch 812/2000, Train Loss: 5.342375637526918, Val Loss: 5.710578959195986, Val MAE: 1.5937176942825317\n",
      "Epoch 813/2000, Train Loss: 5.342146331201885, Val Loss: 5.710610031170218, Val MAE: 1.5931366682052612\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 814/2000, Train Loss: 5.341944320700187, Val Loss: 5.71021038845227, Val MAE: 1.5933903455734253\n",
      "Epoch 815/2000, Train Loss: 5.34173566467908, Val Loss: 5.710146229019223, Val MAE: 1.593044400215149\n",
      "Epoch 816/2000, Train Loss: 5.341531084088857, Val Loss: 5.710120730170416, Val MAE: 1.593071699142456\n",
      "Epoch 817/2000, Train Loss: 5.341212525659678, Val Loss: 5.709828254917711, Val MAE: 1.5930074453353882\n",
      "Epoch 818/2000, Train Loss: 5.340940525462978, Val Loss: 5.709694546570471, Val MAE: 1.5933135747909546\n",
      "Epoch 819/2000, Train Loss: 5.34081418341436, Val Loss: 5.709605790743041, Val MAE: 1.5930482149124146\n",
      "Epoch 820/2000, Train Loss: 5.340724657825145, Val Loss: 5.709686454237419, Val MAE: 1.5926222801208496\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 821/2000, Train Loss: 5.3402979709496385, Val Loss: 5.7093562121055905, Val MAE: 1.5930944681167603\n",
      "Epoch 822/2000, Train Loss: 5.340094532285418, Val Loss: 5.709370983075294, Val MAE: 1.5927255153656006\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 823/2000, Train Loss: 5.339877056119142, Val Loss: 5.709147788288032, Val MAE: 1.5928575992584229\n",
      "Epoch 824/2000, Train Loss: 5.339745094036234, Val Loss: 5.709106295435071, Val MAE: 1.5924973487854004\n",
      "Epoch 825/2000, Train Loss: 5.339381779020392, Val Loss: 5.708879194811943, Val MAE: 1.5925768613815308\n",
      "Epoch 826/2000, Train Loss: 5.339318542057658, Val Loss: 5.708515734437409, Val MAE: 1.5928245782852173\n",
      "Epoch 827/2000, Train Loss: 5.339061098145734, Val Loss: 5.708154148237056, Val MAE: 1.5930290222167969\n",
      "Epoch 828/2000, Train Loss: 5.338835633363127, Val Loss: 5.7083779062608695, Val MAE: 1.592712163925171\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 829/2000, Train Loss: 5.33863959557401, Val Loss: 5.7083008877636825, Val MAE: 1.5922825336456299\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 830/2000, Train Loss: 5.3383818679423, Val Loss: 5.707780692282073, Val MAE: 1.5931729078292847\n",
      "Epoch 831/2000, Train Loss: 5.338090723455162, Val Loss: 5.707773156486884, Val MAE: 1.5925956964492798\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 832/2000, Train Loss: 5.337910515921457, Val Loss: 5.707814863515557, Val MAE: 1.592254877090454\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 833/2000, Train Loss: 5.337720272660172, Val Loss: 5.707343296857055, Val MAE: 1.5927480459213257\n",
      "Epoch 834/2000, Train Loss: 5.337365867720784, Val Loss: 5.7072968711729075, Val MAE: 1.592591643333435\n",
      "Epoch 835/2000, Train Loss: 5.337174265255482, Val Loss: 5.707402988448056, Val MAE: 1.592097520828247\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 836/2000, Train Loss: 5.336856183663461, Val Loss: 5.707220769757889, Val MAE: 1.592145323753357\n",
      "Epoch 837/2000, Train Loss: 5.336712107366445, Val Loss: 5.706980338701779, Val MAE: 1.5921845436096191\n",
      "Epoch 838/2000, Train Loss: 5.336500963302333, Val Loss: 5.7067145886588895, Val MAE: 1.592372179031372\n",
      "Epoch 839/2000, Train Loss: 5.33630596391757, Val Loss: 5.706493904421089, Val MAE: 1.592342495918274\n",
      "Epoch 840/2000, Train Loss: 5.336208268674976, Val Loss: 5.706662424204911, Val MAE: 1.5918245315551758\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 841/2000, Train Loss: 5.335872466257137, Val Loss: 5.706311292376722, Val MAE: 1.5922385454177856\n",
      "Epoch 842/2000, Train Loss: 5.335524083190531, Val Loss: 5.7062179369663975, Val MAE: 1.5917813777923584\n",
      "Epoch 843/2000, Train Loss: 5.335426349022454, Val Loss: 5.705994194937409, Val MAE: 1.592164397239685\n",
      "Epoch 844/2000, Train Loss: 5.335112311318254, Val Loss: 5.705959487945662, Val MAE: 1.591702938079834\n",
      "Epoch 845/2000, Train Loss: 5.334858936293707, Val Loss: 5.705806871574224, Val MAE: 1.591774821281433\n",
      "Epoch 846/2000, Train Loss: 5.334770077135595, Val Loss: 5.705669078987309, Val MAE: 1.5917470455169678\n",
      "Epoch 847/2000, Train Loss: 5.334409726999915, Val Loss: 5.705486470542916, Val MAE: 1.5917893648147583\n",
      "Epoch 848/2000, Train Loss: 5.334331015144579, Val Loss: 5.705187267484286, Val MAE: 1.5920159816741943\n",
      "Epoch 849/2000, Train Loss: 5.33406896879772, Val Loss: 5.7052207782819, Val MAE: 1.5917298793792725\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 850/2000, Train Loss: 5.3337812423706055, Val Loss: 5.705144342980618, Val MAE: 1.591762661933899\n",
      "Epoch 851/2000, Train Loss: 5.3337630997402075, Val Loss: 5.704909367253292, Val MAE: 1.5918070077896118\n",
      "Epoch 852/2000, Train Loss: 5.333266504963547, Val Loss: 5.704738878420004, Val MAE: 1.5917274951934814\n",
      "Epoch 853/2000, Train Loss: 5.333030863767942, Val Loss: 5.70469857889031, Val MAE: 1.591585636138916\n",
      "Epoch 854/2000, Train Loss: 5.332971020033458, Val Loss: 5.704571719153212, Val MAE: 1.591787338256836\n",
      "Epoch 855/2000, Train Loss: 5.332817637526091, Val Loss: 5.704648740703542, Val MAE: 1.5911915302276611\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 856/2000, Train Loss: 5.332645402166056, Val Loss: 5.7041041540261075, Val MAE: 1.591648817062378\n",
      "Epoch 857/2000, Train Loss: 5.332247625346925, Val Loss: 5.704281754632244, Val MAE: 1.5914807319641113\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 858/2000, Train Loss: 5.332034905975256, Val Loss: 5.70391117920387, Val MAE: 1.5916965007781982\n",
      "Epoch 859/2000, Train Loss: 5.331834226825722, Val Loss: 5.704112645910785, Val MAE: 1.5913200378417969\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 860/2000, Train Loss: 5.331636312392125, Val Loss: 5.703809298871125, Val MAE: 1.5914767980575562\n",
      "Epoch 861/2000, Train Loss: 5.331352897884979, Val Loss: 5.703649442221411, Val MAE: 1.5914911031723022\n",
      "Epoch 862/2000, Train Loss: 5.331194999098861, Val Loss: 5.703750552113997, Val MAE: 1.5909110307693481\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 863/2000, Train Loss: 5.330923495705408, Val Loss: 5.703547178149588, Val MAE: 1.5910176038742065\n",
      "Epoch 864/2000, Train Loss: 5.330725351410127, Val Loss: 5.70329765023077, Val MAE: 1.590925931930542\n",
      "Epoch 865/2000, Train Loss: 5.330510633080715, Val Loss: 5.703368995623486, Val MAE: 1.5907572507858276\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 866/2000, Train Loss: 5.3302376639415145, Val Loss: 5.703313125610716, Val MAE: 1.5903593301773071\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 867/2000, Train Loss: 5.330045284644388, Val Loss: 5.7031911019156105, Val MAE: 1.590223789215088\n",
      "Epoch 868/2000, Train Loss: 5.329835155159549, Val Loss: 5.703036474753228, Val MAE: 1.5903253555297852\n",
      "Epoch 869/2000, Train Loss: 5.3296070034280785, Val Loss: 5.702959618391612, Val MAE: 1.5903921127319336\n",
      "Epoch 870/2000, Train Loss: 5.329427429188481, Val Loss: 5.702971657709608, Val MAE: 1.5900298357009888\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 871/2000, Train Loss: 5.329097357876782, Val Loss: 5.702670404533728, Val MAE: 1.5902217626571655\n",
      "Epoch 872/2000, Train Loss: 5.3289168436371215, Val Loss: 5.702535698674505, Val MAE: 1.5902069807052612\n",
      "Epoch 873/2000, Train Loss: 5.3288137964762745, Val Loss: 5.7025431224147844, Val MAE: 1.5899471044540405\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 874/2000, Train Loss: 5.328600111517749, Val Loss: 5.701926579305885, Val MAE: 1.5908113718032837\n",
      "Epoch 875/2000, Train Loss: 5.328220103174595, Val Loss: 5.701979690689924, Val MAE: 1.5903081893920898\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 876/2000, Train Loss: 5.328084581591225, Val Loss: 5.701833378193211, Val MAE: 1.5902047157287598\n",
      "Epoch 877/2000, Train Loss: 5.327916982893705, Val Loss: 5.701839558301716, Val MAE: 1.590218186378479\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 878/2000, Train Loss: 5.327571172264577, Val Loss: 5.701516981427458, Val MAE: 1.590113878250122\n",
      "Epoch 879/2000, Train Loss: 5.327447463726175, Val Loss: 5.701330896683425, Val MAE: 1.5905052423477173\n",
      "Epoch 880/2000, Train Loss: 5.327150392163228, Val Loss: 5.701181120464197, Val MAE: 1.590854525566101\n",
      "Epoch 881/2000, Train Loss: 5.327042192157769, Val Loss: 5.701281711094605, Val MAE: 1.5901912450790405\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 882/2000, Train Loss: 5.32683055640777, Val Loss: 5.700871088909446, Val MAE: 1.5905554294586182\n",
      "Epoch 883/2000, Train Loss: 5.326466676552979, Val Loss: 5.70082286765816, Val MAE: 1.5904014110565186\n",
      "Epoch 884/2000, Train Loss: 5.326300085387878, Val Loss: 5.7008338568772015, Val MAE: 1.590000033378601\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 885/2000, Train Loss: 5.326149988140882, Val Loss: 5.700717265270536, Val MAE: 1.589943766593933\n",
      "Epoch 886/2000, Train Loss: 5.325916485581744, Val Loss: 5.700402419364781, Val MAE: 1.5903884172439575\n",
      "Epoch 887/2000, Train Loss: 5.325805365074528, Val Loss: 5.700059363191281, Val MAE: 1.59028160572052\n",
      "Epoch 888/2000, Train Loss: 5.32544064035221, Val Loss: 5.700205608382138, Val MAE: 1.5898385047912598\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 889/2000, Train Loss: 5.325274458491911, Val Loss: 5.700028453762743, Val MAE: 1.5900473594665527\n",
      "Epoch 890/2000, Train Loss: 5.325072919793904, Val Loss: 5.6999414851177, Val MAE: 1.5898830890655518\n",
      "Epoch 891/2000, Train Loss: 5.324905486445792, Val Loss: 5.700010138185017, Val MAE: 1.5894416570663452\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 892/2000, Train Loss: 5.324646934667664, Val Loss: 5.69960413590666, Val MAE: 1.5898022651672363\n",
      "Epoch 893/2000, Train Loss: 5.324387713102787, Val Loss: 5.6997517780244165, Val MAE: 1.5895593166351318\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 894/2000, Train Loss: 5.3241777128102825, Val Loss: 5.6993983480908454, Val MAE: 1.5899245738983154\n",
      "Epoch 895/2000, Train Loss: 5.3240117120709245, Val Loss: 5.699216189506586, Val MAE: 1.5897608995437622\n",
      "Epoch 896/2000, Train Loss: 5.323874439679763, Val Loss: 5.699240528981256, Val MAE: 1.589736819267273\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 897/2000, Train Loss: 5.323594587273702, Val Loss: 5.699056759066538, Val MAE: 1.5896873474121094\n",
      "Epoch 898/2000, Train Loss: 5.323345582305672, Val Loss: 5.698923068764742, Val MAE: 1.5897773504257202\n",
      "Epoch 899/2000, Train Loss: 5.3231722924341875, Val Loss: 5.698788731287744, Val MAE: 1.5896735191345215\n",
      "Epoch 900/2000, Train Loss: 5.32294094520585, Val Loss: 5.698701522629196, Val MAE: 1.5893973112106323\n",
      "Epoch 901/2000, Train Loss: 5.322994651297798, Val Loss: 5.6982467031442425, Val MAE: 1.589746356010437\n",
      "Epoch 902/2000, Train Loss: 5.322423245687035, Val Loss: 5.69812701920486, Val MAE: 1.589836835861206\n",
      "Epoch 903/2000, Train Loss: 5.322295486549522, Val Loss: 5.698113461745624, Val MAE: 1.5897269248962402\n",
      "Epoch 904/2000, Train Loss: 5.3221418748516, Val Loss: 5.697653714669954, Val MAE: 1.5897623300552368\n",
      "Epoch 905/2000, Train Loss: 5.321858908490846, Val Loss: 5.697477459861963, Val MAE: 1.5899728536605835\n",
      "Epoch 906/2000, Train Loss: 5.3218065022918895, Val Loss: 5.697771909495741, Val MAE: 1.5896124839782715\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 907/2000, Train Loss: 5.321446450640835, Val Loss: 5.69729876682299, Val MAE: 1.5896193981170654\n",
      "Epoch 908/2000, Train Loss: 5.321193891709158, Val Loss: 5.6970609855761225, Val MAE: 1.5898603200912476\n",
      "Epoch 909/2000, Train Loss: 5.321281471695387, Val Loss: 5.697177047166256, Val MAE: 1.5894197225570679\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 910/2000, Train Loss: 5.32075602348886, Val Loss: 5.697013589584864, Val MAE: 1.5895366668701172\n",
      "Epoch 911/2000, Train Loss: 5.320636879466605, Val Loss: 5.696641127131765, Val MAE: 1.5900222063064575\n",
      "Epoch 912/2000, Train Loss: 5.320291525624656, Val Loss: 5.696562719955722, Val MAE: 1.589661955833435\n",
      "Epoch 913/2000, Train Loss: 5.320153647120112, Val Loss: 5.6963563606553125, Val MAE: 1.5898001194000244\n",
      "Epoch 914/2000, Train Loss: 5.319931794018245, Val Loss: 5.696350718355689, Val MAE: 1.5898030996322632\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 915/2000, Train Loss: 5.319808376749832, Val Loss: 5.696438576012212, Val MAE: 1.5892016887664795\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 916/2000, Train Loss: 5.319668218000602, Val Loss: 5.696030255942534, Val MAE: 1.5897712707519531\n",
      "Epoch 917/2000, Train Loss: 5.319279700916137, Val Loss: 5.696093532379249, Val MAE: 1.5892812013626099\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 918/2000, Train Loss: 5.319206755233432, Val Loss: 5.695929550021796, Val MAE: 1.5890793800354004\n",
      "Epoch 919/2000, Train Loss: 5.31882559556847, Val Loss: 5.695664524075088, Val MAE: 1.5895227193832397\n",
      "Epoch 920/2000, Train Loss: 5.318772352555535, Val Loss: 5.695758070557489, Val MAE: 1.588979959487915\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 921/2000, Train Loss: 5.318586434552906, Val Loss: 5.695512883251231, Val MAE: 1.5891106128692627\n",
      "Epoch 922/2000, Train Loss: 5.318236314101088, Val Loss: 5.695487151178745, Val MAE: 1.5886824131011963\n",
      "Epoch 923/2000, Train Loss: 5.318047943373619, Val Loss: 5.695239884044052, Val MAE: 1.589263677597046\n",
      "Epoch 924/2000, Train Loss: 5.317857259264469, Val Loss: 5.695075483041436, Val MAE: 1.5888854265213013\n",
      "Epoch 925/2000, Train Loss: 5.317625425970949, Val Loss: 5.694803341220643, Val MAE: 1.5889325141906738\n",
      "Epoch 926/2000, Train Loss: 5.317536828217584, Val Loss: 5.695068182292699, Val MAE: 1.5884865522384644\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 927/2000, Train Loss: 5.3172188835358805, Val Loss: 5.694729758694267, Val MAE: 1.5886340141296387\n",
      "Epoch 928/2000, Train Loss: 5.316957406977547, Val Loss: 5.694721689282572, Val MAE: 1.588514804840088\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 929/2000, Train Loss: 5.316768180644487, Val Loss: 5.6946442946563804, Val MAE: 1.5885218381881714\n",
      "Epoch 930/2000, Train Loss: 5.316595769448653, Val Loss: 5.694479766788833, Val MAE: 1.5884860754013062\n",
      "Epoch 931/2000, Train Loss: 5.3164680435320255, Val Loss: 5.694407399869111, Val MAE: 1.5881056785583496\n",
      "Epoch 932/2000, Train Loss: 5.316117855128397, Val Loss: 5.694179115856824, Val MAE: 1.588518500328064\n",
      "Epoch 933/2000, Train Loss: 5.315974116744834, Val Loss: 5.6940588188736445, Val MAE: 1.5885801315307617\n",
      "Epoch 934/2000, Train Loss: 5.315811515945016, Val Loss: 5.6938030395303665, Val MAE: 1.5885368585586548\n",
      "Epoch 935/2000, Train Loss: 5.3156198401588695, Val Loss: 5.693870944121927, Val MAE: 1.588536262512207\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 936/2000, Train Loss: 5.315463239252358, Val Loss: 5.693857257382585, Val MAE: 1.588278889656067\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 937/2000, Train Loss: 5.315276188887315, Val Loss: 5.693373839288312, Val MAE: 1.5888707637786865\n",
      "Epoch 938/2000, Train Loss: 5.315028398495002, Val Loss: 5.693224401648985, Val MAE: 1.588863492012024\n",
      "Epoch 939/2000, Train Loss: 5.314756667253251, Val Loss: 5.693351745058637, Val MAE: 1.5885177850723267\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 940/2000, Train Loss: 5.314536522929724, Val Loss: 5.69325054343505, Val MAE: 1.5884294509887695\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 941/2000, Train Loss: 5.314363577263528, Val Loss: 5.6931416354959525, Val MAE: 1.5881991386413574\n",
      "Epoch 942/2000, Train Loss: 5.314107452287882, Val Loss: 5.692874920067437, Val MAE: 1.5885818004608154\n",
      "Epoch 943/2000, Train Loss: 5.313941822682863, Val Loss: 5.692828080705182, Val MAE: 1.5882693529129028\n",
      "Epoch 944/2000, Train Loss: 5.31376560031319, Val Loss: 5.692688445751456, Val MAE: 1.588205337524414\n",
      "Epoch 945/2000, Train Loss: 5.313589278616425, Val Loss: 5.692440137494959, Val MAE: 1.5883928537368774\n",
      "Epoch 946/2000, Train Loss: 5.313685064329219, Val Loss: 5.69259375201635, Val MAE: 1.587958574295044\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 947/2000, Train Loss: 5.313111435100279, Val Loss: 5.692305263166034, Val MAE: 1.5878469944000244\n",
      "Epoch 948/2000, Train Loss: 5.312961786083568, Val Loss: 5.692135977179997, Val MAE: 1.5882209539413452\n",
      "Epoch 949/2000, Train Loss: 5.312926430252553, Val Loss: 5.692074855761061, Val MAE: 1.5880606174468994\n",
      "Epoch 950/2000, Train Loss: 5.312546763094607, Val Loss: 5.69200325663848, Val MAE: 1.5878421068191528\n",
      "Epoch 951/2000, Train Loss: 5.312336849881091, Val Loss: 5.692006514524466, Val MAE: 1.587680459022522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 952/2000, Train Loss: 5.312293678920593, Val Loss: 5.691657660204336, Val MAE: 1.588173270225525\n",
      "Epoch 953/2000, Train Loss: 5.31196765778855, Val Loss: 5.691750488514565, Val MAE: 1.5876630544662476\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 954/2000, Train Loss: 5.311674469294135, Val Loss: 5.691470187920678, Val MAE: 1.587933897972107\n",
      "Epoch 955/2000, Train Loss: 5.311668938939459, Val Loss: 5.691463884866201, Val MAE: 1.587668538093567\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 956/2000, Train Loss: 5.311339429194955, Val Loss: 5.6914117298450675, Val MAE: 1.5874230861663818\n",
      "Epoch 957/2000, Train Loss: 5.31117735103685, Val Loss: 5.691177475707611, Val MAE: 1.587482213973999\n",
      "Epoch 958/2000, Train Loss: 5.3108665216648605, Val Loss: 5.691264350388757, Val MAE: 1.5874260663986206\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 959/2000, Train Loss: 5.3106879199750825, Val Loss: 5.690955252776817, Val MAE: 1.587357521057129\n",
      "Epoch 960/2000, Train Loss: 5.310458418854855, Val Loss: 5.690847142665029, Val MAE: 1.587475299835205\n",
      "Epoch 961/2000, Train Loss: 5.310284205258186, Val Loss: 5.690777356015797, Val MAE: 1.5873723030090332\n",
      "Epoch 962/2000, Train Loss: 5.310122463084039, Val Loss: 5.690457029991558, Val MAE: 1.5876026153564453\n",
      "Epoch 963/2000, Train Loss: 5.309884747009224, Val Loss: 5.690426925909264, Val MAE: 1.5874340534210205\n",
      "Epoch 964/2000, Train Loss: 5.3097391205719875, Val Loss: 5.690383734961897, Val MAE: 1.5874333381652832\n",
      "Epoch 965/2000, Train Loss: 5.309582200543634, Val Loss: 5.690365776374071, Val MAE: 1.5869605541229248\n",
      "Epoch 966/2000, Train Loss: 5.30941841285217, Val Loss: 5.689879572810748, Val MAE: 1.5873416662216187\n",
      "Epoch 967/2000, Train Loss: 5.309041488346796, Val Loss: 5.689806829488606, Val MAE: 1.5871493816375732\n",
      "Epoch 968/2000, Train Loss: 5.309026915283794, Val Loss: 5.689567153379094, Val MAE: 1.5873634815216064\n",
      "Epoch 969/2000, Train Loss: 5.308727757684116, Val Loss: 5.689957256619719, Val MAE: 1.5866705179214478\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 970/2000, Train Loss: 5.3084396201904855, Val Loss: 5.689509124338445, Val MAE: 1.5870213508605957\n",
      "Epoch 971/2000, Train Loss: 5.308378061889176, Val Loss: 5.689397797489749, Val MAE: 1.5871528387069702\n",
      "Epoch 972/2000, Train Loss: 5.308249548067097, Val Loss: 5.689349023027158, Val MAE: 1.586905837059021\n",
      "Epoch 973/2000, Train Loss: 5.307840982849207, Val Loss: 5.688953246636493, Val MAE: 1.5873003005981445\n",
      "Epoch 974/2000, Train Loss: 5.307803961527675, Val Loss: 5.688868395277849, Val MAE: 1.5872645378112793\n",
      "Epoch 975/2000, Train Loss: 5.307506194339513, Val Loss: 5.688938892853734, Val MAE: 1.5870816707611084\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 976/2000, Train Loss: 5.3072624021982495, Val Loss: 5.68863866430357, Val MAE: 1.587073802947998\n",
      "Epoch 977/2000, Train Loss: 5.307264927153349, Val Loss: 5.68869189931712, Val MAE: 1.5867384672164917\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 978/2000, Train Loss: 5.30681969111105, Val Loss: 5.688632696593573, Val MAE: 1.5868490934371948\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 979/2000, Train Loss: 5.306743418809647, Val Loss: 5.688562769636467, Val MAE: 1.5864564180374146\n",
      "Epoch 980/2000, Train Loss: 5.3066882162677995, Val Loss: 5.688201324928791, Val MAE: 1.586837887763977\n",
      "Epoch 981/2000, Train Loss: 5.306278507949432, Val Loss: 5.688125991219774, Val MAE: 1.5867424011230469\n",
      "Epoch 982/2000, Train Loss: 5.306098867519736, Val Loss: 5.688172083017658, Val MAE: 1.586898922920227\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 983/2000, Train Loss: 5.3058725910065965, Val Loss: 5.6880915304845265, Val MAE: 1.5865283012390137\n",
      "Epoch 984/2000, Train Loss: 5.305660536388206, Val Loss: 5.688226455121959, Val MAE: 1.5864511728286743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 985/2000, Train Loss: 5.30559032723402, Val Loss: 5.68817604194176, Val MAE: 1.5863851308822632\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 986/2000, Train Loss: 5.305297895101994, Val Loss: 5.687720199198169, Val MAE: 1.5867953300476074\n",
      "Epoch 987/2000, Train Loss: 5.305232894999473, Val Loss: 5.687972510945542, Val MAE: 1.5862445831298828\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 988/2000, Train Loss: 5.304879573430082, Val Loss: 5.687652507050687, Val MAE: 1.5865721702575684\n",
      "Epoch 989/2000, Train Loss: 5.304886455186901, Val Loss: 5.687655536999031, Val MAE: 1.5861355066299438\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 990/2000, Train Loss: 5.304626627928099, Val Loss: 5.687456204120172, Val MAE: 1.586275339126587\n",
      "Epoch 991/2000, Train Loss: 5.304264544098416, Val Loss: 5.687257895411337, Val MAE: 1.5863765478134155\n",
      "Epoch 992/2000, Train Loss: 5.3040279820299245, Val Loss: 5.68699006444635, Val MAE: 1.5864900350570679\n",
      "Epoch 993/2000, Train Loss: 5.3038936825724745, Val Loss: 5.686849454355167, Val MAE: 1.586534023284912\n",
      "Epoch 994/2000, Train Loss: 5.303717723956501, Val Loss: 5.6865460745056104, Val MAE: 1.5867681503295898\n",
      "Epoch 995/2000, Train Loss: 5.303501675449401, Val Loss: 5.686591073506104, Val MAE: 1.5867184400558472\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 996/2000, Train Loss: 5.30324290286983, Val Loss: 5.686366632808603, Val MAE: 1.586567997932434\n",
      "Epoch 997/2000, Train Loss: 5.303140719619794, Val Loss: 5.6861453225853245, Val MAE: 1.5869994163513184\n",
      "Epoch 998/2000, Train Loss: 5.302938977318025, Val Loss: 5.68597858478899, Val MAE: 1.586578130722046\n",
      "Epoch 999/2000, Train Loss: 5.302716610383015, Val Loss: 5.685926128372505, Val MAE: 1.586639165878296\n",
      "Epoch 1000/2000, Train Loss: 5.302587648162868, Val Loss: 5.685654837558394, Val MAE: 1.5868111848831177\n",
      "Epoch 1001/2000, Train Loss: 5.302357000167063, Val Loss: 5.685883508317332, Val MAE: 1.5863324403762817\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1002/2000, Train Loss: 5.302168172484632, Val Loss: 5.685874360006884, Val MAE: 1.586203694343567\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1003/2000, Train Loss: 5.302101629708201, Val Loss: 5.685577444937251, Val MAE: 1.5861928462982178\n",
      "Epoch 1004/2000, Train Loss: 5.301891506934653, Val Loss: 5.685622054502504, Val MAE: 1.5861655473709106\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1005/2000, Train Loss: 5.301586358501909, Val Loss: 5.68564274911669, Val MAE: 1.5858101844787598\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1006/2000, Train Loss: 5.3014428087894885, Val Loss: 5.685012464858706, Val MAE: 1.586400032043457\n",
      "Epoch 1007/2000, Train Loss: 5.301235874801518, Val Loss: 5.685177563253893, Val MAE: 1.5857906341552734\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1008/2000, Train Loss: 5.301067549271285, Val Loss: 5.685106092272184, Val MAE: 1.58623468875885\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1009/2000, Train Loss: 5.300741438627411, Val Loss: 5.6849695492410515, Val MAE: 1.5859463214874268\n",
      "Epoch 1010/2000, Train Loss: 5.30052015900528, Val Loss: 5.6847650124848075, Val MAE: 1.5865445137023926\n",
      "Epoch 1011/2000, Train Loss: 5.300318842786538, Val Loss: 5.684581409763853, Val MAE: 1.586484432220459\n",
      "Epoch 1012/2000, Train Loss: 5.300101087053085, Val Loss: 5.684496255050377, Val MAE: 1.5862587690353394\n",
      "Epoch 1013/2000, Train Loss: 5.299900863306528, Val Loss: 5.684394953174329, Val MAE: 1.586330771446228\n",
      "Epoch 1014/2000, Train Loss: 5.2996962162745325, Val Loss: 5.6843104490081835, Val MAE: 1.5861601829528809\n",
      "Epoch 1015/2000, Train Loss: 5.299503235478036, Val Loss: 5.684133299401411, Val MAE: 1.5862839221954346\n",
      "Epoch 1016/2000, Train Loss: 5.299328275669804, Val Loss: 5.683998925560111, Val MAE: 1.5864802598953247\n",
      "Epoch 1017/2000, Train Loss: 5.299073203527785, Val Loss: 5.683934684512447, Val MAE: 1.5864149332046509\n",
      "Epoch 1018/2000, Train Loss: 5.29893808915194, Val Loss: 5.683849303182841, Val MAE: 1.5862376689910889\n",
      "Epoch 1019/2000, Train Loss: 5.298709564188851, Val Loss: 5.683931669556402, Val MAE: 1.5860860347747803\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1020/2000, Train Loss: 5.298482646579729, Val Loss: 5.683967867666793, Val MAE: 1.5857077836990356\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1021/2000, Train Loss: 5.298266774579558, Val Loss: 5.6837701198249055, Val MAE: 1.586004376411438\n",
      "Epoch 1022/2000, Train Loss: 5.298106097571033, Val Loss: 5.683537952989249, Val MAE: 1.5859143733978271\n",
      "Epoch 1023/2000, Train Loss: 5.297943808930093, Val Loss: 5.683490808175973, Val MAE: 1.5862581729888916\n",
      "Epoch 1024/2000, Train Loss: 5.29784767666306, Val Loss: 5.683279435842409, Val MAE: 1.586079716682434\n",
      "Epoch 1025/2000, Train Loss: 5.297428326066492, Val Loss: 5.683318627354566, Val MAE: 1.5860711336135864\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1026/2000, Train Loss: 5.297298678875305, Val Loss: 5.6830021677031795, Val MAE: 1.586179256439209\n",
      "Epoch 1027/2000, Train Loss: 5.297074085385929, Val Loss: 5.682919345135353, Val MAE: 1.5860894918441772\n",
      "Epoch 1028/2000, Train Loss: 5.296885223559809, Val Loss: 5.682867106631262, Val MAE: 1.5861499309539795\n",
      "Epoch 1029/2000, Train Loss: 5.296667495636266, Val Loss: 5.682709670941764, Val MAE: 1.5862261056900024\n",
      "Epoch 1030/2000, Train Loss: 5.296403863747131, Val Loss: 5.6825625395191555, Val MAE: 1.586279034614563\n",
      "Epoch 1031/2000, Train Loss: 5.296258104426353, Val Loss: 5.682350293849951, Val MAE: 1.5863003730773926\n",
      "Epoch 1032/2000, Train Loss: 5.296051886449138, Val Loss: 5.682123181378805, Val MAE: 1.586215853691101\n",
      "Epoch 1033/2000, Train Loss: 5.295886316239037, Val Loss: 5.682253633298276, Val MAE: 1.5863789319992065\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1034/2000, Train Loss: 5.2956125526508755, Val Loss: 5.682078864915291, Val MAE: 1.5862070322036743\n",
      "Epoch 1035/2000, Train Loss: 5.295397791033643, Val Loss: 5.681680655488545, Val MAE: 1.5865765810012817\n",
      "Epoch 1036/2000, Train Loss: 5.295194015798226, Val Loss: 5.681594805068562, Val MAE: 1.5865967273712158\n",
      "Epoch 1037/2000, Train Loss: 5.295486038793232, Val Loss: 5.681835013245224, Val MAE: 1.5858798027038574\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1038/2000, Train Loss: 5.295035130536363, Val Loss: 5.681334122969835, Val MAE: 1.5864043235778809\n",
      "Epoch 1039/2000, Train Loss: 5.294859508994933, Val Loss: 5.681592582836064, Val MAE: 1.5859266519546509\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1040/2000, Train Loss: 5.294443103648004, Val Loss: 5.681273803616153, Val MAE: 1.5861186981201172\n",
      "Epoch 1041/2000, Train Loss: 5.294468177363203, Val Loss: 5.681221473244352, Val MAE: 1.5859100818634033\n",
      "Epoch 1042/2000, Train Loss: 5.2940398037727245, Val Loss: 5.681056186055554, Val MAE: 1.585902452468872\n",
      "Epoch 1043/2000, Train Loss: 5.293996407396771, Val Loss: 5.68079180719291, Val MAE: 1.586424708366394\n",
      "Epoch 1044/2000, Train Loss: 5.293640002828513, Val Loss: 5.680968705257144, Val MAE: 1.5857642889022827\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1045/2000, Train Loss: 5.293400765472025, Val Loss: 5.680769604098177, Val MAE: 1.5858830213546753\n",
      "Epoch 1046/2000, Train Loss: 5.293268515474774, Val Loss: 5.680659982680545, Val MAE: 1.5862551927566528\n",
      "Epoch 1047/2000, Train Loss: 5.293102917412819, Val Loss: 5.680676739605924, Val MAE: 1.5857079029083252\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1048/2000, Train Loss: 5.292861397714367, Val Loss: 5.680397520872796, Val MAE: 1.5858662128448486\n",
      "Epoch 1049/2000, Train Loss: 5.292696655216056, Val Loss: 5.680475140110068, Val MAE: 1.5859813690185547\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1050/2000, Train Loss: 5.292421724492609, Val Loss: 5.680391658974715, Val MAE: 1.5853818655014038\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1051/2000, Train Loss: 5.292207136865572, Val Loss: 5.680263641184988, Val MAE: 1.5854047536849976\n",
      "Epoch 1052/2000, Train Loss: 5.2920626904409085, Val Loss: 5.680338606648489, Val MAE: 1.585200548171997\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1053/2000, Train Loss: 5.2919913824807585, Val Loss: 5.6801381277928655, Val MAE: 1.585329532623291\n",
      "Epoch 1054/2000, Train Loss: 5.291574872773269, Val Loss: 5.6800494385421825, Val MAE: 1.5852693319320679\n",
      "Epoch 1055/2000, Train Loss: 5.291489193088819, Val Loss: 5.6798545902293025, Val MAE: 1.5855495929718018\n",
      "Epoch 1056/2000, Train Loss: 5.291442199629181, Val Loss: 5.679590883058146, Val MAE: 1.5853238105773926\n",
      "Epoch 1057/2000, Train Loss: 5.291089507243233, Val Loss: 5.6796535292258685, Val MAE: 1.5851938724517822\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1058/2000, Train Loss: 5.290887674087374, Val Loss: 5.679631730467537, Val MAE: 1.5849323272705078\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1059/2000, Train Loss: 5.290681158128882, Val Loss: 5.6793773856002625, Val MAE: 1.5849061012268066\n",
      "Epoch 1060/2000, Train Loss: 5.290453844050301, Val Loss: 5.679356274801656, Val MAE: 1.584813117980957\n",
      "Epoch 1061/2000, Train Loss: 5.290206197111813, Val Loss: 5.67920397942037, Val MAE: 1.5850542783737183\n",
      "Epoch 1062/2000, Train Loss: 5.290110702333443, Val Loss: 5.678776591742804, Val MAE: 1.5852984189987183\n",
      "Epoch 1063/2000, Train Loss: 5.289861678070455, Val Loss: 5.6789451719515185, Val MAE: 1.5847141742706299\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1064/2000, Train Loss: 5.289763784844803, Val Loss: 5.678704319561658, Val MAE: 1.5851210355758667\n",
      "Epoch 1065/2000, Train Loss: 5.289666705698635, Val Loss: 5.678569044177321, Val MAE: 1.5852168798446655\n",
      "Epoch 1066/2000, Train Loss: 5.289306416467912, Val Loss: 5.67859718508859, Val MAE: 1.584999918937683\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1067/2000, Train Loss: 5.289122257326624, Val Loss: 5.678197171527676, Val MAE: 1.585343599319458\n",
      "Epoch 1068/2000, Train Loss: 5.288854653192691, Val Loss: 5.678152199139653, Val MAE: 1.5851763486862183\n",
      "Epoch 1069/2000, Train Loss: 5.288675782044617, Val Loss: 5.678043522328047, Val MAE: 1.585105061531067\n",
      "Epoch 1070/2000, Train Loss: 5.288434453450148, Val Loss: 5.6778498173033425, Val MAE: 1.5850460529327393\n",
      "Epoch 1071/2000, Train Loss: 5.288392450841357, Val Loss: 5.6778473589763, Val MAE: 1.585015058517456\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1072/2000, Train Loss: 5.288150101627455, Val Loss: 5.677866348751824, Val MAE: 1.584675908088684\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1073/2000, Train Loss: 5.287834466132876, Val Loss: 5.6775535101737455, Val MAE: 1.5848009586334229\n",
      "Epoch 1074/2000, Train Loss: 5.287688233162801, Val Loss: 5.677379839008372, Val MAE: 1.5845764875411987\n",
      "Epoch 1075/2000, Train Loss: 5.287605406993045, Val Loss: 5.67712530120068, Val MAE: 1.5852407217025757\n",
      "Epoch 1076/2000, Train Loss: 5.287399670508947, Val Loss: 5.677293014909149, Val MAE: 1.5847972631454468\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1077/2000, Train Loss: 5.287069532755477, Val Loss: 5.677208759232399, Val MAE: 1.5845215320587158\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1078/2000, Train Loss: 5.28693833911528, Val Loss: 5.676986753484889, Val MAE: 1.584684133529663\n",
      "Epoch 1079/2000, Train Loss: 5.286712584740506, Val Loss: 5.677069276480135, Val MAE: 1.584488868713379\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1080/2000, Train Loss: 5.286473684794126, Val Loss: 5.67691944603344, Val MAE: 1.5844666957855225\n",
      "Epoch 1081/2000, Train Loss: 5.286342955071854, Val Loss: 5.676857641940817, Val MAE: 1.5845026969909668\n",
      "Epoch 1082/2000, Train Loss: 5.286096801563185, Val Loss: 5.676683728027781, Val MAE: 1.5844435691833496\n",
      "Epoch 1083/2000, Train Loss: 5.285931425477148, Val Loss: 5.676664216529339, Val MAE: 1.584266185760498\n",
      "Epoch 1084/2000, Train Loss: 5.285746901241681, Val Loss: 5.676357031688777, Val MAE: 1.5845600366592407\n",
      "Epoch 1085/2000, Train Loss: 5.285763779465033, Val Loss: 5.67615549097732, Val MAE: 1.5849883556365967\n",
      "Epoch 1086/2000, Train Loss: 5.2854526970774085, Val Loss: 5.675920274006117, Val MAE: 1.584867238998413\n",
      "Epoch 1087/2000, Train Loss: 5.285176052332428, Val Loss: 5.675901776515745, Val MAE: 1.5847909450531006\n",
      "Epoch 1088/2000, Train Loss: 5.284965461530289, Val Loss: 5.675893625580572, Val MAE: 1.5844069719314575\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1089/2000, Train Loss: 5.284826660894492, Val Loss: 5.6760433510356, Val MAE: 1.5840355157852173\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1090/2000, Train Loss: 5.284814900029804, Val Loss: 5.675868415185436, Val MAE: 1.5838860273361206\n",
      "Epoch 1091/2000, Train Loss: 5.284414922746447, Val Loss: 5.675915522130622, Val MAE: 1.5838032960891724\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1092/2000, Train Loss: 5.284169341878602, Val Loss: 5.67564396500952, Val MAE: 1.5839686393737793\n",
      "Epoch 1093/2000, Train Loss: 5.284031694326327, Val Loss: 5.675700756706958, Val MAE: 1.5836368799209595\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1094/2000, Train Loss: 5.283886259664204, Val Loss: 5.675380061297971, Val MAE: 1.583984613418579\n",
      "Epoch 1095/2000, Train Loss: 5.2836227947184105, Val Loss: 5.675442176127652, Val MAE: 1.5834699869155884\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1096/2000, Train Loss: 5.283485814100584, Val Loss: 5.675393281088692, Val MAE: 1.583532691001892\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1097/2000, Train Loss: 5.283337190904557, Val Loss: 5.675254357107189, Val MAE: 1.5834400653839111\n",
      "Epoch 1098/2000, Train Loss: 5.283101881666137, Val Loss: 5.67487270997934, Val MAE: 1.5835610628128052\n",
      "Epoch 1099/2000, Train Loss: 5.282873947716028, Val Loss: 5.6746894011348035, Val MAE: 1.5837042331695557\n",
      "Epoch 1100/2000, Train Loss: 5.2828409378164505, Val Loss: 5.674529539700313, Val MAE: 1.5836721658706665\n",
      "Epoch 1101/2000, Train Loss: 5.282545507415595, Val Loss: 5.674552554566561, Val MAE: 1.5834020376205444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1102/2000, Train Loss: 5.282479660348268, Val Loss: 5.67465981604127, Val MAE: 1.5828313827514648\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1103/2000, Train Loss: 5.282196972589943, Val Loss: 5.674524801677885, Val MAE: 1.5828760862350464\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1104/2000, Train Loss: 5.2820349907388495, Val Loss: 5.674377977665776, Val MAE: 1.5831228494644165\n",
      "Epoch 1105/2000, Train Loss: 5.281790478643945, Val Loss: 5.674420295642056, Val MAE: 1.582890272140503\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1106/2000, Train Loss: 5.2816521321108105, Val Loss: 5.674110775967256, Val MAE: 1.5830755233764648\n",
      "Epoch 1107/2000, Train Loss: 5.281495430534948, Val Loss: 5.674126789247224, Val MAE: 1.5826822519302368\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1108/2000, Train Loss: 5.281270690236101, Val Loss: 5.673816693530899, Val MAE: 1.5831377506256104\n",
      "Epoch 1109/2000, Train Loss: 5.281186704481261, Val Loss: 5.673883620159706, Val MAE: 1.5825953483581543\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1110/2000, Train Loss: 5.280966074847571, Val Loss: 5.673398177148005, Val MAE: 1.5831259489059448\n",
      "Epoch 1111/2000, Train Loss: 5.280608667109233, Val Loss: 5.673350084402146, Val MAE: 1.5828841924667358\n",
      "Epoch 1112/2000, Train Loss: 5.280403237876382, Val Loss: 5.673232520756736, Val MAE: 1.5830626487731934\n",
      "Epoch 1113/2000, Train Loss: 5.28017980728579, Val Loss: 5.673126945163861, Val MAE: 1.583135724067688\n",
      "Epoch 1114/2000, Train Loss: 5.280047751748167, Val Loss: 5.673073321854303, Val MAE: 1.5828605890274048\n",
      "Epoch 1115/2000, Train Loss: 5.279866470911401, Val Loss: 5.672953353653625, Val MAE: 1.582686424255371\n",
      "Epoch 1116/2000, Train Loss: 5.279685386883214, Val Loss: 5.672871413899854, Val MAE: 1.5826380252838135\n",
      "Epoch 1117/2000, Train Loss: 5.279476968441775, Val Loss: 5.672638036561304, Val MAE: 1.582810878753662\n",
      "Epoch 1118/2000, Train Loss: 5.279240193793171, Val Loss: 5.672627459425445, Val MAE: 1.5827471017837524\n",
      "Epoch 1119/2000, Train Loss: 5.279102956636282, Val Loss: 5.672587536526018, Val MAE: 1.5824004411697388\n",
      "Epoch 1120/2000, Train Loss: 5.279259334727964, Val Loss: 5.672183611478645, Val MAE: 1.5830137729644775\n",
      "Epoch 1121/2000, Train Loss: 5.2788061253375185, Val Loss: 5.672353643661975, Val MAE: 1.5823041200637817\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1122/2000, Train Loss: 5.27861687148817, Val Loss: 5.67208729214989, Val MAE: 1.5825753211975098\n",
      "Epoch 1123/2000, Train Loss: 5.278326375311651, Val Loss: 5.671934153417564, Val MAE: 1.582664132118225\n",
      "Epoch 1124/2000, Train Loss: 5.278090288402495, Val Loss: 5.671834265602474, Val MAE: 1.5824273824691772\n",
      "Epoch 1125/2000, Train Loss: 5.277952509980064, Val Loss: 5.671869501380381, Val MAE: 1.58220636844635\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1126/2000, Train Loss: 5.277788671953925, Val Loss: 5.671548917661749, Val MAE: 1.5825445652008057\n",
      "Epoch 1127/2000, Train Loss: 5.277468767911091, Val Loss: 5.6716139141117035, Val MAE: 1.5821282863616943\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1128/2000, Train Loss: 5.277395365860662, Val Loss: 5.6713744329020885, Val MAE: 1.5821456909179688\n",
      "Epoch 1129/2000, Train Loss: 5.277246550386175, Val Loss: 5.671434461022371, Val MAE: 1.581697940826416\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1130/2000, Train Loss: 5.276921394255528, Val Loss: 5.6712002815729985, Val MAE: 1.582216739654541\n",
      "Epoch 1131/2000, Train Loss: 5.2767336101787015, Val Loss: 5.671108943105473, Val MAE: 1.5820846557617188\n",
      "Epoch 1132/2000, Train Loss: 5.276542685721476, Val Loss: 5.671077134321225, Val MAE: 1.5819498300552368\n",
      "Epoch 1133/2000, Train Loss: 5.276405672721675, Val Loss: 5.670705294527045, Val MAE: 1.5824215412139893\n",
      "Epoch 1134/2000, Train Loss: 5.276142221701808, Val Loss: 5.670735057659835, Val MAE: 1.5820133686065674\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1135/2000, Train Loss: 5.275978649442755, Val Loss: 5.670583283955898, Val MAE: 1.5821243524551392\n",
      "Epoch 1136/2000, Train Loss: 5.275771417389614, Val Loss: 5.6703416863257, Val MAE: 1.582375407218933\n",
      "Epoch 1137/2000, Train Loss: 5.2757670340782985, Val Loss: 5.670542751567079, Val MAE: 1.5818012952804565\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1138/2000, Train Loss: 5.275397638298104, Val Loss: 5.6702944271790505, Val MAE: 1.5818074941635132\n",
      "Epoch 1139/2000, Train Loss: 5.275200939111354, Val Loss: 5.669957313606863, Val MAE: 1.5822750329971313\n",
      "Epoch 1140/2000, Train Loss: 5.27499946031161, Val Loss: 5.669893376597571, Val MAE: 1.582319974899292\n",
      "Epoch 1141/2000, Train Loss: 5.274860537782008, Val Loss: 5.669731257523236, Val MAE: 1.5819950103759766\n",
      "Epoch 1142/2000, Train Loss: 5.2747562483613715, Val Loss: 5.669831149576272, Val MAE: 1.5819412469863892\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1143/2000, Train Loss: 5.274301297726722, Val Loss: 5.669277228620074, Val MAE: 1.5825663805007935\n",
      "Epoch 1144/2000, Train Loss: 5.2743501901458805, Val Loss: 5.669265298805105, Val MAE: 1.5822724103927612\n",
      "Epoch 1145/2000, Train Loss: 5.27403830844361, Val Loss: 5.668984107650384, Val MAE: 1.582637906074524\n",
      "Epoch 1146/2000, Train Loss: 5.273919894777495, Val Loss: 5.668826423653769, Val MAE: 1.582737922668457\n",
      "Epoch 1147/2000, Train Loss: 5.273641273893159, Val Loss: 5.668582827217353, Val MAE: 1.5829633474349976\n",
      "Epoch 1148/2000, Train Loss: 5.273485629009915, Val Loss: 5.668546595886942, Val MAE: 1.5830342769622803\n",
      "Epoch 1149/2000, Train Loss: 5.273238014453402, Val Loss: 5.668579088301834, Val MAE: 1.5824865102767944\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1150/2000, Train Loss: 5.273178880605624, Val Loss: 5.668544822603191, Val MAE: 1.5823663473129272\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1151/2000, Train Loss: 5.272993256221935, Val Loss: 5.668469949554231, Val MAE: 1.5822862386703491\n",
      "Epoch 1152/2000, Train Loss: 5.272772589843933, Val Loss: 5.668446655439309, Val MAE: 1.5820236206054688\n",
      "Epoch 1153/2000, Train Loss: 5.272653492800708, Val Loss: 5.668395106945563, Val MAE: 1.5819646120071411\n",
      "Epoch 1154/2000, Train Loss: 5.272457139077613, Val Loss: 5.668141725048741, Val MAE: 1.5818963050842285\n",
      "Epoch 1155/2000, Train Loss: 5.272240284376795, Val Loss: 5.668329390682941, Val MAE: 1.5814162492752075\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1156/2000, Train Loss: 5.272133804391086, Val Loss: 5.668186481666128, Val MAE: 1.581758975982666\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1157/2000, Train Loss: 5.2720003607070085, Val Loss: 5.667910822396614, Val MAE: 1.5819069147109985\n",
      "Epoch 1158/2000, Train Loss: 5.271870581219517, Val Loss: 5.668159304864545, Val MAE: 1.5813084840774536\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1159/2000, Train Loss: 5.271458668074584, Val Loss: 5.6678606447641275, Val MAE: 1.5816034078598022\n",
      "Epoch 1160/2000, Train Loss: 5.2712469631144065, Val Loss: 5.667758799565313, Val MAE: 1.581573247909546\n",
      "Epoch 1161/2000, Train Loss: 5.271177243213264, Val Loss: 5.667527387038283, Val MAE: 1.5817089080810547\n",
      "Epoch 1162/2000, Train Loss: 5.2710292961797105, Val Loss: 5.667423047056985, Val MAE: 1.5816426277160645\n",
      "Epoch 1163/2000, Train Loss: 5.2709257372829965, Val Loss: 5.667403496141098, Val MAE: 1.5811097621917725\n",
      "Epoch 1164/2000, Train Loss: 5.270674473210844, Val Loss: 5.667345507201434, Val MAE: 1.5814623832702637\n",
      "Epoch 1165/2000, Train Loss: 5.270346495234405, Val Loss: 5.667198613012602, Val MAE: 1.581458568572998\n",
      "Epoch 1166/2000, Train Loss: 5.2701769854970415, Val Loss: 5.666992457224689, Val MAE: 1.5816848278045654\n",
      "Epoch 1167/2000, Train Loss: 5.270074505104639, Val Loss: 5.666937071066749, Val MAE: 1.581610918045044\n",
      "Epoch 1168/2000, Train Loss: 5.269990651897777, Val Loss: 5.6669793436104365, Val MAE: 1.581300139427185\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1169/2000, Train Loss: 5.269804801511396, Val Loss: 5.666771524770909, Val MAE: 1.5812495946884155\n",
      "Epoch 1170/2000, Train Loss: 5.269433990320129, Val Loss: 5.666365295694144, Val MAE: 1.5820382833480835\n",
      "Epoch 1171/2000, Train Loss: 5.2693283542791445, Val Loss: 5.666128121977188, Val MAE: 1.5820860862731934\n",
      "Epoch 1172/2000, Train Loss: 5.269271899578355, Val Loss: 5.665783291836397, Val MAE: 1.5821290016174316\n",
      "Epoch 1173/2000, Train Loss: 5.26893301130935, Val Loss: 5.6659581663958525, Val MAE: 1.5819454193115234\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1174/2000, Train Loss: 5.268797895255683, Val Loss: 5.665789893825484, Val MAE: 1.5819511413574219\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1175/2000, Train Loss: 5.26853211407591, Val Loss: 5.665668824716081, Val MAE: 1.582050085067749\n",
      "Epoch 1176/2000, Train Loss: 5.268474042793129, Val Loss: 5.665611308102214, Val MAE: 1.5817646980285645\n",
      "Epoch 1177/2000, Train Loss: 5.268680128222364, Val Loss: 5.665762225558998, Val MAE: 1.5813038349151611\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1178/2000, Train Loss: 5.268130459808951, Val Loss: 5.665487515634718, Val MAE: 1.5817404985427856\n",
      "Epoch 1179/2000, Train Loss: 5.267913643194705, Val Loss: 5.665575652448774, Val MAE: 1.5812770128250122\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1180/2000, Train Loss: 5.267781359473221, Val Loss: 5.665220399333068, Val MAE: 1.5819207429885864\n",
      "Epoch 1181/2000, Train Loss: 5.2675474364517605, Val Loss: 5.665217807533544, Val MAE: 1.5817904472351074\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1182/2000, Train Loss: 5.267242676267148, Val Loss: 5.665264649675526, Val MAE: 1.5814292430877686\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1183/2000, Train Loss: 5.267106365100504, Val Loss: 5.6653331353212355, Val MAE: 1.5811820030212402\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1184/2000, Train Loss: 5.266966544600291, Val Loss: 5.665132708095629, Val MAE: 1.5812255144119263\n",
      "Epoch 1185/2000, Train Loss: 5.266753017524864, Val Loss: 5.665177711379638, Val MAE: 1.5809797048568726\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1186/2000, Train Loss: 5.266633078046956, Val Loss: 5.665355082134224, Val MAE: 1.5805249214172363\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1187/2000, Train Loss: 5.2664536012049545, Val Loss: 5.665166978761326, Val MAE: 1.5806580781936646\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1188/2000, Train Loss: 5.266232715032203, Val Loss: 5.665119842378371, Val MAE: 1.5806291103363037\n",
      "Epoch 1189/2000, Train Loss: 5.266145044443559, Val Loss: 5.665060788393021, Val MAE: 1.580601692199707\n",
      "Epoch 1190/2000, Train Loss: 5.265900619427643, Val Loss: 5.664936823354584, Val MAE: 1.5804282426834106\n",
      "Epoch 1191/2000, Train Loss: 5.265751020745607, Val Loss: 5.664992609838827, Val MAE: 1.5802987813949585\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1192/2000, Train Loss: 5.2655840940160035, Val Loss: 5.66480172947277, Val MAE: 1.580401062965393\n",
      "Epoch 1193/2000, Train Loss: 5.265419696603838, Val Loss: 5.664494186275231, Val MAE: 1.5804954767227173\n",
      "Epoch 1194/2000, Train Loss: 5.265179254738568, Val Loss: 5.664452686586876, Val MAE: 1.580495834350586\n",
      "Epoch 1195/2000, Train Loss: 5.264966559191978, Val Loss: 5.664355736761283, Val MAE: 1.5804355144500732\n",
      "Epoch 1196/2000, Train Loss: 5.264885067855867, Val Loss: 5.664103629984623, Val MAE: 1.5806000232696533\n",
      "Epoch 1197/2000, Train Loss: 5.264786927319177, Val Loss: 5.664158833591945, Val MAE: 1.5805344581604004\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1198/2000, Train Loss: 5.264440002951464, Val Loss: 5.6641006703952765, Val MAE: 1.5804096460342407\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1199/2000, Train Loss: 5.264326721744417, Val Loss: 5.664012162601547, Val MAE: 1.5800062417984009\n",
      "Epoch 1200/2000, Train Loss: 5.264080884169727, Val Loss: 5.663559342196228, Val MAE: 1.5803478956222534\n",
      "Epoch 1201/2000, Train Loss: 5.2640825714719375, Val Loss: 5.663434630778222, Val MAE: 1.5805832147598267\n",
      "Epoch 1202/2000, Train Loss: 5.263693985428968, Val Loss: 5.663469906019143, Val MAE: 1.580294132232666\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1203/2000, Train Loss: 5.26361965713662, Val Loss: 5.6634898403551235, Val MAE: 1.5799751281738281\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1204/2000, Train Loss: 5.2635324919081174, Val Loss: 5.66300609334164, Val MAE: 1.5806819200515747\n",
      "Epoch 1205/2000, Train Loss: 5.263253351905496, Val Loss: 5.663030954767075, Val MAE: 1.580191969871521\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1206/2000, Train Loss: 5.26299966698713, Val Loss: 5.6630684325545575, Val MAE: 1.580093264579773\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1207/2000, Train Loss: 5.262897671317652, Val Loss: 5.66306080028916, Val MAE: 1.5799448490142822\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1208/2000, Train Loss: 5.262765777018773, Val Loss: 5.663026333083071, Val MAE: 1.5795882940292358\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1209/2000, Train Loss: 5.262677514829911, Val Loss: 5.663028274654248, Val MAE: 1.5796443223953247\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1210/2000, Train Loss: 5.262414240988101, Val Loss: 5.662616193340094, Val MAE: 1.579988718032837\n",
      "Epoch 1211/2000, Train Loss: 5.262201134039431, Val Loss: 5.662718327265996, Val MAE: 1.5799269676208496\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1212/2000, Train Loss: 5.2620666451558185, Val Loss: 5.662537045343936, Val MAE: 1.579590082168579\n",
      "Epoch 1213/2000, Train Loss: 5.261856227253291, Val Loss: 5.662256910440025, Val MAE: 1.5802892446517944\n",
      "Epoch 1214/2000, Train Loss: 5.261683765387216, Val Loss: 5.662412736667406, Val MAE: 1.5794963836669922\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1215/2000, Train Loss: 5.2614772402007, Val Loss: 5.662406837493637, Val MAE: 1.5795637369155884\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1216/2000, Train Loss: 5.261313867938091, Val Loss: 5.662165990663231, Val MAE: 1.5796394348144531\n",
      "Epoch 1217/2000, Train Loss: 5.261117662366724, Val Loss: 5.661983390284606, Val MAE: 1.5800544023513794\n",
      "Epoch 1218/2000, Train Loss: 5.261006489343663, Val Loss: 5.661727528460894, Val MAE: 1.5801665782928467\n",
      "Epoch 1219/2000, Train Loss: 5.260809534624544, Val Loss: 5.661851356609152, Val MAE: 1.5800392627716064\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1220/2000, Train Loss: 5.260715555758815, Val Loss: 5.661443163632253, Val MAE: 1.5805753469467163\n",
      "Epoch 1221/2000, Train Loss: 5.260456689892312, Val Loss: 5.661855105868902, Val MAE: 1.5798276662826538\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1222/2000, Train Loss: 5.260280425241512, Val Loss: 5.661642443514016, Val MAE: 1.5798656940460205\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1223/2000, Train Loss: 5.260144455847314, Val Loss: 5.661556201243619, Val MAE: 1.5798317193984985\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1224/2000, Train Loss: 5.259942386957393, Val Loss: 5.6613422085385805, Val MAE: 1.57989501953125\n",
      "Epoch 1225/2000, Train Loss: 5.259724439490773, Val Loss: 5.661204524862292, Val MAE: 1.5799717903137207\n",
      "Epoch 1226/2000, Train Loss: 5.259659235952271, Val Loss: 5.661213247418768, Val MAE: 1.5797538757324219\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1227/2000, Train Loss: 5.259341002349531, Val Loss: 5.6608316653183115, Val MAE: 1.5801434516906738\n",
      "Epoch 1228/2000, Train Loss: 5.259235061112632, Val Loss: 5.660830353332587, Val MAE: 1.5797752141952515\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1229/2000, Train Loss: 5.259048006217468, Val Loss: 5.660697657216215, Val MAE: 1.5795978307724\n",
      "Epoch 1230/2000, Train Loss: 5.258921133976935, Val Loss: 5.6607613027642625, Val MAE: 1.5792344808578491\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1231/2000, Train Loss: 5.258901005811376, Val Loss: 5.660737468561995, Val MAE: 1.5794554948806763\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1232/2000, Train Loss: 5.258644613055257, Val Loss: 5.6601271263867705, Val MAE: 1.5800247192382812\n",
      "Epoch 1233/2000, Train Loss: 5.258291031087812, Val Loss: 5.6602255470526694, Val MAE: 1.5796090364456177\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1234/2000, Train Loss: 5.258237478852188, Val Loss: 5.6600702206293745, Val MAE: 1.5797815322875977\n",
      "Epoch 1235/2000, Train Loss: 5.258020643278547, Val Loss: 5.660134042940737, Val MAE: 1.5795254707336426\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1236/2000, Train Loss: 5.257894632096529, Val Loss: 5.65981120750627, Val MAE: 1.580018401145935\n",
      "Epoch 1237/2000, Train Loss: 5.25776186801446, Val Loss: 5.659753766583011, Val MAE: 1.5801780223846436\n",
      "Epoch 1238/2000, Train Loss: 5.257579024957151, Val Loss: 5.659640930114536, Val MAE: 1.5801520347595215\n",
      "Epoch 1239/2000, Train Loss: 5.2573303132724964, Val Loss: 5.659562269436474, Val MAE: 1.5800114870071411\n",
      "Epoch 1240/2000, Train Loss: 5.257320200700645, Val Loss: 5.659341402329072, Val MAE: 1.5800094604492188\n",
      "Epoch 1241/2000, Train Loss: 5.257072196889309, Val Loss: 5.659334228900959, Val MAE: 1.5797234773635864\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1242/2000, Train Loss: 5.256880466909496, Val Loss: 5.659232989591561, Val MAE: 1.579965353012085\n",
      "Epoch 1243/2000, Train Loss: 5.256754334924256, Val Loss: 5.659129118153809, Val MAE: 1.5799729824066162\n",
      "Epoch 1244/2000, Train Loss: 5.256652151506438, Val Loss: 5.658970651777877, Val MAE: 1.5798343420028687\n",
      "Epoch 1245/2000, Train Loss: 5.256422085463371, Val Loss: 5.65881274193981, Val MAE: 1.5800305604934692\n",
      "Epoch 1246/2000, Train Loss: 5.256318857097693, Val Loss: 5.658995029530758, Val MAE: 1.5795857906341553\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1247/2000, Train Loss: 5.256113560077257, Val Loss: 5.658776823260369, Val MAE: 1.5794631242752075\n",
      "Epoch 1248/2000, Train Loss: 5.25590865063382, Val Loss: 5.658856032547236, Val MAE: 1.579313039779663\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1249/2000, Train Loss: 5.2558553285954455, Val Loss: 5.658746847683501, Val MAE: 1.5797817707061768\n",
      "Epoch 1250/2000, Train Loss: 5.2555281114276235, Val Loss: 5.658660498142972, Val MAE: 1.5796871185302734\n",
      "Epoch 1251/2000, Train Loss: 5.255419808655537, Val Loss: 5.658613019261156, Val MAE: 1.5795475244522095\n",
      "Epoch 1252/2000, Train Loss: 5.2552039883322985, Val Loss: 5.658420971181779, Val MAE: 1.5796478986740112\n",
      "Epoch 1253/2000, Train Loss: 5.254998701966035, Val Loss: 5.658302989210193, Val MAE: 1.5795872211456299\n",
      "Epoch 1254/2000, Train Loss: 5.254859387580984, Val Loss: 5.658308889842179, Val MAE: 1.5792720317840576\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1255/2000, Train Loss: 5.254662394271947, Val Loss: 5.65816349014412, Val MAE: 1.5792337656021118\n",
      "Epoch 1256/2000, Train Loss: 5.254581930434343, Val Loss: 5.658064110517866, Val MAE: 1.5793401002883911\n",
      "Epoch 1257/2000, Train Loss: 5.254389628317052, Val Loss: 5.6581998571343375, Val MAE: 1.5789587497711182\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1258/2000, Train Loss: 5.254235182405776, Val Loss: 5.658070809159439, Val MAE: 1.5789794921875\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1259/2000, Train Loss: 5.254042006478521, Val Loss: 5.65798910891791, Val MAE: 1.5790507793426514\n",
      "Epoch 1260/2000, Train Loss: 5.254047076019244, Val Loss: 5.657829514701796, Val MAE: 1.579004168510437\n",
      "Epoch 1261/2000, Train Loss: 5.253731658809374, Val Loss: 5.6579124649092325, Val MAE: 1.578758716583252\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1262/2000, Train Loss: 5.253710847770052, Val Loss: 5.657831206006377, Val MAE: 1.578936219215393\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1263/2000, Train Loss: 5.253409580644129, Val Loss: 5.657747262935026, Val MAE: 1.57877516746521\n",
      "Epoch 1264/2000, Train Loss: 5.253321969282618, Val Loss: 5.657534899833735, Val MAE: 1.5791051387786865\n",
      "Epoch 1265/2000, Train Loss: 5.253180391934453, Val Loss: 5.657779845577132, Val MAE: 1.5786409378051758\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1266/2000, Train Loss: 5.252933437433317, Val Loss: 5.657437018662782, Val MAE: 1.5787596702575684\n",
      "Epoch 1267/2000, Train Loss: 5.252751060810667, Val Loss: 5.657513001050789, Val MAE: 1.5783486366271973\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1268/2000, Train Loss: 5.252623883542001, Val Loss: 5.65736255541854, Val MAE: 1.5783714056015015\n",
      "Epoch 1269/2000, Train Loss: 5.252544808438091, Val Loss: 5.657065558005181, Val MAE: 1.5790537595748901\n",
      "Epoch 1270/2000, Train Loss: 5.2522806362901076, Val Loss: 5.6570853587319725, Val MAE: 1.5790055990219116\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1271/2000, Train Loss: 5.252191101976216, Val Loss: 5.656746299396232, Val MAE: 1.5790599584579468\n",
      "Epoch 1272/2000, Train Loss: 5.251997318006082, Val Loss: 5.656721532390387, Val MAE: 1.5791687965393066\n",
      "Epoch 1273/2000, Train Loss: 5.251777050949838, Val Loss: 5.656638964737227, Val MAE: 1.5792055130004883\n",
      "Epoch 1274/2000, Train Loss: 5.251611823686657, Val Loss: 5.6564739679160105, Val MAE: 1.578802227973938\n",
      "Epoch 1275/2000, Train Loss: 5.251481689088031, Val Loss: 5.656571909420716, Val MAE: 1.5788443088531494\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1276/2000, Train Loss: 5.251312690750971, Val Loss: 5.656429145952248, Val MAE: 1.578693151473999\n",
      "Epoch 1277/2000, Train Loss: 5.251203281287154, Val Loss: 5.656352419690983, Val MAE: 1.5787937641143799\n",
      "Epoch 1278/2000, Train Loss: 5.250955242744891, Val Loss: 5.656249501471855, Val MAE: 1.5789138078689575\n",
      "Epoch 1279/2000, Train Loss: 5.251031415551418, Val Loss: 5.655794228344518, Val MAE: 1.5795040130615234\n",
      "Epoch 1280/2000, Train Loss: 5.250672662534989, Val Loss: 5.655628833475463, Val MAE: 1.5796623229980469\n",
      "Epoch 1281/2000, Train Loss: 5.2504790109869965, Val Loss: 5.65575139438705, Val MAE: 1.5793203115463257\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1282/2000, Train Loss: 5.250281816465095, Val Loss: 5.65577700114396, Val MAE: 1.579071283340454\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1283/2000, Train Loss: 5.250157634361289, Val Loss: 5.655680259342223, Val MAE: 1.5788333415985107\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1284/2000, Train Loss: 5.249991540083324, Val Loss: 5.655793241951444, Val MAE: 1.5787032842636108\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1285/2000, Train Loss: 5.250466054128476, Val Loss: 5.655740846688959, Val MAE: 1.578477382659912\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1286/2000, Train Loss: 5.2497577467575445, Val Loss: 5.655237359920409, Val MAE: 1.579127311706543\n",
      "Epoch 1287/2000, Train Loss: 5.249474059017001, Val Loss: 5.655174028253701, Val MAE: 1.578919768333435\n",
      "Epoch 1288/2000, Train Loss: 5.249398101139539, Val Loss: 5.654977999695944, Val MAE: 1.5790623426437378\n",
      "Epoch 1289/2000, Train Loss: 5.249134856659623, Val Loss: 5.6549816169414315, Val MAE: 1.5789238214492798\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1290/2000, Train Loss: 5.248916316250527, Val Loss: 5.6550091976237225, Val MAE: 1.5786585807800293\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1291/2000, Train Loss: 5.24885755219483, Val Loss: 5.654825074319081, Val MAE: 1.5788484811782837\n",
      "Epoch 1292/2000, Train Loss: 5.248639743400241, Val Loss: 5.654749102822137, Val MAE: 1.5785915851593018\n",
      "Epoch 1293/2000, Train Loss: 5.248525084113672, Val Loss: 5.654752848254066, Val MAE: 1.5785080194473267\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1294/2000, Train Loss: 5.248301666106748, Val Loss: 5.654590220125079, Val MAE: 1.5785013437271118\n",
      "Epoch 1295/2000, Train Loss: 5.24822207020673, Val Loss: 5.654667052152689, Val MAE: 1.5781729221343994\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1296/2000, Train Loss: 5.2480043428704235, Val Loss: 5.654490800534549, Val MAE: 1.5781364440917969\n",
      "Epoch 1297/2000, Train Loss: 5.247883080261682, Val Loss: 5.654450928490461, Val MAE: 1.578458309173584\n",
      "Epoch 1298/2000, Train Loss: 5.2477681638488125, Val Loss: 5.654357485268094, Val MAE: 1.5785446166992188\n",
      "Epoch 1299/2000, Train Loss: 5.247600133615342, Val Loss: 5.6545816169601695, Val MAE: 1.5780982971191406\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1300/2000, Train Loss: 5.247335941333489, Val Loss: 5.654298401207005, Val MAE: 1.5782004594802856\n",
      "Epoch 1301/2000, Train Loss: 5.2471524294961265, Val Loss: 5.654240874978016, Val MAE: 1.5783590078353882\n",
      "Epoch 1302/2000, Train Loss: 5.247228082848803, Val Loss: 5.654465052831792, Val MAE: 1.5776139497756958\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1303/2000, Train Loss: 5.24675618452895, Val Loss: 5.654095186370593, Val MAE: 1.578001856803894\n",
      "Epoch 1304/2000, Train Loss: 5.246645786492108, Val Loss: 5.654035669856844, Val MAE: 1.5781306028366089\n",
      "Epoch 1305/2000, Train Loss: 5.246579644044129, Val Loss: 5.6541338770761405, Val MAE: 1.5776591300964355\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1306/2000, Train Loss: 5.2464475135078406, Val Loss: 5.654179153229118, Val MAE: 1.5775340795516968\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1307/2000, Train Loss: 5.246192060667893, Val Loss: 5.654040881435441, Val MAE: 1.5778260231018066\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1308/2000, Train Loss: 5.2461657857995565, Val Loss: 5.654234862792382, Val MAE: 1.576904296875\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1309/2000, Train Loss: 5.245951072671395, Val Loss: 5.654122951335134, Val MAE: 1.5769122838974\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1310/2000, Train Loss: 5.2456331776532386, Val Loss: 5.65390850243583, Val MAE: 1.5774585008621216\n",
      "Epoch 1311/2000, Train Loss: 5.2457602831111, Val Loss: 5.653821957904264, Val MAE: 1.5772876739501953\n",
      "Epoch 1312/2000, Train Loss: 5.245384309595526, Val Loss: 5.653587975040852, Val MAE: 1.5774836540222168\n",
      "Epoch 1313/2000, Train Loss: 5.2452715032122486, Val Loss: 5.6534337225492575, Val MAE: 1.5777502059936523\n",
      "Epoch 1314/2000, Train Loss: 5.245064727526832, Val Loss: 5.653126514240507, Val MAE: 1.5777580738067627\n",
      "Epoch 1315/2000, Train Loss: 5.244910474397362, Val Loss: 5.6531249396389045, Val MAE: 1.577734112739563\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1316/2000, Train Loss: 5.244726238244021, Val Loss: 5.653239292608124, Val MAE: 1.5774569511413574\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1317/2000, Train Loss: 5.244542951402657, Val Loss: 5.653195074300883, Val MAE: 1.5773824453353882\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1318/2000, Train Loss: 5.244386080534503, Val Loss: 5.653122764798479, Val MAE: 1.5773223638534546\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1319/2000, Train Loss: 5.244290037108172, Val Loss: 5.652991515917516, Val MAE: 1.577173113822937\n",
      "Epoch 1320/2000, Train Loss: 5.244049072475353, Val Loss: 5.6529052787964496, Val MAE: 1.5773781538009644\n",
      "Epoch 1321/2000, Train Loss: 5.243875400689519, Val Loss: 5.652660709592181, Val MAE: 1.5774328708648682\n",
      "Epoch 1322/2000, Train Loss: 5.243726713065108, Val Loss: 5.652616870375948, Val MAE: 1.5773770809173584\n",
      "Epoch 1323/2000, Train Loss: 5.243556647129583, Val Loss: 5.652596195265423, Val MAE: 1.5772932767868042\n",
      "Epoch 1324/2000, Train Loss: 5.243367647470344, Val Loss: 5.652500267149112, Val MAE: 1.5769858360290527\n",
      "Epoch 1325/2000, Train Loss: 5.2432005266166755, Val Loss: 5.652353646649498, Val MAE: 1.5770412683486938\n",
      "Epoch 1326/2000, Train Loss: 5.243085113568343, Val Loss: 5.65242819720452, Val MAE: 1.5770900249481201\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1327/2000, Train Loss: 5.2429008087920606, Val Loss: 5.652211085827708, Val MAE: 1.5771598815917969\n",
      "Epoch 1328/2000, Train Loss: 5.242716548645857, Val Loss: 5.652129405955656, Val MAE: 1.5774255990982056\n",
      "Epoch 1329/2000, Train Loss: 5.2425806609616155, Val Loss: 5.6519356640653875, Val MAE: 1.5775288343429565\n",
      "Epoch 1330/2000, Train Loss: 5.242403779375477, Val Loss: 5.651920781131913, Val MAE: 1.5773123502731323\n",
      "Epoch 1331/2000, Train Loss: 5.242230179022937, Val Loss: 5.651960728228639, Val MAE: 1.5772178173065186\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1332/2000, Train Loss: 5.242092070405035, Val Loss: 5.651906141356226, Val MAE: 1.577202558517456\n",
      "Epoch 1333/2000, Train Loss: 5.241968961268726, Val Loss: 5.651762207941542, Val MAE: 1.5771715641021729\n",
      "Epoch 1334/2000, Train Loss: 5.241892411287698, Val Loss: 5.6519952728114, Val MAE: 1.5766611099243164\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1335/2000, Train Loss: 5.241622798967999, Val Loss: 5.651754739634487, Val MAE: 1.5766165256500244\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1336/2000, Train Loss: 5.241389368219664, Val Loss: 5.651597377949534, Val MAE: 1.5767409801483154\n",
      "Epoch 1337/2000, Train Loss: 5.241317505403608, Val Loss: 5.651354943029013, Val MAE: 1.577115774154663\n",
      "Epoch 1338/2000, Train Loss: 5.241304179130517, Val Loss: 5.650784905304967, Val MAE: 1.5777297019958496\n",
      "Epoch 1339/2000, Train Loss: 5.240977606078758, Val Loss: 5.651017876894467, Val MAE: 1.5773100852966309\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1340/2000, Train Loss: 5.240925129830711, Val Loss: 5.650578454592542, Val MAE: 1.5778043270111084\n",
      "Epoch 1341/2000, Train Loss: 5.240646491832585, Val Loss: 5.650688434594997, Val MAE: 1.5773342847824097\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1342/2000, Train Loss: 5.240545290947631, Val Loss: 5.650465169038612, Val MAE: 1.5775508880615234\n",
      "Epoch 1343/2000, Train Loss: 5.24039118294981, Val Loss: 5.650427569900084, Val MAE: 1.577571988105774\n",
      "Epoch 1344/2000, Train Loss: 5.240160841072721, Val Loss: 5.650249697700917, Val MAE: 1.5774575471878052\n",
      "Epoch 1345/2000, Train Loss: 5.240139982551022, Val Loss: 5.6500375441728385, Val MAE: 1.5777581930160522\n",
      "Epoch 1346/2000, Train Loss: 5.239899613167684, Val Loss: 5.650108184927465, Val MAE: 1.5772958993911743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1347/2000, Train Loss: 5.239704379726681, Val Loss: 5.65009107607038, Val MAE: 1.5773108005523682\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1348/2000, Train Loss: 5.239512634478682, Val Loss: 5.649973404471298, Val MAE: 1.5773746967315674\n",
      "Epoch 1349/2000, Train Loss: 5.239491176135434, Val Loss: 5.6500216140112745, Val MAE: 1.5769227743148804\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1350/2000, Train Loss: 5.239432915426492, Val Loss: 5.64952373614005, Val MAE: 1.5779502391815186\n",
      "Epoch 1351/2000, Train Loss: 5.239053173185988, Val Loss: 5.649396236701114, Val MAE: 1.5776987075805664\n",
      "Epoch 1352/2000, Train Loss: 5.238991170429496, Val Loss: 5.649208795905842, Val MAE: 1.5777525901794434\n",
      "Epoch 1353/2000, Train Loss: 5.238779261278317, Val Loss: 5.6494673551131465, Val MAE: 1.5771507024765015\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1354/2000, Train Loss: 5.2385676100084595, Val Loss: 5.649216019961448, Val MAE: 1.5771249532699585\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1355/2000, Train Loss: 5.238426132658516, Val Loss: 5.649105632296031, Val MAE: 1.5773903131484985\n",
      "Epoch 1356/2000, Train Loss: 5.2383324722435, Val Loss: 5.649191696162617, Val MAE: 1.577271819114685\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1357/2000, Train Loss: 5.238180795746064, Val Loss: 5.648725288131186, Val MAE: 1.5776740312576294\n",
      "Epoch 1358/2000, Train Loss: 5.238059500587565, Val Loss: 5.648930423075635, Val MAE: 1.5774405002593994\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1359/2000, Train Loss: 5.237887432841999, Val Loss: 5.6485004538060695, Val MAE: 1.5778954029083252\n",
      "Epoch 1360/2000, Train Loss: 5.2376099387832635, Val Loss: 5.648601017960715, Val MAE: 1.5770153999328613\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1361/2000, Train Loss: 5.237542129250834, Val Loss: 5.648680057501939, Val MAE: 1.577238917350769\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1362/2000, Train Loss: 5.2374050615540195, Val Loss: 5.6487598728605, Val MAE: 1.5767831802368164\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1363/2000, Train Loss: 5.237204811088138, Val Loss: 5.648464160774826, Val MAE: 1.5771629810333252\n",
      "Epoch 1364/2000, Train Loss: 5.236983083151832, Val Loss: 5.648379180409493, Val MAE: 1.5771358013153076\n",
      "Epoch 1365/2000, Train Loss: 5.2369176338123316, Val Loss: 5.648318850848288, Val MAE: 1.5771992206573486\n",
      "Epoch 1366/2000, Train Loss: 5.236676212434279, Val Loss: 5.64833173584136, Val MAE: 1.5769457817077637\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1367/2000, Train Loss: 5.236637611107286, Val Loss: 5.64807034281598, Val MAE: 1.5771760940551758\n",
      "Epoch 1368/2000, Train Loss: 5.2363707977311025, Val Loss: 5.647876495797335, Val MAE: 1.5773922204971313\n",
      "Epoch 1369/2000, Train Loss: 5.236333206117698, Val Loss: 5.647883844758392, Val MAE: 1.5769538879394531\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1370/2000, Train Loss: 5.236275366709989, Val Loss: 5.647458522235946, Val MAE: 1.5776150226593018\n",
      "Epoch 1371/2000, Train Loss: 5.235894632205252, Val Loss: 5.647554777904388, Val MAE: 1.5770028829574585\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1372/2000, Train Loss: 5.235829578999647, Val Loss: 5.647429596940312, Val MAE: 1.5768383741378784\n",
      "Epoch 1373/2000, Train Loss: 5.235759529070817, Val Loss: 5.647496399529484, Val MAE: 1.5768033266067505\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1374/2000, Train Loss: 5.235459151694172, Val Loss: 5.647431376740473, Val MAE: 1.5768156051635742\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1375/2000, Train Loss: 5.235295110315945, Val Loss: 5.647368587348439, Val MAE: 1.5766760110855103\n",
      "Epoch 1376/2000, Train Loss: 5.235156805392808, Val Loss: 5.647231855159141, Val MAE: 1.5768893957138062\n",
      "Epoch 1377/2000, Train Loss: 5.235088923714011, Val Loss: 5.647218735848727, Val MAE: 1.5764163732528687\n",
      "Epoch 1378/2000, Train Loss: 5.234960755355588, Val Loss: 5.647098199672291, Val MAE: 1.5764111280441284\n",
      "Epoch 1379/2000, Train Loss: 5.234683778737314, Val Loss: 5.647005770184578, Val MAE: 1.576615571975708\n",
      "Epoch 1380/2000, Train Loss: 5.2346388419525125, Val Loss: 5.646902688690647, Val MAE: 1.5765262842178345\n",
      "Epoch 1381/2000, Train Loss: 5.234458418986397, Val Loss: 5.647038365586089, Val MAE: 1.5761065483093262\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1382/2000, Train Loss: 5.234281777832225, Val Loss: 5.647067111794373, Val MAE: 1.5760554075241089\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1383/2000, Train Loss: 5.234063257892563, Val Loss: 5.646919605063006, Val MAE: 1.5761640071868896\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1384/2000, Train Loss: 5.234131094270152, Val Loss: 5.646978641868731, Val MAE: 1.5760873556137085\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1385/2000, Train Loss: 5.23380339707731, Val Loss: 5.64673909531274, Val MAE: 1.5762786865234375\n",
      "Epoch 1386/2000, Train Loss: 5.233610010247764, Val Loss: 5.646835473061337, Val MAE: 1.576018214225769\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1387/2000, Train Loss: 5.233437761297367, Val Loss: 5.646706799255963, Val MAE: 1.5760692358016968\n",
      "Epoch 1388/2000, Train Loss: 5.233351547729793, Val Loss: 5.64641556645023, Val MAE: 1.5764286518096924\n",
      "Epoch 1389/2000, Train Loss: 5.233228435153948, Val Loss: 5.646373642693966, Val MAE: 1.576572299003601\n",
      "Epoch 1390/2000, Train Loss: 5.23300199924767, Val Loss: 5.64627643777143, Val MAE: 1.576263666152954\n",
      "Epoch 1391/2000, Train Loss: 5.233015331560923, Val Loss: 5.64627120491197, Val MAE: 1.576104998588562\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1392/2000, Train Loss: 5.23266491631569, Val Loss: 5.646221265971478, Val MAE: 1.5761089324951172\n",
      "Epoch 1393/2000, Train Loss: 5.232528817435539, Val Loss: 5.646110998517877, Val MAE: 1.5762253999710083\n",
      "Epoch 1394/2000, Train Loss: 5.232414371106593, Val Loss: 5.64595703917359, Val MAE: 1.5760160684585571\n",
      "Epoch 1395/2000, Train Loss: 5.232253250146231, Val Loss: 5.645859719188571, Val MAE: 1.5760680437088013\n",
      "Epoch 1396/2000, Train Loss: 5.232067336338159, Val Loss: 5.645888134381457, Val MAE: 1.5755562782287598\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1397/2000, Train Loss: 5.231932924131035, Val Loss: 5.645627299124312, Val MAE: 1.576078176498413\n",
      "Epoch 1398/2000, Train Loss: 5.23181920786461, Val Loss: 5.6456463108642385, Val MAE: 1.5757750272750854\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1399/2000, Train Loss: 5.231646885425587, Val Loss: 5.645499126171118, Val MAE: 1.575985074043274\n",
      "Epoch 1400/2000, Train Loss: 5.23164705715072, Val Loss: 5.645161357995931, Val MAE: 1.5765905380249023\n",
      "Epoch 1401/2000, Train Loss: 5.231487868790221, Val Loss: 5.6454997353870935, Val MAE: 1.5756034851074219\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1402/2000, Train Loss: 5.231540149739726, Val Loss: 5.645557780741552, Val MAE: 1.575472354888916\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1403/2000, Train Loss: 5.231042506240104, Val Loss: 5.645232975893064, Val MAE: 1.5756944417953491\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1404/2000, Train Loss: 5.23094683008912, Val Loss: 5.645021161765134, Val MAE: 1.5760550498962402\n",
      "Epoch 1405/2000, Train Loss: 5.23077896528895, Val Loss: 5.645102380794852, Val MAE: 1.5757825374603271\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1406/2000, Train Loss: 5.230576994719428, Val Loss: 5.645108561450188, Val MAE: 1.5753939151763916\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1407/2000, Train Loss: 5.23039227887663, Val Loss: 5.644938669620304, Val MAE: 1.575835108757019\n",
      "Epoch 1408/2000, Train Loss: 5.230358468404041, Val Loss: 5.644944527371579, Val MAE: 1.5755592584609985\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1409/2000, Train Loss: 5.23013941860803, Val Loss: 5.644734692774052, Val MAE: 1.5754454135894775\n",
      "Epoch 1410/2000, Train Loss: 5.230008113728201, Val Loss: 5.644809998002256, Val MAE: 1.5753880739212036\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1411/2000, Train Loss: 5.22977595325929, Val Loss: 5.644802015673495, Val MAE: 1.5751157999038696\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1412/2000, Train Loss: 5.229803490353503, Val Loss: 5.644612266246332, Val MAE: 1.575337529182434\n",
      "Epoch 1413/2000, Train Loss: 5.229649004845616, Val Loss: 5.644715696527689, Val MAE: 1.5749489068984985\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1414/2000, Train Loss: 5.229324077066614, Val Loss: 5.644493161766172, Val MAE: 1.5752737522125244\n",
      "Epoch 1415/2000, Train Loss: 5.229255641370823, Val Loss: 5.644317385481402, Val MAE: 1.5754443407058716\n",
      "Epoch 1416/2000, Train Loss: 5.2290943157161145, Val Loss: 5.644373703166979, Val MAE: 1.575226068496704\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1417/2000, Train Loss: 5.228851767344679, Val Loss: 5.644242495799648, Val MAE: 1.575400471687317\n",
      "Epoch 1418/2000, Train Loss: 5.2287259910578125, Val Loss: 5.644207489645445, Val MAE: 1.5751032829284668\n",
      "Epoch 1419/2000, Train Loss: 5.228754650615286, Val Loss: 5.644373942588083, Val MAE: 1.575000286102295\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1420/2000, Train Loss: 5.228561920821037, Val Loss: 5.6440763942874534, Val MAE: 1.5752573013305664\n",
      "Epoch 1421/2000, Train Loss: 5.228466437284079, Val Loss: 5.643886295753881, Val MAE: 1.5755040645599365\n",
      "Epoch 1422/2000, Train Loss: 5.228075701614906, Val Loss: 5.64379882420604, Val MAE: 1.5754387378692627\n",
      "Epoch 1423/2000, Train Loss: 5.228134565501042, Val Loss: 5.643951236062458, Val MAE: 1.5750617980957031\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1424/2000, Train Loss: 5.227917456442332, Val Loss: 5.644013262806682, Val MAE: 1.5745558738708496\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1425/2000, Train Loss: 5.2277614427737, Val Loss: 5.643695142427716, Val MAE: 1.5752068758010864\n",
      "Epoch 1426/2000, Train Loss: 5.227596479096436, Val Loss: 5.643550640490441, Val MAE: 1.574973702430725\n",
      "Epoch 1427/2000, Train Loss: 5.227361473040544, Val Loss: 5.643553610697434, Val MAE: 1.5750893354415894\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1428/2000, Train Loss: 5.227198011182212, Val Loss: 5.643416486976708, Val MAE: 1.5748862028121948\n",
      "Epoch 1429/2000, Train Loss: 5.227095565352953, Val Loss: 5.642973723169131, Val MAE: 1.5756630897521973\n",
      "Epoch 1430/2000, Train Loss: 5.226837960537851, Val Loss: 5.64298066824948, Val MAE: 1.5754629373550415\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1431/2000, Train Loss: 5.226691386969806, Val Loss: 5.642894659886302, Val MAE: 1.5753569602966309\n",
      "Epoch 1432/2000, Train Loss: 5.2266409449137745, Val Loss: 5.642686767361215, Val MAE: 1.575549602508545\n",
      "Epoch 1433/2000, Train Loss: 5.226552041796712, Val Loss: 5.642812502220137, Val MAE: 1.5752520561218262\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1434/2000, Train Loss: 5.226358423092095, Val Loss: 5.64259087141683, Val MAE: 1.5758392810821533\n",
      "Epoch 1435/2000, Train Loss: 5.22617951159239, Val Loss: 5.6425302974218985, Val MAE: 1.5753087997436523\n",
      "Epoch 1436/2000, Train Loss: 5.226004547841275, Val Loss: 5.642576817206651, Val MAE: 1.575188398361206\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1437/2000, Train Loss: 5.225872660085905, Val Loss: 5.642260032646153, Val MAE: 1.5756343603134155\n",
      "Epoch 1438/2000, Train Loss: 5.225661572396629, Val Loss: 5.642296404193301, Val MAE: 1.5751525163650513\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1439/2000, Train Loss: 5.225510460486469, Val Loss: 5.642402360244264, Val MAE: 1.5749397277832031\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1440/2000, Train Loss: 5.225366303989537, Val Loss: 5.642159382957931, Val MAE: 1.5750007629394531\n",
      "Epoch 1441/2000, Train Loss: 5.22520618653482, Val Loss: 5.642098735505288, Val MAE: 1.5751419067382812\n",
      "Epoch 1442/2000, Train Loss: 5.225033228452737, Val Loss: 5.642077098290126, Val MAE: 1.5748937129974365\n",
      "Epoch 1443/2000, Train Loss: 5.224966192144814, Val Loss: 5.642076789147993, Val MAE: 1.5749465227127075\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1444/2000, Train Loss: 5.224741827342646, Val Loss: 5.641952488901783, Val MAE: 1.5749294757843018\n",
      "Epoch 1445/2000, Train Loss: 5.224702979543526, Val Loss: 5.641745781087365, Val MAE: 1.5751451253890991\n",
      "Epoch 1446/2000, Train Loss: 5.22445646328963, Val Loss: 5.641832504524004, Val MAE: 1.5746420621871948\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1447/2000, Train Loss: 5.224411742105021, Val Loss: 5.641632638803316, Val MAE: 1.574988842010498\n",
      "Epoch 1448/2000, Train Loss: 5.224231926846219, Val Loss: 5.641599903139499, Val MAE: 1.5750254392623901\n",
      "Epoch 1449/2000, Train Loss: 5.224125485953775, Val Loss: 5.641623940626416, Val MAE: 1.5746432542800903\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1450/2000, Train Loss: 5.223958909385729, Val Loss: 5.64145157411011, Val MAE: 1.574620246887207\n",
      "Epoch 1451/2000, Train Loss: 5.223887718705373, Val Loss: 5.641372875655098, Val MAE: 1.5749123096466064\n",
      "Epoch 1452/2000, Train Loss: 5.223619176500872, Val Loss: 5.641266838445212, Val MAE: 1.574890375137329\n",
      "Epoch 1453/2000, Train Loss: 5.2234253343102965, Val Loss: 5.6412350465216035, Val MAE: 1.574609398841858\n",
      "Epoch 1454/2000, Train Loss: 5.223282275612634, Val Loss: 5.641160566188874, Val MAE: 1.5746262073516846\n",
      "Epoch 1455/2000, Train Loss: 5.223214808523781, Val Loss: 5.641150624061214, Val MAE: 1.5748525857925415\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1456/2000, Train Loss: 5.22297303262183, Val Loss: 5.640999637156816, Val MAE: 1.5750049352645874\n",
      "Epoch 1457/2000, Train Loss: 5.222882687416318, Val Loss: 5.641023637368045, Val MAE: 1.5748130083084106\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1458/2000, Train Loss: 5.222687919235498, Val Loss: 5.640888158425644, Val MAE: 1.5746262073516846\n",
      "Epoch 1459/2000, Train Loss: 5.222562900317714, Val Loss: 5.640801483292463, Val MAE: 1.5750319957733154\n",
      "Epoch 1460/2000, Train Loss: 5.222516116585889, Val Loss: 5.64073917643375, Val MAE: 1.5748363733291626\n",
      "Epoch 1461/2000, Train Loss: 5.222399367319707, Val Loss: 5.640591334904735, Val MAE: 1.5751055479049683\n",
      "Epoch 1462/2000, Train Loss: 5.222116212623376, Val Loss: 5.64065894739708, Val MAE: 1.5747947692871094\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1463/2000, Train Loss: 5.222004261594017, Val Loss: 5.640511182014366, Val MAE: 1.5748264789581299\n",
      "Epoch 1464/2000, Train Loss: 5.221837097788092, Val Loss: 5.640337121823877, Val MAE: 1.5752019882202148\n",
      "Epoch 1465/2000, Train Loss: 5.221761655169923, Val Loss: 5.640452517464256, Val MAE: 1.574628233909607\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1466/2000, Train Loss: 5.2215739608566, Val Loss: 5.6401426105871115, Val MAE: 1.5751659870147705\n",
      "Epoch 1467/2000, Train Loss: 5.221376179138764, Val Loss: 5.639928117121031, Val MAE: 1.5753846168518066\n",
      "Epoch 1468/2000, Train Loss: 5.221333122857434, Val Loss: 5.639931607775003, Val MAE: 1.5751370191574097\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1469/2000, Train Loss: 5.221306545142134, Val Loss: 5.639686970991462, Val MAE: 1.5752602815628052\n",
      "Epoch 1470/2000, Train Loss: 5.220975937095014, Val Loss: 5.639685081733841, Val MAE: 1.5751326084136963\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1471/2000, Train Loss: 5.220825920970737, Val Loss: 5.639605280410624, Val MAE: 1.575351357460022\n",
      "Epoch 1472/2000, Train Loss: 5.220790068755932, Val Loss: 5.6397297275722575, Val MAE: 1.5748705863952637\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1473/2000, Train Loss: 5.220607996490285, Val Loss: 5.639319481288257, Val MAE: 1.5753618478775024\n",
      "Epoch 1474/2000, Train Loss: 5.220342568827716, Val Loss: 5.63939096238635, Val MAE: 1.5749248266220093\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1475/2000, Train Loss: 5.220255946626469, Val Loss: 5.639297692190617, Val MAE: 1.5747839212417603\n",
      "Epoch 1476/2000, Train Loss: 5.220063438724247, Val Loss: 5.639334433441498, Val MAE: 1.5747932195663452\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1477/2000, Train Loss: 5.219922038721921, Val Loss: 5.639329497192613, Val MAE: 1.5745849609375\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1478/2000, Train Loss: 5.2198077924313635, Val Loss: 5.639245854833075, Val MAE: 1.5745710134506226\n",
      "Epoch 1479/2000, Train Loss: 5.219678150944773, Val Loss: 5.639173400010173, Val MAE: 1.5741877555847168\n",
      "Epoch 1480/2000, Train Loss: 5.219677630316448, Val Loss: 5.638995098593767, Val MAE: 1.5748486518859863\n",
      "Epoch 1481/2000, Train Loss: 5.219465764956437, Val Loss: 5.638861157569681, Val MAE: 1.5745314359664917\n",
      "Epoch 1482/2000, Train Loss: 5.219351424540037, Val Loss: 5.639064581295766, Val MAE: 1.5741840600967407\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1483/2000, Train Loss: 5.21904612527105, Val Loss: 5.638983564730449, Val MAE: 1.5742064714431763\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1484/2000, Train Loss: 5.218948142617458, Val Loss: 5.638838865022412, Val MAE: 1.574013113975525\n",
      "Epoch 1485/2000, Train Loss: 5.218905373411561, Val Loss: 5.638603159502741, Val MAE: 1.5744162797927856\n",
      "Epoch 1486/2000, Train Loss: 5.218681691483156, Val Loss: 5.638718297494297, Val MAE: 1.5741124153137207\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1487/2000, Train Loss: 5.218500466517878, Val Loss: 5.638578252462437, Val MAE: 1.5741065740585327\n",
      "Epoch 1488/2000, Train Loss: 5.21840086610782, Val Loss: 5.6385667734645555, Val MAE: 1.5740883350372314\n",
      "Epoch 1489/2000, Train Loss: 5.2184957734805275, Val Loss: 5.638643202841829, Val MAE: 1.5736106634140015\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1490/2000, Train Loss: 5.218079316624448, Val Loss: 5.638535477198228, Val MAE: 1.5735268592834473\n",
      "Epoch 1491/2000, Train Loss: 5.2179585892410865, Val Loss: 5.638450738213478, Val MAE: 1.5734367370605469\n",
      "Epoch 1492/2000, Train Loss: 5.217960749605354, Val Loss: 5.637962679068248, Val MAE: 1.5742346048355103\n",
      "Epoch 1493/2000, Train Loss: 5.2175900817001315, Val Loss: 5.63809306190466, Val MAE: 1.573860764503479\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1494/2000, Train Loss: 5.217460858494693, Val Loss: 5.638014041821527, Val MAE: 1.5737864971160889\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1495/2000, Train Loss: 5.217310863846545, Val Loss: 5.638097245211995, Val MAE: 1.5736740827560425\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1496/2000, Train Loss: 5.217232548292886, Val Loss: 5.6379478986657, Val MAE: 1.573801040649414\n",
      "Epoch 1497/2000, Train Loss: 5.217021913951253, Val Loss: 5.637995533576799, Val MAE: 1.5734628438949585\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1498/2000, Train Loss: 5.216947464268448, Val Loss: 5.637954741989801, Val MAE: 1.5738502740859985\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1499/2000, Train Loss: 5.216773584810132, Val Loss: 5.637904549866277, Val MAE: 1.573331356048584\n",
      "Epoch 1500/2000, Train Loss: 5.216641560312227, Val Loss: 5.637717033061413, Val MAE: 1.573761224746704\n",
      "Epoch 1501/2000, Train Loss: 5.216441461605392, Val Loss: 5.637577045264594, Val MAE: 1.5736618041992188\n",
      "Epoch 1502/2000, Train Loss: 5.216482116167684, Val Loss: 5.637372860303348, Val MAE: 1.5735400915145874\n",
      "Epoch 1503/2000, Train Loss: 5.216354401660251, Val Loss: 5.637569308691068, Val MAE: 1.5733108520507812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1504/2000, Train Loss: 5.216151127422636, Val Loss: 5.637417243617025, Val MAE: 1.5735104084014893\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1505/2000, Train Loss: 5.215990824904096, Val Loss: 5.637510828468778, Val MAE: 1.5733562707901\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1506/2000, Train Loss: 5.215752908665055, Val Loss: 5.637340496348314, Val MAE: 1.5733602046966553\n",
      "Epoch 1507/2000, Train Loss: 5.215559837601707, Val Loss: 5.637161370355419, Val MAE: 1.573417067527771\n",
      "Epoch 1508/2000, Train Loss: 5.2154270362719775, Val Loss: 5.637155661479048, Val MAE: 1.573527216911316\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1509/2000, Train Loss: 5.215352023511937, Val Loss: 5.637035166314982, Val MAE: 1.573409080505371\n",
      "Epoch 1510/2000, Train Loss: 5.215195332128846, Val Loss: 5.636840939931913, Val MAE: 1.573376178741455\n",
      "Epoch 1511/2000, Train Loss: 5.215205099278986, Val Loss: 5.636697770349841, Val MAE: 1.5735249519348145\n",
      "Epoch 1512/2000, Train Loss: 5.21494012571908, Val Loss: 5.636569500789729, Val MAE: 1.5735256671905518\n",
      "Epoch 1513/2000, Train Loss: 5.214942920048584, Val Loss: 5.636737707649167, Val MAE: 1.573163390159607\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1514/2000, Train Loss: 5.214673438599041, Val Loss: 5.636580635784963, Val MAE: 1.5729165077209473\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1515/2000, Train Loss: 5.214484924501638, Val Loss: 5.636437704165776, Val MAE: 1.5730887651443481\n",
      "Epoch 1516/2000, Train Loss: 5.214437762924184, Val Loss: 5.636538086909037, Val MAE: 1.5730180740356445\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1517/2000, Train Loss: 5.214233326962261, Val Loss: 5.6363300644203065, Val MAE: 1.5731745958328247\n",
      "Epoch 1518/2000, Train Loss: 5.214094800902117, Val Loss: 5.636462054577078, Val MAE: 1.573007583618164\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1519/2000, Train Loss: 5.213859358780154, Val Loss: 5.636278342125248, Val MAE: 1.573153018951416\n",
      "Epoch 1520/2000, Train Loss: 5.2138378511089245, Val Loss: 5.6361862688527555, Val MAE: 1.5730512142181396\n",
      "Epoch 1521/2000, Train Loss: 5.213711472483776, Val Loss: 5.636221328095194, Val MAE: 1.5730304718017578\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1522/2000, Train Loss: 5.213629651791105, Val Loss: 5.63593290290519, Val MAE: 1.5733323097229004\n",
      "Epoch 1523/2000, Train Loss: 5.2134099110677825, Val Loss: 5.635937549834587, Val MAE: 1.5730639696121216\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1524/2000, Train Loss: 5.213167685173834, Val Loss: 5.635818120582024, Val MAE: 1.573174238204956\n",
      "Epoch 1525/2000, Train Loss: 5.213163919133422, Val Loss: 5.63570332700324, Val MAE: 1.5728293657302856\n",
      "Epoch 1526/2000, Train Loss: 5.212893441057306, Val Loss: 5.635778089136523, Val MAE: 1.5728334188461304\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1527/2000, Train Loss: 5.212787736896727, Val Loss: 5.635670450666994, Val MAE: 1.5727328062057495\n",
      "Epoch 1528/2000, Train Loss: 5.2126279696035684, Val Loss: 5.635483898564217, Val MAE: 1.57292902469635\n",
      "Epoch 1529/2000, Train Loss: 5.212849265332796, Val Loss: 5.635562293892242, Val MAE: 1.572754979133606\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1530/2000, Train Loss: 5.212447446303666, Val Loss: 5.63539578066142, Val MAE: 1.5728472471237183\n",
      "Epoch 1531/2000, Train Loss: 5.212398290130809, Val Loss: 5.635326938299229, Val MAE: 1.5727664232254028\n",
      "Epoch 1532/2000, Train Loss: 5.212167268400038, Val Loss: 5.635344678245553, Val MAE: 1.572464942932129\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1533/2000, Train Loss: 5.211942669709412, Val Loss: 5.635150133560922, Val MAE: 1.5724302530288696\n",
      "Epoch 1534/2000, Train Loss: 5.211772834138917, Val Loss: 5.635126194321416, Val MAE: 1.572546362876892\n",
      "Epoch 1535/2000, Train Loss: 5.212133171019799, Val Loss: 5.635369629351371, Val MAE: 1.572168231010437\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1536/2000, Train Loss: 5.2114884071497745, Val Loss: 5.63499980727467, Val MAE: 1.572760820388794\n",
      "Epoch 1537/2000, Train Loss: 5.211507288525425, Val Loss: 5.635096833793395, Val MAE: 1.5721514225006104\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1538/2000, Train Loss: 5.211221740247497, Val Loss: 5.634965070617309, Val MAE: 1.5724916458129883\n",
      "Epoch 1539/2000, Train Loss: 5.2111341557311475, Val Loss: 5.6349678815595965, Val MAE: 1.572373867034912\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1540/2000, Train Loss: 5.211197202764373, Val Loss: 5.635062284516996, Val MAE: 1.5722867250442505\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1541/2000, Train Loss: 5.210869831908009, Val Loss: 5.63475272492349, Val MAE: 1.572785496711731\n",
      "Epoch 1542/2000, Train Loss: 5.210734041201909, Val Loss: 5.634447286025099, Val MAE: 1.5732917785644531\n",
      "Epoch 1543/2000, Train Loss: 5.2106359063698156, Val Loss: 5.634459525711311, Val MAE: 1.5731152296066284\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1544/2000, Train Loss: 5.210297357356523, Val Loss: 5.634574843063632, Val MAE: 1.572912573814392\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1545/2000, Train Loss: 5.210190561772399, Val Loss: 5.634604153510992, Val MAE: 1.5725533962249756\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1546/2000, Train Loss: 5.2100061044820505, Val Loss: 5.63455511789074, Val MAE: 1.5724612474441528\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1547/2000, Train Loss: 5.209944542153967, Val Loss: 5.634319591276142, Val MAE: 1.572981595993042\n",
      "Epoch 1548/2000, Train Loss: 5.2097259213771725, Val Loss: 5.634434975022934, Val MAE: 1.572821855545044\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1549/2000, Train Loss: 5.2096445729915395, Val Loss: 5.634395569562912, Val MAE: 1.5725898742675781\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1550/2000, Train Loss: 5.209490520445752, Val Loss: 5.634240162026263, Val MAE: 1.5728061199188232\n",
      "Epoch 1551/2000, Train Loss: 5.209346680637819, Val Loss: 5.634190593564182, Val MAE: 1.5728102922439575\n",
      "Epoch 1552/2000, Train Loss: 5.2091648677635325, Val Loss: 5.634252110433506, Val MAE: 1.5725265741348267\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1553/2000, Train Loss: 5.209033657009546, Val Loss: 5.634131678838613, Val MAE: 1.572500467300415\n",
      "Epoch 1554/2000, Train Loss: 5.208859536112572, Val Loss: 5.633944971936923, Val MAE: 1.5727726221084595\n",
      "Epoch 1555/2000, Train Loss: 5.208920228825247, Val Loss: 5.633784463418369, Val MAE: 1.5728422403335571\n",
      "Epoch 1556/2000, Train Loss: 5.208737070001741, Val Loss: 5.634067959832853, Val MAE: 1.572220802307129\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1557/2000, Train Loss: 5.208740564570471, Val Loss: 5.633595520795668, Val MAE: 1.572851538658142\n",
      "Epoch 1558/2000, Train Loss: 5.208339806260733, Val Loss: 5.6337605209890125, Val MAE: 1.5724847316741943\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1559/2000, Train Loss: 5.208294905716898, Val Loss: 5.633764953833837, Val MAE: 1.5723828077316284\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1560/2000, Train Loss: 5.208083277600991, Val Loss: 5.633835652507044, Val MAE: 1.5720237493515015\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1561/2000, Train Loss: 5.207925819830522, Val Loss: 5.633616569847871, Val MAE: 1.572284460067749\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1562/2000, Train Loss: 5.207761425354546, Val Loss: 5.6336141102448885, Val MAE: 1.5721888542175293\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1563/2000, Train Loss: 5.207669301741061, Val Loss: 5.633648123717454, Val MAE: 1.571864366531372\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1564/2000, Train Loss: 5.207460578607053, Val Loss: 5.633640429477809, Val MAE: 1.572144627571106\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1565/2000, Train Loss: 5.20738746245758, Val Loss: 5.633860365239852, Val MAE: 1.5717463493347168\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1566/2000, Train Loss: 5.207238714095994, Val Loss: 5.633573576899114, Val MAE: 1.5720393657684326\n",
      "Epoch 1567/2000, Train Loss: 5.2071376067076995, Val Loss: 5.63338141403067, Val MAE: 1.5719643831253052\n",
      "Epoch 1568/2000, Train Loss: 5.206960597229541, Val Loss: 5.633323808738945, Val MAE: 1.5719901323318481\n",
      "Epoch 1569/2000, Train Loss: 5.206737892129403, Val Loss: 5.633317638518978, Val MAE: 1.571860909461975\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1570/2000, Train Loss: 5.206676766202955, Val Loss: 5.633270780883433, Val MAE: 1.5721355676651\n",
      "Epoch 1571/2000, Train Loss: 5.206500672857834, Val Loss: 5.633051814106991, Val MAE: 1.5720577239990234\n",
      "Epoch 1572/2000, Train Loss: 5.206397454726872, Val Loss: 5.632839903645559, Val MAE: 1.5722936391830444\n",
      "Epoch 1573/2000, Train Loss: 5.206260182503493, Val Loss: 5.632931500126462, Val MAE: 1.5717521905899048\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1574/2000, Train Loss: 5.20614003849902, Val Loss: 5.632816686725033, Val MAE: 1.5718406438827515\n",
      "Epoch 1575/2000, Train Loss: 5.206000238803807, Val Loss: 5.632555999704822, Val MAE: 1.5722215175628662\n",
      "Epoch 1576/2000, Train Loss: 5.205846761331686, Val Loss: 5.632778042910296, Val MAE: 1.5718519687652588\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1577/2000, Train Loss: 5.2056641672632376, Val Loss: 5.632767868561483, Val MAE: 1.5718568563461304\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1578/2000, Train Loss: 5.205608322618714, Val Loss: 5.63264748253589, Val MAE: 1.5717530250549316\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1579/2000, Train Loss: 5.205417616278416, Val Loss: 5.63242292199113, Val MAE: 1.5720828771591187\n",
      "Epoch 1580/2000, Train Loss: 5.205294346658383, Val Loss: 5.632526085373094, Val MAE: 1.5719016790390015\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1581/2000, Train Loss: 5.20523803700535, Val Loss: 5.632295165723617, Val MAE: 1.5722682476043701\n",
      "Epoch 1582/2000, Train Loss: 5.205096176356853, Val Loss: 5.632299565302851, Val MAE: 1.572055459022522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1583/2000, Train Loss: 5.204899093032639, Val Loss: 5.6321542175082255, Val MAE: 1.5720551013946533\n",
      "Epoch 1584/2000, Train Loss: 5.20472117849507, Val Loss: 5.63204200821003, Val MAE: 1.5720610618591309\n",
      "Epoch 1585/2000, Train Loss: 5.204813913423187, Val Loss: 5.632088544582008, Val MAE: 1.5717403888702393\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1586/2000, Train Loss: 5.204483770132904, Val Loss: 5.631911656770867, Val MAE: 1.5717891454696655\n",
      "Epoch 1587/2000, Train Loss: 5.204552599781421, Val Loss: 5.6319489133649645, Val MAE: 1.5719488859176636\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1588/2000, Train Loss: 5.204340930055515, Val Loss: 5.631659143698325, Val MAE: 1.571770429611206\n",
      "Epoch 1589/2000, Train Loss: 5.204015507906109, Val Loss: 5.6317288832289, Val MAE: 1.5717310905456543\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1590/2000, Train Loss: 5.20402202163256, Val Loss: 5.631545357201078, Val MAE: 1.5718377828598022\n",
      "Epoch 1591/2000, Train Loss: 5.203757072988318, Val Loss: 5.631714575926828, Val MAE: 1.5716842412948608\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1592/2000, Train Loss: 5.203727529889509, Val Loss: 5.631828516129325, Val MAE: 1.5712742805480957\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1593/2000, Train Loss: 5.203567198638258, Val Loss: 5.631820492378068, Val MAE: 1.5713183879852295\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1594/2000, Train Loss: 5.203574136979642, Val Loss: 5.631441382958984, Val MAE: 1.5714912414550781\n",
      "Epoch 1595/2000, Train Loss: 5.203268737544651, Val Loss: 5.631795329192728, Val MAE: 1.5709710121154785\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1596/2000, Train Loss: 5.203213674597636, Val Loss: 5.631601343871257, Val MAE: 1.5714117288589478\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1597/2000, Train Loss: 5.203013844090729, Val Loss: 5.631622214519649, Val MAE: 1.5711379051208496\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1598/2000, Train Loss: 5.2030217119038396, Val Loss: 5.631648327733763, Val MAE: 1.5708063840866089\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1599/2000, Train Loss: 5.202789241541112, Val Loss: 5.631463406829659, Val MAE: 1.5709426403045654\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1600/2000, Train Loss: 5.20256401198922, Val Loss: 5.631524380330646, Val MAE: 1.5706286430358887\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1601/2000, Train Loss: 5.2024107123662855, Val Loss: 5.631297335438772, Val MAE: 1.5710885524749756\n",
      "Epoch 1602/2000, Train Loss: 5.202291112144089, Val Loss: 5.631228811013589, Val MAE: 1.5710150003433228\n",
      "Epoch 1603/2000, Train Loss: 5.202129620720516, Val Loss: 5.631181770189458, Val MAE: 1.57121741771698\n",
      "Epoch 1604/2000, Train Loss: 5.202039156892952, Val Loss: 5.631179759671928, Val MAE: 1.5710532665252686\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1605/2000, Train Loss: 5.201882776452319, Val Loss: 5.631198197831072, Val MAE: 1.5706677436828613\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1606/2000, Train Loss: 5.201982528705315, Val Loss: 5.630759193035076, Val MAE: 1.5715417861938477\n",
      "Epoch 1607/2000, Train Loss: 5.201556091993146, Val Loss: 5.631026388809586, Val MAE: 1.5709381103515625\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1608/2000, Train Loss: 5.201602590830707, Val Loss: 5.631111880234622, Val MAE: 1.5704890489578247\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1609/2000, Train Loss: 5.201451054934798, Val Loss: 5.6312826950251145, Val MAE: 1.5705262422561646\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1610/2000, Train Loss: 5.201407900333069, Val Loss: 5.630710493260567, Val MAE: 1.5709185600280762\n",
      "Epoch 1611/2000, Train Loss: 5.20109014826539, Val Loss: 5.630767575553433, Val MAE: 1.570797085762024\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1612/2000, Train Loss: 5.20101407512152, Val Loss: 5.630808746231441, Val MAE: 1.5708812475204468\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1613/2000, Train Loss: 5.200886109653784, Val Loss: 5.63051625375354, Val MAE: 1.571511149406433\n",
      "Epoch 1614/2000, Train Loss: 5.200825539883554, Val Loss: 5.630647181917768, Val MAE: 1.5709254741668701\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1615/2000, Train Loss: 5.200903762318063, Val Loss: 5.630882832890986, Val MAE: 1.5704991817474365\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1616/2000, Train Loss: 5.2005379721784495, Val Loss: 5.630288288589647, Val MAE: 1.571277141571045\n",
      "Epoch 1617/2000, Train Loss: 5.200424753050834, Val Loss: 5.630194926708481, Val MAE: 1.571773648262024\n",
      "Epoch 1618/2000, Train Loss: 5.2003152752996415, Val Loss: 5.630266465451739, Val MAE: 1.570966124534607\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1619/2000, Train Loss: 5.200140009792148, Val Loss: 5.630352617996183, Val MAE: 1.5710135698318481\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1620/2000, Train Loss: 5.199895603400062, Val Loss: 5.629963201986175, Val MAE: 1.571445107460022\n",
      "Epoch 1621/2000, Train Loss: 5.199808768199246, Val Loss: 5.629933237209233, Val MAE: 1.5713751316070557\n",
      "Epoch 1622/2000, Train Loss: 5.1996328167979105, Val Loss: 5.629889274181211, Val MAE: 1.5713411569595337\n",
      "Epoch 1623/2000, Train Loss: 5.199675900427747, Val Loss: 5.629931606694099, Val MAE: 1.5710673332214355\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1624/2000, Train Loss: 5.19949887958234, Val Loss: 5.62982278223796, Val MAE: 1.5710291862487793\n",
      "Epoch 1625/2000, Train Loss: 5.1993826192336385, Val Loss: 5.629719061816869, Val MAE: 1.5711370706558228\n",
      "Epoch 1626/2000, Train Loss: 5.199089144586594, Val Loss: 5.629464946163175, Val MAE: 1.5712970495224\n",
      "Epoch 1627/2000, Train Loss: 5.1991905936214975, Val Loss: 5.629270023071073, Val MAE: 1.5716761350631714\n",
      "Epoch 1628/2000, Train Loss: 5.198834850786438, Val Loss: 5.629445979445717, Val MAE: 1.5712960958480835\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1629/2000, Train Loss: 5.198678410950889, Val Loss: 5.629284661889805, Val MAE: 1.5713757276535034\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1630/2000, Train Loss: 5.198581510492818, Val Loss: 5.629286975853304, Val MAE: 1.571246862411499\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1631/2000, Train Loss: 5.19845400218977, Val Loss: 5.629127599093892, Val MAE: 1.5713618993759155\n",
      "Epoch 1632/2000, Train Loss: 5.198366398714026, Val Loss: 5.629124131497987, Val MAE: 1.5712425708770752\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1633/2000, Train Loss: 5.1982092015093935, Val Loss: 5.629212206201086, Val MAE: 1.5707529783248901\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1634/2000, Train Loss: 5.198120907441903, Val Loss: 5.628910552972318, Val MAE: 1.5712319612503052\n",
      "Epoch 1635/2000, Train Loss: 5.1979778076375895, Val Loss: 5.628998565063199, Val MAE: 1.5709303617477417\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1636/2000, Train Loss: 5.197835757646822, Val Loss: 5.629035957772797, Val MAE: 1.571077585220337\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1637/2000, Train Loss: 5.197933786272751, Val Loss: 5.628633899654088, Val MAE: 1.571694254875183\n",
      "Epoch 1638/2000, Train Loss: 5.197454060407527, Val Loss: 5.6287903190570505, Val MAE: 1.5711438655853271\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1639/2000, Train Loss: 5.197485399615337, Val Loss: 5.628714064346905, Val MAE: 1.5710393190383911\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1640/2000, Train Loss: 5.197352398289165, Val Loss: 5.628616832720759, Val MAE: 1.5711573362350464\n",
      "Epoch 1641/2000, Train Loss: 5.197106073810382, Val Loss: 5.628606350232337, Val MAE: 1.5709525346755981\n",
      "Epoch 1642/2000, Train Loss: 5.196980998182196, Val Loss: 5.628526373467314, Val MAE: 1.571012020111084\n",
      "Epoch 1643/2000, Train Loss: 5.196933528220964, Val Loss: 5.6284295443092285, Val MAE: 1.5710867643356323\n",
      "Epoch 1644/2000, Train Loss: 5.197157918190469, Val Loss: 5.628324397204484, Val MAE: 1.571182131767273\n",
      "Epoch 1645/2000, Train Loss: 5.196768572261684, Val Loss: 5.628374955978598, Val MAE: 1.5709370374679565\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1646/2000, Train Loss: 5.196606300147296, Val Loss: 5.6281444030252805, Val MAE: 1.5711759328842163\n",
      "Epoch 1647/2000, Train Loss: 5.196439316203945, Val Loss: 5.628175086019965, Val MAE: 1.5709822177886963\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1648/2000, Train Loss: 5.196349340622732, Val Loss: 5.6281350021242, Val MAE: 1.5709750652313232\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1649/2000, Train Loss: 5.196122080570036, Val Loss: 5.628147513221164, Val MAE: 1.5711679458618164\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1650/2000, Train Loss: 5.196004632658559, Val Loss: 5.62810530415551, Val MAE: 1.5711520910263062\n",
      "Epoch 1651/2000, Train Loss: 5.196054163824749, Val Loss: 5.628043687762835, Val MAE: 1.5708298683166504\n",
      "Epoch 1652/2000, Train Loss: 5.195750572746862, Val Loss: 5.627993543760492, Val MAE: 1.5709654092788696\n",
      "Epoch 1653/2000, Train Loss: 5.195733094198615, Val Loss: 5.628046519211308, Val MAE: 1.570573091506958\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1654/2000, Train Loss: 5.195522704315723, Val Loss: 5.627952523461175, Val MAE: 1.5706226825714111\n",
      "Epoch 1655/2000, Train Loss: 5.195390600960159, Val Loss: 5.627661385713003, Val MAE: 1.5711528062820435\n",
      "Epoch 1656/2000, Train Loss: 5.195370121123, Val Loss: 5.627499953960425, Val MAE: 1.5709755420684814\n",
      "Epoch 1657/2000, Train Loss: 5.195108211686459, Val Loss: 5.6274957242179715, Val MAE: 1.5707658529281616\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1658/2000, Train Loss: 5.195052811832683, Val Loss: 5.627368513733969, Val MAE: 1.5708258152008057\n",
      "Epoch 1659/2000, Train Loss: 5.194827011485573, Val Loss: 5.627284619558477, Val MAE: 1.5709489583969116\n",
      "Epoch 1660/2000, Train Loss: 5.194808673053288, Val Loss: 5.627349921960714, Val MAE: 1.5706883668899536\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1661/2000, Train Loss: 5.194649109112218, Val Loss: 5.6272445143362795, Val MAE: 1.5707417726516724\n",
      "Epoch 1662/2000, Train Loss: 5.1945617296257796, Val Loss: 5.627151659185733, Val MAE: 1.5707769393920898\n",
      "Epoch 1663/2000, Train Loss: 5.194431579758969, Val Loss: 5.627207130512695, Val MAE: 1.5705885887145996\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1664/2000, Train Loss: 5.19426836185603, Val Loss: 5.62712288358525, Val MAE: 1.5703505277633667\n",
      "Epoch 1665/2000, Train Loss: 5.194206824695284, Val Loss: 5.6268106477978765, Val MAE: 1.5707759857177734\n",
      "Epoch 1666/2000, Train Loss: 5.19405106135777, Val Loss: 5.627110457976294, Val MAE: 1.570346474647522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1667/2000, Train Loss: 5.1938925601494805, Val Loss: 5.626951228177876, Val MAE: 1.5700840950012207\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1668/2000, Train Loss: 5.193722824912773, Val Loss: 5.626889456530594, Val MAE: 1.5706406831741333\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1669/2000, Train Loss: 5.193545262885714, Val Loss: 5.626910437411125, Val MAE: 1.5704752206802368\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1670/2000, Train Loss: 5.193511715915151, Val Loss: 5.626704995603007, Val MAE: 1.5705342292785645\n",
      "Epoch 1671/2000, Train Loss: 5.193391121750898, Val Loss: 5.6267077247028325, Val MAE: 1.570391297340393\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1672/2000, Train Loss: 5.193249769063167, Val Loss: 5.626626131184604, Val MAE: 1.5704883337020874\n",
      "Epoch 1673/2000, Train Loss: 5.193329718480388, Val Loss: 5.626889244952333, Val MAE: 1.5698981285095215\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1674/2000, Train Loss: 5.193067096304843, Val Loss: 5.626605197969562, Val MAE: 1.5703755617141724\n",
      "Epoch 1675/2000, Train Loss: 5.192830840774525, Val Loss: 5.626747428322786, Val MAE: 1.5699585676193237\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1676/2000, Train Loss: 5.192702671577191, Val Loss: 5.626463538918656, Val MAE: 1.5704058408737183\n",
      "Epoch 1677/2000, Train Loss: 5.192597393793258, Val Loss: 5.626519801647656, Val MAE: 1.5700323581695557\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1678/2000, Train Loss: 5.192480956727899, Val Loss: 5.626680316773759, Val MAE: 1.569797396659851\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1679/2000, Train Loss: 5.192309555581218, Val Loss: 5.6266297529870215, Val MAE: 1.5696625709533691\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1680/2000, Train Loss: 5.192271242671915, Val Loss: 5.6265139829037025, Val MAE: 1.5698951482772827\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1681/2000, Train Loss: 5.192134329502554, Val Loss: 5.626323846729888, Val MAE: 1.56972336769104\n",
      "Epoch 1682/2000, Train Loss: 5.191986584646613, Val Loss: 5.626389691862491, Val MAE: 1.5696847438812256\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1683/2000, Train Loss: 5.1918948851866595, Val Loss: 5.626354189367469, Val MAE: 1.5694774389266968\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1684/2000, Train Loss: 5.191818812462245, Val Loss: 5.626116319488313, Val MAE: 1.5697476863861084\n",
      "Epoch 1685/2000, Train Loss: 5.1915883623656045, Val Loss: 5.626016455004704, Val MAE: 1.5701884031295776\n",
      "Epoch 1686/2000, Train Loss: 5.1914433191394735, Val Loss: 5.626240209793097, Val MAE: 1.5695576667785645\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1687/2000, Train Loss: 5.191460338299917, Val Loss: 5.626129927164918, Val MAE: 1.5693180561065674\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1688/2000, Train Loss: 5.19117218688036, Val Loss: 5.626041357169093, Val MAE: 1.5693904161453247\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1689/2000, Train Loss: 5.191077352492932, Val Loss: 5.62596480799013, Val MAE: 1.569698452949524\n",
      "Epoch 1690/2000, Train Loss: 5.190945240748256, Val Loss: 5.625989628950026, Val MAE: 1.569319486618042\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1691/2000, Train Loss: 5.190725062616605, Val Loss: 5.625579851710833, Val MAE: 1.5701593160629272\n",
      "Epoch 1692/2000, Train Loss: 5.190718127337247, Val Loss: 5.6254928970099956, Val MAE: 1.5700711011886597\n",
      "Epoch 1693/2000, Train Loss: 5.1905356010528285, Val Loss: 5.625603357677431, Val MAE: 1.5698492527008057\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1694/2000, Train Loss: 5.190470818396104, Val Loss: 5.625432410690398, Val MAE: 1.5696722269058228\n",
      "Epoch 1695/2000, Train Loss: 5.190558354003928, Val Loss: 5.625855647551539, Val MAE: 1.569088101387024\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1696/2000, Train Loss: 5.190423444657993, Val Loss: 5.625492503017825, Val MAE: 1.56952702999115\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1697/2000, Train Loss: 5.1901159229453055, Val Loss: 5.62535608193014, Val MAE: 1.5697684288024902\n",
      "Epoch 1698/2000, Train Loss: 5.189948590182654, Val Loss: 5.625384180507528, Val MAE: 1.5695509910583496\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1699/2000, Train Loss: 5.189755849818123, Val Loss: 5.6254222890652645, Val MAE: 1.5693583488464355\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1700/2000, Train Loss: 5.189824846577761, Val Loss: 5.625489814611385, Val MAE: 1.5693795680999756\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1701/2000, Train Loss: 5.189624071288998, Val Loss: 5.625172038508483, Val MAE: 1.5698914527893066\n",
      "Epoch 1702/2000, Train Loss: 5.189463349650395, Val Loss: 5.625095988313357, Val MAE: 1.5696700811386108\n",
      "Epoch 1703/2000, Train Loss: 5.189279443860306, Val Loss: 5.625108813018245, Val MAE: 1.5695269107818604\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1704/2000, Train Loss: 5.1891558265954485, Val Loss: 5.6250694369504215, Val MAE: 1.5694388151168823\n",
      "Epoch 1705/2000, Train Loss: 5.1892038556071425, Val Loss: 5.625394468460608, Val MAE: 1.5691035985946655\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1706/2000, Train Loss: 5.188876866119836, Val Loss: 5.6251300884131625, Val MAE: 1.5692588090896606\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1707/2000, Train Loss: 5.188744888768743, Val Loss: 5.624889035655089, Val MAE: 1.5696271657943726\n",
      "Epoch 1708/2000, Train Loss: 5.188772588947304, Val Loss: 5.624643852495637, Val MAE: 1.569718599319458\n",
      "Epoch 1709/2000, Train Loss: 5.188606440727346, Val Loss: 5.6249894273755014, Val MAE: 1.569071650505066\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1710/2000, Train Loss: 5.188463801485312, Val Loss: 5.624564263525359, Val MAE: 1.5697665214538574\n",
      "Epoch 1711/2000, Train Loss: 5.188384443668309, Val Loss: 5.624406216842684, Val MAE: 1.5697044134140015\n",
      "Epoch 1712/2000, Train Loss: 5.1880522374113545, Val Loss: 5.624568967917644, Val MAE: 1.5694072246551514\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1713/2000, Train Loss: 5.18802371719704, Val Loss: 5.624655106791298, Val MAE: 1.569159746170044\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1714/2000, Train Loss: 5.188107427453424, Val Loss: 5.624743124578342, Val MAE: 1.5688289403915405\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1715/2000, Train Loss: 5.187759932626057, Val Loss: 5.624437329510301, Val MAE: 1.5693150758743286\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1716/2000, Train Loss: 5.18761039266781, Val Loss: 5.624284578436011, Val MAE: 1.5694098472595215\n",
      "Epoch 1717/2000, Train Loss: 5.187628339328538, Val Loss: 5.624125660559453, Val MAE: 1.5693844556808472\n",
      "Epoch 1718/2000, Train Loss: 5.187322151316965, Val Loss: 5.6241579356999205, Val MAE: 1.5694128274917603\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1719/2000, Train Loss: 5.187254282221838, Val Loss: 5.6242782642626254, Val MAE: 1.5690752267837524\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1720/2000, Train Loss: 5.18726323751561, Val Loss: 5.624062782352853, Val MAE: 1.5693005323410034\n",
      "Epoch 1721/2000, Train Loss: 5.186959613636294, Val Loss: 5.62428615650088, Val MAE: 1.5688713788986206\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1722/2000, Train Loss: 5.186875590298228, Val Loss: 5.62406247193478, Val MAE: 1.5687341690063477\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1723/2000, Train Loss: 5.1867500999123175, Val Loss: 5.623845573093184, Val MAE: 1.5692346096038818\n",
      "Epoch 1724/2000, Train Loss: 5.186561725745312, Val Loss: 5.623979361687961, Val MAE: 1.568970799446106\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1725/2000, Train Loss: 5.186513207816138, Val Loss: 5.623895053287529, Val MAE: 1.5690155029296875\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1726/2000, Train Loss: 5.186512591719711, Val Loss: 5.623698958153025, Val MAE: 1.5693718194961548\n",
      "Epoch 1727/2000, Train Loss: 5.186211277446975, Val Loss: 5.6235877925285145, Val MAE: 1.5693830251693726\n",
      "Epoch 1728/2000, Train Loss: 5.186083193315241, Val Loss: 5.623633395231098, Val MAE: 1.569209337234497\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1729/2000, Train Loss: 5.185976898561138, Val Loss: 5.623491331114682, Val MAE: 1.5692297220230103\n",
      "Epoch 1730/2000, Train Loss: 5.18584688308791, Val Loss: 5.623313260461212, Val MAE: 1.5695559978485107\n",
      "Epoch 1731/2000, Train Loss: 5.185675752574, Val Loss: 5.623339378368964, Val MAE: 1.5692920684814453\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1732/2000, Train Loss: 5.18555886857867, Val Loss: 5.62335716447699, Val MAE: 1.5692082643508911\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1733/2000, Train Loss: 5.185599816676012, Val Loss: 5.623470192108679, Val MAE: 1.5692172050476074\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1734/2000, Train Loss: 5.1853568673049955, Val Loss: 5.623209841116489, Val MAE: 1.5692687034606934\n",
      "Epoch 1735/2000, Train Loss: 5.185240284646589, Val Loss: 5.623116710636229, Val MAE: 1.5694777965545654\n",
      "Epoch 1736/2000, Train Loss: 5.185106650026309, Val Loss: 5.623059863754369, Val MAE: 1.569580078125\n",
      "Epoch 1737/2000, Train Loss: 5.184980870635471, Val Loss: 5.623155923657096, Val MAE: 1.5693366527557373\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1738/2000, Train Loss: 5.184959761447088, Val Loss: 5.623056322564043, Val MAE: 1.569082498550415\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1739/2000, Train Loss: 5.184847637769792, Val Loss: 5.623092216681632, Val MAE: 1.568986177444458\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1740/2000, Train Loss: 5.184646353765242, Val Loss: 5.622993407583018, Val MAE: 1.5686969757080078\n",
      "Epoch 1741/2000, Train Loss: 5.184449529580714, Val Loss: 5.622874134086323, Val MAE: 1.569191813468933\n",
      "Epoch 1742/2000, Train Loss: 5.184311933369807, Val Loss: 5.622961313215964, Val MAE: 1.5690361261367798\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1743/2000, Train Loss: 5.18424742545652, Val Loss: 5.623021965863508, Val MAE: 1.5683854818344116\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1744/2000, Train Loss: 5.184222322379427, Val Loss: 5.622859059579511, Val MAE: 1.5684356689453125\n",
      "Epoch 1745/2000, Train Loss: 5.183992501969576, Val Loss: 5.6230186429501305, Val MAE: 1.5683174133300781\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1746/2000, Train Loss: 5.183846390725525, Val Loss: 5.6227518844130575, Val MAE: 1.5690714120864868\n",
      "Epoch 1747/2000, Train Loss: 5.1837442835647405, Val Loss: 5.622763173386955, Val MAE: 1.568764090538025\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1748/2000, Train Loss: 5.183722302747562, Val Loss: 5.62243672535507, Val MAE: 1.5692063570022583\n",
      "Epoch 1749/2000, Train Loss: 5.183601869989834, Val Loss: 5.622592598382122, Val MAE: 1.5684517621994019\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1750/2000, Train Loss: 5.18330183774128, Val Loss: 5.622455974921174, Val MAE: 1.5688915252685547\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1751/2000, Train Loss: 5.18327390547792, Val Loss: 5.6225212981030115, Val MAE: 1.5687108039855957\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1752/2000, Train Loss: 5.183077523944917, Val Loss: 5.622258440890444, Val MAE: 1.5690701007843018\n",
      "Epoch 1753/2000, Train Loss: 5.183008216788113, Val Loss: 5.622302267666257, Val MAE: 1.569158673286438\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1754/2000, Train Loss: 5.182843637499987, Val Loss: 5.622141298669923, Val MAE: 1.5692118406295776\n",
      "Epoch 1755/2000, Train Loss: 5.182784011379083, Val Loss: 5.622252643928615, Val MAE: 1.568932056427002\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1756/2000, Train Loss: 5.182634026126741, Val Loss: 5.6221590640894865, Val MAE: 1.569083571434021\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1757/2000, Train Loss: 5.182505851895939, Val Loss: 5.622183879444359, Val MAE: 1.5691429376602173\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1758/2000, Train Loss: 5.1824482868122095, Val Loss: 5.622004788494256, Val MAE: 1.569185733795166\n",
      "Epoch 1759/2000, Train Loss: 5.18227298724492, Val Loss: 5.621970467018789, Val MAE: 1.5691564083099365\n",
      "Epoch 1760/2000, Train Loss: 5.18217084241702, Val Loss: 5.62180671952551, Val MAE: 1.5694009065628052\n",
      "Epoch 1761/2000, Train Loss: 5.182128404581069, Val Loss: 5.621895591919211, Val MAE: 1.568787932395935\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1762/2000, Train Loss: 5.181882447545416, Val Loss: 5.621666279954648, Val MAE: 1.5691933631896973\n",
      "Epoch 1763/2000, Train Loss: 5.181760126230668, Val Loss: 5.6217015858637085, Val MAE: 1.5689892768859863\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1764/2000, Train Loss: 5.181709358928742, Val Loss: 5.621566748628194, Val MAE: 1.5690666437149048\n",
      "Epoch 1765/2000, Train Loss: 5.181650744954269, Val Loss: 5.621682985158871, Val MAE: 1.5686490535736084\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1766/2000, Train Loss: 5.181365388283672, Val Loss: 5.6213934606094975, Val MAE: 1.5691543817520142\n",
      "Epoch 1767/2000, Train Loss: 5.181539997091434, Val Loss: 5.621319754818164, Val MAE: 1.5693042278289795\n",
      "Epoch 1768/2000, Train Loss: 5.181320950986633, Val Loss: 5.6211204202532405, Val MAE: 1.5691765546798706\n",
      "Epoch 1769/2000, Train Loss: 5.180995581772527, Val Loss: 5.62119470851137, Val MAE: 1.5690340995788574\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1770/2000, Train Loss: 5.180985157629036, Val Loss: 5.621210602992171, Val MAE: 1.5691345930099487\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1771/2000, Train Loss: 5.180811456187689, Val Loss: 5.621220320873304, Val MAE: 1.5688954591751099\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1772/2000, Train Loss: 5.180712969516215, Val Loss: 5.620907810394188, Val MAE: 1.569213628768921\n",
      "Epoch 1773/2000, Train Loss: 5.18062405582887, Val Loss: 5.6210131172649, Val MAE: 1.5687133073806763\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1774/2000, Train Loss: 5.180429451235745, Val Loss: 5.621107857799676, Val MAE: 1.5690988302230835\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1775/2000, Train Loss: 5.18033190062145, Val Loss: 5.620950421654485, Val MAE: 1.5690399408340454\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1776/2000, Train Loss: 5.180209540045656, Val Loss: 5.620965012625452, Val MAE: 1.5689984560012817\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1777/2000, Train Loss: 5.180111224008731, Val Loss: 5.62103375879085, Val MAE: 1.5685474872589111\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1778/2000, Train Loss: 5.180126180789741, Val Loss: 5.621066538474611, Val MAE: 1.5688763856887817\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1779/2000, Train Loss: 5.179846784583622, Val Loss: 5.620907519889899, Val MAE: 1.5690113306045532\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1780/2000, Train Loss: 5.179714800269565, Val Loss: 5.620788548289818, Val MAE: 1.5687998533248901\n",
      "Epoch 1781/2000, Train Loss: 5.179845919666116, Val Loss: 5.620960811409382, Val MAE: 1.5685646533966064\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1782/2000, Train Loss: 5.179603993095704, Val Loss: 5.620644286047064, Val MAE: 1.568939447402954\n",
      "Epoch 1783/2000, Train Loss: 5.179403176868071, Val Loss: 5.620921906482554, Val MAE: 1.5686728954315186\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1784/2000, Train Loss: 5.179290576856964, Val Loss: 5.620807038762519, Val MAE: 1.5684571266174316\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1785/2000, Train Loss: 5.179184249888667, Val Loss: 5.6205809338286015, Val MAE: 1.568726897239685\n",
      "Epoch 1786/2000, Train Loss: 5.179126278268544, Val Loss: 5.620495996161703, Val MAE: 1.568951964378357\n",
      "Epoch 1787/2000, Train Loss: 5.178998843118559, Val Loss: 5.620273795651004, Val MAE: 1.5688611268997192\n",
      "Epoch 1788/2000, Train Loss: 5.178907759671141, Val Loss: 5.620538597123338, Val MAE: 1.5686955451965332\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1789/2000, Train Loss: 5.178840007016896, Val Loss: 5.620377971947375, Val MAE: 1.5688612461090088\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1790/2000, Train Loss: 5.178572408923123, Val Loss: 5.620397990039729, Val MAE: 1.5688525438308716\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1791/2000, Train Loss: 5.178443858021167, Val Loss: 5.6203891572511155, Val MAE: 1.5685609579086304\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1792/2000, Train Loss: 5.17833084820191, Val Loss: 5.620023351515834, Val MAE: 1.5690875053405762\n",
      "Epoch 1793/2000, Train Loss: 5.178143619736008, Val Loss: 5.620211912905769, Val MAE: 1.568532943725586\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1794/2000, Train Loss: 5.178082222170095, Val Loss: 5.6201559464227895, Val MAE: 1.5681970119476318\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1795/2000, Train Loss: 5.178042422038916, Val Loss: 5.6198956595466045, Val MAE: 1.5688155889511108\n",
      "Epoch 1796/2000, Train Loss: 5.177804921060948, Val Loss: 5.619910914507845, Val MAE: 1.5685794353485107\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1797/2000, Train Loss: 5.177731085125616, Val Loss: 5.619877485780541, Val MAE: 1.5685681104660034\n",
      "Epoch 1798/2000, Train Loss: 5.177591821914152, Val Loss: 5.6199251010512725, Val MAE: 1.5684900283813477\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1799/2000, Train Loss: 5.177486174044518, Val Loss: 5.619847862955627, Val MAE: 1.5684088468551636\n",
      "Epoch 1800/2000, Train Loss: 5.177393319319873, Val Loss: 5.619547945962768, Val MAE: 1.5684887170791626\n",
      "Epoch 1801/2000, Train Loss: 5.177371931445171, Val Loss: 5.6195798190453, Val MAE: 1.5685480833053589\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1802/2000, Train Loss: 5.177142662703361, Val Loss: 5.619579038443186, Val MAE: 1.5681867599487305\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1803/2000, Train Loss: 5.177003713664163, Val Loss: 5.6195080747389285, Val MAE: 1.5681532621383667\n",
      "Epoch 1804/2000, Train Loss: 5.176989227176803, Val Loss: 5.619458689511005, Val MAE: 1.5683571100234985\n",
      "Epoch 1805/2000, Train Loss: 5.176776753270567, Val Loss: 5.619222227130826, Val MAE: 1.568495273590088\n",
      "Epoch 1806/2000, Train Loss: 5.176637115653009, Val Loss: 5.619231592214436, Val MAE: 1.5682843923568726\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1807/2000, Train Loss: 5.176663694710567, Val Loss: 5.619415042825066, Val MAE: 1.5680123567581177\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1808/2000, Train Loss: 5.176428642514556, Val Loss: 5.619236482119342, Val MAE: 1.5683414936065674\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1809/2000, Train Loss: 5.176314320889768, Val Loss: 5.61900745596543, Val MAE: 1.5684198141098022\n",
      "Epoch 1810/2000, Train Loss: 5.176300529356492, Val Loss: 5.618940679852022, Val MAE: 1.568359375\n",
      "Epoch 1811/2000, Train Loss: 5.176046814740669, Val Loss: 5.618869587277783, Val MAE: 1.5686665773391724\n",
      "Epoch 1812/2000, Train Loss: 5.1759243467842335, Val Loss: 5.618817966237709, Val MAE: 1.5683847665786743\n",
      "Epoch 1813/2000, Train Loss: 5.175841276952702, Val Loss: 5.618678510189056, Val MAE: 1.5685029029846191\n",
      "Epoch 1814/2000, Train Loss: 5.175836905386144, Val Loss: 5.61853343330392, Val MAE: 1.5685598850250244\n",
      "Epoch 1815/2000, Train Loss: 5.175657728874373, Val Loss: 5.618429013804193, Val MAE: 1.56870436668396\n",
      "Epoch 1816/2000, Train Loss: 5.175448936446295, Val Loss: 5.618333121095229, Val MAE: 1.5685131549835205\n",
      "Epoch 1817/2000, Train Loss: 5.175398797069445, Val Loss: 5.618320334486276, Val MAE: 1.5684864521026611\n",
      "Epoch 1818/2000, Train Loss: 5.175270842968956, Val Loss: 5.618229228194336, Val MAE: 1.5687944889068604\n",
      "Epoch 1819/2000, Train Loss: 5.175157910413427, Val Loss: 5.6182999726646905, Val MAE: 1.5684524774551392\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1820/2000, Train Loss: 5.175156040983582, Val Loss: 5.618106704695145, Val MAE: 1.5689499378204346\n",
      "Epoch 1821/2000, Train Loss: 5.174946362161871, Val Loss: 5.618228742699011, Val MAE: 1.5684646368026733\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1822/2000, Train Loss: 5.174886449468882, Val Loss: 5.617939200468749, Val MAE: 1.5688998699188232\n",
      "Epoch 1823/2000, Train Loss: 5.174732633747743, Val Loss: 5.617897904520735, Val MAE: 1.5689876079559326\n",
      "Epoch 1824/2000, Train Loss: 5.174554261751866, Val Loss: 5.617831763415526, Val MAE: 1.5686923265457153\n",
      "Epoch 1825/2000, Train Loss: 5.174686012429137, Val Loss: 5.617990965826796, Val MAE: 1.5682212114334106\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1826/2000, Train Loss: 5.174450238433881, Val Loss: 5.61762646674563, Val MAE: 1.5689351558685303\n",
      "Epoch 1827/2000, Train Loss: 5.174450710279517, Val Loss: 5.617543986950081, Val MAE: 1.56904137134552\n",
      "Epoch 1828/2000, Train Loss: 5.174091419646809, Val Loss: 5.617748628242301, Val MAE: 1.568390965461731\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1829/2000, Train Loss: 5.174173517851322, Val Loss: 5.61762508872999, Val MAE: 1.568615436553955\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1830/2000, Train Loss: 5.173813650760745, Val Loss: 5.617516326521515, Val MAE: 1.568587064743042\n",
      "Epoch 1831/2000, Train Loss: 5.173731983421052, Val Loss: 5.617296191986183, Val MAE: 1.5687220096588135\n",
      "Epoch 1832/2000, Train Loss: 5.17364325848874, Val Loss: 5.6173212879112375, Val MAE: 1.568482518196106\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1833/2000, Train Loss: 5.173590248693134, Val Loss: 5.617239316988064, Val MAE: 1.5684266090393066\n",
      "Epoch 1834/2000, Train Loss: 5.173414356183368, Val Loss: 5.617213053258552, Val MAE: 1.5684385299682617\n",
      "Epoch 1835/2000, Train Loss: 5.173373678215451, Val Loss: 5.6174287214556236, Val MAE: 1.5681798458099365\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1836/2000, Train Loss: 5.173309950154068, Val Loss: 5.617235150267954, Val MAE: 1.5682697296142578\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1837/2000, Train Loss: 5.1730629875993825, Val Loss: 5.617409060216461, Val MAE: 1.5678197145462036\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1838/2000, Train Loss: 5.173006794638906, Val Loss: 5.617258894926547, Val MAE: 1.5681387186050415\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1839/2000, Train Loss: 5.172884464263916, Val Loss: 5.617160851027623, Val MAE: 1.5682317018508911\n",
      "Epoch 1840/2000, Train Loss: 5.172756680965759, Val Loss: 5.61716199741451, Val MAE: 1.5681283473968506\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1841/2000, Train Loss: 5.172737086683323, Val Loss: 5.616841497178836, Val MAE: 1.5683226585388184\n",
      "Epoch 1842/2000, Train Loss: 5.172508473970116, Val Loss: 5.616827672546792, Val MAE: 1.5683388710021973\n",
      "Epoch 1843/2000, Train Loss: 5.172353862793323, Val Loss: 5.616773113426083, Val MAE: 1.5685101747512817\n",
      "Epoch 1844/2000, Train Loss: 5.172183446444566, Val Loss: 5.616746628184202, Val MAE: 1.568526029586792\n",
      "Epoch 1845/2000, Train Loss: 5.172167438088295, Val Loss: 5.616693635371482, Val MAE: 1.5688624382019043\n",
      "Epoch 1846/2000, Train Loss: 5.172033696506831, Val Loss: 5.616706674479928, Val MAE: 1.5684125423431396\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1847/2000, Train Loss: 5.171875342275793, Val Loss: 5.616708870965771, Val MAE: 1.568144679069519\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1848/2000, Train Loss: 5.171833031954352, Val Loss: 5.616680063557917, Val MAE: 1.5682501792907715\n",
      "Epoch 1849/2000, Train Loss: 5.171750934420968, Val Loss: 5.616821804770272, Val MAE: 1.5680750608444214\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1850/2000, Train Loss: 5.171658639417913, Val Loss: 5.616539427492232, Val MAE: 1.5684250593185425\n",
      "Epoch 1851/2000, Train Loss: 5.171480924792897, Val Loss: 5.6164106619558565, Val MAE: 1.568464994430542\n",
      "Epoch 1852/2000, Train Loss: 5.171410499870735, Val Loss: 5.616646740688097, Val MAE: 1.568029761314392\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1853/2000, Train Loss: 5.171325754062296, Val Loss: 5.616370355135075, Val MAE: 1.5679644346237183\n",
      "Epoch 1854/2000, Train Loss: 5.171145124891792, Val Loss: 5.6162555109743675, Val MAE: 1.5684325695037842\n",
      "Epoch 1855/2000, Train Loss: 5.1709606035086235, Val Loss: 5.616265939508739, Val MAE: 1.568018913269043\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1856/2000, Train Loss: 5.1709095708590675, Val Loss: 5.616452823811715, Val MAE: 1.5674623250961304\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1857/2000, Train Loss: 5.1708126615085375, Val Loss: 5.616474875570802, Val MAE: 1.5678024291992188\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1858/2000, Train Loss: 5.170598549460291, Val Loss: 5.616414469772158, Val MAE: 1.5676853656768799\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1859/2000, Train Loss: 5.170561695837119, Val Loss: 5.616376934248373, Val MAE: 1.5675936937332153\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1860/2000, Train Loss: 5.170514529897326, Val Loss: 5.616517315763946, Val MAE: 1.5675444602966309\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1861/2000, Train Loss: 5.170310955329482, Val Loss: 5.616531488317597, Val MAE: 1.5672560930252075\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1862/2000, Train Loss: 5.170245700785176, Val Loss: 5.61607532431044, Val MAE: 1.5676689147949219\n",
      "Epoch 1863/2000, Train Loss: 5.170185614269775, Val Loss: 5.616092098552152, Val MAE: 1.5676275491714478\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1864/2000, Train Loss: 5.1698929904465265, Val Loss: 5.616004684831753, Val MAE: 1.5679246187210083\n",
      "Epoch 1865/2000, Train Loss: 5.169808509650153, Val Loss: 5.615913186461554, Val MAE: 1.56789231300354\n",
      "Epoch 1866/2000, Train Loss: 5.16975211056918, Val Loss: 5.615905371964525, Val MAE: 1.5676956176757812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1867/2000, Train Loss: 5.16973955575888, Val Loss: 5.615827162376966, Val MAE: 1.5674854516983032\n",
      "Epoch 1868/2000, Train Loss: 5.1695506285815735, Val Loss: 5.61581015782801, Val MAE: 1.5673619508743286\n",
      "Epoch 1869/2000, Train Loss: 5.169493392770513, Val Loss: 5.616008042058813, Val MAE: 1.5671155452728271\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1870/2000, Train Loss: 5.169452162211751, Val Loss: 5.615869634331913, Val MAE: 1.5671905279159546\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1871/2000, Train Loss: 5.169321855468535, Val Loss: 5.615948192843604, Val MAE: 1.5669509172439575\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1872/2000, Train Loss: 5.169226087372879, Val Loss: 5.61579477126263, Val MAE: 1.5671069622039795\n",
      "Epoch 1873/2000, Train Loss: 5.169231836263937, Val Loss: 5.6159205067595215, Val MAE: 1.5665841102600098\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1874/2000, Train Loss: 5.168842555602447, Val Loss: 5.615539088343991, Val MAE: 1.5674104690551758\n",
      "Epoch 1875/2000, Train Loss: 5.168795148056213, Val Loss: 5.6157134987801225, Val MAE: 1.5668702125549316\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1876/2000, Train Loss: 5.168802371585478, Val Loss: 5.615321357817096, Val MAE: 1.567421317100525\n",
      "Epoch 1877/2000, Train Loss: 5.168454699747836, Val Loss: 5.615361287643056, Val MAE: 1.5673171281814575\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1878/2000, Train Loss: 5.168513379660733, Val Loss: 5.615282795434698, Val MAE: 1.567331314086914\n",
      "Epoch 1879/2000, Train Loss: 5.168400963455082, Val Loss: 5.615467423054786, Val MAE: 1.5668129920959473\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1880/2000, Train Loss: 5.1681337510927055, Val Loss: 5.61537500874165, Val MAE: 1.566810965538025\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1881/2000, Train Loss: 5.1682580408288255, Val Loss: 5.615034010477752, Val MAE: 1.5676532983779907\n",
      "Epoch 1882/2000, Train Loss: 5.167932270782922, Val Loss: 5.615046312366057, Val MAE: 1.5674055814743042\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1883/2000, Train Loss: 5.167793728615682, Val Loss: 5.615082245809952, Val MAE: 1.567283272743225\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1884/2000, Train Loss: 5.167721764123582, Val Loss: 5.615045560153617, Val MAE: 1.56732976436615\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1885/2000, Train Loss: 5.16766842446807, Val Loss: 5.61485421479842, Val MAE: 1.567643404006958\n",
      "Epoch 1886/2000, Train Loss: 5.167616157696165, Val Loss: 5.615042298075256, Val MAE: 1.5672885179519653\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1887/2000, Train Loss: 5.167456867827402, Val Loss: 5.614861766315017, Val MAE: 1.567010760307312\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1888/2000, Train Loss: 5.167240205153372, Val Loss: 5.614873138908582, Val MAE: 1.5673757791519165\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1889/2000, Train Loss: 5.167253541409508, Val Loss: 5.614784820500864, Val MAE: 1.5674844980239868\n",
      "Epoch 1890/2000, Train Loss: 5.167088411879489, Val Loss: 5.614894630803246, Val MAE: 1.5670782327651978\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1891/2000, Train Loss: 5.166930586917563, Val Loss: 5.614945987860362, Val MAE: 1.566725254058838\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1892/2000, Train Loss: 5.166877841211221, Val Loss: 5.614886934285135, Val MAE: 1.5668067932128906\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1893/2000, Train Loss: 5.166726505982885, Val Loss: 5.614685498519775, Val MAE: 1.566946268081665\n",
      "Epoch 1894/2000, Train Loss: 5.166650425288813, Val Loss: 5.614770499391294, Val MAE: 1.5664712190628052\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1895/2000, Train Loss: 5.166568429431472, Val Loss: 5.614905879311605, Val MAE: 1.5663028955459595\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1896/2000, Train Loss: 5.1664265240690055, Val Loss: 5.614808192767135, Val MAE: 1.566067099571228\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1897/2000, Train Loss: 5.166243644435165, Val Loss: 5.614771299086944, Val MAE: 1.5665388107299805\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1898/2000, Train Loss: 5.166217155057221, Val Loss: 5.614616277476698, Val MAE: 1.5667154788970947\n",
      "Epoch 1899/2000, Train Loss: 5.1660730058588165, Val Loss: 5.614654153448726, Val MAE: 1.5664159059524536\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1900/2000, Train Loss: 5.1659778736578925, Val Loss: 5.614594930811395, Val MAE: 1.5666041374206543\n",
      "Epoch 1901/2000, Train Loss: 5.165956121108467, Val Loss: 5.614546406341985, Val MAE: 1.5663819313049316\n",
      "Epoch 1902/2000, Train Loss: 5.165686810377364, Val Loss: 5.614480503700924, Val MAE: 1.5665513277053833\n",
      "Epoch 1903/2000, Train Loss: 5.165690633631525, Val Loss: 5.614439240608376, Val MAE: 1.5664891004562378\n",
      "Epoch 1904/2000, Train Loss: 5.1654836972107105, Val Loss: 5.614332505048962, Val MAE: 1.5665171146392822\n",
      "Epoch 1905/2000, Train Loss: 5.165479679329138, Val Loss: 5.614564491749174, Val MAE: 1.5661083459854126\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1906/2000, Train Loss: 5.165266232285846, Val Loss: 5.61439621311809, Val MAE: 1.5663706064224243\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1907/2000, Train Loss: 5.165260225177902, Val Loss: 5.6142488261884145, Val MAE: 1.5663398504257202\n",
      "Epoch 1908/2000, Train Loss: 5.1652598267370005, Val Loss: 5.614532388587246, Val MAE: 1.5657806396484375\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1909/2000, Train Loss: 5.165149298794751, Val Loss: 5.614169173421116, Val MAE: 1.5660799741744995\n",
      "Epoch 1910/2000, Train Loss: 5.164882173343581, Val Loss: 5.614240158178391, Val MAE: 1.5656810998916626\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1911/2000, Train Loss: 5.164746284484863, Val Loss: 5.614107640283552, Val MAE: 1.5661150217056274\n",
      "Epoch 1912/2000, Train Loss: 5.164901690949528, Val Loss: 5.61424203772975, Val MAE: 1.5657144784927368\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1913/2000, Train Loss: 5.164572783543307, Val Loss: 5.614170452688083, Val MAE: 1.5657289028167725\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1914/2000, Train Loss: 5.164469867755963, Val Loss: 5.6141240184369074, Val MAE: 1.5657843351364136\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1915/2000, Train Loss: 5.164313433289444, Val Loss: 5.614145950485442, Val MAE: 1.565610647201538\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1916/2000, Train Loss: 5.164166378270229, Val Loss: 5.614086458898101, Val MAE: 1.5656481981277466\n",
      "Epoch 1917/2000, Train Loss: 5.164060452385405, Val Loss: 5.614022855319379, Val MAE: 1.56562077999115\n",
      "Epoch 1918/2000, Train Loss: 5.164020504102499, Val Loss: 5.613933435820658, Val MAE: 1.5655382871627808\n",
      "Epoch 1919/2000, Train Loss: 5.164001413837946, Val Loss: 5.613772753318515, Val MAE: 1.5661554336547852\n",
      "Epoch 1920/2000, Train Loss: 5.16368180031008, Val Loss: 5.613852562479652, Val MAE: 1.565761923789978\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1921/2000, Train Loss: 5.163671986138963, Val Loss: 5.613938553435358, Val MAE: 1.5653412342071533\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1922/2000, Train Loss: 5.1635681161739555, Val Loss: 5.6137248632649035, Val MAE: 1.5658645629882812\n",
      "Epoch 1923/2000, Train Loss: 5.163475740635421, Val Loss: 5.613852743161928, Val MAE: 1.5653775930404663\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1924/2000, Train Loss: 5.163285524219296, Val Loss: 5.6136320841239495, Val MAE: 1.5658063888549805\n",
      "Epoch 1925/2000, Train Loss: 5.163201875706779, Val Loss: 5.613542177170424, Val MAE: 1.5656788349151611\n",
      "Epoch 1926/2000, Train Loss: 5.163091790248272, Val Loss: 5.613247885206424, Val MAE: 1.5660724639892578\n",
      "Epoch 1927/2000, Train Loss: 5.163009815028149, Val Loss: 5.613356034826795, Val MAE: 1.5656198263168335\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1928/2000, Train Loss: 5.1629062727418775, Val Loss: 5.613122192772521, Val MAE: 1.5657343864440918\n",
      "Epoch 1929/2000, Train Loss: 5.162736302106671, Val Loss: 5.613203194299969, Val MAE: 1.5657200813293457\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1930/2000, Train Loss: 5.162595126093651, Val Loss: 5.613073847891723, Val MAE: 1.5660978555679321\n",
      "Epoch 1931/2000, Train Loss: 5.162522055916514, Val Loss: 5.613045174184196, Val MAE: 1.5658777952194214\n",
      "Epoch 1932/2000, Train Loss: 5.162428754555516, Val Loss: 5.61297784695567, Val MAE: 1.5656038522720337\n",
      "Epoch 1933/2000, Train Loss: 5.162363574087075, Val Loss: 5.612845050313422, Val MAE: 1.5662161111831665\n",
      "Epoch 1934/2000, Train Loss: 5.162181299224359, Val Loss: 5.612897702951314, Val MAE: 1.5658800601959229\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1935/2000, Train Loss: 5.162170379284316, Val Loss: 5.612831072978652, Val MAE: 1.5657747983932495\n",
      "Epoch 1936/2000, Train Loss: 5.162194981531389, Val Loss: 5.612516754902102, Val MAE: 1.5666999816894531\n",
      "Epoch 1937/2000, Train Loss: 5.161844382396808, Val Loss: 5.612711364887541, Val MAE: 1.5661872625350952\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1938/2000, Train Loss: 5.161750861576626, Val Loss: 5.612748115616836, Val MAE: 1.5664399862289429\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1939/2000, Train Loss: 5.161635538627362, Val Loss: 5.612744926403787, Val MAE: 1.566437840461731\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1940/2000, Train Loss: 5.161484072025522, Val Loss: 5.612723890202125, Val MAE: 1.5660743713378906\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1941/2000, Train Loss: 5.161588289887027, Val Loss: 5.612669711585074, Val MAE: 1.5660719871520996\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1942/2000, Train Loss: 5.161283417539979, Val Loss: 5.612440209231974, Val MAE: 1.5660251379013062\n",
      "Epoch 1943/2000, Train Loss: 5.161379338653049, Val Loss: 5.61241306970608, Val MAE: 1.566327452659607\n",
      "Epoch 1944/2000, Train Loss: 5.161192849687419, Val Loss: 5.612587307161148, Val MAE: 1.565897822380066\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1945/2000, Train Loss: 5.160962122246717, Val Loss: 5.612194919121375, Val MAE: 1.5660908222198486\n",
      "Epoch 1946/2000, Train Loss: 5.160950061387365, Val Loss: 5.612443342212508, Val MAE: 1.5657529830932617\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1947/2000, Train Loss: 5.160661973221717, Val Loss: 5.612090569570524, Val MAE: 1.5661792755126953\n",
      "Epoch 1948/2000, Train Loss: 5.160700138734311, Val Loss: 5.612084800223692, Val MAE: 1.5661346912384033\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1949/2000, Train Loss: 5.1604966016658, Val Loss: 5.6121116905310835, Val MAE: 1.5658470392227173\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1950/2000, Train Loss: 5.16050224485404, Val Loss: 5.61202673039852, Val MAE: 1.5663845539093018\n",
      "Epoch 1951/2000, Train Loss: 5.160277403214715, Val Loss: 5.612033949441502, Val MAE: 1.565975308418274\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1952/2000, Train Loss: 5.160327101659808, Val Loss: 5.612062852271471, Val MAE: 1.565855622291565\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1953/2000, Train Loss: 5.160130864042031, Val Loss: 5.611966949810675, Val MAE: 1.565897822380066\n",
      "Epoch 1954/2000, Train Loss: 5.160056164867017, Val Loss: 5.611935746642428, Val MAE: 1.565779447555542\n",
      "Epoch 1955/2000, Train Loss: 5.159907553751313, Val Loss: 5.611741349055498, Val MAE: 1.5659881830215454\n",
      "Epoch 1956/2000, Train Loss: 5.159759172832857, Val Loss: 5.61173091186296, Val MAE: 1.565935492515564\n",
      "Epoch 1957/2000, Train Loss: 5.1596909012281085, Val Loss: 5.611725254252052, Val MAE: 1.565791368484497\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1958/2000, Train Loss: 5.1595427026889595, Val Loss: 5.611785910772985, Val MAE: 1.5658197402954102\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1959/2000, Train Loss: 5.159542711916983, Val Loss: 5.611503109700454, Val MAE: 1.5662165880203247\n",
      "Epoch 1960/2000, Train Loss: 5.159377871895239, Val Loss: 5.611875966327999, Val MAE: 1.5656393766403198\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1961/2000, Train Loss: 5.159404022436927, Val Loss: 5.611494992440994, Val MAE: 1.5661650896072388\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1962/2000, Train Loss: 5.159336288742076, Val Loss: 5.611697573089454, Val MAE: 1.5655454397201538\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1963/2000, Train Loss: 5.159070868210252, Val Loss: 5.611547142495073, Val MAE: 1.5659974813461304\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1964/2000, Train Loss: 5.158896811993335, Val Loss: 5.61152978801946, Val MAE: 1.565700888633728\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1965/2000, Train Loss: 5.158840507289878, Val Loss: 5.611519615356711, Val MAE: 1.5661159753799438\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1966/2000, Train Loss: 5.1587645427011255, Val Loss: 5.611664646809254, Val MAE: 1.5658918619155884\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1967/2000, Train Loss: 5.1585902610687535, Val Loss: 5.611374021077739, Val MAE: 1.566282868385315\n",
      "Epoch 1968/2000, Train Loss: 5.158549628569826, Val Loss: 5.611392047700532, Val MAE: 1.566009759902954\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1969/2000, Train Loss: 5.158511444013946, Val Loss: 5.61116543644612, Val MAE: 1.5665920972824097\n",
      "Epoch 1970/2000, Train Loss: 5.158243223783586, Val Loss: 5.611311916227734, Val MAE: 1.5662281513214111\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1971/2000, Train Loss: 5.158161832445696, Val Loss: 5.611218550199762, Val MAE: 1.5662214756011963\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1972/2000, Train Loss: 5.158103295902733, Val Loss: 5.61134642007154, Val MAE: 1.5659030675888062\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1973/2000, Train Loss: 5.158132117370079, Val Loss: 5.611274350400365, Val MAE: 1.5658308267593384\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1974/2000, Train Loss: 5.157768307899942, Val Loss: 5.610946373153899, Val MAE: 1.5660990476608276\n",
      "Epoch 1975/2000, Train Loss: 5.157682810259234, Val Loss: 5.610883242812361, Val MAE: 1.5660239458084106\n",
      "Epoch 1976/2000, Train Loss: 5.1575865097904945, Val Loss: 5.61072377081311, Val MAE: 1.5661492347717285\n",
      "Epoch 1977/2000, Train Loss: 5.157507929560334, Val Loss: 5.610744967783263, Val MAE: 1.5663312673568726\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1978/2000, Train Loss: 5.157483184782239, Val Loss: 5.610786134633449, Val MAE: 1.5658262968063354\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1979/2000, Train Loss: 5.157350648884032, Val Loss: 5.610745633459602, Val MAE: 1.5660704374313354\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1980/2000, Train Loss: 5.157201620661651, Val Loss: 5.610671607635072, Val MAE: 1.56590735912323\n",
      "Epoch 1981/2000, Train Loss: 5.157037918874699, Val Loss: 5.610463940085621, Val MAE: 1.5660796165466309\n",
      "Epoch 1982/2000, Train Loss: 5.1568955146955995, Val Loss: 5.610371430122524, Val MAE: 1.5662957429885864\n",
      "Epoch 1983/2000, Train Loss: 5.156826082876583, Val Loss: 5.610257236691425, Val MAE: 1.5663028955459595\n",
      "Epoch 1984/2000, Train Loss: 5.156783921126326, Val Loss: 5.6103484056867226, Val MAE: 1.5660423040390015\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1985/2000, Train Loss: 5.156686064393314, Val Loss: 5.610447967681316, Val MAE: 1.5658825635910034\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1986/2000, Train Loss: 5.156548994347547, Val Loss: 5.6103609557454375, Val MAE: 1.565726637840271\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1987/2000, Train Loss: 5.156399481570024, Val Loss: 5.610130129330749, Val MAE: 1.5660245418548584\n",
      "Epoch 1988/2000, Train Loss: 5.156300730138157, Val Loss: 5.610108248410969, Val MAE: 1.5659559965133667\n",
      "Epoch 1989/2000, Train Loss: 5.156187142225154, Val Loss: 5.6099664925708685, Val MAE: 1.5658915042877197\n",
      "Epoch 1990/2000, Train Loss: 5.156064382747057, Val Loss: 5.61001712708116, Val MAE: 1.5655897855758667\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1991/2000, Train Loss: 5.15608472575779, Val Loss: 5.60983530065153, Val MAE: 1.5659235715866089\n",
      "Epoch 1992/2000, Train Loss: 5.155910058226239, Val Loss: 5.609796417916951, Val MAE: 1.5658665895462036\n",
      "Epoch 1993/2000, Train Loss: 5.155731110233895, Val Loss: 5.609960907187301, Val MAE: 1.5654243230819702\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1994/2000, Train Loss: 5.1557285629637555, Val Loss: 5.609960428755218, Val MAE: 1.565497636795044\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1995/2000, Train Loss: 5.15567707948329, Val Loss: 5.6099463982410755, Val MAE: 1.5652967691421509\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1996/2000, Train Loss: 5.15542504646508, Val Loss: 5.610012043552296, Val MAE: 1.565170407295227\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1997/2000, Train Loss: 5.155430449808592, Val Loss: 5.610091807508687, Val MAE: 1.564575433731079\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1998/2000, Train Loss: 5.155255719694263, Val Loss: 5.609961972962097, Val MAE: 1.564792513847351\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1999/2000, Train Loss: 5.155146445295158, Val Loss: 5.609867856225471, Val MAE: 1.565034031867981\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 2000/2000, Train Loss: 5.155011126057184, Val Loss: 5.6097297979695355, Val MAE: 1.565129280090332\n",
      "Test Loss (MSE): 3.822767496109009\n",
      "Test Mean Absolute Error (MAE): 1.2239730036719343\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(rnnModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (rnn): RNN(10, 64, batch_first=True)\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=385, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'test_mae': 1.2239730036719343})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 444\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_rnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=6,\n",
    "                kernel_size=2,\n",
    "                num_filters=64,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = ['total_points', 'ict_index', 'clean_sheets', 'goals_conceded', 'bps', 'matchup_difficulty', 'goals_scored', 'assists', 'yellow_cards', 'red_cards'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpremier.cnn.experiment import gridsearch_cnn\n",
    "\n",
    "#gridsearch_cnn(epochs=100, verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized)  {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
