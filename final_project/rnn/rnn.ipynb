{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.rnn.model import build_train_rnn, full_rnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2502, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2502, 7)\n",
      "Shape of a given window (prior to preprocessing): (6, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[2.20565704e+00 1.33883324e+00 1.79139658e-01 7.89628757e-01\n",
      " 1.09493223e+01 1.36122569e-01 5.89275192e-04 2.35710077e-03\n",
      " 2.35710077e-02 1.17855038e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.85633472  1.54349833  0.38346922  1.18920143 10.73299567  1.44422033\n",
      "  0.02426784  0.04849273  0.15170832  0.03430979]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building rnn Architecture ======\n",
      "====== Done Building rnn Architecture ======\n",
      "Epoch 1/2000, Train Loss: 12.621868637562805, Val Loss: 9.203133417186091, Val MAE: 1.5818248987197876\n",
      "Epoch 2/2000, Train Loss: 12.480846977418448, Val Loss: 9.113794753511073, Val MAE: 1.5695126056671143\n",
      "Epoch 3/2000, Train Loss: 12.341843811078109, Val Loss: 9.023330660088588, Val MAE: 1.557302474975586\n",
      "Epoch 4/2000, Train Loss: 12.201476250124347, Val Loss: 8.933013160653033, Val MAE: 1.5458109378814697\n",
      "Epoch 5/2000, Train Loss: 12.060988354397692, Val Loss: 8.841924464702606, Val MAE: 1.5343724489212036\n",
      "Epoch 6/2000, Train Loss: 11.91915565305491, Val Loss: 8.748930043992349, Val MAE: 1.5229930877685547\n",
      "Epoch 7/2000, Train Loss: 11.774866108823879, Val Loss: 8.656586596622306, Val MAE: 1.5114465951919556\n",
      "Epoch 8/2000, Train Loss: 11.629562716849835, Val Loss: 8.559662719399242, Val MAE: 1.4995189905166626\n",
      "Epoch 9/2000, Train Loss: 11.478447102728568, Val Loss: 8.46390484278485, Val MAE: 1.4877842664718628\n",
      "Epoch 10/2000, Train Loss: 11.323865523395364, Val Loss: 8.359702920459085, Val MAE: 1.4749088287353516\n",
      "Epoch 11/2000, Train Loss: 11.165661407808953, Val Loss: 8.255079481763355, Val MAE: 1.4621047973632812\n",
      "Epoch 12/2000, Train Loss: 11.000677579561646, Val Loss: 8.146876284429583, Val MAE: 1.4486415386199951\n",
      "Epoch 13/2000, Train Loss: 10.832116985052593, Val Loss: 8.037364714327506, Val MAE: 1.434967041015625\n",
      "Epoch 14/2000, Train Loss: 10.658201253892333, Val Loss: 7.922223310248326, Val MAE: 1.420430064201355\n",
      "Epoch 15/2000, Train Loss: 10.477077866674392, Val Loss: 7.804240745811139, Val MAE: 1.4053903818130493\n",
      "Epoch 16/2000, Train Loss: 10.289884920274599, Val Loss: 7.682093988939867, Val MAE: 1.3894884586334229\n",
      "Epoch 17/2000, Train Loss: 10.097558897368762, Val Loss: 7.553151146537166, Val MAE: 1.3727408647537231\n",
      "Epoch 18/2000, Train Loss: 9.897437680195454, Val Loss: 7.420898333242384, Val MAE: 1.3554292917251587\n",
      "Epoch 19/2000, Train Loss: 9.69471314555738, Val Loss: 7.288484092284057, Val MAE: 1.3382062911987305\n",
      "Epoch 20/2000, Train Loss: 9.490134612008939, Val Loss: 7.1502455933619355, Val MAE: 1.3207088708877563\n",
      "Epoch 21/2000, Train Loss: 9.27946662902832, Val Loss: 7.018701267646531, Val MAE: 1.3044607639312744\n",
      "Epoch 22/2000, Train Loss: 9.067206748516773, Val Loss: 6.8793477470591915, Val MAE: 1.2886507511138916\n",
      "Epoch 23/2000, Train Loss: 8.854240911431416, Val Loss: 6.738577650765241, Val MAE: 1.274394154548645\n",
      "Epoch 24/2000, Train Loss: 8.641055515163135, Val Loss: 6.600485130083763, Val MAE: 1.2608944177627563\n",
      "Epoch 25/2000, Train Loss: 8.433234827229878, Val Loss: 6.4663691023648795, Val MAE: 1.2480851411819458\n",
      "Epoch 26/2000, Train Loss: 8.23295131877306, Val Loss: 6.334559376361006, Val MAE: 1.2351871728897095\n",
      "Epoch 27/2000, Train Loss: 8.034650567717152, Val Loss: 6.209594449754489, Val MAE: 1.222378134727478\n",
      "Epoch 28/2000, Train Loss: 7.845860095363029, Val Loss: 6.084365074109223, Val MAE: 1.2094817161560059\n",
      "Epoch 29/2000, Train Loss: 7.662064682002809, Val Loss: 5.969128088627832, Val MAE: 1.1991084814071655\n",
      "Epoch 30/2000, Train Loss: 7.489381390168246, Val Loss: 5.859307191331508, Val MAE: 1.1924641132354736\n",
      "Epoch 31/2000, Train Loss: 7.32443420455793, Val Loss: 5.757372715917684, Val MAE: 1.1866434812545776\n",
      "Epoch 32/2000, Train Loss: 7.172601460068265, Val Loss: 5.657561197927443, Val MAE: 1.1827723979949951\n",
      "Epoch 33/2000, Train Loss: 7.030807252839617, Val Loss: 5.570262491905083, Val MAE: 1.1830754280090332\n",
      "Epoch 34/2000, Train Loss: 6.897975212247501, Val Loss: 5.488646177518166, Val MAE: 1.1842288970947266\n",
      "Epoch 35/2000, Train Loss: 6.776730399161976, Val Loss: 5.413494751008891, Val MAE: 1.1883565187454224\n",
      "Epoch 36/2000, Train Loss: 6.666643104697515, Val Loss: 5.343276940361928, Val MAE: 1.1932004690170288\n",
      "Epoch 37/2000, Train Loss: 6.567513623680555, Val Loss: 5.28920446977777, Val MAE: 1.1978837251663208\n",
      "Epoch 38/2000, Train Loss: 6.481123815532472, Val Loss: 5.23539263595969, Val MAE: 1.2037806510925293\n",
      "Epoch 39/2000, Train Loss: 6.4007512581843375, Val Loss: 5.189763848660356, Val MAE: 1.2096701860427856\n",
      "Epoch 40/2000, Train Loss: 6.330981135452238, Val Loss: 5.1478802519329525, Val MAE: 1.2167086601257324\n",
      "Epoch 41/2000, Train Loss: 6.268904265176235, Val Loss: 5.113919122340316, Val MAE: 1.2231158018112183\n",
      "Epoch 42/2000, Train Loss: 6.213942333814714, Val Loss: 5.084931552208076, Val MAE: 1.2298533916473389\n",
      "Epoch 43/2000, Train Loss: 6.165829353144604, Val Loss: 5.0594926680548715, Val MAE: 1.2366575002670288\n",
      "Epoch 44/2000, Train Loss: 6.124011657172572, Val Loss: 5.038923524597944, Val MAE: 1.2433216571807861\n",
      "Epoch 45/2000, Train Loss: 6.086191884067007, Val Loss: 5.018056213047545, Val MAE: 1.2513885498046875\n",
      "Epoch 46/2000, Train Loss: 6.052872287983797, Val Loss: 5.002575412847228, Val MAE: 1.257747769355774\n",
      "Epoch 47/2000, Train Loss: 6.023144727353896, Val Loss: 4.989157654471317, Val MAE: 1.2653824090957642\n",
      "Epoch 48/2000, Train Loss: 5.997937629291661, Val Loss: 4.978466888201439, Val MAE: 1.272121787071228\n",
      "Epoch 49/2000, Train Loss: 5.975672055767255, Val Loss: 4.969633808378446, Val MAE: 1.2787556648254395\n",
      "Epoch 50/2000, Train Loss: 5.955267084307607, Val Loss: 4.963546555729236, Val MAE: 1.2838751077651978\n",
      "Epoch 51/2000, Train Loss: 5.938967160487326, Val Loss: 4.9575477579892695, Val MAE: 1.2897740602493286\n",
      "Epoch 52/2000, Train Loss: 5.923866928672388, Val Loss: 4.952915094666562, Val MAE: 1.2952102422714233\n",
      "Epoch 53/2000, Train Loss: 5.90952361796122, Val Loss: 4.950260102546821, Val MAE: 1.2981981039047241\n",
      "Epoch 54/2000, Train Loss: 5.897053332499934, Val Loss: 4.946752479924994, Val MAE: 1.3034816980361938\n",
      "Epoch 55/2000, Train Loss: 5.886503166920865, Val Loss: 4.9446180872998, Val MAE: 1.3071107864379883\n",
      "Epoch 56/2000, Train Loss: 5.875951836094061, Val Loss: 4.943221258915077, Val MAE: 1.30928373336792\n",
      "Epoch 57/2000, Train Loss: 5.866506308086484, Val Loss: 4.9412010948536755, Val MAE: 1.31562340259552\n",
      "Epoch 58/2000, Train Loss: 5.85751911521713, Val Loss: 4.939990729194577, Val MAE: 1.32082998752594\n",
      "Epoch 59/2000, Train Loss: 5.849245710325945, Val Loss: 4.940043008125435, Val MAE: 1.3224847316741943\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 60/2000, Train Loss: 5.84223714913389, Val Loss: 4.939372559321129, Val MAE: 1.3257662057876587\n",
      "Epoch 61/2000, Train Loss: 5.835764980249049, Val Loss: 4.938924071142229, Val MAE: 1.3271667957305908\n",
      "Epoch 62/2000, Train Loss: 5.829045861477083, Val Loss: 4.9382806553679, Val MAE: 1.3303241729736328\n",
      "Epoch 63/2000, Train Loss: 5.82282739923841, Val Loss: 4.937375338198775, Val MAE: 1.3337726593017578\n",
      "Epoch 64/2000, Train Loss: 5.817140002052688, Val Loss: 4.93696022276151, Val MAE: 1.3355510234832764\n",
      "Epoch 65/2000, Train Loss: 5.812210805142622, Val Loss: 4.9378030401165205, Val MAE: 1.3391510248184204\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 66/2000, Train Loss: 5.806845468672793, Val Loss: 4.937075279526791, Val MAE: 1.3392081260681152\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 67/2000, Train Loss: 5.802113437887483, Val Loss: 4.937025392661661, Val MAE: 1.3442922830581665\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 68/2000, Train Loss: 5.798062402039326, Val Loss: 4.9364795284756156, Val MAE: 1.3420851230621338\n",
      "Epoch 69/2000, Train Loss: 5.792654911285551, Val Loss: 4.936241258605052, Val MAE: 1.3439385890960693\n",
      "Epoch 70/2000, Train Loss: 5.7892322140961445, Val Loss: 4.936399100796651, Val MAE: 1.3466377258300781\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 71/2000, Train Loss: 5.784402326829496, Val Loss: 4.935808137311774, Val MAE: 1.3455208539962769\n",
      "Epoch 72/2000, Train Loss: 5.780715578295327, Val Loss: 4.9354772014133, Val MAE: 1.347191333770752\n",
      "Epoch 73/2000, Train Loss: 5.776738175533759, Val Loss: 4.935838599528297, Val MAE: 1.3476438522338867\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 74/2000, Train Loss: 5.773542830341723, Val Loss: 4.935896482710111, Val MAE: 1.3517568111419678\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 75/2000, Train Loss: 5.768844262216395, Val Loss: 4.9355789530075205, Val MAE: 1.3510996103286743\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 76/2000, Train Loss: 5.7648137535199915, Val Loss: 4.9345651723570745, Val MAE: 1.3547202348709106\n",
      "Epoch 77/2000, Train Loss: 5.761600964846198, Val Loss: 4.934468766390267, Val MAE: 1.3552539348602295\n",
      "Epoch 78/2000, Train Loss: 5.758259312859897, Val Loss: 4.934581087807477, Val MAE: 1.3532254695892334\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 79/2000, Train Loss: 5.754532825435072, Val Loss: 4.934405413118459, Val MAE: 1.353413701057434\n",
      "Epoch 80/2000, Train Loss: 5.751535423199615, Val Loss: 4.933479124408657, Val MAE: 1.3531322479248047\n",
      "Epoch 81/2000, Train Loss: 5.7482395554662675, Val Loss: 4.932977531319958, Val MAE: 1.3543063402175903\n",
      "Epoch 82/2000, Train Loss: 5.745625351282344, Val Loss: 4.932268614890211, Val MAE: 1.3531078100204468\n",
      "Epoch 83/2000, Train Loss: 5.741652122943188, Val Loss: 4.933350344633652, Val MAE: 1.3554290533065796\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 84/2000, Train Loss: 5.7385888445301845, Val Loss: 4.933278528916634, Val MAE: 1.3552919626235962\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 85/2000, Train Loss: 5.73515184054821, Val Loss: 4.932633594739235, Val MAE: 1.3590221405029297\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 86/2000, Train Loss: 5.731851956946677, Val Loss: 4.932255826966237, Val MAE: 1.3558735847473145\n",
      "Epoch 87/2000, Train Loss: 5.729048256132487, Val Loss: 4.931319088855032, Val MAE: 1.355338215827942\n",
      "Epoch 88/2000, Train Loss: 5.725549029767052, Val Loss: 4.931561244140237, Val MAE: 1.356076955795288\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 89/2000, Train Loss: 5.7226397244549405, Val Loss: 4.931368492417416, Val MAE: 1.3566629886627197\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 90/2000, Train Loss: 5.719842548021374, Val Loss: 4.930681798619739, Val MAE: 1.3559565544128418\n",
      "Epoch 91/2000, Train Loss: 5.717112060670363, Val Loss: 4.930219988701707, Val MAE: 1.3605413436889648\n",
      "Epoch 92/2000, Train Loss: 5.713957197979921, Val Loss: 4.93076196221982, Val MAE: 1.353737473487854\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 93/2000, Train Loss: 5.71171682674561, Val Loss: 4.930074123204765, Val MAE: 1.3594473600387573\n",
      "Epoch 94/2000, Train Loss: 5.708082973579551, Val Loss: 4.9294297093051975, Val MAE: 1.3561807870864868\n",
      "Epoch 95/2000, Train Loss: 5.705849667320278, Val Loss: 4.9295174762354055, Val MAE: 1.3572171926498413\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 96/2000, Train Loss: 5.703383502785711, Val Loss: 4.929032256643651, Val MAE: 1.3558189868927002\n",
      "Epoch 97/2000, Train Loss: 5.70054507540784, Val Loss: 4.929154937550173, Val MAE: 1.3569822311401367\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 98/2000, Train Loss: 5.697749535186789, Val Loss: 4.928068273350344, Val MAE: 1.3574835062026978\n",
      "Epoch 99/2000, Train Loss: 5.6950424304736655, Val Loss: 4.929321063575098, Val MAE: 1.3621854782104492\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 100/2000, Train Loss: 5.692574412113005, Val Loss: 4.928778419252169, Val MAE: 1.3582961559295654\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 101/2000, Train Loss: 5.6897665301651115, Val Loss: 4.928897384465751, Val MAE: 1.3612972497940063\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 102/2000, Train Loss: 5.68842710062788, Val Loss: 4.929934443457652, Val MAE: 1.3622143268585205\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 103/2000, Train Loss: 5.685492862705443, Val Loss: 4.928641882185208, Val MAE: 1.3616198301315308\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 104/2000, Train Loss: 5.683008940600745, Val Loss: 4.9289830593739525, Val MAE: 1.36297607421875\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 105/2000, Train Loss: 5.680704115814596, Val Loss: 4.9292077707032025, Val MAE: 1.3609638214111328\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 106/2000, Train Loss: 5.678000385202546, Val Loss: 4.928418253639997, Val MAE: 1.3611007928848267\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 107/2000, Train Loss: 5.675867617256117, Val Loss: 4.927795154765501, Val MAE: 1.3580595254898071\n",
      "Epoch 108/2000, Train Loss: 5.673777283118864, Val Loss: 4.928176918676344, Val MAE: 1.3603235483169556\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 109/2000, Train Loss: 5.671532448373321, Val Loss: 4.92774350602748, Val MAE: 1.3606432676315308\n",
      "Epoch 110/2000, Train Loss: 5.669682308119171, Val Loss: 4.928608669264842, Val MAE: 1.3637633323669434\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 111/2000, Train Loss: 5.6673093311892355, Val Loss: 4.928433873289722, Val MAE: 1.36679208278656\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 112/2000, Train Loss: 5.665144491833251, Val Loss: 4.928412350153519, Val MAE: 1.360693097114563\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 113/2000, Train Loss: 5.663385134864072, Val Loss: 4.927538226620626, Val MAE: 1.3633030652999878\n",
      "Epoch 114/2000, Train Loss: 5.6619323341885055, Val Loss: 4.927727565927021, Val MAE: 1.359253168106079\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 115/2000, Train Loss: 5.659408882417618, Val Loss: 4.928695936526283, Val MAE: 1.363756537437439\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 116/2000, Train Loss: 5.657843611258843, Val Loss: 4.927963539705438, Val MAE: 1.3601646423339844\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 117/2000, Train Loss: 5.656256787463194, Val Loss: 4.92814733537577, Val MAE: 1.362148404121399\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 118/2000, Train Loss: 5.653782973064661, Val Loss: 4.928708409656912, Val MAE: 1.3611539602279663\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 119/2000, Train Loss: 5.651961555454112, Val Loss: 4.928686389478586, Val MAE: 1.3643176555633545\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 120/2000, Train Loss: 5.650482389811141, Val Loss: 4.929676650540303, Val MAE: 1.3650994300842285\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 121/2000, Train Loss: 5.648472980577118, Val Loss: 4.930520990339376, Val MAE: 1.3645750284194946\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 122/2000, Train Loss: 5.646459557991645, Val Loss: 4.930315257331072, Val MAE: 1.364970326423645\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 123/2000, Train Loss: 5.645083974063774, Val Loss: 4.931104592145499, Val MAE: 1.3653043508529663\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 124/2000, Train Loss: 5.643893458656993, Val Loss: 4.931634541891389, Val MAE: 1.360509991645813\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 125/2000, Train Loss: 5.642060791917622, Val Loss: 4.93084071046215, Val MAE: 1.3642420768737793\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 126/2000, Train Loss: 5.640408713913232, Val Loss: 4.93071208646742, Val MAE: 1.3611000776290894\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 127/2000, Train Loss: 5.638625999976849, Val Loss: 4.931784836114463, Val MAE: 1.3671131134033203\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 128/2000, Train Loss: 5.637363603297964, Val Loss: 4.931051516532898, Val MAE: 1.3639782667160034\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 129/2000, Train Loss: 5.6365927878103985, Val Loss: 4.932881467625246, Val MAE: 1.366806149482727\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 130/2000, Train Loss: 5.634588897941316, Val Loss: 4.932267808510085, Val MAE: 1.3691428899765015\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 131/2000, Train Loss: 5.633035653414313, Val Loss: 4.933118712902069, Val MAE: 1.3677207231521606\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 132/2000, Train Loss: 5.6312630931564325, Val Loss: 4.933751867989362, Val MAE: 1.3686093091964722\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 133/2000, Train Loss: 5.630062000672638, Val Loss: 4.933788814383038, Val MAE: 1.368293046951294\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 3.5122549533843994\n",
      "Test Mean Absolute Error (MAE): 1.1559463560717775\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(rnnModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (rnn): RNN(10, 64, batch_first=True, bidirectional=True)\n",
       "   (attention): TemporalAttention(\n",
       "     (W): Linear(in_features=128, out_features=64, bias=False)\n",
       "     (v): Linear(in_features=64, out_features=1, bias=False)\n",
       "   )\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=129, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'train_mse': 5.628432273864746,\n",
       "  'train_mae': 1.6680772253112273,\n",
       "  'val_mse': 4.933788776397705,\n",
       "  'val_mae': 1.3682930790961294,\n",
       "  'test_mse': 3.5122549533843994,\n",
       "  'test_mae': 1.1559463560717775,\n",
       "  'spear_corr': 0.690968143143042})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 444\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_rnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=6,\n",
    "                num_filters=64,\n",
    "                num_dense=64,\n",
    "                bidirectional=True,\n",
    "                temporal_attention=True,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = ['total_points', 'ict_index', 'clean_sheets', 'goals_conceded', 'bps', 'matchup_difficulty', 'goals_scored', 'assists', 'yellow_cards', 'red_cards'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.00001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Running GridSearch Experiment ========\n",
      "===== Total Number of Iterations:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 444, 'position': 'DEF', 'window_size': 3, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 444\n",
      "position DEF\n",
      "window_size 3\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  0\n",
      "======= Generating CNN Data for Season: ['2020-21'], Position: DEF =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type DEF = 245.\n",
      "65 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (6124, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (6662, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (6124, 7)\n",
      "Shape of a given window (prior to preprocessing): (3, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.53270791e+00 4.18286004e+01 2.07910751e-02 2.81440162e-02\n",
      " 1.34888438e-01 7.88640974e+00 7.07403651e-02 4.56389452e-03\n",
      " 5.07099391e-03 0.00000000e+00]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.6527274  43.61212885  0.14445036  0.17140691  0.34160437  9.94942984\n",
      "  0.25639065  0.06740227  0.07103013  1.        ]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building rnn Architecture ======\n",
      "====== Done Building rnn Architecture ======\n",
      "Epoch 1/2000, Train Loss: 9.087921791160824, Val Loss: 9.334738846619924, Val MAE: 1.5535359382629395\n",
      "Epoch 2/2000, Train Loss: 8.905077113673888, Val Loss: 9.16857551508225, Val MAE: 1.5383179187774658\n",
      "Epoch 3/2000, Train Loss: 8.729940387601689, Val Loss: 9.007825231361084, Val MAE: 1.5241814851760864\n",
      "Epoch 4/2000, Train Loss: 8.558454511971744, Val Loss: 8.847492942099388, Val MAE: 1.5097397565841675\n",
      "Epoch 5/2000, Train Loss: 8.386191701073708, Val Loss: 8.689911785339698, Val MAE: 1.4953409433364868\n",
      "Epoch 6/2000, Train Loss: 8.21516094507595, Val Loss: 8.52767556473995, Val MAE: 1.4800755977630615\n",
      "Epoch 7/2000, Train Loss: 8.045296425196064, Val Loss: 8.36928427823079, Val MAE: 1.4648218154907227\n",
      "Epoch 8/2000, Train Loss: 7.875608109665127, Val Loss: 8.208580947189759, Val MAE: 1.4487814903259277\n",
      "Epoch 9/2000, Train Loss: 7.706039687397726, Val Loss: 8.047935355435579, Val MAE: 1.432932734489441\n",
      "Epoch 10/2000, Train Loss: 7.538369485556718, Val Loss: 7.889583136829046, Val MAE: 1.4185000658035278\n",
      "Epoch 11/2000, Train Loss: 7.372630349194484, Val Loss: 7.735425474666632, Val MAE: 1.4065760374069214\n",
      "Epoch 12/2000, Train Loss: 7.2094016272391785, Val Loss: 7.581415740419657, Val MAE: 1.3970553874969482\n",
      "Epoch 13/2000, Train Loss: 7.051036251623592, Val Loss: 7.4328244411792515, Val MAE: 1.3908575773239136\n",
      "Epoch 14/2000, Train Loss: 6.898931932528244, Val Loss: 7.291811485473926, Val MAE: 1.3874239921569824\n",
      "Epoch 15/2000, Train Loss: 6.755759277727744, Val Loss: 7.157087327196048, Val MAE: 1.3867902755737305\n",
      "Epoch 16/2000, Train Loss: 6.621755682034711, Val Loss: 7.034500217208495, Val MAE: 1.3879122734069824\n",
      "Epoch 17/2000, Train Loss: 6.493691523299978, Val Loss: 6.9168905349114, Val MAE: 1.3901656866073608\n",
      "Epoch 18/2000, Train Loss: 6.381453566438114, Val Loss: 6.813405789167453, Val MAE: 1.3948200941085815\n",
      "Epoch 19/2000, Train Loss: 6.279807229102546, Val Loss: 6.720034156701503, Val MAE: 1.4018445014953613\n",
      "Epoch 20/2000, Train Loss: 6.187795860492519, Val Loss: 6.634132955624507, Val MAE: 1.4116792678833008\n",
      "Epoch 21/2000, Train Loss: 6.1068073885255405, Val Loss: 6.5609942501936205, Val MAE: 1.4242128133773804\n",
      "Epoch 22/2000, Train Loss: 6.036415196622332, Val Loss: 6.496277867677884, Val MAE: 1.4385325908660889\n",
      "Epoch 23/2000, Train Loss: 5.976260504930373, Val Loss: 6.441521919537813, Val MAE: 1.4532740116119385\n",
      "Epoch 24/2000, Train Loss: 5.924798326734279, Val Loss: 6.395153658359479, Val MAE: 1.4674172401428223\n",
      "Epoch 25/2000, Train Loss: 5.881579844647853, Val Loss: 6.356365588689462, Val MAE: 1.4812625646591187\n",
      "Epoch 26/2000, Train Loss: 5.843995749260007, Val Loss: 6.322381740655654, Val MAE: 1.4957081079483032\n",
      "Epoch 27/2000, Train Loss: 5.815123814742209, Val Loss: 6.294689365839347, Val MAE: 1.508766531944275\n",
      "Epoch 28/2000, Train Loss: 5.79111927356499, Val Loss: 6.271918197778555, Val MAE: 1.520950436592102\n",
      "Epoch 29/2000, Train Loss: 5.772121166696354, Val Loss: 6.25461234435057, Val MAE: 1.5318841934204102\n",
      "Epoch 30/2000, Train Loss: 5.7566230950668515, Val Loss: 6.24077037022664, Val MAE: 1.5408076047897339\n",
      "Epoch 31/2000, Train Loss: 5.743547939294225, Val Loss: 6.227097688577114, Val MAE: 1.5508157014846802\n",
      "Epoch 32/2000, Train Loss: 5.733434174248874, Val Loss: 6.218003217684917, Val MAE: 1.5577996969223022\n",
      "Epoch 33/2000, Train Loss: 5.72534590660375, Val Loss: 6.2101551010058476, Val MAE: 1.5644495487213135\n",
      "Epoch 34/2000, Train Loss: 5.718960985056419, Val Loss: 6.204196816835648, Val MAE: 1.5686582326889038\n",
      "Epoch 35/2000, Train Loss: 5.713390535500848, Val Loss: 6.197590680611439, Val MAE: 1.575285792350769\n",
      "Epoch 36/2000, Train Loss: 5.708961461304435, Val Loss: 6.1929177617415405, Val MAE: 1.5787428617477417\n",
      "Epoch 37/2000, Train Loss: 5.705009674210609, Val Loss: 6.1891052276660234, Val MAE: 1.5811973810195923\n",
      "Epoch 38/2000, Train Loss: 5.7016388975449575, Val Loss: 6.185798743443612, Val MAE: 1.5832875967025757\n",
      "Epoch 39/2000, Train Loss: 5.698466501114977, Val Loss: 6.182392153678796, Val MAE: 1.5851384401321411\n",
      "Epoch 40/2000, Train Loss: 5.695638992677258, Val Loss: 6.179437395853874, Val MAE: 1.5868101119995117\n",
      "Epoch 41/2000, Train Loss: 5.6930187580931335, Val Loss: 6.177166083531502, Val MAE: 1.5863622426986694\n",
      "Epoch 42/2000, Train Loss: 5.690669543428837, Val Loss: 6.17421172887851, Val MAE: 1.5877254009246826\n",
      "Epoch 43/2000, Train Loss: 5.688121960824587, Val Loss: 6.171602937197074, Val MAE: 1.589048981666565\n",
      "Epoch 44/2000, Train Loss: 5.686133738584566, Val Loss: 6.169474221192873, Val MAE: 1.5893605947494507\n",
      "Epoch 45/2000, Train Loss: 5.683998305956698, Val Loss: 6.167194153712346, Val MAE: 1.5891532897949219\n",
      "Epoch 46/2000, Train Loss: 5.68176549623766, Val Loss: 6.165194860788492, Val MAE: 1.5898600816726685\n",
      "Epoch 47/2000, Train Loss: 5.679930141928179, Val Loss: 6.163281294627067, Val MAE: 1.5891629457473755\n",
      "Epoch 48/2000, Train Loss: 5.678512663454623, Val Loss: 6.16141046835826, Val MAE: 1.5894806385040283\n",
      "Epoch 49/2000, Train Loss: 5.676233303461427, Val Loss: 6.159967675575843, Val MAE: 1.5881316661834717\n",
      "Epoch 50/2000, Train Loss: 5.674399710464267, Val Loss: 6.157105386257172, Val MAE: 1.5905054807662964\n",
      "Epoch 51/2000, Train Loss: 5.672932755664378, Val Loss: 6.1553053614420765, Val MAE: 1.5906568765640259\n",
      "Epoch 52/2000, Train Loss: 5.6710162175931265, Val Loss: 6.153499239530319, Val MAE: 1.5903366804122925\n",
      "Epoch 53/2000, Train Loss: 5.669457816709422, Val Loss: 6.152433512761043, Val MAE: 1.5882484912872314\n",
      "Epoch 54/2000, Train Loss: 5.6678157168334184, Val Loss: 6.1502997942459885, Val MAE: 1.589802861213684\n",
      "Epoch 55/2000, Train Loss: 5.666025208558946, Val Loss: 6.1489417540721405, Val MAE: 1.5884089469909668\n",
      "Epoch 56/2000, Train Loss: 5.664516911083207, Val Loss: 6.147151744365692, Val MAE: 1.5888575315475464\n",
      "Epoch 57/2000, Train Loss: 5.663228700782999, Val Loss: 6.145489210348863, Val MAE: 1.5882139205932617\n",
      "Epoch 58/2000, Train Loss: 5.661853250849661, Val Loss: 6.143990054497352, Val MAE: 1.5876764059066772\n",
      "Epoch 59/2000, Train Loss: 5.660613055263378, Val Loss: 6.1430120596518885, Val MAE: 1.5849164724349976\n",
      "Epoch 60/2000, Train Loss: 5.65889451058938, Val Loss: 6.140787632648761, Val MAE: 1.5880202054977417\n",
      "Epoch 61/2000, Train Loss: 5.657228626208087, Val Loss: 6.139219560684302, Val MAE: 1.588258981704712\n",
      "Epoch 62/2000, Train Loss: 5.655724071233363, Val Loss: 6.137923058485374, Val MAE: 1.5863193273544312\n",
      "Epoch 63/2000, Train Loss: 5.654708912802611, Val Loss: 6.136441462162214, Val MAE: 1.5870920419692993\n",
      "Epoch 64/2000, Train Loss: 5.653066332670318, Val Loss: 6.135263652679248, Val MAE: 1.5860481262207031\n",
      "Epoch 65/2000, Train Loss: 5.651812329321229, Val Loss: 6.134050940855955, Val MAE: 1.586057424545288\n",
      "Epoch 66/2000, Train Loss: 5.650610795944222, Val Loss: 6.132310110177749, Val MAE: 1.5859460830688477\n",
      "Epoch 67/2000, Train Loss: 5.649197947301722, Val Loss: 6.131111970008948, Val MAE: 1.5861475467681885\n",
      "Epoch 68/2000, Train Loss: 5.648030692414559, Val Loss: 6.130081632198432, Val MAE: 1.5843430757522583\n",
      "Epoch 69/2000, Train Loss: 5.646887988986327, Val Loss: 6.129342090472197, Val MAE: 1.5847982168197632\n",
      "Epoch 70/2000, Train Loss: 5.6457705505632, Val Loss: 6.127899720424261, Val MAE: 1.5834128856658936\n",
      "Epoch 71/2000, Train Loss: 5.644256862697191, Val Loss: 6.126597382166447, Val MAE: 1.5836610794067383\n",
      "Epoch 72/2000, Train Loss: 5.643442823291154, Val Loss: 6.125533475019993, Val MAE: 1.5835239887237549\n",
      "Epoch 73/2000, Train Loss: 5.6426646942206, Val Loss: 6.1246850478343475, Val MAE: 1.582582712173462\n",
      "Epoch 74/2000, Train Loss: 5.640925710134196, Val Loss: 6.123426396724505, Val MAE: 1.583209753036499\n",
      "Epoch 75/2000, Train Loss: 5.64041400153007, Val Loss: 6.122881485560002, Val MAE: 1.5814528465270996\n",
      "Epoch 76/2000, Train Loss: 5.638893736572602, Val Loss: 6.121657317112653, Val MAE: 1.5825868844985962\n",
      "Epoch 77/2000, Train Loss: 5.638046616424333, Val Loss: 6.1202001376029775, Val MAE: 1.582707166671753\n",
      "Epoch 78/2000, Train Loss: 5.636681561402964, Val Loss: 6.119531871722295, Val MAE: 1.5825390815734863\n",
      "Epoch 79/2000, Train Loss: 5.635548044815947, Val Loss: 6.118736896759424, Val MAE: 1.5816634893417358\n",
      "Epoch 80/2000, Train Loss: 5.634818456610855, Val Loss: 6.11765531087533, Val MAE: 1.5814307928085327\n",
      "Epoch 81/2000, Train Loss: 5.6341128583383115, Val Loss: 6.116435524133536, Val MAE: 1.581351399421692\n",
      "Epoch 82/2000, Train Loss: 5.63307456186445, Val Loss: 6.116269912475195, Val MAE: 1.5809481143951416\n",
      "Epoch 83/2000, Train Loss: 5.631695150868773, Val Loss: 6.11521241512054, Val MAE: 1.5804563760757446\n",
      "Epoch 84/2000, Train Loss: 5.6308851013246715, Val Loss: 6.113684608080448, Val MAE: 1.5806446075439453\n",
      "Epoch 85/2000, Train Loss: 5.630006344096146, Val Loss: 6.112980637794886, Val MAE: 1.5807949304580688\n",
      "Epoch 86/2000, Train Loss: 5.628991903814241, Val Loss: 6.112341046638978, Val MAE: 1.579230546951294\n",
      "Epoch 87/2000, Train Loss: 5.62841846426087, Val Loss: 6.111926415944711, Val MAE: 1.577643632888794\n",
      "Epoch 88/2000, Train Loss: 5.627337695614646, Val Loss: 6.110741600317833, Val MAE: 1.5790880918502808\n",
      "Epoch 89/2000, Train Loss: 5.626702022105373, Val Loss: 6.110025080045064, Val MAE: 1.5785259008407593\n",
      "Epoch 90/2000, Train Loss: 5.625558765830089, Val Loss: 6.109363920566363, Val MAE: 1.5790196657180786\n",
      "Epoch 91/2000, Train Loss: 5.624805173752916, Val Loss: 6.10857474834491, Val MAE: 1.5789506435394287\n",
      "Epoch 92/2000, Train Loss: 5.624177036927144, Val Loss: 6.108081671213492, Val MAE: 1.5782802104949951\n",
      "Epoch 93/2000, Train Loss: 5.623005096648059, Val Loss: 6.107217820179768, Val MAE: 1.5784138441085815\n",
      "Epoch 94/2000, Train Loss: 5.622319436112755, Val Loss: 6.106518382292528, Val MAE: 1.5780158042907715\n",
      "Epoch 95/2000, Train Loss: 5.6218797111143015, Val Loss: 6.105713936610099, Val MAE: 1.5782514810562134\n",
      "Epoch 96/2000, Train Loss: 5.62074597474954, Val Loss: 6.105129002913451, Val MAE: 1.5780564546585083\n",
      "Epoch 97/2000, Train Loss: 5.620007814456269, Val Loss: 6.104267208087139, Val MAE: 1.5785958766937256\n",
      "Epoch 98/2000, Train Loss: 5.619663131743324, Val Loss: 6.103672922268892, Val MAE: 1.5783284902572632\n",
      "Epoch 99/2000, Train Loss: 5.619060939540537, Val Loss: 6.102857512999804, Val MAE: 1.5765440464019775\n",
      "Epoch 100/2000, Train Loss: 5.617984789087177, Val Loss: 6.102443838119507, Val MAE: 1.5783005952835083\n",
      "Epoch 101/2000, Train Loss: 5.617453925989638, Val Loss: 6.102013347699092, Val MAE: 1.5763885974884033\n",
      "Epoch 102/2000, Train Loss: 5.61630695930056, Val Loss: 6.100910464311258, Val MAE: 1.5775227546691895\n",
      "Epoch 103/2000, Train Loss: 5.615617977080263, Val Loss: 6.1005555427991425, Val MAE: 1.5776700973510742\n",
      "Epoch 104/2000, Train Loss: 5.614747155784443, Val Loss: 6.099969749572949, Val MAE: 1.576997995376587\n",
      "Epoch 105/2000, Train Loss: 5.6142634109311915, Val Loss: 6.099599863015689, Val MAE: 1.5768013000488281\n",
      "Epoch 106/2000, Train Loss: 5.613648833718276, Val Loss: 6.09883206991049, Val MAE: 1.577102780342102\n",
      "Epoch 107/2000, Train Loss: 5.613000448145922, Val Loss: 6.098142386705447, Val MAE: 1.5768228769302368\n",
      "Epoch 108/2000, Train Loss: 5.612079437306357, Val Loss: 6.09744906486609, Val MAE: 1.5780495405197144\n",
      "Epoch 109/2000, Train Loss: 5.61166830557325, Val Loss: 6.097323041695815, Val MAE: 1.5761462450027466\n",
      "Epoch 110/2000, Train Loss: 5.611203638677497, Val Loss: 6.097061989246271, Val MAE: 1.5755293369293213\n",
      "Epoch 111/2000, Train Loss: 5.610742393805056, Val Loss: 6.095933691354898, Val MAE: 1.5771551132202148\n",
      "Epoch 112/2000, Train Loss: 5.609938389809172, Val Loss: 6.096120516459147, Val MAE: 1.5753177404403687\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 113/2000, Train Loss: 5.609494839856063, Val Loss: 6.094976064792046, Val MAE: 1.5765897035598755\n",
      "Epoch 114/2000, Train Loss: 5.608402292963818, Val Loss: 6.095029466580122, Val MAE: 1.5749547481536865\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 115/2000, Train Loss: 5.607618729054304, Val Loss: 6.0946783047456, Val MAE: 1.5744614601135254\n",
      "Epoch 116/2000, Train Loss: 5.607271293086764, Val Loss: 6.093935272021171, Val MAE: 1.5756447315216064\n",
      "Epoch 117/2000, Train Loss: 5.606847442156428, Val Loss: 6.093534928407425, Val MAE: 1.5755475759506226\n",
      "Epoch 118/2000, Train Loss: 5.606098829417747, Val Loss: 6.09303546379774, Val MAE: 1.5751638412475586\n",
      "Epoch 119/2000, Train Loss: 5.605718841752622, Val Loss: 6.092869104483189, Val MAE: 1.5733517408370972\n",
      "Epoch 120/2000, Train Loss: 5.605065190588251, Val Loss: 6.092656559516222, Val MAE: 1.5732618570327759\n",
      "Epoch 121/2000, Train Loss: 5.604557366116035, Val Loss: 6.091691691753192, Val MAE: 1.57498300075531\n",
      "Epoch 122/2000, Train Loss: 5.603695945587201, Val Loss: 6.091283525870397, Val MAE: 1.5755709409713745\n",
      "Epoch 123/2000, Train Loss: 5.603431544117157, Val Loss: 6.090983218107468, Val MAE: 1.5762690305709839\n",
      "Epoch 124/2000, Train Loss: 5.602832915831585, Val Loss: 6.090638517416441, Val MAE: 1.57447350025177\n",
      "Epoch 125/2000, Train Loss: 5.602355922708443, Val Loss: 6.090822793581547, Val MAE: 1.573989987373352\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 126/2000, Train Loss: 5.601661305498564, Val Loss: 6.08994367367182, Val MAE: 1.573511004447937\n",
      "Epoch 127/2000, Train Loss: 5.601136326987113, Val Loss: 6.089785041870215, Val MAE: 1.57453453540802\n",
      "Epoch 128/2000, Train Loss: 5.6005774712075995, Val Loss: 6.089158209164937, Val MAE: 1.5739201307296753\n",
      "Epoch 129/2000, Train Loss: 5.600084590438447, Val Loss: 6.08877659088526, Val MAE: 1.574301838874817\n",
      "Epoch 130/2000, Train Loss: 5.600153100602349, Val Loss: 6.088240337983156, Val MAE: 1.572871446609497\n",
      "Epoch 131/2000, Train Loss: 5.598937166703714, Val Loss: 6.088194229663947, Val MAE: 1.5742067098617554\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 132/2000, Train Loss: 5.598726318478255, Val Loss: 6.0879094016857636, Val MAE: 1.5728800296783447\n",
      "Epoch 133/2000, Train Loss: 5.598244519767614, Val Loss: 6.0873441919302325, Val MAE: 1.5732847452163696\n",
      "Epoch 134/2000, Train Loss: 5.597586577331829, Val Loss: 6.087193523614834, Val MAE: 1.5739431381225586\n",
      "Epoch 135/2000, Train Loss: 5.5973518916538785, Val Loss: 6.086751281298124, Val MAE: 1.5730634927749634\n",
      "Epoch 136/2000, Train Loss: 5.596741927636637, Val Loss: 6.0861302702854845, Val MAE: 1.574237585067749\n",
      "Epoch 137/2000, Train Loss: 5.596357116499331, Val Loss: 6.0862093152144014, Val MAE: 1.573665976524353\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 138/2000, Train Loss: 5.595812114246367, Val Loss: 6.085751241904038, Val MAE: 1.5733015537261963\n",
      "Epoch 139/2000, Train Loss: 5.595556105820434, Val Loss: 6.0853421529134115, Val MAE: 1.5721782445907593\n",
      "Epoch 140/2000, Train Loss: 5.5948295330251705, Val Loss: 6.085133960613837, Val MAE: 1.5726706981658936\n",
      "Epoch 141/2000, Train Loss: 5.59420970047763, Val Loss: 6.085090159452879, Val MAE: 1.5726101398468018\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 142/2000, Train Loss: 5.594014544547492, Val Loss: 6.0845194932741995, Val MAE: 1.5729329586029053\n",
      "Epoch 143/2000, Train Loss: 5.59373977325006, Val Loss: 6.084670112377558, Val MAE: 1.5722147226333618\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 144/2000, Train Loss: 5.593348769147424, Val Loss: 6.0845346606694735, Val MAE: 1.5717889070510864\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 145/2000, Train Loss: 5.592799807475932, Val Loss: 6.083817350558745, Val MAE: 1.5725057125091553\n",
      "Epoch 146/2000, Train Loss: 5.592042356976079, Val Loss: 6.083972774102137, Val MAE: 1.5709482431411743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 147/2000, Train Loss: 5.591705053995409, Val Loss: 6.083320527810317, Val MAE: 1.5722229480743408\n",
      "Epoch 148/2000, Train Loss: 5.591272212495083, Val Loss: 6.082610919536688, Val MAE: 1.5740762948989868\n",
      "Epoch 149/2000, Train Loss: 5.590976786363605, Val Loss: 6.0825926102124726, Val MAE: 1.5735788345336914\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 150/2000, Train Loss: 5.590270024797762, Val Loss: 6.082030044763516, Val MAE: 1.5737955570220947\n",
      "Epoch 151/2000, Train Loss: 5.590339967182705, Val Loss: 6.0819914995095665, Val MAE: 1.5739045143127441\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 152/2000, Train Loss: 5.589534545168322, Val Loss: 6.081715157704475, Val MAE: 1.5729819536209106\n",
      "Epoch 153/2000, Train Loss: 5.589644093836991, Val Loss: 6.081882134767679, Val MAE: 1.5712746381759644\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 154/2000, Train Loss: 5.5889012810938015, Val Loss: 6.081550406798338, Val MAE: 1.571733832359314\n",
      "Epoch 155/2000, Train Loss: 5.588429586438927, Val Loss: 6.080842861762414, Val MAE: 1.5724807977676392\n",
      "Epoch 156/2000, Train Loss: 5.588216573838299, Val Loss: 6.080726493016267, Val MAE: 1.571531057357788\n",
      "Epoch 157/2000, Train Loss: 5.587736889859687, Val Loss: 6.080850563293848, Val MAE: 1.5712707042694092\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 158/2000, Train Loss: 5.587295693188692, Val Loss: 6.080500021653298, Val MAE: 1.5715703964233398\n",
      "Epoch 159/2000, Train Loss: 5.587053940957643, Val Loss: 6.080286114032452, Val MAE: 1.5716781616210938\n",
      "Epoch 160/2000, Train Loss: 5.586667631518032, Val Loss: 6.08047841328841, Val MAE: 1.5703593492507935\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 161/2000, Train Loss: 5.586234600979258, Val Loss: 6.080469323732914, Val MAE: 1.5703824758529663\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 162/2000, Train Loss: 5.585941557324005, Val Loss: 6.079929390014747, Val MAE: 1.5705546140670776\n",
      "Epoch 163/2000, Train Loss: 5.5853234522262465, Val Loss: 6.079924612778884, Val MAE: 1.5699306726455688\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 164/2000, Train Loss: 5.585398705671061, Val Loss: 6.07957580272968, Val MAE: 1.5692132711410522\n",
      "Epoch 165/2000, Train Loss: 5.5847021135518515, Val Loss: 6.078968297212552, Val MAE: 1.5696791410446167\n",
      "Epoch 166/2000, Train Loss: 5.584237557646325, Val Loss: 6.079160799429967, Val MAE: 1.5703716278076172\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 167/2000, Train Loss: 5.58402089078454, Val Loss: 6.079228912561367, Val MAE: 1.5692754983901978\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 168/2000, Train Loss: 5.58395332722656, Val Loss: 6.078902884324392, Val MAE: 1.569259762763977\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 169/2000, Train Loss: 5.583250110608336, Val Loss: 6.0779147481307, Val MAE: 1.569982647895813\n",
      "Epoch 170/2000, Train Loss: 5.582905119187386, Val Loss: 6.078089951551878, Val MAE: 1.570610523223877\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 171/2000, Train Loss: 5.582371795368142, Val Loss: 6.078037281219776, Val MAE: 1.5704861879348755\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 172/2000, Train Loss: 5.582187299102429, Val Loss: 6.077788289388021, Val MAE: 1.5705902576446533\n",
      "Epoch 173/2000, Train Loss: 5.582046966952509, Val Loss: 6.077590500391446, Val MAE: 1.5695596933364868\n",
      "Epoch 174/2000, Train Loss: 5.581155532115215, Val Loss: 6.077438967655866, Val MAE: 1.5710464715957642\n",
      "Epoch 175/2000, Train Loss: 5.5812043852393165, Val Loss: 6.077116267497723, Val MAE: 1.5693334341049194\n",
      "Epoch 176/2000, Train Loss: 5.580736436438837, Val Loss: 6.0771163812050455, Val MAE: 1.5689036846160889\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 177/2000, Train Loss: 5.580912419201851, Val Loss: 6.077101880159134, Val MAE: 1.5685185194015503\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 178/2000, Train Loss: 5.580213617371119, Val Loss: 6.076778979790516, Val MAE: 1.5706841945648193\n",
      "Epoch 179/2000, Train Loss: 5.579607746841759, Val Loss: 6.077006092438331, Val MAE: 1.5686006546020508\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 180/2000, Train Loss: 5.579195487321705, Val Loss: 6.076605371022835, Val MAE: 1.569679617881775\n",
      "Epoch 181/2000, Train Loss: 5.579068269771696, Val Loss: 6.076199958874629, Val MAE: 1.5694758892059326\n",
      "Epoch 182/2000, Train Loss: 5.5787407924506915, Val Loss: 6.07606572952026, Val MAE: 1.5691847801208496\n",
      "Epoch 183/2000, Train Loss: 5.5780547593617005, Val Loss: 6.076454692009168, Val MAE: 1.5687615871429443\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 184/2000, Train Loss: 5.577859671551157, Val Loss: 6.076008947690328, Val MAE: 1.5700536966323853\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 185/2000, Train Loss: 5.577643862496537, Val Loss: 6.074928897466415, Val MAE: 1.5698719024658203\n",
      "Epoch 186/2000, Train Loss: 5.577186845378591, Val Loss: 6.075746391369746, Val MAE: 1.5683847665786743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 187/2000, Train Loss: 5.577179241233022, Val Loss: 6.075110172614073, Val MAE: 1.5701189041137695\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 188/2000, Train Loss: 5.576615072651194, Val Loss: 6.0752215379323715, Val MAE: 1.570038914680481\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 189/2000, Train Loss: 5.576604609948394, Val Loss: 6.075109955592033, Val MAE: 1.5700769424438477\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 190/2000, Train Loss: 5.575824036890147, Val Loss: 6.075349849309677, Val MAE: 1.5696487426757812\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 191/2000, Train Loss: 5.575594604574246, Val Loss: 6.074934612482022, Val MAE: 1.5700469017028809\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 192/2000, Train Loss: 5.575262178259532, Val Loss: 6.074595216604379, Val MAE: 1.5705084800720215\n",
      "Epoch 193/2000, Train Loss: 5.5755895305850265, Val Loss: 6.074546220669379, Val MAE: 1.5677071809768677\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 194/2000, Train Loss: 5.574823127026987, Val Loss: 6.073702053840344, Val MAE: 1.5707337856292725\n",
      "Epoch 195/2000, Train Loss: 5.574208928679999, Val Loss: 6.073662012968308, Val MAE: 1.5699094533920288\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 196/2000, Train Loss: 5.574094515772597, Val Loss: 6.073499766374246, Val MAE: 1.5711859464645386\n",
      "Epoch 197/2000, Train Loss: 5.573482316170223, Val Loss: 6.073033316013141, Val MAE: 1.5706573724746704\n",
      "Epoch 198/2000, Train Loss: 5.573531982850056, Val Loss: 6.073052081389305, Val MAE: 1.5707435607910156\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 199/2000, Train Loss: 5.572898225894766, Val Loss: 6.073195767402649, Val MAE: 1.5708914995193481\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 200/2000, Train Loss: 5.57258178066037, Val Loss: 6.073354645570119, Val MAE: 1.5690652132034302\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 201/2000, Train Loss: 5.572673514674397, Val Loss: 6.073274643604572, Val MAE: 1.5696247816085815\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 202/2000, Train Loss: 5.572096309014861, Val Loss: 6.072873023228768, Val MAE: 1.5700117349624634\n",
      "Epoch 203/2000, Train Loss: 5.571742211266637, Val Loss: 6.073233591593229, Val MAE: 1.5699663162231445\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 204/2000, Train Loss: 5.571879178597923, Val Loss: 6.07273376079706, Val MAE: 1.5706077814102173\n",
      "Epoch 205/2000, Train Loss: 5.5712832781207275, Val Loss: 6.072907392183939, Val MAE: 1.5697506666183472\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 206/2000, Train Loss: 5.5708297781370915, Val Loss: 6.072601738037207, Val MAE: 1.5696871280670166\n",
      "Epoch 207/2000, Train Loss: 5.5709127110798455, Val Loss: 6.07207128848785, Val MAE: 1.5700442790985107\n",
      "Epoch 208/2000, Train Loss: 5.5704835419883665, Val Loss: 6.072389394809038, Val MAE: 1.5684564113616943\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 209/2000, Train Loss: 5.570108645145103, Val Loss: 6.0720839659372965, Val MAE: 1.569039225578308\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 210/2000, Train Loss: 5.5700167914615175, Val Loss: 6.071665051961556, Val MAE: 1.5694316625595093\n",
      "Epoch 211/2000, Train Loss: 5.569689616799288, Val Loss: 6.071792008326604, Val MAE: 1.5698977708816528\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 212/2000, Train Loss: 5.569344167083386, Val Loss: 6.071814145186009, Val MAE: 1.5704425573349\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 213/2000, Train Loss: 5.5687264632074065, Val Loss: 6.071625535610394, Val MAE: 1.5692028999328613\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 214/2000, Train Loss: 5.568592612807815, Val Loss: 6.072024227411319, Val MAE: 1.5691813230514526\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 215/2000, Train Loss: 5.568212485089857, Val Loss: 6.071501379440992, Val MAE: 1.570162296295166\n",
      "Epoch 216/2000, Train Loss: 5.5681640445627165, Val Loss: 6.071352015397488, Val MAE: 1.5705026388168335\n",
      "Epoch 217/2000, Train Loss: 5.567915191127001, Val Loss: 6.0708577733773454, Val MAE: 1.569764494895935\n",
      "Epoch 218/2000, Train Loss: 5.567478976325968, Val Loss: 6.070877705476223, Val MAE: 1.5693503618240356\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 219/2000, Train Loss: 5.567084552041089, Val Loss: 6.071008301392579, Val MAE: 1.5706660747528076\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 220/2000, Train Loss: 5.566869018620966, Val Loss: 6.070886910267365, Val MAE: 1.5714629888534546\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 221/2000, Train Loss: 5.566462140417336, Val Loss: 6.071151662178528, Val MAE: 1.5701102018356323\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 222/2000, Train Loss: 5.566347047629043, Val Loss: 6.0711045836791016, Val MAE: 1.5706707239151\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 223/2000, Train Loss: 5.566479190830624, Val Loss: 6.070818308072212, Val MAE: 1.5706415176391602\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 224/2000, Train Loss: 5.565941072733311, Val Loss: 6.07033044558305, Val MAE: 1.57005774974823\n",
      "Epoch 225/2000, Train Loss: 5.565588414964923, Val Loss: 6.070662394242409, Val MAE: 1.5696725845336914\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 226/2000, Train Loss: 5.564947893129023, Val Loss: 6.070318946777246, Val MAE: 1.570035696029663\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 227/2000, Train Loss: 5.564953621618668, Val Loss: 6.070090236419286, Val MAE: 1.5709288120269775\n",
      "Epoch 228/2000, Train Loss: 5.564728925718633, Val Loss: 6.070159488152235, Val MAE: 1.5699628591537476\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 229/2000, Train Loss: 5.564511343253231, Val Loss: 6.070078340860514, Val MAE: 1.569983959197998\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 230/2000, Train Loss: 5.563877919202869, Val Loss: 6.070136636342758, Val MAE: 1.5690057277679443\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 231/2000, Train Loss: 5.563674219655386, Val Loss: 6.0704687427251764, Val MAE: 1.5679662227630615\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 232/2000, Train Loss: 5.56342888246632, Val Loss: 6.06980854181143, Val MAE: 1.569209337234497\n",
      "Epoch 233/2000, Train Loss: 5.562935436225635, Val Loss: 6.069561243668581, Val MAE: 1.570584774017334\n",
      "Epoch 234/2000, Train Loss: 5.562833916595215, Val Loss: 6.069721776705522, Val MAE: 1.5692105293273926\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 235/2000, Train Loss: 5.562438762589048, Val Loss: 6.0696565444652855, Val MAE: 1.5708893537521362\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 236/2000, Train Loss: 5.562185792980836, Val Loss: 6.070044310276325, Val MAE: 1.5692830085754395\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 237/2000, Train Loss: 5.562133314329422, Val Loss: 6.0700884996316375, Val MAE: 1.5701640844345093\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 238/2000, Train Loss: 5.561842332804196, Val Loss: 6.0694265631529, Val MAE: 1.570920705795288\n",
      "Epoch 239/2000, Train Loss: 5.561415005308847, Val Loss: 6.069398642808963, Val MAE: 1.5714330673217773\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 240/2000, Train Loss: 5.5610891448419135, Val Loss: 6.0692131959475, Val MAE: 1.5709314346313477\n",
      "Epoch 241/2000, Train Loss: 5.561014905004822, Val Loss: 6.0694172192842535, Val MAE: 1.5720946788787842\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 242/2000, Train Loss: 5.560493774066889, Val Loss: 6.069515517430427, Val MAE: 1.570557951927185\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 243/2000, Train Loss: 5.560434682225892, Val Loss: 6.069028338102194, Val MAE: 1.5726382732391357\n",
      "Epoch 244/2000, Train Loss: 5.55989527452472, Val Loss: 6.069214520698939, Val MAE: 1.5710607767105103\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 245/2000, Train Loss: 5.559674603466953, Val Loss: 6.068957789433308, Val MAE: 1.5708057880401611\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 246/2000, Train Loss: 5.559500616095176, Val Loss: 6.069038976155794, Val MAE: 1.5710359811782837\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 247/2000, Train Loss: 5.55933073457275, Val Loss: 6.069423488470224, Val MAE: 1.5697828531265259\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 248/2000, Train Loss: 5.559001520728118, Val Loss: 6.068574025386419, Val MAE: 1.5696805715560913\n",
      "Epoch 249/2000, Train Loss: 5.558803816625445, Val Loss: 6.06778844503256, Val MAE: 1.5715651512145996\n",
      "Epoch 250/2000, Train Loss: 5.558420885878245, Val Loss: 6.067469157928076, Val MAE: 1.572282314300537\n",
      "Epoch 251/2000, Train Loss: 5.558286039985745, Val Loss: 6.067564896436838, Val MAE: 1.5720546245574951\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 252/2000, Train Loss: 5.557815816345888, Val Loss: 6.067924689329588, Val MAE: 1.5709129571914673\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 253/2000, Train Loss: 5.5576745929865305, Val Loss: 6.067978857725095, Val MAE: 1.5712281465530396\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 254/2000, Train Loss: 5.557385123374379, Val Loss: 6.067750686865587, Val MAE: 1.570717215538025\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 255/2000, Train Loss: 5.557484967499494, Val Loss: 6.067874612258031, Val MAE: 1.5705913305282593\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 256/2000, Train Loss: 5.557244693516238, Val Loss: 6.068311131306183, Val MAE: 1.5698367357254028\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 257/2000, Train Loss: 5.55678890320851, Val Loss: 6.067641935898707, Val MAE: 1.5703351497650146\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 258/2000, Train Loss: 5.556671719556347, Val Loss: 6.067538102773519, Val MAE: 1.5710302591323853\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 259/2000, Train Loss: 5.555902393628271, Val Loss: 6.067294276066315, Val MAE: 1.571131944656372\n",
      "Epoch 260/2000, Train Loss: 5.556093365989597, Val Loss: 6.067266913255056, Val MAE: 1.5709283351898193\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 261/2000, Train Loss: 5.555735964769825, Val Loss: 6.067169957588881, Val MAE: 1.5704070329666138\n",
      "Epoch 262/2000, Train Loss: 5.555345601951359, Val Loss: 6.067350315130674, Val MAE: 1.5697945356369019\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 263/2000, Train Loss: 5.555043922230741, Val Loss: 6.067220627038907, Val MAE: 1.5700154304504395\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 264/2000, Train Loss: 5.554818409421598, Val Loss: 6.067222658181802, Val MAE: 1.5704479217529297\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 265/2000, Train Loss: 5.554583462706207, Val Loss: 6.0671659301488825, Val MAE: 1.570237398147583\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 266/2000, Train Loss: 5.55438208747686, Val Loss: 6.066380627338702, Val MAE: 1.5721025466918945\n",
      "Epoch 267/2000, Train Loss: 5.55419021703496, Val Loss: 6.066421883534162, Val MAE: 1.5713564157485962\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 268/2000, Train Loss: 5.554070537286244, Val Loss: 6.066895138606047, Val MAE: 1.570104718208313\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 269/2000, Train Loss: 5.553516870036812, Val Loss: 6.066832575431237, Val MAE: 1.5711113214492798\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 270/2000, Train Loss: 5.553271645284528, Val Loss: 6.06652812346434, Val MAE: 1.5707519054412842\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 271/2000, Train Loss: 5.5530197845265405, Val Loss: 6.066859212594155, Val MAE: 1.5698161125183105\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 272/2000, Train Loss: 5.553207586565138, Val Loss: 6.067097008533967, Val MAE: 1.5714541673660278\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 273/2000, Train Loss: 5.552649824184011, Val Loss: 6.066481966850085, Val MAE: 1.5710020065307617\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 274/2000, Train Loss: 5.552314707013828, Val Loss: 6.066687923822648, Val MAE: 1.5700767040252686\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 275/2000, Train Loss: 5.551899963101167, Val Loss: 6.066279394504352, Val MAE: 1.5705722570419312\n",
      "Epoch 276/2000, Train Loss: 5.551933704130043, Val Loss: 6.0666344196368485, Val MAE: 1.570426106452942\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 277/2000, Train Loss: 5.551432117729377, Val Loss: 6.0664116776906525, Val MAE: 1.5706177949905396\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 278/2000, Train Loss: 5.551346552944025, Val Loss: 6.066144405267178, Val MAE: 1.571630835533142\n",
      "Epoch 279/2000, Train Loss: 5.550973487899768, Val Loss: 6.066580110635513, Val MAE: 1.5705629587173462\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 280/2000, Train Loss: 5.55096756208686, Val Loss: 6.066599100064009, Val MAE: 1.569550633430481\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 281/2000, Train Loss: 5.550473967245515, Val Loss: 6.066641463988867, Val MAE: 1.5710963010787964\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 282/2000, Train Loss: 5.550265373213148, Val Loss: 6.066349568733802, Val MAE: 1.5715782642364502\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 283/2000, Train Loss: 5.550135108034583, Val Loss: 6.066704028080672, Val MAE: 1.5701388120651245\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 284/2000, Train Loss: 5.5496371432818306, Val Loss: 6.066043674945831, Val MAE: 1.5712051391601562\n",
      "Epoch 285/2000, Train Loss: 5.54962783930253, Val Loss: 6.066041924403264, Val MAE: 1.571212649345398\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 286/2000, Train Loss: 5.549083657838072, Val Loss: 6.065430888457176, Val MAE: 1.5720465183258057\n",
      "Epoch 287/2000, Train Loss: 5.548919953368872, Val Loss: 6.0659378880109545, Val MAE: 1.5714271068572998\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 288/2000, Train Loss: 5.548555172548331, Val Loss: 6.0658987060571326, Val MAE: 1.5706913471221924\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 289/2000, Train Loss: 5.548479441838967, Val Loss: 6.065985872500982, Val MAE: 1.5700716972351074\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 290/2000, Train Loss: 5.548450102545711, Val Loss: 6.065293038197053, Val MAE: 1.571306586265564\n",
      "Epoch 291/2000, Train Loss: 5.547954732386762, Val Loss: 6.0664135358272455, Val MAE: 1.5698398351669312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 292/2000, Train Loss: 5.547641380413158, Val Loss: 6.066404838134081, Val MAE: 1.5694998502731323\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 293/2000, Train Loss: 5.54727469158646, Val Loss: 6.065505736302107, Val MAE: 1.5725107192993164\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 294/2000, Train Loss: 5.547146995489728, Val Loss: 6.065474345133855, Val MAE: 1.5716489553451538\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 295/2000, Train Loss: 5.546762645277474, Val Loss: 6.065753951133826, Val MAE: 1.5704174041748047\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 296/2000, Train Loss: 5.54663205317746, Val Loss: 6.066274966337742, Val MAE: 1.5698513984680176\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 297/2000, Train Loss: 5.546562497548747, Val Loss: 6.065645111524142, Val MAE: 1.5715711116790771\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 298/2000, Train Loss: 5.5465399898091405, Val Loss: 6.065346288986695, Val MAE: 1.571291446685791\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 299/2000, Train Loss: 5.54615406850656, Val Loss: 6.064868057079805, Val MAE: 1.5714116096496582\n",
      "Epoch 300/2000, Train Loss: 5.545338414876552, Val Loss: 6.065373042913584, Val MAE: 1.5716568231582642\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 301/2000, Train Loss: 5.545577531188348, Val Loss: 6.065727896873767, Val MAE: 1.5708246231079102\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 302/2000, Train Loss: 5.545158098234502, Val Loss: 6.065357480904995, Val MAE: 1.5712140798568726\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 303/2000, Train Loss: 5.544918887469233, Val Loss: 6.066057608066461, Val MAE: 1.5702531337738037\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 304/2000, Train Loss: 5.5447636558019315, Val Loss: 6.065671929640648, Val MAE: 1.5719642639160156\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 305/2000, Train Loss: 5.544248177343748, Val Loss: 6.065237867220854, Val MAE: 1.5709547996520996\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 306/2000, Train Loss: 5.54460258702185, Val Loss: 6.065354305047255, Val MAE: 1.570164442062378\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 307/2000, Train Loss: 5.543721014665624, Val Loss: 6.065544762978187, Val MAE: 1.5699949264526367\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 308/2000, Train Loss: 5.54390299951445, Val Loss: 6.065489269525576, Val MAE: 1.570120930671692\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 309/2000, Train Loss: 5.543400661759921, Val Loss: 6.0655479712364, Val MAE: 1.5705050230026245\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 310/2000, Train Loss: 5.543072613707183, Val Loss: 6.065302243905189, Val MAE: 1.5716742277145386\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 311/2000, Train Loss: 5.54285323764498, Val Loss: 6.065205303522257, Val MAE: 1.5710173845291138\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 312/2000, Train Loss: 5.543194266359778, Val Loss: 6.065718399867033, Val MAE: 1.5707974433898926\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 313/2000, Train Loss: 5.54253456622806, Val Loss: 6.06557263258176, Val MAE: 1.571130633354187\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 314/2000, Train Loss: 5.542207621896853, Val Loss: 6.064995693549132, Val MAE: 1.5707626342773438\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 315/2000, Train Loss: 5.54221485493529, Val Loss: 6.065214355480976, Val MAE: 1.5698388814926147\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 316/2000, Train Loss: 5.541804519728015, Val Loss: 6.065005970918215, Val MAE: 1.571239709854126\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 317/2000, Train Loss: 5.541683068409586, Val Loss: 6.065397313313606, Val MAE: 1.5703719854354858\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 318/2000, Train Loss: 5.541082043839763, Val Loss: 6.065604397883782, Val MAE: 1.5713165998458862\n",
      "EarlyStopping counter: 19 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:43<10:54, 163.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319/2000, Train Loss: 5.540799441024603, Val Loss: 6.065416482473031, Val MAE: 1.5705751180648804\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 5.599146366119385\n",
      "Test Mean Absolute Error (MAE): 1.5717979601181264\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 445, 'position': 'DEF', 'window_size': 3, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 445\n",
      "position DEF\n",
      "window_size 3\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  1\n",
      "======= Generating CNN Data for Season: ['2020-21'], Position: DEF =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type DEF = 245.\n",
      "65 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (6124, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (6662, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (6124, 7)\n",
      "Shape of a given window (prior to preprocessing): (3, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.55797468e+00 4.13012658e+01 2.07594937e-02 2.98734177e-02\n",
      " 1.36202532e-01 8.06379747e+00 6.68354430e-02 3.03797468e-03\n",
      " 3.79746835e-03 0.00000000e+00]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.66587548 43.57949667  0.14434288  0.18034772  0.3430035  10.12853999\n",
      "  0.24973679  0.05503404  0.06150648  1.        ]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building rnn Architecture ======\n",
      "====== Done Building rnn Architecture ======\n",
      "Epoch 1/2000, Train Loss: 9.503770207518523, Val Loss: 9.63886586493461, Val MAE: 1.7104476690292358\n",
      "Epoch 2/2000, Train Loss: 9.326000047675313, Val Loss: 9.445647919349389, Val MAE: 1.6918649673461914\n",
      "Epoch 3/2000, Train Loss: 9.146878788649774, Val Loss: 9.250772220439748, Val MAE: 1.6744714975357056\n",
      "Epoch 4/2000, Train Loss: 8.964970561376226, Val Loss: 9.049756641827873, Val MAE: 1.65617036819458\n",
      "Epoch 5/2000, Train Loss: 8.778250931643178, Val Loss: 8.844117980736723, Val MAE: 1.6371488571166992\n",
      "Epoch 6/2000, Train Loss: 8.585385126164306, Val Loss: 8.633435016405315, Val MAE: 1.6171396970748901\n",
      "Epoch 7/2000, Train Loss: 8.388763660901443, Val Loss: 8.416183654132833, Val MAE: 1.5957489013671875\n",
      "Epoch 8/2000, Train Loss: 8.190531629297702, Val Loss: 8.198746615316628, Val MAE: 1.573919415473938\n",
      "Epoch 9/2000, Train Loss: 7.992744503304822, Val Loss: 7.9842843125503755, Val MAE: 1.553039312362671\n",
      "Epoch 10/2000, Train Loss: 7.797082172353887, Val Loss: 7.772110154645444, Val MAE: 1.535819411277771\n",
      "Epoch 11/2000, Train Loss: 7.606175958322534, Val Loss: 7.5719637875379435, Val MAE: 1.5238028764724731\n",
      "Epoch 12/2000, Train Loss: 7.422776131377871, Val Loss: 7.371777157113913, Val MAE: 1.5151219367980957\n",
      "Epoch 13/2000, Train Loss: 7.248068181428615, Val Loss: 7.185950750387029, Val MAE: 1.5093566179275513\n",
      "Epoch 14/2000, Train Loss: 7.084207572075764, Val Loss: 7.01436222331486, Val MAE: 1.5044653415679932\n",
      "Epoch 15/2000, Train Loss: 6.933785914849604, Val Loss: 6.857631171050461, Val MAE: 1.5016759634017944\n",
      "Epoch 16/2000, Train Loss: 6.794915326366341, Val Loss: 6.713976870218008, Val MAE: 1.5001530647277832\n",
      "Epoch 17/2000, Train Loss: 6.6699294503039726, Val Loss: 6.585491718664593, Val MAE: 1.5015538930892944\n",
      "Epoch 18/2000, Train Loss: 6.556335718621241, Val Loss: 6.46884354700326, Val MAE: 1.5071616172790527\n",
      "Epoch 19/2000, Train Loss: 6.4556200955932885, Val Loss: 6.366850916827062, Val MAE: 1.5169090032577515\n",
      "Epoch 20/2000, Train Loss: 6.3673416696456036, Val Loss: 6.275372790906755, Val MAE: 1.5284284353256226\n",
      "Epoch 21/2000, Train Loss: 6.289195419933303, Val Loss: 6.198691658761749, Val MAE: 1.5415911674499512\n",
      "Epoch 22/2000, Train Loss: 6.2227405073359146, Val Loss: 6.1348251365173, Val MAE: 1.5537244081497192\n",
      "Epoch 23/2000, Train Loss: 6.166333952664279, Val Loss: 6.075814776084313, Val MAE: 1.5673878192901611\n",
      "Epoch 24/2000, Train Loss: 6.118181514319869, Val Loss: 6.031056416729066, Val MAE: 1.5788520574569702\n",
      "Epoch 25/2000, Train Loss: 6.078682399531293, Val Loss: 5.994884906439482, Val MAE: 1.5893921852111816\n",
      "Epoch 26/2000, Train Loss: 6.045783148463077, Val Loss: 5.963055433162676, Val MAE: 1.6002546548843384\n",
      "Epoch 27/2000, Train Loss: 6.018416472993758, Val Loss: 5.937956208078638, Val MAE: 1.6095118522644043\n",
      "Epoch 28/2000, Train Loss: 5.995714026925848, Val Loss: 5.917031183217166, Val MAE: 1.6181715726852417\n",
      "Epoch 29/2000, Train Loss: 5.97716179923339, Val Loss: 5.899926328287963, Val MAE: 1.626371145248413\n",
      "Epoch 30/2000, Train Loss: 5.961654203053613, Val Loss: 5.886406677467832, Val MAE: 1.6329455375671387\n",
      "Epoch 31/2000, Train Loss: 5.948821815625162, Val Loss: 5.875693197666987, Val MAE: 1.639360785484314\n",
      "Epoch 32/2000, Train Loss: 5.938306804270471, Val Loss: 5.866935322991308, Val MAE: 1.644309639930725\n",
      "Epoch 33/2000, Train Loss: 5.929597118352478, Val Loss: 5.859057635241051, Val MAE: 1.6487104892730713\n",
      "Epoch 34/2000, Train Loss: 5.9216641581531135, Val Loss: 5.8531935043986865, Val MAE: 1.6533048152923584\n",
      "Epoch 35/2000, Train Loss: 5.914854700869926, Val Loss: 5.847300025566322, Val MAE: 1.6571588516235352\n",
      "Epoch 36/2000, Train Loss: 5.908957277625668, Val Loss: 5.842384264523472, Val MAE: 1.6601414680480957\n",
      "Epoch 37/2000, Train Loss: 5.903315743685819, Val Loss: 5.837891163995468, Val MAE: 1.663435697555542\n",
      "Epoch 38/2000, Train Loss: 5.898464312112279, Val Loss: 5.834227701885786, Val MAE: 1.6651066541671753\n",
      "Epoch 39/2000, Train Loss: 5.893928873381426, Val Loss: 5.830704687524285, Val MAE: 1.666741132736206\n",
      "Epoch 40/2000, Train Loss: 5.889833855733998, Val Loss: 5.827520165157257, Val MAE: 1.6681853532791138\n",
      "Epoch 41/2000, Train Loss: 5.885655803302311, Val Loss: 5.824323622402239, Val MAE: 1.6682944297790527\n",
      "Epoch 42/2000, Train Loss: 5.881808897472163, Val Loss: 5.821109975027028, Val MAE: 1.6711477041244507\n",
      "Epoch 43/2000, Train Loss: 5.87814706537692, Val Loss: 5.8183574303889, Val MAE: 1.6710560321807861\n",
      "Epoch 44/2000, Train Loss: 5.8746963555592275, Val Loss: 5.815744020580404, Val MAE: 1.6710842847824097\n",
      "Epoch 45/2000, Train Loss: 5.871427919371013, Val Loss: 5.8130554388610784, Val MAE: 1.672515630722046\n",
      "Epoch 46/2000, Train Loss: 5.8683742142984, Val Loss: 5.810574156855052, Val MAE: 1.6724562644958496\n",
      "Epoch 47/2000, Train Loss: 5.864752180250731, Val Loss: 5.80801418660847, Val MAE: 1.6730552911758423\n",
      "Epoch 48/2000, Train Loss: 5.862209534855141, Val Loss: 5.805570721014167, Val MAE: 1.6725432872772217\n",
      "Epoch 49/2000, Train Loss: 5.859094419143274, Val Loss: 5.803266706932647, Val MAE: 1.6719319820404053\n",
      "Epoch 50/2000, Train Loss: 5.856237749696303, Val Loss: 5.8009752009508695, Val MAE: 1.6724051237106323\n",
      "Epoch 51/2000, Train Loss: 5.853418357571841, Val Loss: 5.7986014286422, Val MAE: 1.672841191291809\n",
      "Epoch 52/2000, Train Loss: 5.850313092118318, Val Loss: 5.796423247186838, Val MAE: 1.6727409362792969\n",
      "Epoch 53/2000, Train Loss: 5.84757950967629, Val Loss: 5.794272528743713, Val MAE: 1.6728161573410034\n",
      "Epoch 54/2000, Train Loss: 5.844846530107675, Val Loss: 5.7920953537575395, Val MAE: 1.6724859476089478\n",
      "Epoch 55/2000, Train Loss: 5.842752460866247, Val Loss: 5.789923375163978, Val MAE: 1.6713778972625732\n",
      "Epoch 56/2000, Train Loss: 5.83953202033358, Val Loss: 5.78810049977933, Val MAE: 1.6720702648162842\n",
      "Epoch 57/2000, Train Loss: 5.837586543633549, Val Loss: 5.785835536763512, Val MAE: 1.6714518070220947\n",
      "Epoch 58/2000, Train Loss: 5.834704727853447, Val Loss: 5.784184495132986, Val MAE: 1.6729726791381836\n",
      "Epoch 59/2000, Train Loss: 5.832454527002074, Val Loss: 5.782054808167729, Val MAE: 1.6716737747192383\n",
      "Epoch 60/2000, Train Loss: 5.829968531226272, Val Loss: 5.780205140696104, Val MAE: 1.6721229553222656\n",
      "Epoch 61/2000, Train Loss: 5.827585854719389, Val Loss: 5.778310373014219, Val MAE: 1.671130657196045\n",
      "Epoch 62/2000, Train Loss: 5.825190103001532, Val Loss: 5.77670365615015, Val MAE: 1.6720350980758667\n",
      "Epoch 63/2000, Train Loss: 5.823154337080565, Val Loss: 5.774925387774108, Val MAE: 1.6718147993087769\n",
      "Epoch 64/2000, Train Loss: 5.82090214485639, Val Loss: 5.77327716583924, Val MAE: 1.6704859733581543\n",
      "Epoch 65/2000, Train Loss: 5.818738910070075, Val Loss: 5.771599615989815, Val MAE: 1.6706777811050415\n",
      "Epoch 66/2000, Train Loss: 5.816778471816479, Val Loss: 5.770010362789321, Val MAE: 1.6704031229019165\n",
      "Epoch 67/2000, Train Loss: 5.814690486975178, Val Loss: 5.768263060480012, Val MAE: 1.6703318357467651\n",
      "Epoch 68/2000, Train Loss: 5.812483020816081, Val Loss: 5.766816691750127, Val MAE: 1.6704025268554688\n",
      "Epoch 69/2000, Train Loss: 5.810788769028785, Val Loss: 5.765246291770403, Val MAE: 1.6699939966201782\n",
      "Epoch 70/2000, Train Loss: 5.808807668181768, Val Loss: 5.7637153685724165, Val MAE: 1.6700447797775269\n",
      "Epoch 71/2000, Train Loss: 5.807043310829196, Val Loss: 5.762231553721795, Val MAE: 1.6691834926605225\n",
      "Epoch 72/2000, Train Loss: 5.805049935101413, Val Loss: 5.760805668436058, Val MAE: 1.6694281101226807\n",
      "Epoch 73/2000, Train Loss: 5.802993465625242, Val Loss: 5.759384910335743, Val MAE: 1.6703622341156006\n",
      "Epoch 74/2000, Train Loss: 5.801215171813965, Val Loss: 5.757987418504903, Val MAE: 1.6695199012756348\n",
      "Epoch 75/2000, Train Loss: 5.799650831894728, Val Loss: 5.756528071695712, Val MAE: 1.668850064277649\n",
      "Epoch 76/2000, Train Loss: 5.797708187859489, Val Loss: 5.7553711341782465, Val MAE: 1.669535756111145\n",
      "Epoch 77/2000, Train Loss: 5.79602985865219, Val Loss: 5.754078447044049, Val MAE: 1.6680943965911865\n",
      "Epoch 78/2000, Train Loss: 5.794483089236961, Val Loss: 5.75282345496032, Val MAE: 1.6684465408325195\n",
      "Epoch 79/2000, Train Loss: 5.792693464241364, Val Loss: 5.7514899466019385, Val MAE: 1.6684165000915527\n",
      "Epoch 80/2000, Train Loss: 5.79100758808825, Val Loss: 5.7502398558238355, Val MAE: 1.6679233312606812\n",
      "Epoch 81/2000, Train Loss: 5.789913251011382, Val Loss: 5.7492151101321864, Val MAE: 1.6680787801742554\n",
      "Epoch 82/2000, Train Loss: 5.788144062269102, Val Loss: 5.748080728056725, Val MAE: 1.668967604637146\n",
      "Epoch 83/2000, Train Loss: 5.786262923925459, Val Loss: 5.746894984873101, Val MAE: 1.6672486066818237\n",
      "Epoch 84/2000, Train Loss: 5.785151567753191, Val Loss: 5.745851585988209, Val MAE: 1.6684223413467407\n",
      "Epoch 85/2000, Train Loss: 5.783392502872954, Val Loss: 5.744680507833064, Val MAE: 1.6673444509506226\n",
      "Epoch 86/2000, Train Loss: 5.781970268829278, Val Loss: 5.743794752052951, Val MAE: 1.6666712760925293\n",
      "Epoch 87/2000, Train Loss: 5.780290222378029, Val Loss: 5.7426865676726395, Val MAE: 1.6671009063720703\n",
      "Epoch 88/2000, Train Loss: 5.7790263161260125, Val Loss: 5.741602398703814, Val MAE: 1.6658642292022705\n",
      "Epoch 89/2000, Train Loss: 5.777902124211652, Val Loss: 5.740683531166271, Val MAE: 1.666621446609497\n",
      "Epoch 90/2000, Train Loss: 5.776508156948678, Val Loss: 5.739598825602614, Val MAE: 1.6664531230926514\n",
      "Epoch 91/2000, Train Loss: 5.775088717758918, Val Loss: 5.738549299241643, Val MAE: 1.6656712293624878\n",
      "Epoch 92/2000, Train Loss: 5.773938336036279, Val Loss: 5.737651500798313, Val MAE: 1.6663599014282227\n",
      "Epoch 93/2000, Train Loss: 5.772289626923952, Val Loss: 5.736592588570825, Val MAE: 1.6657699346542358\n",
      "Epoch 94/2000, Train Loss: 5.771560387464347, Val Loss: 5.736035036068979, Val MAE: 1.6669846773147583\n",
      "Epoch 95/2000, Train Loss: 5.770085606806079, Val Loss: 5.735111159541487, Val MAE: 1.666555643081665\n",
      "Epoch 96/2000, Train Loss: 5.769756549255439, Val Loss: 5.734213663406182, Val MAE: 1.6643126010894775\n",
      "Epoch 97/2000, Train Loss: 5.767889409338325, Val Loss: 5.733430251555663, Val MAE: 1.6656404733657837\n",
      "Epoch 98/2000, Train Loss: 5.766500609561735, Val Loss: 5.732693686335322, Val MAE: 1.6671764850616455\n",
      "Epoch 99/2000, Train Loss: 5.7652723736699985, Val Loss: 5.731757573639934, Val MAE: 1.6659985780715942\n",
      "Epoch 100/2000, Train Loss: 5.764464073769322, Val Loss: 5.731035422395069, Val MAE: 1.6649744510650635\n",
      "Epoch 101/2000, Train Loss: 5.763324660876774, Val Loss: 5.730346776956313, Val MAE: 1.665309190750122\n",
      "Epoch 102/2000, Train Loss: 5.7621005736783735, Val Loss: 5.729565544348014, Val MAE: 1.6653984785079956\n",
      "Epoch 103/2000, Train Loss: 5.760838183012303, Val Loss: 5.728836660819733, Val MAE: 1.6653891801834106\n",
      "Epoch 104/2000, Train Loss: 5.759937147737075, Val Loss: 5.7281837953070465, Val MAE: 1.666447639465332\n",
      "Epoch 105/2000, Train Loss: 5.759166343621746, Val Loss: 5.72746813052426, Val MAE: 1.6660181283950806\n",
      "Epoch 106/2000, Train Loss: 5.757892587636536, Val Loss: 5.726913861136534, Val MAE: 1.6651493310928345\n",
      "Epoch 107/2000, Train Loss: 5.757137248169483, Val Loss: 5.726178820463751, Val MAE: 1.665656328201294\n",
      "Epoch 108/2000, Train Loss: 5.755881778994321, Val Loss: 5.725243626265226, Val MAE: 1.6655391454696655\n",
      "Epoch 109/2000, Train Loss: 5.7547940409656135, Val Loss: 5.7246450648687315, Val MAE: 1.664923906326294\n",
      "Epoch 110/2000, Train Loss: 5.753958189539972, Val Loss: 5.724070915533887, Val MAE: 1.665146827697754\n",
      "Epoch 111/2000, Train Loss: 5.7535560131073, Val Loss: 5.723543181078119, Val MAE: 1.6659090518951416\n",
      "Epoch 112/2000, Train Loss: 5.752342484596017, Val Loss: 5.722653334594508, Val MAE: 1.6635023355484009\n",
      "Epoch 113/2000, Train Loss: 5.751192980400791, Val Loss: 5.722131120550311, Val MAE: 1.664678931236267\n",
      "Epoch 114/2000, Train Loss: 5.750515673129044, Val Loss: 5.721604220234384, Val MAE: 1.6640907526016235\n",
      "Epoch 115/2000, Train Loss: 5.750752908017667, Val Loss: 5.720707078781146, Val MAE: 1.662529706954956\n",
      "Epoch 116/2000, Train Loss: 5.748938611950643, Val Loss: 5.720348349584695, Val MAE: 1.6646311283111572\n",
      "Epoch 117/2000, Train Loss: 5.74808222186723, Val Loss: 5.7196076342857545, Val MAE: 1.661887526512146\n",
      "Epoch 118/2000, Train Loss: 5.747379772463559, Val Loss: 5.719218281768406, Val MAE: 1.6639822721481323\n",
      "Epoch 119/2000, Train Loss: 5.746467392875235, Val Loss: 5.7185614571950865, Val MAE: 1.6618424654006958\n",
      "Epoch 120/2000, Train Loss: 5.745926740411095, Val Loss: 5.7180633706749635, Val MAE: 1.6641333103179932\n",
      "Epoch 121/2000, Train Loss: 5.745159217439559, Val Loss: 5.717455969255451, Val MAE: 1.6632224321365356\n",
      "Epoch 122/2000, Train Loss: 5.744019804547012, Val Loss: 5.71708365760275, Val MAE: 1.663726568222046\n",
      "Epoch 123/2000, Train Loss: 5.7432188515095985, Val Loss: 5.716314651368518, Val MAE: 1.6625299453735352\n",
      "Epoch 124/2000, Train Loss: 5.742602812561169, Val Loss: 5.715877522763453, Val MAE: 1.6629773378372192\n",
      "Epoch 125/2000, Train Loss: 5.741851244728996, Val Loss: 5.715278318952841, Val MAE: 1.6624350547790527\n",
      "Epoch 126/2000, Train Loss: 5.741676633053414, Val Loss: 5.714910819342262, Val MAE: 1.6628994941711426\n",
      "Epoch 127/2000, Train Loss: 5.740444413365772, Val Loss: 5.714558501107396, Val MAE: 1.6639286279678345\n",
      "Epoch 128/2000, Train Loss: 5.73996871158415, Val Loss: 5.7139109137428585, Val MAE: 1.6613494157791138\n",
      "Epoch 129/2000, Train Loss: 5.738848176296587, Val Loss: 5.713661958206449, Val MAE: 1.6623833179473877\n",
      "Epoch 130/2000, Train Loss: 5.738169288845314, Val Loss: 5.713060043570472, Val MAE: 1.6630953550338745\n",
      "Epoch 131/2000, Train Loss: 5.737803326829415, Val Loss: 5.7127092653199245, Val MAE: 1.661865234375\n",
      "Epoch 132/2000, Train Loss: 5.737170919972894, Val Loss: 5.712170771364988, Val MAE: 1.6627352237701416\n",
      "Epoch 133/2000, Train Loss: 5.736576065618036, Val Loss: 5.711518218661915, Val MAE: 1.6623334884643555\n",
      "Epoch 134/2000, Train Loss: 5.735754105488109, Val Loss: 5.711085208228984, Val MAE: 1.6625782251358032\n",
      "Epoch 135/2000, Train Loss: 5.734958532098107, Val Loss: 5.71061632760223, Val MAE: 1.6618268489837646\n",
      "Epoch 136/2000, Train Loss: 5.734366134399885, Val Loss: 5.710000869838356, Val MAE: 1.6613832712173462\n",
      "Epoch 137/2000, Train Loss: 5.733772004753482, Val Loss: 5.709669108683093, Val MAE: 1.662235975265503\n",
      "Epoch 138/2000, Train Loss: 5.733381664175294, Val Loss: 5.70920451356526, Val MAE: 1.6609365940093994\n",
      "Epoch 139/2000, Train Loss: 5.732607430823574, Val Loss: 5.70891381473076, Val MAE: 1.6610454320907593\n",
      "Epoch 140/2000, Train Loss: 5.732161077633829, Val Loss: 5.708538275895896, Val MAE: 1.6619006395339966\n",
      "Epoch 141/2000, Train Loss: 5.731548524638105, Val Loss: 5.7080999512000625, Val MAE: 1.661380410194397\n",
      "Epoch 142/2000, Train Loss: 5.730857002577593, Val Loss: 5.7075988749025415, Val MAE: 1.6610770225524902\n",
      "Epoch 143/2000, Train Loss: 5.730274081755315, Val Loss: 5.707165027422225, Val MAE: 1.6599774360656738\n",
      "Epoch 144/2000, Train Loss: 5.7296987682712235, Val Loss: 5.706704052157978, Val MAE: 1.6601117849349976\n",
      "Epoch 145/2000, Train Loss: 5.72905559056656, Val Loss: 5.7064033185295635, Val MAE: 1.6611566543579102\n",
      "Epoch 146/2000, Train Loss: 5.7290214147861835, Val Loss: 5.705934759702854, Val MAE: 1.6598255634307861\n",
      "Epoch 147/2000, Train Loss: 5.728001310961887, Val Loss: 5.705414195230592, Val MAE: 1.6597532033920288\n",
      "Epoch 148/2000, Train Loss: 5.727570781623739, Val Loss: 5.7050376434802095, Val MAE: 1.660988688468933\n",
      "Epoch 149/2000, Train Loss: 5.726872278205098, Val Loss: 5.70444560203993, Val MAE: 1.6600571870803833\n",
      "Epoch 150/2000, Train Loss: 5.726137873359714, Val Loss: 5.704257973447079, Val MAE: 1.6615135669708252\n",
      "Epoch 151/2000, Train Loss: 5.725712126047076, Val Loss: 5.703746053427267, Val MAE: 1.6606504917144775\n",
      "Epoch 152/2000, Train Loss: 5.725305406007473, Val Loss: 5.703359218919231, Val MAE: 1.6603952646255493\n",
      "Epoch 153/2000, Train Loss: 5.724702900201739, Val Loss: 5.703045128513515, Val MAE: 1.659702181816101\n",
      "Epoch 154/2000, Train Loss: 5.724260510852158, Val Loss: 5.702468338918624, Val MAE: 1.6603041887283325\n",
      "Epoch 155/2000, Train Loss: 5.724083446721148, Val Loss: 5.702116997771453, Val MAE: 1.660317301750183\n",
      "Epoch 156/2000, Train Loss: 5.723432315078601, Val Loss: 5.70169218496655, Val MAE: 1.6598848104476929\n",
      "Epoch 157/2000, Train Loss: 5.72274385884995, Val Loss: 5.7013219193637905, Val MAE: 1.659254550933838\n",
      "Epoch 158/2000, Train Loss: 5.722915423599109, Val Loss: 5.70081434964139, Val MAE: 1.658563256263733\n",
      "Epoch 159/2000, Train Loss: 5.721852271042207, Val Loss: 5.700779383942312, Val MAE: 1.658677577972412\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 160/2000, Train Loss: 5.7214644837484485, Val Loss: 5.7000434960266135, Val MAE: 1.6585988998413086\n",
      "Epoch 161/2000, Train Loss: 5.72078999758817, Val Loss: 5.699800596310636, Val MAE: 1.658223032951355\n",
      "Epoch 162/2000, Train Loss: 5.72033695397398, Val Loss: 5.699402603893439, Val MAE: 1.6581474542617798\n",
      "Epoch 163/2000, Train Loss: 5.71982279538058, Val Loss: 5.6988571324458634, Val MAE: 1.6591596603393555\n",
      "Epoch 164/2000, Train Loss: 5.719751411597634, Val Loss: 5.698537360393802, Val MAE: 1.6593964099884033\n",
      "Epoch 165/2000, Train Loss: 5.719063398596473, Val Loss: 5.698281075092886, Val MAE: 1.659576654434204\n",
      "Epoch 166/2000, Train Loss: 5.719008493003341, Val Loss: 5.697786188228904, Val MAE: 1.6582469940185547\n",
      "Epoch 167/2000, Train Loss: 5.718026306135539, Val Loss: 5.697587146742965, Val MAE: 1.657973051071167\n",
      "Epoch 168/2000, Train Loss: 5.71808537092503, Val Loss: 5.697008879503787, Val MAE: 1.6571576595306396\n",
      "Epoch 169/2000, Train Loss: 5.7171291790344645, Val Loss: 5.696673337738383, Val MAE: 1.6590808629989624\n",
      "Epoch 170/2000, Train Loss: 5.717474477406641, Val Loss: 5.69649885068809, Val MAE: 1.6600167751312256\n",
      "Epoch 171/2000, Train Loss: 5.71610027787969, Val Loss: 5.6961274647054685, Val MAE: 1.658835530281067\n",
      "Epoch 172/2000, Train Loss: 5.715871412848586, Val Loss: 5.695766239669724, Val MAE: 1.6578054428100586\n",
      "Epoch 173/2000, Train Loss: 5.71560217315405, Val Loss: 5.695212562005389, Val MAE: 1.6575554609298706\n",
      "Epoch 174/2000, Train Loss: 5.714796467499586, Val Loss: 5.694949542948156, Val MAE: 1.6578114032745361\n",
      "Epoch 175/2000, Train Loss: 5.7149752121139725, Val Loss: 5.694429552849664, Val MAE: 1.658092975616455\n",
      "Epoch 176/2000, Train Loss: 5.7139901784023, Val Loss: 5.69401039542757, Val MAE: 1.6571277379989624\n",
      "Epoch 177/2000, Train Loss: 5.713821434239459, Val Loss: 5.693721728086166, Val MAE: 1.658632755279541\n",
      "Epoch 178/2000, Train Loss: 5.713184630818304, Val Loss: 5.693405246837913, Val MAE: 1.6577088832855225\n",
      "Epoch 179/2000, Train Loss: 5.713362388148707, Val Loss: 5.6930977799313975, Val MAE: 1.6566437482833862\n",
      "Epoch 180/2000, Train Loss: 5.712491915089443, Val Loss: 5.692597822426066, Val MAE: 1.6580572128295898\n",
      "Epoch 181/2000, Train Loss: 5.712062835693359, Val Loss: 5.692325499458705, Val MAE: 1.657321810722351\n",
      "Epoch 182/2000, Train Loss: 5.711720508625854, Val Loss: 5.69174575484304, Val MAE: 1.6567590236663818\n",
      "Epoch 183/2000, Train Loss: 5.711149216748544, Val Loss: 5.691599763805508, Val MAE: 1.6592460870742798\n",
      "Epoch 184/2000, Train Loss: 5.710564300352257, Val Loss: 5.691213147133704, Val MAE: 1.6577526330947876\n",
      "Epoch 185/2000, Train Loss: 5.710377702628988, Val Loss: 5.690964952742487, Val MAE: 1.6576783657073975\n",
      "Epoch 186/2000, Train Loss: 5.709776949777477, Val Loss: 5.690228680285012, Val MAE: 1.6568126678466797\n",
      "Epoch 187/2000, Train Loss: 5.70970357987324, Val Loss: 5.690428241704793, Val MAE: 1.6569753885269165\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 188/2000, Train Loss: 5.709462260359709, Val Loss: 5.689835183542408, Val MAE: 1.6557215452194214\n",
      "Epoch 189/2000, Train Loss: 5.7086487688157, Val Loss: 5.689427327838115, Val MAE: 1.6569887399673462\n",
      "Epoch 190/2000, Train Loss: 5.708542955079268, Val Loss: 5.6892223331565575, Val MAE: 1.6565436124801636\n",
      "Epoch 191/2000, Train Loss: 5.708867270515879, Val Loss: 5.68898097032607, Val MAE: 1.6572285890579224\n",
      "Epoch 192/2000, Train Loss: 5.70784864341635, Val Loss: 5.688550508680331, Val MAE: 1.655703067779541\n",
      "Epoch 193/2000, Train Loss: 5.707699493164533, Val Loss: 5.688286802849812, Val MAE: 1.654970407485962\n",
      "Epoch 194/2000, Train Loss: 5.706824562097961, Val Loss: 5.6878457749417874, Val MAE: 1.6549330949783325\n",
      "Epoch 195/2000, Train Loss: 5.706359138572794, Val Loss: 5.687246488380953, Val MAE: 1.6561312675476074\n",
      "Epoch 196/2000, Train Loss: 5.706314972318742, Val Loss: 5.687224789860925, Val MAE: 1.6563547849655151\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 197/2000, Train Loss: 5.705744855729494, Val Loss: 5.686712692863522, Val MAE: 1.6565743684768677\n",
      "Epoch 198/2000, Train Loss: 5.705324872474838, Val Loss: 5.686544718959671, Val MAE: 1.6559115648269653\n",
      "Epoch 199/2000, Train Loss: 5.705003298326736, Val Loss: 5.686130157924127, Val MAE: 1.6556885242462158\n",
      "Epoch 200/2000, Train Loss: 5.704705039835186, Val Loss: 5.685986461761062, Val MAE: 1.6557908058166504\n",
      "Epoch 201/2000, Train Loss: 5.704276673069084, Val Loss: 5.6853863430688865, Val MAE: 1.654852271080017\n",
      "Epoch 202/2000, Train Loss: 5.704121868515855, Val Loss: 5.685330837792854, Val MAE: 1.6557648181915283\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 203/2000, Train Loss: 5.7038922940056755, Val Loss: 5.684856866478002, Val MAE: 1.6551374197006226\n",
      "Epoch 204/2000, Train Loss: 5.70355599829804, Val Loss: 5.684575186270039, Val MAE: 1.6543536186218262\n",
      "Epoch 205/2000, Train Loss: 5.702843229150982, Val Loss: 5.684191161749911, Val MAE: 1.6558626890182495\n",
      "Epoch 206/2000, Train Loss: 5.702578152854012, Val Loss: 5.683757260250494, Val MAE: 1.6551882028579712\n",
      "Epoch 207/2000, Train Loss: 5.702166545758688, Val Loss: 5.683228051670096, Val MAE: 1.65394926071167\n",
      "Epoch 208/2000, Train Loss: 5.702609364681832, Val Loss: 5.683374169396191, Val MAE: 1.6528196334838867\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 209/2000, Train Loss: 5.7013781018194125, Val Loss: 5.682984219914223, Val MAE: 1.656493067741394\n",
      "Epoch 210/2000, Train Loss: 5.701422869896573, Val Loss: 5.682551590885981, Val MAE: 1.6538280248641968\n",
      "Epoch 211/2000, Train Loss: 5.700782551114255, Val Loss: 5.682303177338663, Val MAE: 1.6541308164596558\n",
      "Epoch 212/2000, Train Loss: 5.700492099518293, Val Loss: 5.6818228623183, Val MAE: 1.655579686164856\n",
      "Epoch 213/2000, Train Loss: 5.700087084119016, Val Loss: 5.681678585698577, Val MAE: 1.6545629501342773\n",
      "Epoch 214/2000, Train Loss: 5.699627506575395, Val Loss: 5.681202444380606, Val MAE: 1.654068112373352\n",
      "Epoch 215/2000, Train Loss: 5.699426469298712, Val Loss: 5.68083686327062, Val MAE: 1.6537318229675293\n",
      "Epoch 216/2000, Train Loss: 5.699289433231438, Val Loss: 5.680543148146514, Val MAE: 1.6548478603363037\n",
      "Epoch 217/2000, Train Loss: 5.6987830930869485, Val Loss: 5.68006255335159, Val MAE: 1.6529260873794556\n",
      "Epoch 218/2000, Train Loss: 5.698655534945921, Val Loss: 5.68004536777926, Val MAE: 1.653365969657898\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 219/2000, Train Loss: 5.6980431647027645, Val Loss: 5.679543049252844, Val MAE: 1.655020833015442\n",
      "Epoch 220/2000, Train Loss: 5.69754571011413, Val Loss: 5.679193894348953, Val MAE: 1.654050588607788\n",
      "Epoch 221/2000, Train Loss: 5.697372496390658, Val Loss: 5.6791059699803785, Val MAE: 1.6533139944076538\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 222/2000, Train Loss: 5.697684922407377, Val Loss: 5.67866273867074, Val MAE: 1.6530914306640625\n",
      "Epoch 223/2000, Train Loss: 5.696892069299841, Val Loss: 5.6782488992607485, Val MAE: 1.6540546417236328\n",
      "Epoch 224/2000, Train Loss: 5.696533329161253, Val Loss: 5.678087394679771, Val MAE: 1.6540236473083496\n",
      "Epoch 225/2000, Train Loss: 5.696391584589618, Val Loss: 5.677804765732665, Val MAE: 1.6534594297409058\n",
      "Epoch 226/2000, Train Loss: 5.695859785121968, Val Loss: 5.677384284211368, Val MAE: 1.6522656679153442\n",
      "Epoch 227/2000, Train Loss: 5.695536509484446, Val Loss: 5.677259955081769, Val MAE: 1.6512196063995361\n",
      "Epoch 228/2000, Train Loss: 5.695112079250655, Val Loss: 5.677039284378634, Val MAE: 1.6522613763809204\n",
      "Epoch 229/2000, Train Loss: 5.695094545507221, Val Loss: 5.676421390973099, Val MAE: 1.6517566442489624\n",
      "Epoch 230/2000, Train Loss: 5.694592702756369, Val Loss: 5.676279442913266, Val MAE: 1.6524696350097656\n",
      "Epoch 231/2000, Train Loss: 5.694342897852087, Val Loss: 5.676528636409168, Val MAE: 1.6507015228271484\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 232/2000, Train Loss: 5.693887335613436, Val Loss: 5.6756016505889315, Val MAE: 1.6524924039840698\n",
      "Epoch 233/2000, Train Loss: 5.69383185130384, Val Loss: 5.675224801236154, Val MAE: 1.6526577472686768\n",
      "Epoch 234/2000, Train Loss: 5.693477726192726, Val Loss: 5.674924177329256, Val MAE: 1.651560664176941\n",
      "Epoch 235/2000, Train Loss: 5.692880472947848, Val Loss: 5.67476483948041, Val MAE: 1.6530812978744507\n",
      "Epoch 236/2000, Train Loss: 5.692784860795815, Val Loss: 5.674567173300873, Val MAE: 1.6532456874847412\n",
      "Epoch 237/2000, Train Loss: 5.692472316094957, Val Loss: 5.674115139383713, Val MAE: 1.6511180400848389\n",
      "Epoch 238/2000, Train Loss: 5.691958832845814, Val Loss: 5.674034927542464, Val MAE: 1.6525161266326904\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 239/2000, Train Loss: 5.691870879496772, Val Loss: 5.673606768958223, Val MAE: 1.6528074741363525\n",
      "Epoch 240/2000, Train Loss: 5.691624566847008, Val Loss: 5.673082710785339, Val MAE: 1.6523023843765259\n",
      "Epoch 241/2000, Train Loss: 5.691390189830427, Val Loss: 5.672863261939166, Val MAE: 1.650620937347412\n",
      "Epoch 242/2000, Train Loss: 5.691193451440282, Val Loss: 5.672591296676187, Val MAE: 1.6511341333389282\n",
      "Epoch 243/2000, Train Loss: 5.690641707785854, Val Loss: 5.6724863256278795, Val MAE: 1.6504261493682861\n",
      "Epoch 244/2000, Train Loss: 5.690502390462396, Val Loss: 5.672006330399061, Val MAE: 1.6511424779891968\n",
      "Epoch 245/2000, Train Loss: 5.690133556920526, Val Loss: 5.6719843649780035, Val MAE: 1.6501210927963257\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 246/2000, Train Loss: 5.6896858110301824, Val Loss: 5.671695202569906, Val MAE: 1.6502023935317993\n",
      "Epoch 247/2000, Train Loss: 5.6897531196409386, Val Loss: 5.671387299512715, Val MAE: 1.6508384943008423\n",
      "Epoch 248/2000, Train Loss: 5.689289170739935, Val Loss: 5.671405824678394, Val MAE: 1.6508471965789795\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 249/2000, Train Loss: 5.688726452478753, Val Loss: 5.670848005256389, Val MAE: 1.650335431098938\n",
      "Epoch 250/2000, Train Loss: 5.6884895389825765, Val Loss: 5.670551278031836, Val MAE: 1.6492714881896973\n",
      "Epoch 251/2000, Train Loss: 5.688649766770753, Val Loss: 5.670610660134751, Val MAE: 1.6510343551635742\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 252/2000, Train Loss: 5.688590047643048, Val Loss: 5.670250593984694, Val MAE: 1.650868535041809\n",
      "Epoch 253/2000, Train Loss: 5.687686085175837, Val Loss: 5.669923813891656, Val MAE: 1.649756908416748\n",
      "Epoch 254/2000, Train Loss: 5.687457472217241, Val Loss: 5.669780526200216, Val MAE: 1.6492691040039062\n",
      "Epoch 255/2000, Train Loss: 5.6870640384993365, Val Loss: 5.669327931720118, Val MAE: 1.6507155895233154\n",
      "Epoch 256/2000, Train Loss: 5.687011357446074, Val Loss: 5.669298962982842, Val MAE: 1.6482914686203003\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 257/2000, Train Loss: 5.686399125842796, Val Loss: 5.669053726290402, Val MAE: 1.6495754718780518\n",
      "Epoch 258/2000, Train Loss: 5.6863275425024495, Val Loss: 5.668727461731633, Val MAE: 1.6488815546035767\n",
      "Epoch 259/2000, Train Loss: 5.686281796594023, Val Loss: 5.668440550049248, Val MAE: 1.6480084657669067\n",
      "Epoch 260/2000, Train Loss: 5.685911766758049, Val Loss: 5.66773407884296, Val MAE: 1.6488085985183716\n",
      "Epoch 261/2000, Train Loss: 5.685226614779838, Val Loss: 5.667887602809611, Val MAE: 1.6478254795074463\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 262/2000, Train Loss: 5.6851710853072515, Val Loss: 5.667422887774732, Val MAE: 1.6492987871170044\n",
      "Epoch 263/2000, Train Loss: 5.685238855000635, Val Loss: 5.667488378320793, Val MAE: 1.64888596534729\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 264/2000, Train Loss: 5.68469901599548, Val Loss: 5.666854436269008, Val MAE: 1.648738980293274\n",
      "Epoch 265/2000, Train Loss: 5.684100694068203, Val Loss: 5.666806392419476, Val MAE: 1.6497398614883423\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 266/2000, Train Loss: 5.6844406253965944, Val Loss: 5.666236925683677, Val MAE: 1.6492023468017578\n",
      "Epoch 267/2000, Train Loss: 5.683859793625214, Val Loss: 5.666246765844323, Val MAE: 1.6485391855239868\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 268/2000, Train Loss: 5.683230496713243, Val Loss: 5.666016283424736, Val MAE: 1.6479376554489136\n",
      "Epoch 269/2000, Train Loss: 5.683226895227306, Val Loss: 5.665804327108129, Val MAE: 1.6497195959091187\n",
      "Epoch 270/2000, Train Loss: 5.682771817177928, Val Loss: 5.6651657546209275, Val MAE: 1.650139331817627\n",
      "Epoch 271/2000, Train Loss: 5.682438518507365, Val Loss: 5.664825733248933, Val MAE: 1.6491509675979614\n",
      "Epoch 272/2000, Train Loss: 5.682119585869071, Val Loss: 5.664903921191132, Val MAE: 1.6489088535308838\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 273/2000, Train Loss: 5.682205714843347, Val Loss: 5.664733406974048, Val MAE: 1.6487886905670166\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 274/2000, Train Loss: 5.681979899889572, Val Loss: 5.664422720594186, Val MAE: 1.6481167078018188\n",
      "Epoch 275/2000, Train Loss: 5.681922407402341, Val Loss: 5.664025592214795, Val MAE: 1.6492103338241577\n",
      "Epoch 276/2000, Train Loss: 5.681237003351623, Val Loss: 5.663896162316337, Val MAE: 1.649216890335083\n",
      "Epoch 277/2000, Train Loss: 5.680880006714539, Val Loss: 5.663631752898513, Val MAE: 1.6486703157424927\n",
      "Epoch 278/2000, Train Loss: 5.680832191710955, Val Loss: 5.663389546134352, Val MAE: 1.64735746383667\n",
      "Epoch 279/2000, Train Loss: 5.680183946298608, Val Loss: 5.66319637991421, Val MAE: 1.6481753587722778\n",
      "Epoch 280/2000, Train Loss: 5.680161251370603, Val Loss: 5.663116523501044, Val MAE: 1.6475017070770264\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 281/2000, Train Loss: 5.680629991749835, Val Loss: 5.663232778726249, Val MAE: 1.6458773612976074\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 282/2000, Train Loss: 5.679549264487716, Val Loss: 5.662985810706337, Val MAE: 1.647721529006958\n",
      "Epoch 283/2000, Train Loss: 5.679305348627368, Val Loss: 5.662546697951404, Val MAE: 1.6495494842529297\n",
      "Epoch 284/2000, Train Loss: 5.679162850989119, Val Loss: 5.662303374671355, Val MAE: 1.6485930681228638\n",
      "Epoch 285/2000, Train Loss: 5.678817201816038, Val Loss: 5.661956306924716, Val MAE: 1.6481159925460815\n",
      "Epoch 286/2000, Train Loss: 5.678438781116502, Val Loss: 5.661730075563485, Val MAE: 1.6473641395568848\n",
      "Epoch 287/2000, Train Loss: 5.678355125603697, Val Loss: 5.6613243002258, Val MAE: 1.6472915410995483\n",
      "Epoch 288/2000, Train Loss: 5.677852371190613, Val Loss: 5.6612544433143235, Val MAE: 1.6467087268829346\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 289/2000, Train Loss: 5.677940198503402, Val Loss: 5.660968259168222, Val MAE: 1.6461524963378906\n",
      "Epoch 290/2000, Train Loss: 5.677454490493573, Val Loss: 5.661009938653382, Val MAE: 1.6466106176376343\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 291/2000, Train Loss: 5.6773741591869475, Val Loss: 5.660868413985464, Val MAE: 1.6458396911621094\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 292/2000, Train Loss: 5.676816652524839, Val Loss: 5.660384162629217, Val MAE: 1.6466443538665771\n",
      "Epoch 293/2000, Train Loss: 5.676927287147959, Val Loss: 5.6599234649397445, Val MAE: 1.6485671997070312\n",
      "Epoch 294/2000, Train Loss: 5.676226434203497, Val Loss: 5.659950268991645, Val MAE: 1.6462820768356323\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 295/2000, Train Loss: 5.6761398966617, Val Loss: 5.659991943484246, Val MAE: 1.6475201845169067\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 296/2000, Train Loss: 5.675808942265448, Val Loss: 5.659691904089264, Val MAE: 1.647537112236023\n",
      "Epoch 297/2000, Train Loss: 5.675309401776822, Val Loss: 5.659583303584826, Val MAE: 1.6480494737625122\n",
      "Epoch 298/2000, Train Loss: 5.675263541910617, Val Loss: 5.659453431931019, Val MAE: 1.6474944353103638\n",
      "Epoch 299/2000, Train Loss: 5.67498754404715, Val Loss: 5.659178436051131, Val MAE: 1.646195411682129\n",
      "Epoch 300/2000, Train Loss: 5.674760826883862, Val Loss: 5.658764780964105, Val MAE: 1.6464993953704834\n",
      "Epoch 301/2000, Train Loss: 5.675035872648466, Val Loss: 5.658867652804156, Val MAE: 1.6443579196929932\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 302/2000, Train Loss: 5.674753210092956, Val Loss: 5.658187953303347, Val MAE: 1.646239161491394\n",
      "Epoch 303/2000, Train Loss: 5.673998134251733, Val Loss: 5.658082522710114, Val MAE: 1.6448665857315063\n",
      "Epoch 304/2000, Train Loss: 5.673785879748508, Val Loss: 5.65763346770532, Val MAE: 1.645907998085022\n",
      "Epoch 305/2000, Train Loss: 5.674033001130898, Val Loss: 5.657389153373716, Val MAE: 1.6466833353042603\n",
      "Epoch 306/2000, Train Loss: 5.673113377608916, Val Loss: 5.657234412007062, Val MAE: 1.6462575197219849\n",
      "Epoch 307/2000, Train Loss: 5.672965385840328, Val Loss: 5.657209948049584, Val MAE: 1.647251844406128\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 308/2000, Train Loss: 5.673045915654052, Val Loss: 5.656600519536548, Val MAE: 1.646608591079712\n",
      "Epoch 309/2000, Train Loss: 5.673230028362526, Val Loss: 5.656510183819604, Val MAE: 1.6468143463134766\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 310/2000, Train Loss: 5.672202622837958, Val Loss: 5.656549851686259, Val MAE: 1.6459293365478516\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 311/2000, Train Loss: 5.672585224790195, Val Loss: 5.656559732762932, Val MAE: 1.6448631286621094\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 312/2000, Train Loss: 5.671752707023453, Val Loss: 5.655940755015773, Val MAE: 1.6473186016082764\n",
      "Epoch 313/2000, Train Loss: 5.6712178238688065, Val Loss: 5.65595544365848, Val MAE: 1.6465885639190674\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 314/2000, Train Loss: 5.671406141986931, Val Loss: 5.655788549983608, Val MAE: 1.6445869207382202\n",
      "Epoch 315/2000, Train Loss: 5.671170877465068, Val Loss: 5.655632434362625, Val MAE: 1.6455488204956055\n",
      "Epoch 316/2000, Train Loss: 5.671000138253367, Val Loss: 5.6551765373146425, Val MAE: 1.6452571153640747\n",
      "Epoch 317/2000, Train Loss: 5.670498580134388, Val Loss: 5.655003965245285, Val MAE: 1.6455466747283936\n",
      "Epoch 318/2000, Train Loss: 5.670205858835565, Val Loss: 5.654721793031203, Val MAE: 1.646478533744812\n",
      "Epoch 319/2000, Train Loss: 5.670474167962431, Val Loss: 5.655229431960794, Val MAE: 1.6441078186035156\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 320/2000, Train Loss: 5.669720796761534, Val Loss: 5.654361845420934, Val MAE: 1.6460762023925781\n",
      "Epoch 321/2000, Train Loss: 5.669259733040428, Val Loss: 5.654437608758659, Val MAE: 1.6443922519683838\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 322/2000, Train Loss: 5.66928920451765, Val Loss: 5.654211198698265, Val MAE: 1.6447792053222656\n",
      "Epoch 323/2000, Train Loss: 5.669102817905107, Val Loss: 5.653901700203011, Val MAE: 1.6440590620040894\n",
      "Epoch 324/2000, Train Loss: 5.668674859181375, Val Loss: 5.653778729144967, Val MAE: 1.644867181777954\n",
      "Epoch 325/2000, Train Loss: 5.668596259297779, Val Loss: 5.653765002707921, Val MAE: 1.643740177154541\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 326/2000, Train Loss: 5.6683537203834975, Val Loss: 5.653612539057401, Val MAE: 1.6448982954025269\n",
      "Epoch 327/2000, Train Loss: 5.668166282943692, Val Loss: 5.652981241933036, Val MAE: 1.6438453197479248\n",
      "Epoch 328/2000, Train Loss: 5.667702511018593, Val Loss: 5.652886955495517, Val MAE: 1.6462132930755615\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 329/2000, Train Loss: 5.667895432610869, Val Loss: 5.652934132897043, Val MAE: 1.6444581747055054\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 330/2000, Train Loss: 5.667390050341904, Val Loss: 5.652591943453916, Val MAE: 1.6449228525161743\n",
      "Epoch 331/2000, Train Loss: 5.66705362072075, Val Loss: 5.652615144753181, Val MAE: 1.6442816257476807\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 332/2000, Train Loss: 5.666857987248425, Val Loss: 5.65273194110669, Val MAE: 1.6448514461517334\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 333/2000, Train Loss: 5.666594890770933, Val Loss: 5.651875745366413, Val MAE: 1.6449123620986938\n",
      "Epoch 334/2000, Train Loss: 5.666323501107977, Val Loss: 5.651870965728099, Val MAE: 1.6450986862182617\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 335/2000, Train Loss: 5.6669009309508205, Val Loss: 5.651615437593631, Val MAE: 1.6438517570495605\n",
      "Epoch 336/2000, Train Loss: 5.6656442665318565, Val Loss: 5.651558856736864, Val MAE: 1.6447978019714355\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 337/2000, Train Loss: 5.665687203932439, Val Loss: 5.651729572550017, Val MAE: 1.6453614234924316\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 338/2000, Train Loss: 5.665651403334698, Val Loss: 5.651307201048835, Val MAE: 1.644078016281128\n",
      "Epoch 339/2000, Train Loss: 5.665015608203569, Val Loss: 5.6505340768528844, Val MAE: 1.6449267864227295\n",
      "Epoch 340/2000, Train Loss: 5.664758591399844, Val Loss: 5.650995853623622, Val MAE: 1.6431585550308228\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 341/2000, Train Loss: 5.664427191150346, Val Loss: 5.6509689928172335, Val MAE: 1.6451210975646973\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 342/2000, Train Loss: 5.664414253528948, Val Loss: 5.650759312667344, Val MAE: 1.645058274269104\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 343/2000, Train Loss: 5.663923401139381, Val Loss: 5.6504059460525795, Val MAE: 1.6450754404067993\n",
      "Epoch 344/2000, Train Loss: 5.663757949148506, Val Loss: 5.650197120184923, Val MAE: 1.6449576616287231\n",
      "Epoch 345/2000, Train Loss: 5.6638958370107915, Val Loss: 5.650020811768949, Val MAE: 1.643601655960083\n",
      "Epoch 346/2000, Train Loss: 5.6635349353504605, Val Loss: 5.649909568995964, Val MAE: 1.6423861980438232\n",
      "Epoch 347/2000, Train Loss: 5.663622358296936, Val Loss: 5.64937234902336, Val MAE: 1.6428711414337158\n",
      "Epoch 348/2000, Train Loss: 5.663330607477263, Val Loss: 5.649557800879833, Val MAE: 1.6428059339523315\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 349/2000, Train Loss: 5.662763612385889, Val Loss: 5.6494027305399195, Val MAE: 1.644387125968933\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 350/2000, Train Loss: 5.662461199949491, Val Loss: 5.649202707987541, Val MAE: 1.6440470218658447\n",
      "Epoch 351/2000, Train Loss: 5.6623468294017645, Val Loss: 5.648475554983332, Val MAE: 1.6438580751419067\n",
      "Epoch 352/2000, Train Loss: 5.661887471371285, Val Loss: 5.648920784961428, Val MAE: 1.642736554145813\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 353/2000, Train Loss: 5.662008266617023, Val Loss: 5.648399598485239, Val MAE: 1.6437857151031494\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 354/2000, Train Loss: 5.661160022676778, Val Loss: 5.648068802098867, Val MAE: 1.643869161605835\n",
      "Epoch 355/2000, Train Loss: 5.661885209020539, Val Loss: 5.648269216404493, Val MAE: 1.643521785736084\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 356/2000, Train Loss: 5.661559240933557, Val Loss: 5.647880698696486, Val MAE: 1.6436647176742554\n",
      "Epoch 357/2000, Train Loss: 5.660930178764108, Val Loss: 5.647705416490392, Val MAE: 1.64458167552948\n",
      "Epoch 358/2000, Train Loss: 5.6606379458557665, Val Loss: 5.647626481175576, Val MAE: 1.6434053182601929\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 359/2000, Train Loss: 5.660153885769949, Val Loss: 5.64771465832829, Val MAE: 1.6431883573532104\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 360/2000, Train Loss: 5.6600030791917035, Val Loss: 5.647113715557905, Val MAE: 1.644706130027771\n",
      "Epoch 361/2000, Train Loss: 5.659330230452416, Val Loss: 5.647301007296522, Val MAE: 1.6435415744781494\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 362/2000, Train Loss: 5.65965269630701, Val Loss: 5.647316827834494, Val MAE: 1.6425914764404297\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 363/2000, Train Loss: 5.659857466357395, Val Loss: 5.646909098015364, Val MAE: 1.6438935995101929\n",
      "Epoch 364/2000, Train Loss: 5.6592753332617, Val Loss: 5.646828076040638, Val MAE: 1.6431560516357422\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 365/2000, Train Loss: 5.6591134869579705, Val Loss: 5.647010067881913, Val MAE: 1.6440634727478027\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 366/2000, Train Loss: 5.658799888804095, Val Loss: 5.6464460659891715, Val MAE: 1.6436264514923096\n",
      "Epoch 367/2000, Train Loss: 5.658178654011126, Val Loss: 5.646714276982058, Val MAE: 1.6422384977340698\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 368/2000, Train Loss: 5.658199078185968, Val Loss: 5.646570846036708, Val MAE: 1.6414469480514526\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 369/2000, Train Loss: 5.657876301441949, Val Loss: 5.646079764420475, Val MAE: 1.6419727802276611\n",
      "Epoch 370/2000, Train Loss: 5.657374571073423, Val Loss: 5.645992397841195, Val MAE: 1.6418242454528809\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 371/2000, Train Loss: 5.657072753108021, Val Loss: 5.645873180881085, Val MAE: 1.6429312229156494\n",
      "Epoch 372/2000, Train Loss: 5.657392488177127, Val Loss: 5.645412243701833, Val MAE: 1.6426304578781128\n",
      "Epoch 373/2000, Train Loss: 5.65684534274534, Val Loss: 5.645329674493057, Val MAE: 1.6419264078140259\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 374/2000, Train Loss: 5.65647092054594, Val Loss: 5.645911389103444, Val MAE: 1.6425750255584717\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 375/2000, Train Loss: 5.656251926254071, Val Loss: 5.645548627430881, Val MAE: 1.641401767730713\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 376/2000, Train Loss: 5.655867468418004, Val Loss: 5.645030994126059, Val MAE: 1.6432592868804932\n",
      "Epoch 377/2000, Train Loss: 5.6556409581642315, Val Loss: 5.645313759804231, Val MAE: 1.641799807548523\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 378/2000, Train Loss: 5.6555374519415365, Val Loss: 5.645069115351651, Val MAE: 1.640440821647644\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 379/2000, Train Loss: 5.655452869011967, Val Loss: 5.644886034042294, Val MAE: 1.642350435256958\n",
      "Epoch 380/2000, Train Loss: 5.655176536627278, Val Loss: 5.644915873148934, Val MAE: 1.641318440437317\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 381/2000, Train Loss: 5.654785805336704, Val Loss: 5.644596775800028, Val MAE: 1.641977310180664\n",
      "Epoch 382/2000, Train Loss: 5.654773056769686, Val Loss: 5.644538457418131, Val MAE: 1.6406992673873901\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 383/2000, Train Loss: 5.654234250736657, Val Loss: 5.644434665837857, Val MAE: 1.641732096672058\n",
      "Epoch 384/2000, Train Loss: 5.653970422198594, Val Loss: 5.644267101285546, Val MAE: 1.6426188945770264\n",
      "Epoch 385/2000, Train Loss: 5.653810883408601, Val Loss: 5.643862925383797, Val MAE: 1.6409598588943481\n",
      "Epoch 386/2000, Train Loss: 5.653380834058518, Val Loss: 5.643926175839635, Val MAE: 1.6422597169876099\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 387/2000, Train Loss: 5.653463875144589, Val Loss: 5.643822391642992, Val MAE: 1.640971302986145\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 388/2000, Train Loss: 5.652887721418809, Val Loss: 5.643812932925291, Val MAE: 1.641135573387146\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 389/2000, Train Loss: 5.6532958215553855, Val Loss: 5.643755736950886, Val MAE: 1.6410365104675293\n",
      "Epoch 390/2000, Train Loss: 5.65319338033903, Val Loss: 5.642957779654183, Val MAE: 1.641347885131836\n",
      "Epoch 391/2000, Train Loss: 5.652474656504157, Val Loss: 5.643187802337559, Val MAE: 1.6420204639434814\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 392/2000, Train Loss: 5.652165940679643, Val Loss: 5.643007737088265, Val MAE: 1.6419669389724731\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 393/2000, Train Loss: 5.6515193941309585, Val Loss: 5.643291363016785, Val MAE: 1.6410653591156006\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 394/2000, Train Loss: 5.651717537825328, Val Loss: 5.643141975815245, Val MAE: 1.6410411596298218\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 395/2000, Train Loss: 5.651502827715769, Val Loss: 5.642616792422969, Val MAE: 1.6406769752502441\n",
      "Epoch 396/2000, Train Loss: 5.651054421185397, Val Loss: 5.642792941191842, Val MAE: 1.6418416500091553\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 397/2000, Train Loss: 5.65108987938465, Val Loss: 5.64233867778169, Val MAE: 1.6422110795974731\n",
      "Epoch 398/2000, Train Loss: 5.650836107489296, Val Loss: 5.642411666653276, Val MAE: 1.6418153047561646\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 399/2000, Train Loss: 5.650526194845527, Val Loss: 5.6427431600658515, Val MAE: 1.6405149698257446\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 400/2000, Train Loss: 5.650024659833194, Val Loss: 5.642295685578831, Val MAE: 1.6409378051757812\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 401/2000, Train Loss: 5.6498946607900615, Val Loss: 5.642020162037157, Val MAE: 1.6405764818191528\n",
      "Epoch 402/2000, Train Loss: 5.650318733921135, Val Loss: 5.642261758982293, Val MAE: 1.6402840614318848\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 403/2000, Train Loss: 5.649329141373151, Val Loss: 5.642153508894251, Val MAE: 1.64024019241333\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 404/2000, Train Loss: 5.649480630647768, Val Loss: 5.6416318903035165, Val MAE: 1.640685796737671\n",
      "Epoch 405/2000, Train Loss: 5.649177298146722, Val Loss: 5.641762222229134, Val MAE: 1.6413360834121704\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 406/2000, Train Loss: 5.648411882081221, Val Loss: 5.641677785879994, Val MAE: 1.6412038803100586\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 407/2000, Train Loss: 5.648831456768355, Val Loss: 5.641727324015675, Val MAE: 1.641627550125122\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 408/2000, Train Loss: 5.648164447708802, Val Loss: 5.64197899426055, Val MAE: 1.6406453847885132\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 409/2000, Train Loss: 5.647840611209953, Val Loss: 5.641476876316389, Val MAE: 1.6402523517608643\n",
      "Epoch 410/2000, Train Loss: 5.647619896523228, Val Loss: 5.641751859322745, Val MAE: 1.6398468017578125\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 411/2000, Train Loss: 5.647489364976925, Val Loss: 5.640872344432043, Val MAE: 1.6411200761795044\n",
      "Epoch 412/2000, Train Loss: 5.647781858360189, Val Loss: 5.640418269820942, Val MAE: 1.6397100687026978\n",
      "Epoch 413/2000, Train Loss: 5.647047113216921, Val Loss: 5.640718683198726, Val MAE: 1.6407462358474731\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 414/2000, Train Loss: 5.646458169962341, Val Loss: 5.64080111548515, Val MAE: 1.6405924558639526\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 415/2000, Train Loss: 5.64631787585792, Val Loss: 5.640510947946866, Val MAE: 1.6410969495773315\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 416/2000, Train Loss: 5.646205407407315, Val Loss: 5.640623716223836, Val MAE: 1.640313982963562\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 417/2000, Train Loss: 5.645999081859505, Val Loss: 5.640432082087911, Val MAE: 1.6397157907485962\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 418/2000, Train Loss: 5.645564980443879, Val Loss: 5.639905317240563, Val MAE: 1.6420378684997559\n",
      "Epoch 419/2000, Train Loss: 5.645379083272119, Val Loss: 5.639629893626573, Val MAE: 1.6424345970153809\n",
      "Epoch 420/2000, Train Loss: 5.645063679648916, Val Loss: 5.63968865387859, Val MAE: 1.6403992176055908\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 421/2000, Train Loss: 5.645552094287284, Val Loss: 5.639655046078911, Val MAE: 1.6407591104507446\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 422/2000, Train Loss: 5.644488287392167, Val Loss: 5.639563798961805, Val MAE: 1.6406216621398926\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 423/2000, Train Loss: 5.644135052412092, Val Loss: 5.639633266740647, Val MAE: 1.639349102973938\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 424/2000, Train Loss: 5.644193644040482, Val Loss: 5.640010966109984, Val MAE: 1.6381293535232544\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 425/2000, Train Loss: 5.644405421706549, Val Loss: 5.639889356585767, Val MAE: 1.6383252143859863\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 426/2000, Train Loss: 5.643759202326972, Val Loss: 5.639498206670845, Val MAE: 1.6392015218734741\n",
      "Epoch 427/2000, Train Loss: 5.643445769070529, Val Loss: 5.6389860259094045, Val MAE: 1.6394716501235962\n",
      "Epoch 428/2000, Train Loss: 5.643279244721199, Val Loss: 5.6393057489157945, Val MAE: 1.639719009399414\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 429/2000, Train Loss: 5.6428971049018894, Val Loss: 5.638844861451713, Val MAE: 1.6405826807022095\n",
      "Epoch 430/2000, Train Loss: 5.642671183867601, Val Loss: 5.639081801181427, Val MAE: 1.6392014026641846\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 431/2000, Train Loss: 5.642621039293936, Val Loss: 5.638700665937155, Val MAE: 1.640406608581543\n",
      "Epoch 432/2000, Train Loss: 5.642223194307167, Val Loss: 5.639329177309521, Val MAE: 1.6380865573883057\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 433/2000, Train Loss: 5.641991684615349, Val Loss: 5.638129460930212, Val MAE: 1.6389278173446655\n",
      "Epoch 434/2000, Train Loss: 5.641638061548645, Val Loss: 5.638433330019264, Val MAE: 1.6399997472763062\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 435/2000, Train Loss: 5.641752861669936, Val Loss: 5.6386123813200975, Val MAE: 1.638502597808838\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 436/2000, Train Loss: 5.641271337013412, Val Loss: 5.639052313084168, Val MAE: 1.6373480558395386\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 437/2000, Train Loss: 5.640874755540083, Val Loss: 5.638241142742288, Val MAE: 1.638991355895996\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 438/2000, Train Loss: 5.640614659775721, Val Loss: 5.638112400018817, Val MAE: 1.6395537853240967\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 439/2000, Train Loss: 5.640475503148487, Val Loss: 5.6386185342945705, Val MAE: 1.637963891029358\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 440/2000, Train Loss: 5.640066389470373, Val Loss: 5.638394663724881, Val MAE: 1.6383544206619263\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 441/2000, Train Loss: 5.64018268522187, Val Loss: 5.637317213381668, Val MAE: 1.6395808458328247\n",
      "Epoch 442/2000, Train Loss: 5.639974054260926, Val Loss: 5.637766158358889, Val MAE: 1.6397173404693604\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 443/2000, Train Loss: 5.639431855752079, Val Loss: 5.638040671816074, Val MAE: 1.6380927562713623\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 444/2000, Train Loss: 5.639254446071675, Val Loss: 5.638079423251752, Val MAE: 1.6382170915603638\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 445/2000, Train Loss: 5.639048081662686, Val Loss: 5.6385519319573785, Val MAE: 1.6376844644546509\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 446/2000, Train Loss: 5.63869111863527, Val Loss: 5.6376440657233395, Val MAE: 1.63981294631958\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 447/2000, Train Loss: 5.638846324929057, Val Loss: 5.638236918789126, Val MAE: 1.6383672952651978\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 448/2000, Train Loss: 5.6383253204665, Val Loss: 5.637551456747221, Val MAE: 1.6392967700958252\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 449/2000, Train Loss: 5.638118809015215, Val Loss: 5.636911858413583, Val MAE: 1.6393420696258545\n",
      "Epoch 450/2000, Train Loss: 5.637839090456522, Val Loss: 5.637434634168432, Val MAE: 1.6388404369354248\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 451/2000, Train Loss: 5.6372107934321605, Val Loss: 5.637516573052351, Val MAE: 1.6394864320755005\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 452/2000, Train Loss: 5.6373410287932675, Val Loss: 5.637632337686951, Val MAE: 1.6393849849700928\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 453/2000, Train Loss: 5.636815620413961, Val Loss: 5.6374150143125705, Val MAE: 1.6381222009658813\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 454/2000, Train Loss: 5.636727207032594, Val Loss: 5.637498216084246, Val MAE: 1.638929843902588\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 455/2000, Train Loss: 5.636381743762986, Val Loss: 5.637526525382971, Val MAE: 1.6402502059936523\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 456/2000, Train Loss: 5.636599896762864, Val Loss: 5.637147852216055, Val MAE: 1.638978123664856\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 457/2000, Train Loss: 5.635994094583957, Val Loss: 5.637251196704106, Val MAE: 1.639350175857544\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 458/2000, Train Loss: 5.636370897293091, Val Loss: 5.636779897345352, Val MAE: 1.6389824151992798\n",
      "Epoch 459/2000, Train Loss: 5.635269810974861, Val Loss: 5.636838147820802, Val MAE: 1.6381276845932007\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 460/2000, Train Loss: 5.6352326796443455, Val Loss: 5.637618574790839, Val MAE: 1.6376619338989258\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 461/2000, Train Loss: 5.6348602299123085, Val Loss: 5.636599500196124, Val MAE: 1.6399086713790894\n",
      "Epoch 462/2000, Train Loss: 5.63444924249523, Val Loss: 5.636542211180321, Val MAE: 1.6397157907485962\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 463/2000, Train Loss: 5.63493429406624, Val Loss: 5.636158614509066, Val MAE: 1.6397299766540527\n",
      "Epoch 464/2000, Train Loss: 5.634493930749431, Val Loss: 5.636157890817627, Val MAE: 1.639474868774414\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 465/2000, Train Loss: 5.634225326487671, Val Loss: 5.636668722383171, Val MAE: 1.6387969255447388\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 466/2000, Train Loss: 5.633808452127264, Val Loss: 5.636592244363566, Val MAE: 1.6382867097854614\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 467/2000, Train Loss: 5.634346436824043, Val Loss: 5.6362914616703526, Val MAE: 1.6391950845718384\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 468/2000, Train Loss: 5.633637799040336, Val Loss: 5.636171525571404, Val MAE: 1.6389310359954834\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 469/2000, Train Loss: 5.633312506297611, Val Loss: 5.636413386490899, Val MAE: 1.6387351751327515\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 470/2000, Train Loss: 5.6330305101587905, Val Loss: 5.636537723080644, Val MAE: 1.6376793384552002\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 471/2000, Train Loss: 5.632258596924433, Val Loss: 5.636331924941482, Val MAE: 1.638824224472046\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 472/2000, Train Loss: 5.632228706901819, Val Loss: 5.636000675533029, Val MAE: 1.639397144317627\n",
      "Epoch 473/2000, Train Loss: 5.631912483517819, Val Loss: 5.6361131032448215, Val MAE: 1.6394389867782593\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 474/2000, Train Loss: 5.631656541698304, Val Loss: 5.636365646709221, Val MAE: 1.6380404233932495\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 475/2000, Train Loss: 5.631444574977858, Val Loss: 5.6355719328959575, Val MAE: 1.6380168199539185\n",
      "Epoch 476/2000, Train Loss: 5.631390772201941, Val Loss: 5.635447586665411, Val MAE: 1.6393563747406006\n",
      "Epoch 477/2000, Train Loss: 5.630934996751962, Val Loss: 5.635588310591982, Val MAE: 1.6385462284088135\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 478/2000, Train Loss: 5.630748486203769, Val Loss: 5.635611529049579, Val MAE: 1.63761568069458\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 479/2000, Train Loss: 5.6303757213811, Val Loss: 5.635392217565715, Val MAE: 1.6386269330978394\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 480/2000, Train Loss: 5.630243292989185, Val Loss: 5.636209990862705, Val MAE: 1.6373001337051392\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 481/2000, Train Loss: 5.630053054918802, Val Loss: 5.635963044630242, Val MAE: 1.6388201713562012\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 482/2000, Train Loss: 5.630494310992405, Val Loss: 5.635353105900682, Val MAE: 1.639390468597412\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 483/2000, Train Loss: 5.629423659278433, Val Loss: 5.635520939037337, Val MAE: 1.639090895652771\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 484/2000, Train Loss: 5.629294912195416, Val Loss: 5.6357902973622505, Val MAE: 1.6380335092544556\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 485/2000, Train Loss: 5.629548295478989, Val Loss: 5.635380889450478, Val MAE: 1.6382189989089966\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 486/2000, Train Loss: 5.6290020217979535, Val Loss: 5.635592586154044, Val MAE: 1.6393868923187256\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 487/2000, Train Loss: 5.6287727765574855, Val Loss: 5.635490619554752, Val MAE: 1.639871597290039\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 488/2000, Train Loss: 5.628336622851536, Val Loss: 5.635424701867575, Val MAE: 1.638904094696045\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 489/2000, Train Loss: 5.6285911415116905, Val Loss: 5.635803528158747, Val MAE: 1.6389034986495972\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 490/2000, Train Loss: 5.62802039159027, Val Loss: 5.63522671285505, Val MAE: 1.6379719972610474\n",
      "Epoch 491/2000, Train Loss: 5.628070194815749, Val Loss: 5.635216263449238, Val MAE: 1.6393630504608154\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 492/2000, Train Loss: 5.62740577054969, Val Loss: 5.63547740616526, Val MAE: 1.638127326965332\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 493/2000, Train Loss: 5.62706860151585, Val Loss: 5.635297183181723, Val MAE: 1.6397740840911865\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 494/2000, Train Loss: 5.626801235560278, Val Loss: 5.635214027395144, Val MAE: 1.6391369104385376\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 495/2000, Train Loss: 5.626635778317892, Val Loss: 5.634893791666386, Val MAE: 1.6390643119812012\n",
      "Epoch 496/2000, Train Loss: 5.626812353008119, Val Loss: 5.63570296976701, Val MAE: 1.637412428855896\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 497/2000, Train Loss: 5.626074559888125, Val Loss: 5.634944136894101, Val MAE: 1.6395453214645386\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 498/2000, Train Loss: 5.625935635377657, Val Loss: 5.634738757555996, Val MAE: 1.6381436586380005\n",
      "Epoch 499/2000, Train Loss: 5.625811145169094, Val Loss: 5.634827629267328, Val MAE: 1.6385477781295776\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 500/2000, Train Loss: 5.62531193342503, Val Loss: 5.635247758229829, Val MAE: 1.6383581161499023\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 501/2000, Train Loss: 5.6253408429906235, Val Loss: 5.635395620883812, Val MAE: 1.638232946395874\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 502/2000, Train Loss: 5.625353654575768, Val Loss: 5.635279672117533, Val MAE: 1.6393568515777588\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 503/2000, Train Loss: 5.625458779314016, Val Loss: 5.634900560791441, Val MAE: 1.6392911672592163\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 504/2000, Train Loss: 5.624509818753482, Val Loss: 5.635732231630594, Val MAE: 1.6376266479492188\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 505/2000, Train Loss: 5.624200795715601, Val Loss: 5.6355180769814455, Val MAE: 1.6379948854446411\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 506/2000, Train Loss: 5.623795000992158, Val Loss: 5.6350238764316645, Val MAE: 1.6385142803192139\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 507/2000, Train Loss: 5.62421743145073, Val Loss: 5.635543296517701, Val MAE: 1.6382484436035156\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 508/2000, Train Loss: 5.624227967031201, Val Loss: 5.6355607820069835, Val MAE: 1.638334035873413\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 509/2000, Train Loss: 5.623292820044026, Val Loss: 5.635388187327648, Val MAE: 1.638291358947754\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 510/2000, Train Loss: 5.62287331887804, Val Loss: 5.634512845106395, Val MAE: 1.63883376121521\n",
      "Epoch 511/2000, Train Loss: 5.622925159689613, Val Loss: 5.63531305001085, Val MAE: 1.6381620168685913\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 512/2000, Train Loss: 5.622481413349706, Val Loss: 5.635496136331895, Val MAE: 1.6381218433380127\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 513/2000, Train Loss: 5.62223702590371, Val Loss: 5.635365746752748, Val MAE: 1.6381570100784302\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 514/2000, Train Loss: 5.622086111144347, Val Loss: 5.6355157848232365, Val MAE: 1.6375713348388672\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 515/2000, Train Loss: 5.6221798621610395, Val Loss: 5.635205151525175, Val MAE: 1.6378827095031738\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 516/2000, Train Loss: 5.6218777480104425, Val Loss: 5.63627243581418, Val MAE: 1.6373167037963867\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 517/2000, Train Loss: 5.621455327004588, Val Loss: 5.635874914916397, Val MAE: 1.6379363536834717\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 518/2000, Train Loss: 5.621879840212246, Val Loss: 5.634201491846965, Val MAE: 1.638889193534851\n",
      "Epoch 519/2000, Train Loss: 5.621042550923016, Val Loss: 5.63510918194507, Val MAE: 1.6381152868270874\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 520/2000, Train Loss: 5.620971280572698, Val Loss: 5.634702656556767, Val MAE: 1.6388944387435913\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 521/2000, Train Loss: 5.620228903934294, Val Loss: 5.634976676860119, Val MAE: 1.6382014751434326\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 522/2000, Train Loss: 5.620571667927478, Val Loss: 5.635170990622702, Val MAE: 1.6378144025802612\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 523/2000, Train Loss: 5.619816393579155, Val Loss: 5.634856615564637, Val MAE: 1.6383967399597168\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 524/2000, Train Loss: 5.619903219954033, Val Loss: 5.635455879649394, Val MAE: 1.637740969657898\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 525/2000, Train Loss: 5.6196811734842305, Val Loss: 5.634917441989859, Val MAE: 1.6384696960449219\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 526/2000, Train Loss: 5.619355193318775, Val Loss: 5.63466580336452, Val MAE: 1.6378055810928345\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 527/2000, Train Loss: 5.619050240201572, Val Loss: 5.6354458149001285, Val MAE: 1.6378042697906494\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 528/2000, Train Loss: 5.618879402261474, Val Loss: 5.634066968085065, Val MAE: 1.6388686895370483\n",
      "Epoch 529/2000, Train Loss: 5.618859494835269, Val Loss: 5.634969224844879, Val MAE: 1.6391584873199463\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 530/2000, Train Loss: 5.618154067825116, Val Loss: 5.634725760816763, Val MAE: 1.6377354860305786\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 531/2000, Train Loss: 5.617963324559417, Val Loss: 5.6350030817942685, Val MAE: 1.6373709440231323\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 532/2000, Train Loss: 5.618011726681882, Val Loss: 5.635153479214198, Val MAE: 1.637877106666565\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 533/2000, Train Loss: 5.618166773329747, Val Loss: 5.6348235408592595, Val MAE: 1.6387825012207031\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 534/2000, Train Loss: 5.6173033735300475, Val Loss: 5.63485507246236, Val MAE: 1.6382367610931396\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 535/2000, Train Loss: 5.6173411203376, Val Loss: 5.6349506712158774, Val MAE: 1.637693166732788\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 536/2000, Train Loss: 5.617128478272896, Val Loss: 5.634087699674672, Val MAE: 1.6391198635101318\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 537/2000, Train Loss: 5.616687148678145, Val Loss: 5.634855643622529, Val MAE: 1.6384477615356445\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 538/2000, Train Loss: 5.616446626343916, Val Loss: 5.634173339211283, Val MAE: 1.6396149396896362\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 539/2000, Train Loss: 5.616603261048573, Val Loss: 5.635486555765162, Val MAE: 1.6381950378417969\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 540/2000, Train Loss: 5.616373137755541, Val Loss: 5.635221744704614, Val MAE: 1.63748300075531\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 541/2000, Train Loss: 5.616357847457415, Val Loss: 5.633685992927851, Val MAE: 1.6388503313064575\n",
      "Epoch 542/2000, Train Loss: 5.615670042416073, Val Loss: 5.634552384849844, Val MAE: 1.6393948793411255\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 543/2000, Train Loss: 5.61534580575212, Val Loss: 5.634593385672233, Val MAE: 1.6377136707305908\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 544/2000, Train Loss: 5.61516414864998, Val Loss: 5.634940739046778, Val MAE: 1.6384726762771606\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 545/2000, Train Loss: 5.615012736047417, Val Loss: 5.6348921770041045, Val MAE: 1.6387618780136108\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 546/2000, Train Loss: 5.614716395407521, Val Loss: 5.634562006791688, Val MAE: 1.6380019187927246\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 547/2000, Train Loss: 5.614835734934534, Val Loss: 5.634738436157652, Val MAE: 1.639455795288086\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 548/2000, Train Loss: 5.614149421322188, Val Loss: 5.635387351906193, Val MAE: 1.638268232345581\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 549/2000, Train Loss: 5.614763363867604, Val Loss: 5.635209567299703, Val MAE: 1.6384559869766235\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 550/2000, Train Loss: 5.614249257789309, Val Loss: 5.635303336462252, Val MAE: 1.638811707496643\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 551/2000, Train Loss: 5.61347205418322, Val Loss: 5.63500695957758, Val MAE: 1.6391706466674805\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 552/2000, Train Loss: 5.613452264390853, Val Loss: 5.6353239520753, Val MAE: 1.6378748416900635\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 553/2000, Train Loss: 5.613181002864754, Val Loss: 5.635518902092598, Val MAE: 1.6380488872528076\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 554/2000, Train Loss: 5.613481109888018, Val Loss: 5.634894370611886, Val MAE: 1.639560580253601\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 555/2000, Train Loss: 5.613176408843322, Val Loss: 5.635857989894411, Val MAE: 1.637643575668335\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 556/2000, Train Loss: 5.612687214880788, Val Loss: 5.63601015626772, Val MAE: 1.6386549472808838\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 557/2000, Train Loss: 5.6127858939149835, Val Loss: 5.636470786222904, Val MAE: 1.6378384828567505\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 558/2000, Train Loss: 5.61204409494274, Val Loss: 5.635420692326628, Val MAE: 1.6395035982131958\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 559/2000, Train Loss: 5.611882684514386, Val Loss: 5.634527158714229, Val MAE: 1.640076756477356\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 560/2000, Train Loss: 5.612047510000052, Val Loss: 5.635546129230358, Val MAE: 1.639085292816162\n",
      "EarlyStopping counter: 19 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [07:25<11:39, 233.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/2000, Train Loss: 5.611470561720727, Val Loss: 5.636028660889308, Val MAE: 1.6383919715881348\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.010101318359375\n",
      "Test Mean Absolute Error (MAE): 1.5117122630810074\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 446, 'position': 'DEF', 'window_size': 3, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 446\n",
      "position DEF\n",
      "window_size 3\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  2\n",
      "======= Generating CNN Data for Season: ['2020-21'], Position: DEF =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type DEF = 245.\n",
      "65 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (6124, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (6662, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (6124, 7)\n",
      "Shape of a given window (prior to preprocessing): (3, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.52335269e+00 4.15147690e+01 1.91870740e-02 2.70133805e-02\n",
      " 1.33552133e-01 7.99823277e+00 6.79121434e-02 3.78692249e-03\n",
      " 3.78692249e-03 0.00000000e+00]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.6339447  43.39363048  0.13901026  0.16973     0.34017049 10.02432018\n",
      "  0.25159508  0.06142135  0.06142135  1.        ]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building rnn Architecture ======\n",
      "====== Done Building rnn Architecture ======\n",
      "Epoch 1/2000, Train Loss: 8.917804460946982, Val Loss: 9.424760184737641, Val MAE: 1.6994796991348267\n",
      "Epoch 2/2000, Train Loss: 8.73969755662263, Val Loss: 9.234839178492734, Val MAE: 1.680776834487915\n",
      "Epoch 3/2000, Train Loss: 8.565930775289774, Val Loss: 9.04660805622074, Val MAE: 1.6640040874481201\n",
      "Epoch 4/2000, Train Loss: 8.392891746414188, Val Loss: 8.859758229956098, Val MAE: 1.6488155126571655\n",
      "Epoch 5/2000, Train Loss: 8.218910048173543, Val Loss: 8.672567776489748, Val MAE: 1.6335043907165527\n",
      "Epoch 6/2000, Train Loss: 8.043674059696915, Val Loss: 8.484019345240077, Val MAE: 1.6177295446395874\n",
      "Epoch 7/2000, Train Loss: 7.867087809227339, Val Loss: 8.291174581220625, Val MAE: 1.6005915403366089\n",
      "Epoch 8/2000, Train Loss: 7.689253416357216, Val Loss: 8.100600125470018, Val MAE: 1.5837438106536865\n",
      "Epoch 9/2000, Train Loss: 7.512090978536048, Val Loss: 7.907931464539651, Val MAE: 1.5672155618667603\n",
      "Epoch 10/2000, Train Loss: 7.336355430556954, Val Loss: 7.71926508170049, Val MAE: 1.5525251626968384\n",
      "Epoch 11/2000, Train Loss: 7.1638517063003695, Val Loss: 7.536888466040395, Val MAE: 1.5406901836395264\n",
      "Epoch 12/2000, Train Loss: 6.997423614148023, Val Loss: 7.357134816889736, Val MAE: 1.5309984683990479\n",
      "Epoch 13/2000, Train Loss: 6.837787013246304, Val Loss: 7.186545035562276, Val MAE: 1.5249897241592407\n",
      "Epoch 14/2000, Train Loss: 6.686753117118665, Val Loss: 7.0275583837642515, Val MAE: 1.5221890211105347\n",
      "Epoch 15/2000, Train Loss: 6.546305264544428, Val Loss: 6.876638856279766, Val MAE: 1.5220166444778442\n",
      "Epoch 16/2000, Train Loss: 6.41701974889717, Val Loss: 6.7407962053995085, Val MAE: 1.523979663848877\n",
      "Epoch 17/2000, Train Loss: 6.300168880878357, Val Loss: 6.6157868021646316, Val MAE: 1.5278664827346802\n",
      "Epoch 18/2000, Train Loss: 6.195425318258004, Val Loss: 6.505317378457041, Val MAE: 1.5343434810638428\n",
      "Epoch 19/2000, Train Loss: 6.103324346414085, Val Loss: 6.407587562943973, Val MAE: 1.5420130491256714\n",
      "Epoch 20/2000, Train Loss: 6.02327183920572, Val Loss: 6.323360692086015, Val MAE: 1.5518347024917603\n",
      "Epoch 21/2000, Train Loss: 5.954047802808093, Val Loss: 6.247750739855519, Val MAE: 1.5634640455245972\n",
      "Epoch 22/2000, Train Loss: 5.895316833000801, Val Loss: 6.184487583241148, Val MAE: 1.5756421089172363\n",
      "Epoch 23/2000, Train Loss: 5.8455547059986115, Val Loss: 6.1326738027825884, Val MAE: 1.5867985486984253\n",
      "Epoch 24/2000, Train Loss: 5.804359199645393, Val Loss: 6.088686571433194, Val MAE: 1.597788691520691\n",
      "Epoch 25/2000, Train Loss: 5.7702064656177265, Val Loss: 6.052399112904507, Val MAE: 1.6074438095092773\n",
      "Epoch 26/2000, Train Loss: 5.74181614529455, Val Loss: 6.021576845928826, Val MAE: 1.61698579788208\n",
      "Epoch 27/2000, Train Loss: 5.718515720859583, Val Loss: 5.996721707528184, Val MAE: 1.6248421669006348\n",
      "Epoch 28/2000, Train Loss: 5.699605620303325, Val Loss: 5.976330461067753, Val MAE: 1.6314421892166138\n",
      "Epoch 29/2000, Train Loss: 5.684413153818274, Val Loss: 5.958825728922337, Val MAE: 1.6377681493759155\n",
      "Epoch 30/2000, Train Loss: 5.671531995181163, Val Loss: 5.945059407132034, Val MAE: 1.6418458223342896\n",
      "Epoch 31/2000, Train Loss: 5.661056601081678, Val Loss: 5.933997692856901, Val MAE: 1.645424246788025\n",
      "Epoch 32/2000, Train Loss: 5.651961027119731, Val Loss: 5.9243598574625524, Val MAE: 1.6479445695877075\n",
      "Epoch 33/2000, Train Loss: 5.644390798092805, Val Loss: 5.915837307172069, Val MAE: 1.650866985321045\n",
      "Epoch 34/2000, Train Loss: 5.637672083475352, Val Loss: 5.90828296203809, Val MAE: 1.6537036895751953\n",
      "Epoch 35/2000, Train Loss: 5.631895620090569, Val Loss: 5.902629616476161, Val MAE: 1.6547297239303589\n",
      "Epoch 36/2000, Train Loss: 5.6266874329846805, Val Loss: 5.897013667610075, Val MAE: 1.6564626693725586\n",
      "Epoch 37/2000, Train Loss: 5.621857004246802, Val Loss: 5.892873471635056, Val MAE: 1.6558061838150024\n",
      "Epoch 38/2000, Train Loss: 5.617931306018562, Val Loss: 5.888325567043615, Val MAE: 1.657414197921753\n",
      "Epoch 39/2000, Train Loss: 5.613806843594015, Val Loss: 5.884252973364438, Val MAE: 1.6576834917068481\n",
      "Epoch 40/2000, Train Loss: 5.609813378009696, Val Loss: 5.880859830426894, Val MAE: 1.6574081182479858\n",
      "Epoch 41/2000, Train Loss: 5.606739019909109, Val Loss: 5.8774201073196926, Val MAE: 1.658022403717041\n",
      "Epoch 42/2000, Train Loss: 5.6032804733378985, Val Loss: 5.8742082224510055, Val MAE: 1.6579471826553345\n",
      "Epoch 43/2000, Train Loss: 5.600218793045451, Val Loss: 5.870956216277489, Val MAE: 1.6584912538528442\n",
      "Epoch 44/2000, Train Loss: 5.5975111757305145, Val Loss: 5.868860149322373, Val MAE: 1.657122254371643\n",
      "Epoch 45/2000, Train Loss: 5.594085142404994, Val Loss: 5.865633351287451, Val MAE: 1.6576522588729858\n",
      "Epoch 46/2000, Train Loss: 5.591489079952109, Val Loss: 5.863448610666702, Val MAE: 1.6564408540725708\n",
      "Epoch 47/2000, Train Loss: 5.588572711562377, Val Loss: 5.8608573270654585, Val MAE: 1.656976580619812\n",
      "Epoch 48/2000, Train Loss: 5.586161797366691, Val Loss: 5.858721162050943, Val MAE: 1.6560776233673096\n",
      "Epoch 49/2000, Train Loss: 5.58377644049084, Val Loss: 5.855886041544584, Val MAE: 1.6572744846343994\n",
      "Epoch 50/2000, Train Loss: 5.580979502204353, Val Loss: 5.853930000160809, Val MAE: 1.6561572551727295\n",
      "Epoch 51/2000, Train Loss: 5.578659319661654, Val Loss: 5.8524524735823285, Val MAE: 1.6540837287902832\n",
      "Epoch 52/2000, Train Loss: 5.57670873840399, Val Loss: 5.8505801977141685, Val MAE: 1.6532864570617676\n",
      "Epoch 53/2000, Train Loss: 5.574149542904607, Val Loss: 5.848275653210261, Val MAE: 1.6540753841400146\n",
      "Epoch 54/2000, Train Loss: 5.5720863238881, Val Loss: 5.845897429565958, Val MAE: 1.6547268629074097\n",
      "Epoch 55/2000, Train Loss: 5.570048260813128, Val Loss: 5.8445166315001655, Val MAE: 1.653472900390625\n",
      "Epoch 56/2000, Train Loss: 5.568329285829237, Val Loss: 5.842842079111213, Val MAE: 1.6531082391738892\n",
      "Epoch 57/2000, Train Loss: 5.566106348289031, Val Loss: 5.841181964580641, Val MAE: 1.6525061130523682\n",
      "Epoch 58/2000, Train Loss: 5.564146092852295, Val Loss: 5.839885985109575, Val MAE: 1.6512678861618042\n",
      "Epoch 59/2000, Train Loss: 5.562324907960978, Val Loss: 5.838576142492777, Val MAE: 1.6502628326416016\n",
      "Epoch 60/2000, Train Loss: 5.560476225351129, Val Loss: 5.836948634264483, Val MAE: 1.6505744457244873\n",
      "Epoch 61/2000, Train Loss: 5.558606785599622, Val Loss: 5.8354529150790935, Val MAE: 1.6501401662826538\n",
      "Epoch 62/2000, Train Loss: 5.556860822849865, Val Loss: 5.833655923509384, Val MAE: 1.6510177850723267\n",
      "Epoch 63/2000, Train Loss: 5.555442846184604, Val Loss: 5.8326368650004525, Val MAE: 1.6499286890029907\n",
      "Epoch 64/2000, Train Loss: 5.5538300962000475, Val Loss: 5.831034906251527, Val MAE: 1.6502734422683716\n",
      "Epoch 65/2000, Train Loss: 5.552242475324301, Val Loss: 5.829725447796032, Val MAE: 1.649989366531372\n",
      "Epoch 66/2000, Train Loss: 5.550661907332172, Val Loss: 5.828576527779648, Val MAE: 1.6491494178771973\n",
      "Epoch 67/2000, Train Loss: 5.5495673170333175, Val Loss: 5.827829478412504, Val MAE: 1.647947907447815\n",
      "Epoch 68/2000, Train Loss: 5.548090779254504, Val Loss: 5.826635519153725, Val MAE: 1.6479461193084717\n",
      "Epoch 69/2000, Train Loss: 5.546517800407818, Val Loss: 5.825419106645565, Val MAE: 1.6478387117385864\n",
      "Epoch 70/2000, Train Loss: 5.5452045588057475, Val Loss: 5.824116514767494, Val MAE: 1.648213267326355\n",
      "Epoch 71/2000, Train Loss: 5.544035826546132, Val Loss: 5.823015773349882, Val MAE: 1.6481261253356934\n",
      "Epoch 72/2000, Train Loss: 5.542681591019276, Val Loss: 5.822211675111723, Val MAE: 1.6475085020065308\n",
      "Epoch 73/2000, Train Loss: 5.541759792540448, Val Loss: 5.821040065415476, Val MAE: 1.647619366645813\n",
      "Epoch 74/2000, Train Loss: 5.540069913589264, Val Loss: 5.820153460585207, Val MAE: 1.6473346948623657\n",
      "Epoch 75/2000, Train Loss: 5.538885234968367, Val Loss: 5.819618886980665, Val MAE: 1.646095871925354\n",
      "Epoch 76/2000, Train Loss: 5.538075287198746, Val Loss: 5.818792607097613, Val MAE: 1.6454602479934692\n",
      "Epoch 77/2000, Train Loss: 5.53694032068407, Val Loss: 5.817913319989609, Val MAE: 1.6455129384994507\n",
      "Epoch 78/2000, Train Loss: 5.535661114659441, Val Loss: 5.817102035855237, Val MAE: 1.6456019878387451\n",
      "Epoch 79/2000, Train Loss: 5.534516145853822, Val Loss: 5.816254850073149, Val MAE: 1.6456695795059204\n",
      "Epoch 80/2000, Train Loss: 5.5340841183648015, Val Loss: 5.816041971797588, Val MAE: 1.643658995628357\n",
      "Epoch 81/2000, Train Loss: 5.532494297074709, Val Loss: 5.81474133498245, Val MAE: 1.6442831754684448\n",
      "Epoch 82/2000, Train Loss: 5.531886672502424, Val Loss: 5.813802568020922, Val MAE: 1.6450608968734741\n",
      "Epoch 83/2000, Train Loss: 5.530980221295207, Val Loss: 5.813397348194722, Val MAE: 1.6444517374038696\n",
      "Epoch 84/2000, Train Loss: 5.529788203596774, Val Loss: 5.812681586257612, Val MAE: 1.643976092338562\n",
      "Epoch 85/2000, Train Loss: 5.529064679518854, Val Loss: 5.812681682909659, Val MAE: 1.6421948671340942\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 86/2000, Train Loss: 5.527730252662269, Val Loss: 5.811341195170737, Val MAE: 1.6435168981552124\n",
      "Epoch 87/2000, Train Loss: 5.5269087489214765, Val Loss: 5.81078485925661, Val MAE: 1.6434459686279297\n",
      "Epoch 88/2000, Train Loss: 5.526243794006926, Val Loss: 5.8104572507186605, Val MAE: 1.6425440311431885\n",
      "Epoch 89/2000, Train Loss: 5.5255574075330784, Val Loss: 5.80975513598948, Val MAE: 1.642791748046875\n",
      "Epoch 90/2000, Train Loss: 5.5248504361705795, Val Loss: 5.809352437068898, Val MAE: 1.642033338546753\n",
      "Epoch 91/2000, Train Loss: 5.524023661664512, Val Loss: 5.808393273160885, Val MAE: 1.6430864334106445\n",
      "Epoch 92/2000, Train Loss: 5.522887137510146, Val Loss: 5.807738861416761, Val MAE: 1.6430031061172485\n",
      "Epoch 93/2000, Train Loss: 5.522500465016361, Val Loss: 5.807344793279328, Val MAE: 1.6424858570098877\n",
      "Epoch 94/2000, Train Loss: 5.521492249684906, Val Loss: 5.807064597158573, Val MAE: 1.641886830329895\n",
      "Epoch 95/2000, Train Loss: 5.5208353434753, Val Loss: 5.806637325678367, Val MAE: 1.6415040493011475\n",
      "Epoch 96/2000, Train Loss: 5.5201799531183235, Val Loss: 5.805752294502478, Val MAE: 1.6426700353622437\n",
      "Epoch 97/2000, Train Loss: 5.519192621925851, Val Loss: 5.805441334891427, Val MAE: 1.6421493291854858\n",
      "Epoch 98/2000, Train Loss: 5.518645631458744, Val Loss: 5.804723501664236, Val MAE: 1.6428146362304688\n",
      "Epoch 99/2000, Train Loss: 5.51832926643114, Val Loss: 5.804790747020396, Val MAE: 1.6410166025161743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 100/2000, Train Loss: 5.517665419839026, Val Loss: 5.804145779649442, Val MAE: 1.641875147819519\n",
      "Epoch 101/2000, Train Loss: 5.516854692284236, Val Loss: 5.8036852941213315, Val MAE: 1.641280174255371\n",
      "Epoch 102/2000, Train Loss: 5.5161174331233624, Val Loss: 5.8034164377938025, Val MAE: 1.6413817405700684\n",
      "Epoch 103/2000, Train Loss: 5.515769560259186, Val Loss: 5.8029953807955605, Val MAE: 1.6415365934371948\n",
      "Epoch 104/2000, Train Loss: 5.515196449574029, Val Loss: 5.802464994732732, Val MAE: 1.640971064567566\n",
      "Epoch 105/2000, Train Loss: 5.514312911681542, Val Loss: 5.802327521876543, Val MAE: 1.640765905380249\n",
      "Epoch 106/2000, Train Loss: 5.514195527409971, Val Loss: 5.802163588075473, Val MAE: 1.6402655839920044\n",
      "Epoch 107/2000, Train Loss: 5.51344319683099, Val Loss: 5.8021724198092395, Val MAE: 1.63922119140625\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 108/2000, Train Loss: 5.512856479270949, Val Loss: 5.80064492014603, Val MAE: 1.642189383506775\n",
      "Epoch 109/2000, Train Loss: 5.512122717461284, Val Loss: 5.800500310943095, Val MAE: 1.6418288946151733\n",
      "Epoch 110/2000, Train Loss: 5.512080668748619, Val Loss: 5.800582261480071, Val MAE: 1.6404234170913696\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 111/2000, Train Loss: 5.511035041432115, Val Loss: 5.8002017295230575, Val MAE: 1.640064001083374\n",
      "Epoch 112/2000, Train Loss: 5.511203531050073, Val Loss: 5.799835797224846, Val MAE: 1.6406563520431519\n",
      "Epoch 113/2000, Train Loss: 5.510232525149315, Val Loss: 5.799482010357497, Val MAE: 1.6405047178268433\n",
      "Epoch 114/2000, Train Loss: 5.5097782683182706, Val Loss: 5.79908745651294, Val MAE: 1.640727162361145\n",
      "Epoch 115/2000, Train Loss: 5.509260174470913, Val Loss: 5.798794511191886, Val MAE: 1.64026939868927\n",
      "Epoch 116/2000, Train Loss: 5.508377259490322, Val Loss: 5.798711236583032, Val MAE: 1.640018343925476\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 117/2000, Train Loss: 5.50790411615123, Val Loss: 5.79827838161192, Val MAE: 1.6401188373565674\n",
      "Epoch 118/2000, Train Loss: 5.507519139780175, Val Loss: 5.79860891203914, Val MAE: 1.6388227939605713\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 119/2000, Train Loss: 5.5070460627375475, Val Loss: 5.798310882698044, Val MAE: 1.6384899616241455\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 120/2000, Train Loss: 5.506990901330261, Val Loss: 5.798014640196383, Val MAE: 1.6388839483261108\n",
      "Epoch 121/2000, Train Loss: 5.506583111056449, Val Loss: 5.797595181627255, Val MAE: 1.6392096281051636\n",
      "Epoch 122/2000, Train Loss: 5.505893010278996, Val Loss: 5.796926558361824, Val MAE: 1.6399415731430054\n",
      "Epoch 123/2000, Train Loss: 5.505382294056672, Val Loss: 5.796968415733176, Val MAE: 1.6392278671264648\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 124/2000, Train Loss: 5.505012709591175, Val Loss: 5.796698018172523, Val MAE: 1.6397918462753296\n",
      "Epoch 125/2000, Train Loss: 5.504441714699112, Val Loss: 5.796431719759782, Val MAE: 1.6392171382904053\n",
      "Epoch 126/2000, Train Loss: 5.503894922859617, Val Loss: 5.796421173369755, Val MAE: 1.6385356187820435\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 127/2000, Train Loss: 5.503696117301645, Val Loss: 5.796470470807734, Val MAE: 1.6375606060028076\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 128/2000, Train Loss: 5.503591546114414, Val Loss: 5.795937536617668, Val MAE: 1.6387840509414673\n",
      "Epoch 129/2000, Train Loss: 5.503141141808233, Val Loss: 5.796095401526568, Val MAE: 1.6371268033981323\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 130/2000, Train Loss: 5.502403372760137, Val Loss: 5.7955619235158045, Val MAE: 1.6384371519088745\n",
      "Epoch 131/2000, Train Loss: 5.502167676590053, Val Loss: 5.795294219366322, Val MAE: 1.6385533809661865\n",
      "Epoch 132/2000, Train Loss: 5.501549326388547, Val Loss: 5.795266818816728, Val MAE: 1.6379035711288452\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 133/2000, Train Loss: 5.5019384912170946, Val Loss: 5.7949346557039725, Val MAE: 1.6380202770233154\n",
      "Epoch 134/2000, Train Loss: 5.50113660210016, Val Loss: 5.795058645293681, Val MAE: 1.637073040008545\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 135/2000, Train Loss: 5.500577721267227, Val Loss: 5.794871701270514, Val MAE: 1.6377183198928833\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 136/2000, Train Loss: 5.50027733833424, Val Loss: 5.794094851263217, Val MAE: 1.6387536525726318\n",
      "Epoch 137/2000, Train Loss: 5.500072689721325, Val Loss: 5.794445052523121, Val MAE: 1.6378021240234375\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 138/2000, Train Loss: 5.499461963809587, Val Loss: 5.7943641419132375, Val MAE: 1.6370570659637451\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 139/2000, Train Loss: 5.499569116893981, Val Loss: 5.793625904093039, Val MAE: 1.6389374732971191\n",
      "Epoch 140/2000, Train Loss: 5.498917124672575, Val Loss: 5.794182681058599, Val MAE: 1.6365655660629272\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 141/2000, Train Loss: 5.499095124855429, Val Loss: 5.793782378864105, Val MAE: 1.6371970176696777\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 142/2000, Train Loss: 5.498285828401124, Val Loss: 5.793643622615538, Val MAE: 1.6370753049850464\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 143/2000, Train Loss: 5.497896672538634, Val Loss: 5.793631255129655, Val MAE: 1.6368099451065063\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 144/2000, Train Loss: 5.497669486727258, Val Loss: 5.793622836002254, Val MAE: 1.6361706256866455\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 145/2000, Train Loss: 5.497138648186285, Val Loss: 5.792858680752014, Val MAE: 1.6383395195007324\n",
      "Epoch 146/2000, Train Loss: 5.496902237545813, Val Loss: 5.792868801077793, Val MAE: 1.6374362707138062\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 147/2000, Train Loss: 5.496790545804141, Val Loss: 5.7927608493048846, Val MAE: 1.6374720335006714\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 148/2000, Train Loss: 5.496390782572757, Val Loss: 5.792268104626629, Val MAE: 1.6380226612091064\n",
      "Epoch 149/2000, Train Loss: 5.495831463515612, Val Loss: 5.7926762542944825, Val MAE: 1.6362799406051636\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 150/2000, Train Loss: 5.495376283507932, Val Loss: 5.7925349511421205, Val MAE: 1.6364951133728027\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 151/2000, Train Loss: 5.495162090167481, Val Loss: 5.792305135512826, Val MAE: 1.6368873119354248\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 152/2000, Train Loss: 5.495148547233018, Val Loss: 5.792537296483577, Val MAE: 1.6359708309173584\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 153/2000, Train Loss: 5.494639636983984, Val Loss: 5.7921719811068195, Val MAE: 1.6366627216339111\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 154/2000, Train Loss: 5.494308820374222, Val Loss: 5.79211132919429, Val MAE: 1.636675238609314\n",
      "Epoch 155/2000, Train Loss: 5.494085947498005, Val Loss: 5.791630954142944, Val MAE: 1.637601375579834\n",
      "Epoch 156/2000, Train Loss: 5.493677373315946, Val Loss: 5.791712670390464, Val MAE: 1.636398196220398\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 157/2000, Train Loss: 5.493539090998187, Val Loss: 5.79182770223171, Val MAE: 1.6363234519958496\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 158/2000, Train Loss: 5.49344388148695, Val Loss: 5.7914757300371384, Val MAE: 1.6367515325546265\n",
      "Epoch 159/2000, Train Loss: 5.493347505668412, Val Loss: 5.792107658251652, Val MAE: 1.6350408792495728\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 160/2000, Train Loss: 5.492845648106657, Val Loss: 5.791715331686239, Val MAE: 1.635297179222107\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 161/2000, Train Loss: 5.492328039866177, Val Loss: 5.7909409282603574, Val MAE: 1.6372137069702148\n",
      "Epoch 162/2000, Train Loss: 5.49201535648479, Val Loss: 5.791181820948358, Val MAE: 1.6361842155456543\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 163/2000, Train Loss: 5.491515736837751, Val Loss: 5.791108468467414, Val MAE: 1.63624107837677\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 164/2000, Train Loss: 5.491665438443397, Val Loss: 5.791086836454576, Val MAE: 1.6363667249679565\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 165/2000, Train Loss: 5.491435326686967, Val Loss: 5.790358703361252, Val MAE: 1.6379224061965942\n",
      "Epoch 166/2000, Train Loss: 5.491089533097202, Val Loss: 5.790676943685399, Val MAE: 1.6370855569839478\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 167/2000, Train Loss: 5.490829455993217, Val Loss: 5.7903130771717715, Val MAE: 1.6375856399536133\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 168/2000, Train Loss: 5.49045412847376, Val Loss: 5.790159923132602, Val MAE: 1.637516736984253\n",
      "Epoch 169/2000, Train Loss: 5.490017724174606, Val Loss: 5.789874415431289, Val MAE: 1.6381196975708008\n",
      "Epoch 170/2000, Train Loss: 5.489557414797025, Val Loss: 5.790258782272388, Val MAE: 1.6370458602905273\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 171/2000, Train Loss: 5.4897555697595815, Val Loss: 5.790160561464321, Val MAE: 1.6370232105255127\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 172/2000, Train Loss: 5.489242348342422, Val Loss: 5.7903768766989225, Val MAE: 1.6362974643707275\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 173/2000, Train Loss: 5.489040889289724, Val Loss: 5.789902346649561, Val MAE: 1.637503981590271\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 174/2000, Train Loss: 5.4888091146242415, Val Loss: 5.790369651652514, Val MAE: 1.6359423398971558\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 175/2000, Train Loss: 5.489110706273324, Val Loss: 5.79026540018489, Val MAE: 1.6354581117630005\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 176/2000, Train Loss: 5.488582435689587, Val Loss: 5.790295354061359, Val MAE: 1.635503888130188\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 177/2000, Train Loss: 5.488126639380025, Val Loss: 5.789709825252706, Val MAE: 1.636543869972229\n",
      "Epoch 178/2000, Train Loss: 5.487883064401958, Val Loss: 5.789349384687129, Val MAE: 1.6375184059143066\n",
      "Epoch 179/2000, Train Loss: 5.4872859922896495, Val Loss: 5.789476858032598, Val MAE: 1.6373374462127686\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 180/2000, Train Loss: 5.487432157211241, Val Loss: 5.789442924599834, Val MAE: 1.637149691581726\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 181/2000, Train Loss: 5.486930926619014, Val Loss: 5.789607144380854, Val MAE: 1.636314034461975\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 182/2000, Train Loss: 5.4868001632889385, Val Loss: 5.789163815814613, Val MAE: 1.6376495361328125\n",
      "Epoch 183/2000, Train Loss: 5.48657388003881, Val Loss: 5.78957382266991, Val MAE: 1.6365110874176025\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 184/2000, Train Loss: 5.48646222985514, Val Loss: 5.789040526340526, Val MAE: 1.637231469154358\n",
      "Epoch 185/2000, Train Loss: 5.486415502038788, Val Loss: 5.789519866664414, Val MAE: 1.63593327999115\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 186/2000, Train Loss: 5.485908392429221, Val Loss: 5.7892270819203375, Val MAE: 1.6371487379074097\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 187/2000, Train Loss: 5.4854923348292, Val Loss: 5.78902572695455, Val MAE: 1.637044906616211\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 188/2000, Train Loss: 5.485391504647409, Val Loss: 5.788882451916903, Val MAE: 1.6374679803848267\n",
      "Epoch 189/2000, Train Loss: 5.485279300662472, Val Loss: 5.788864036337392, Val MAE: 1.637336015701294\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 190/2000, Train Loss: 5.485245277683322, Val Loss: 5.789162777722685, Val MAE: 1.6366692781448364\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 191/2000, Train Loss: 5.484563328033288, Val Loss: 5.788852692567973, Val MAE: 1.6367607116699219\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 192/2000, Train Loss: 5.484787248563806, Val Loss: 5.78868580231535, Val MAE: 1.6372838020324707\n",
      "Epoch 193/2000, Train Loss: 5.484692429174213, Val Loss: 5.788929906237041, Val MAE: 1.636393666267395\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 194/2000, Train Loss: 5.484042114914364, Val Loss: 5.788566497597196, Val MAE: 1.6372792720794678\n",
      "Epoch 195/2000, Train Loss: 5.483709685199705, Val Loss: 5.788529135463634, Val MAE: 1.63738214969635\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 196/2000, Train Loss: 5.483762403061977, Val Loss: 5.788741290072282, Val MAE: 1.63638436794281\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 197/2000, Train Loss: 5.483251695347591, Val Loss: 5.788135094241961, Val MAE: 1.6379669904708862\n",
      "Epoch 198/2000, Train Loss: 5.483376596708923, Val Loss: 5.788447862408573, Val MAE: 1.6372334957122803\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 199/2000, Train Loss: 5.482599627405544, Val Loss: 5.788508713742416, Val MAE: 1.6371058225631714\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 200/2000, Train Loss: 5.482538590285924, Val Loss: 5.7885129070954875, Val MAE: 1.636780023574829\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 201/2000, Train Loss: 5.482491478434376, Val Loss: 5.788420333703257, Val MAE: 1.637198567390442\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 202/2000, Train Loss: 5.482256565190854, Val Loss: 5.788702165568158, Val MAE: 1.6362608671188354\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 203/2000, Train Loss: 5.482039340865046, Val Loss: 5.788293430788384, Val MAE: 1.6368553638458252\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 204/2000, Train Loss: 5.481669009533552, Val Loss: 5.788140979005863, Val MAE: 1.6376674175262451\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 205/2000, Train Loss: 5.4816185532118835, Val Loss: 5.7879860577023585, Val MAE: 1.6376110315322876\n",
      "Epoch 206/2000, Train Loss: 5.481337147167414, Val Loss: 5.788109965627204, Val MAE: 1.6374591588974\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 207/2000, Train Loss: 5.481185319362811, Val Loss: 5.788298108625137, Val MAE: 1.6372095346450806\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 208/2000, Train Loss: 5.480795737121775, Val Loss: 5.78784815847606, Val MAE: 1.6377074718475342\n",
      "Epoch 209/2000, Train Loss: 5.480940488678657, Val Loss: 5.787793546093053, Val MAE: 1.63809335231781\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 210/2000, Train Loss: 5.480477467494255, Val Loss: 5.787763469260499, Val MAE: 1.6380479335784912\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 211/2000, Train Loss: 5.48076643061579, Val Loss: 5.788004496221438, Val MAE: 1.636940836906433\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 212/2000, Train Loss: 5.479871219898887, Val Loss: 5.787676677862905, Val MAE: 1.6379419565200806\n",
      "Epoch 213/2000, Train Loss: 5.4797312462686545, Val Loss: 5.787882563852208, Val MAE: 1.6373860836029053\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 214/2000, Train Loss: 5.4796201398076185, Val Loss: 5.787838162941532, Val MAE: 1.6375176906585693\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 215/2000, Train Loss: 5.479493422590702, Val Loss: 5.78770119016793, Val MAE: 1.6376409530639648\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 216/2000, Train Loss: 5.479182915539654, Val Loss: 5.787899703074144, Val MAE: 1.6375521421432495\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 217/2000, Train Loss: 5.4791844842069395, Val Loss: 5.787516350467821, Val MAE: 1.6382999420166016\n",
      "Epoch 218/2000, Train Loss: 5.478714384680556, Val Loss: 5.787390505525835, Val MAE: 1.6383788585662842\n",
      "Epoch 219/2000, Train Loss: 5.478618160826866, Val Loss: 5.788010172082339, Val MAE: 1.636715292930603\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 220/2000, Train Loss: 5.478252224761184, Val Loss: 5.787680339017994, Val MAE: 1.6373426914215088\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 221/2000, Train Loss: 5.4780319013865215, Val Loss: 5.787707422695686, Val MAE: 1.6375713348388672\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 222/2000, Train Loss: 5.477925450730513, Val Loss: 5.787514450459851, Val MAE: 1.638155221939087\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 223/2000, Train Loss: 5.477831700723184, Val Loss: 5.787530234106234, Val MAE: 1.6377471685409546\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 224/2000, Train Loss: 5.477594338175428, Val Loss: 5.787621850459678, Val MAE: 1.6374701261520386\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 225/2000, Train Loss: 5.47728090707725, Val Loss: 5.7877812917757065, Val MAE: 1.6371181011199951\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 226/2000, Train Loss: 5.477223377976482, Val Loss: 5.787980462588738, Val MAE: 1.636350154876709\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 227/2000, Train Loss: 5.476826825117038, Val Loss: 5.787521804273702, Val MAE: 1.63735032081604\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 228/2000, Train Loss: 5.476791362464019, Val Loss: 5.78730585882494, Val MAE: 1.637344241142273\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 229/2000, Train Loss: 5.477020471527326, Val Loss: 5.786855130259848, Val MAE: 1.6390235424041748\n",
      "Epoch 230/2000, Train Loss: 5.476359807950862, Val Loss: 5.787643847670442, Val MAE: 1.637073278427124\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 231/2000, Train Loss: 5.4765865565197105, Val Loss: 5.787545338741398, Val MAE: 1.6375726461410522\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 232/2000, Train Loss: 5.4760563395162745, Val Loss: 5.787426957417329, Val MAE: 1.6377488374710083\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 233/2000, Train Loss: 5.4757906207991205, Val Loss: 5.7871134909702615, Val MAE: 1.6385412216186523\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 234/2000, Train Loss: 5.475680308063182, Val Loss: 5.787337034004021, Val MAE: 1.63761305809021\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 235/2000, Train Loss: 5.475444044871805, Val Loss: 5.787103828212403, Val MAE: 1.6381529569625854\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 236/2000, Train Loss: 5.47517770651504, Val Loss: 5.787567853468821, Val MAE: 1.6374400854110718\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 237/2000, Train Loss: 5.475076137582516, Val Loss: 5.787229329366727, Val MAE: 1.6381301879882812\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 238/2000, Train Loss: 5.474846328512313, Val Loss: 5.787207001214458, Val MAE: 1.6381970643997192\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 239/2000, Train Loss: 5.474656362635665, Val Loss: 5.787469372678981, Val MAE: 1.6373097896575928\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 240/2000, Train Loss: 5.474545777318506, Val Loss: 5.787540837080529, Val MAE: 1.6373817920684814\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 241/2000, Train Loss: 5.474644030184331, Val Loss: 5.787400913972592, Val MAE: 1.6372747421264648\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 242/2000, Train Loss: 5.473873895241472, Val Loss: 5.787204707257636, Val MAE: 1.6378904581069946\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 243/2000, Train Loss: 5.47444532076605, Val Loss: 5.787348930464103, Val MAE: 1.6373862028121948\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 244/2000, Train Loss: 5.474084468258301, Val Loss: 5.787141884038508, Val MAE: 1.6384453773498535\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 245/2000, Train Loss: 5.4735521962607985, Val Loss: 5.786900709040888, Val MAE: 1.639054775238037\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 246/2000, Train Loss: 5.473294099084695, Val Loss: 5.78718262195893, Val MAE: 1.6380971670150757\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 247/2000, Train Loss: 5.473349713114242, Val Loss: 5.78695432431733, Val MAE: 1.6384197473526\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 248/2000, Train Loss: 5.473288284367727, Val Loss: 5.786967862027054, Val MAE: 1.6382259130477905\n",
      "EarlyStopping counter: 19 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [09:30<06:07, 183.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/2000, Train Loss: 5.473042835614657, Val Loss: 5.787392699772001, Val MAE: 1.637574315071106\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.480794429779053\n",
      "Test Mean Absolute Error (MAE): 1.590791333636467\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 447, 'position': 'DEF', 'window_size': 3, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 447\n",
      "position DEF\n",
      "window_size 3\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  3\n",
      "======= Generating CNN Data for Season: ['2020-21'], Position: DEF =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type DEF = 245.\n",
      "65 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (6124, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (6662, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (6124, 7)\n",
      "Shape of a given window (prior to preprocessing): (3, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.60161372e+00 4.23464448e+01 2.14321735e-02 3.02571861e-02\n",
      " 1.41956631e-01 8.25668180e+00 6.70700958e-02 4.28643469e-03\n",
      " 2.77357539e-03 0.00000000e+00]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.71547691 43.68105181  0.14655075  0.17990944  0.34900565 10.29514887\n",
      "  0.25014336  0.0653304   0.05259166  1.        ]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building rnn Architecture ======\n",
      "====== Done Building rnn Architecture ======\n",
      "Epoch 1/2000, Train Loss: 9.530986792162844, Val Loss: 8.718777947404126, Val MAE: 1.5801382064819336\n",
      "Epoch 2/2000, Train Loss: 9.343111299631888, Val Loss: 8.558636723514029, Val MAE: 1.5688056945800781\n",
      "Epoch 3/2000, Train Loss: 9.156957532230177, Val Loss: 8.401192959206556, Val MAE: 1.5578155517578125\n",
      "Epoch 4/2000, Train Loss: 8.97205118338267, Val Loss: 8.2412592250958, Val MAE: 1.5468579530715942\n",
      "Epoch 5/2000, Train Loss: 8.787641732316269, Val Loss: 8.084296012591757, Val MAE: 1.535634994506836\n",
      "Epoch 6/2000, Train Loss: 8.602962823551998, Val Loss: 7.925856581118949, Val MAE: 1.5238937139511108\n",
      "Epoch 7/2000, Train Loss: 8.415930683152718, Val Loss: 7.767270820204438, Val MAE: 1.5117183923721313\n",
      "Epoch 8/2000, Train Loss: 8.22816759034207, Val Loss: 7.605258289683385, Val MAE: 1.498799443244934\n",
      "Epoch 9/2000, Train Loss: 8.039558472340566, Val Loss: 7.4460083319912815, Val MAE: 1.4865803718566895\n",
      "Epoch 10/2000, Train Loss: 7.852127395178142, Val Loss: 7.289786996822817, Val MAE: 1.475974440574646\n",
      "Epoch 11/2000, Train Loss: 7.6676393161740215, Val Loss: 7.135171219522331, Val MAE: 1.4682749509811401\n",
      "Epoch 12/2000, Train Loss: 7.487045085221006, Val Loss: 6.9876759531595685, Val MAE: 1.4640558958053589\n",
      "Epoch 13/2000, Train Loss: 7.312504151411224, Val Loss: 6.844968098979844, Val MAE: 1.4630684852600098\n",
      "Epoch 14/2000, Train Loss: 7.146557023650722, Val Loss: 6.710270120016382, Val MAE: 1.4650086164474487\n",
      "Epoch 15/2000, Train Loss: 6.98926485839643, Val Loss: 6.584420059281041, Val MAE: 1.4693697690963745\n",
      "Epoch 16/2000, Train Loss: 6.841913226403688, Val Loss: 6.469831397801773, Val MAE: 1.476043462753296\n",
      "Epoch 17/2000, Train Loss: 6.706880800556719, Val Loss: 6.364823683873352, Val MAE: 1.4855170249938965\n",
      "Epoch 18/2000, Train Loss: 6.585602818873891, Val Loss: 6.272125031316754, Val MAE: 1.4951061010360718\n",
      "Epoch 19/2000, Train Loss: 6.477750777152547, Val Loss: 6.190678875091096, Val MAE: 1.5064321756362915\n",
      "Epoch 20/2000, Train Loss: 6.382572901876349, Val Loss: 6.123751622325969, Val MAE: 1.5196646451950073\n",
      "Epoch 21/2000, Train Loss: 6.30051958247235, Val Loss: 6.067194154996257, Val MAE: 1.5325040817260742\n",
      "Epoch 22/2000, Train Loss: 6.230649078101442, Val Loss: 6.016604151133213, Val MAE: 1.5475261211395264\n",
      "Epoch 23/2000, Train Loss: 6.171712797984743, Val Loss: 5.977915257545129, Val MAE: 1.5612850189208984\n",
      "Epoch 24/2000, Train Loss: 6.123238148396475, Val Loss: 5.946808257813413, Val MAE: 1.573001503944397\n",
      "Epoch 25/2000, Train Loss: 6.083172243938112, Val Loss: 5.922301353226254, Val MAE: 1.584126353263855\n",
      "Epoch 26/2000, Train Loss: 6.050553294650295, Val Loss: 5.901971081129979, Val MAE: 1.5951995849609375\n",
      "Epoch 27/2000, Train Loss: 6.024369910097959, Val Loss: 5.887911211459297, Val MAE: 1.6029924154281616\n",
      "Epoch 28/2000, Train Loss: 6.003121850783365, Val Loss: 5.877003148537452, Val MAE: 1.6091557741165161\n",
      "Epoch 29/2000, Train Loss: 5.986030600572887, Val Loss: 5.867983615157972, Val MAE: 1.617035984992981\n",
      "Epoch 30/2000, Train Loss: 5.97167274199034, Val Loss: 5.861147132258021, Val MAE: 1.6226398944854736\n",
      "Epoch 31/2000, Train Loss: 5.960205155506468, Val Loss: 5.856318835554452, Val MAE: 1.6255546808242798\n",
      "Epoch 32/2000, Train Loss: 5.950625784564436, Val Loss: 5.851855471604198, Val MAE: 1.6297649145126343\n",
      "Epoch 33/2000, Train Loss: 5.942198117574056, Val Loss: 5.8485227608417025, Val MAE: 1.6323081254959106\n",
      "Epoch 34/2000, Train Loss: 5.9353344116294595, Val Loss: 5.8456012772986385, Val MAE: 1.6348164081573486\n",
      "Epoch 35/2000, Train Loss: 5.929028131459889, Val Loss: 5.843181971225478, Val MAE: 1.636705994606018\n",
      "Epoch 36/2000, Train Loss: 5.923097929410767, Val Loss: 5.840931363127491, Val MAE: 1.6376101970672607\n",
      "Epoch 37/2000, Train Loss: 5.918713960731239, Val Loss: 5.8388040780866, Val MAE: 1.6398972272872925\n",
      "Epoch 38/2000, Train Loss: 5.913694044999909, Val Loss: 5.83656943922471, Val MAE: 1.639574408531189\n",
      "Epoch 39/2000, Train Loss: 5.90938145026826, Val Loss: 5.834665753272112, Val MAE: 1.6391311883926392\n",
      "Epoch 40/2000, Train Loss: 5.905319375949993, Val Loss: 5.832670706933245, Val MAE: 1.6396100521087646\n",
      "Epoch 41/2000, Train Loss: 5.9016225923571675, Val Loss: 5.830975722397933, Val MAE: 1.6401396989822388\n",
      "Epoch 42/2000, Train Loss: 5.897697421542385, Val Loss: 5.828911848322633, Val MAE: 1.6406757831573486\n",
      "Epoch 43/2000, Train Loss: 5.894549593590853, Val Loss: 5.82720932312111, Val MAE: 1.6400878429412842\n",
      "Epoch 44/2000, Train Loss: 5.891194209717868, Val Loss: 5.825198836813674, Val MAE: 1.6391348838806152\n",
      "Epoch 45/2000, Train Loss: 5.888258493783181, Val Loss: 5.823120246246874, Val MAE: 1.6388332843780518\n",
      "Epoch 46/2000, Train Loss: 5.884970986006553, Val Loss: 5.821434159983112, Val MAE: 1.6387174129486084\n",
      "Epoch 47/2000, Train Loss: 5.8820503652095795, Val Loss: 5.819520153623939, Val MAE: 1.6377063989639282\n",
      "Epoch 48/2000, Train Loss: 5.879297233464425, Val Loss: 5.81841006368961, Val MAE: 1.637887716293335\n",
      "Epoch 49/2000, Train Loss: 5.876291835517214, Val Loss: 5.815952219742804, Val MAE: 1.6375867128372192\n",
      "Epoch 50/2000, Train Loss: 5.8736365981269305, Val Loss: 5.814245774983274, Val MAE: 1.6382334232330322\n",
      "Epoch 51/2000, Train Loss: 5.870882616754164, Val Loss: 5.812378263814773, Val MAE: 1.6365476846694946\n",
      "Epoch 52/2000, Train Loss: 5.8687277530369, Val Loss: 5.810108517390532, Val MAE: 1.636222004890442\n",
      "Epoch 53/2000, Train Loss: 5.865896961145234, Val Loss: 5.808690044735652, Val MAE: 1.6370017528533936\n",
      "Epoch 54/2000, Train Loss: 5.863468768303854, Val Loss: 5.80736538645658, Val MAE: 1.6378945112228394\n",
      "Epoch 55/2000, Train Loss: 5.861324643879606, Val Loss: 5.80595369903624, Val MAE: 1.637929081916809\n",
      "Epoch 56/2000, Train Loss: 5.858830558626275, Val Loss: 5.803933125932106, Val MAE: 1.635854959487915\n",
      "Epoch 57/2000, Train Loss: 5.856780332431459, Val Loss: 5.802112550803726, Val MAE: 1.6360143423080444\n",
      "Epoch 58/2000, Train Loss: 5.854676816547126, Val Loss: 5.800845972655575, Val MAE: 1.634437084197998\n",
      "Epoch 59/2000, Train Loss: 5.852302917262964, Val Loss: 5.79932199768533, Val MAE: 1.6347652673721313\n",
      "Epoch 60/2000, Train Loss: 5.85040971898196, Val Loss: 5.797687502596079, Val MAE: 1.6349728107452393\n",
      "Epoch 61/2000, Train Loss: 5.848390816596517, Val Loss: 5.796406195546872, Val MAE: 1.634110689163208\n",
      "Epoch 62/2000, Train Loss: 5.846199531304209, Val Loss: 5.795288054293135, Val MAE: 1.635086178779602\n",
      "Epoch 63/2000, Train Loss: 5.844171775014777, Val Loss: 5.793611677885831, Val MAE: 1.6347895860671997\n",
      "Epoch 64/2000, Train Loss: 5.8427250782648725, Val Loss: 5.792723871006467, Val MAE: 1.634390115737915\n",
      "Epoch 65/2000, Train Loss: 5.840421268814488, Val Loss: 5.790986165026216, Val MAE: 1.635420322418213\n",
      "Epoch 66/2000, Train Loss: 5.838960674771092, Val Loss: 5.789567464035521, Val MAE: 1.6338748931884766\n",
      "Epoch 67/2000, Train Loss: 5.837025680040059, Val Loss: 5.788462338233506, Val MAE: 1.6340028047561646\n",
      "Epoch 68/2000, Train Loss: 5.835541778489163, Val Loss: 5.786701760202084, Val MAE: 1.6345409154891968\n",
      "Epoch 69/2000, Train Loss: 5.833705092731275, Val Loss: 5.786086541301799, Val MAE: 1.6344550848007202\n",
      "Epoch 70/2000, Train Loss: 5.832085093908143, Val Loss: 5.784902921030098, Val MAE: 1.6314054727554321\n",
      "Epoch 71/2000, Train Loss: 5.830677689167491, Val Loss: 5.783300006707613, Val MAE: 1.6312249898910522\n",
      "Epoch 72/2000, Train Loss: 5.828955187086473, Val Loss: 5.782243862660891, Val MAE: 1.6327428817749023\n",
      "Epoch 73/2000, Train Loss: 5.827714774692268, Val Loss: 5.781790320719693, Val MAE: 1.6332416534423828\n",
      "Epoch 74/2000, Train Loss: 5.8262210781114145, Val Loss: 5.78019537053949, Val MAE: 1.6312191486358643\n",
      "Epoch 75/2000, Train Loss: 5.82479842399296, Val Loss: 5.7797724296306745, Val MAE: 1.6324121952056885\n",
      "Epoch 76/2000, Train Loss: 5.8235448964855125, Val Loss: 5.778756669689171, Val MAE: 1.6325645446777344\n",
      "Epoch 77/2000, Train Loss: 5.822145396157315, Val Loss: 5.778006092945166, Val MAE: 1.6333247423171997\n",
      "Epoch 78/2000, Train Loss: 5.8209332415932105, Val Loss: 5.776717657226979, Val MAE: 1.631295919418335\n",
      "Epoch 79/2000, Train Loss: 5.819663984733715, Val Loss: 5.776425805274899, Val MAE: 1.63225257396698\n",
      "Epoch 80/2000, Train Loss: 5.818221853490462, Val Loss: 5.775364457281208, Val MAE: 1.6324799060821533\n",
      "Epoch 81/2000, Train Loss: 5.817080805176182, Val Loss: 5.774402780973376, Val MAE: 1.631931185722351\n",
      "Epoch 82/2000, Train Loss: 5.815961749930131, Val Loss: 5.773681366373831, Val MAE: 1.6307748556137085\n",
      "Epoch 83/2000, Train Loss: 5.814887366796794, Val Loss: 5.7730772596097415, Val MAE: 1.631223440170288\n",
      "Epoch 84/2000, Train Loss: 5.813687899656463, Val Loss: 5.772499896057566, Val MAE: 1.6298086643218994\n",
      "Epoch 85/2000, Train Loss: 5.812766138921704, Val Loss: 5.771920763213059, Val MAE: 1.6325229406356812\n",
      "Epoch 86/2000, Train Loss: 5.8116350508572765, Val Loss: 5.770827290914085, Val MAE: 1.631027102470398\n",
      "Epoch 87/2000, Train Loss: 5.810452536532753, Val Loss: 5.77003554950384, Val MAE: 1.6311904191970825\n",
      "Epoch 88/2000, Train Loss: 5.80981258237571, Val Loss: 5.7693377146103515, Val MAE: 1.6315449476242065\n",
      "Epoch 89/2000, Train Loss: 5.8086970689003925, Val Loss: 5.769093896323547, Val MAE: 1.632426142692566\n",
      "Epoch 90/2000, Train Loss: 5.807692549730602, Val Loss: 5.768100884027425, Val MAE: 1.6315946578979492\n",
      "Epoch 91/2000, Train Loss: 5.806708478091056, Val Loss: 5.767723999631506, Val MAE: 1.6326673030853271\n",
      "Epoch 92/2000, Train Loss: 5.806196314201021, Val Loss: 5.766762632321598, Val MAE: 1.6310662031173706\n",
      "Epoch 93/2000, Train Loss: 5.804992667415686, Val Loss: 5.766507092899677, Val MAE: 1.6303118467330933\n",
      "Epoch 94/2000, Train Loss: 5.804171043529845, Val Loss: 5.76625889993443, Val MAE: 1.6318597793579102\n",
      "Epoch 95/2000, Train Loss: 5.803407008187813, Val Loss: 5.765134094129077, Val MAE: 1.628981590270996\n",
      "Epoch 96/2000, Train Loss: 5.802369980435622, Val Loss: 5.764861410286183, Val MAE: 1.6310725212097168\n",
      "Epoch 97/2000, Train Loss: 5.8019920671195315, Val Loss: 5.763903709379356, Val MAE: 1.6283037662506104\n",
      "Epoch 98/2000, Train Loss: 5.801014173449132, Val Loss: 5.7637390423379475, Val MAE: 1.6318168640136719\n",
      "Epoch 99/2000, Train Loss: 5.8003064833189315, Val Loss: 5.763426638611277, Val MAE: 1.632456660270691\n",
      "Epoch 100/2000, Train Loss: 5.799576370339644, Val Loss: 5.763040700514129, Val MAE: 1.6298977136611938\n",
      "Epoch 101/2000, Train Loss: 5.798665015321029, Val Loss: 5.7625488170804955, Val MAE: 1.6301509141921997\n",
      "Epoch 102/2000, Train Loss: 5.798407410320483, Val Loss: 5.762429187727502, Val MAE: 1.6309313774108887\n",
      "Epoch 103/2000, Train Loss: 5.797283052352437, Val Loss: 5.76176776029727, Val MAE: 1.6301839351654053\n",
      "Epoch 104/2000, Train Loss: 5.796915783171068, Val Loss: 5.761633830172874, Val MAE: 1.6297500133514404\n",
      "Epoch 105/2000, Train Loss: 5.795774617738891, Val Loss: 5.760973084337461, Val MAE: 1.6313285827636719\n",
      "Epoch 106/2000, Train Loss: 5.795054696108165, Val Loss: 5.7605452847775505, Val MAE: 1.6303825378417969\n",
      "Epoch 107/2000, Train Loss: 5.794445326453761, Val Loss: 5.760370573131931, Val MAE: 1.633489966392517\n",
      "Epoch 108/2000, Train Loss: 5.794375376743183, Val Loss: 5.759761093030444, Val MAE: 1.6321322917938232\n",
      "Epoch 109/2000, Train Loss: 5.793668128942189, Val Loss: 5.759856975148386, Val MAE: 1.6306557655334473\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 110/2000, Train Loss: 5.792800844761363, Val Loss: 5.759566167460375, Val MAE: 1.632500171661377\n",
      "Epoch 111/2000, Train Loss: 5.792044259999928, Val Loss: 5.7586329217537235, Val MAE: 1.6317094564437866\n",
      "Epoch 112/2000, Train Loss: 5.791281764967399, Val Loss: 5.758484471829277, Val MAE: 1.6307342052459717\n",
      "Epoch 113/2000, Train Loss: 5.791050087987331, Val Loss: 5.758422984654979, Val MAE: 1.6303696632385254\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 114/2000, Train Loss: 5.7905183758652, Val Loss: 5.757845140542779, Val MAE: 1.6318539381027222\n",
      "Epoch 115/2000, Train Loss: 5.789970184627332, Val Loss: 5.758002474157548, Val MAE: 1.6301968097686768\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 116/2000, Train Loss: 5.789312680562337, Val Loss: 5.757467605886168, Val MAE: 1.6303457021713257\n",
      "Epoch 117/2000, Train Loss: 5.788959799105661, Val Loss: 5.75660616186585, Val MAE: 1.6285828351974487\n",
      "Epoch 118/2000, Train Loss: 5.788107400400596, Val Loss: 5.756764582557653, Val MAE: 1.630415916442871\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 119/2000, Train Loss: 5.787600062395397, Val Loss: 5.7568470673198116, Val MAE: 1.6307507753372192\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 120/2000, Train Loss: 5.7872152119352105, Val Loss: 5.756473539398332, Val MAE: 1.630072832107544\n",
      "Epoch 121/2000, Train Loss: 5.7866270521230865, Val Loss: 5.756411630919636, Val MAE: 1.6312005519866943\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 122/2000, Train Loss: 5.786172615854364, Val Loss: 5.755997877425005, Val MAE: 1.630652904510498\n",
      "Epoch 123/2000, Train Loss: 5.785861733712648, Val Loss: 5.7558950282725405, Val MAE: 1.6301501989364624\n",
      "Epoch 124/2000, Train Loss: 5.785391164453406, Val Loss: 5.754997298402606, Val MAE: 1.6285268068313599\n",
      "Epoch 125/2000, Train Loss: 5.78480605284373, Val Loss: 5.754967540599498, Val MAE: 1.6300266981124878\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 126/2000, Train Loss: 5.784207558422758, Val Loss: 5.75505017644799, Val MAE: 1.6297715902328491\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 127/2000, Train Loss: 5.784298862281599, Val Loss: 5.754090512039293, Val MAE: 1.6303318738937378\n",
      "Epoch 128/2000, Train Loss: 5.783543670386599, Val Loss: 5.7542380870900995, Val MAE: 1.6298621892929077\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 129/2000, Train Loss: 5.783236792213039, Val Loss: 5.754703601149669, Val MAE: 1.6315088272094727\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 130/2000, Train Loss: 5.7828000010105605, Val Loss: 5.753711632186899, Val MAE: 1.6300032138824463\n",
      "Epoch 131/2000, Train Loss: 5.782301561874256, Val Loss: 5.753919096681773, Val MAE: 1.6295570135116577\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 132/2000, Train Loss: 5.7817176507230394, Val Loss: 5.7535843203885255, Val MAE: 1.630293369293213\n",
      "Epoch 133/2000, Train Loss: 5.7819781052438834, Val Loss: 5.753220158578827, Val MAE: 1.6302598714828491\n",
      "Epoch 134/2000, Train Loss: 5.781007305571907, Val Loss: 5.752951334107597, Val MAE: 1.6283963918685913\n",
      "Epoch 135/2000, Train Loss: 5.780824993786059, Val Loss: 5.752990050989076, Val MAE: 1.630988359451294\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 136/2000, Train Loss: 5.780266645707582, Val Loss: 5.752646604760095, Val MAE: 1.6303552389144897\n",
      "Epoch 137/2000, Train Loss: 5.7798856528181775, Val Loss: 5.752229835433314, Val MAE: 1.6304231882095337\n",
      "Epoch 138/2000, Train Loss: 5.779811605026848, Val Loss: 5.7524785747292295, Val MAE: 1.6310150623321533\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 139/2000, Train Loss: 5.778984611494499, Val Loss: 5.751978719087103, Val MAE: 1.6303962469100952\n",
      "Epoch 140/2000, Train Loss: 5.7785873664052865, Val Loss: 5.752061287923998, Val MAE: 1.6304354667663574\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 141/2000, Train Loss: 5.778080833585639, Val Loss: 5.751668302130218, Val MAE: 1.628542184829712\n",
      "Epoch 142/2000, Train Loss: 5.778020237621508, Val Loss: 5.751814247496188, Val MAE: 1.629893183708191\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 143/2000, Train Loss: 5.777510681696105, Val Loss: 5.751629800244284, Val MAE: 1.6312329769134521\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 144/2000, Train Loss: 5.777182733803465, Val Loss: 5.751328805862655, Val MAE: 1.631447672843933\n",
      "Epoch 145/2000, Train Loss: 5.77681619451757, Val Loss: 5.750496664525009, Val MAE: 1.6295565366744995\n",
      "Epoch 146/2000, Train Loss: 5.776533362112548, Val Loss: 5.750377169815689, Val MAE: 1.6290256977081299\n",
      "Epoch 147/2000, Train Loss: 5.7760457124626425, Val Loss: 5.750516360971008, Val MAE: 1.6281737089157104\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 148/2000, Train Loss: 5.7757593727948375, Val Loss: 5.750281968982327, Val MAE: 1.629427194595337\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 149/2000, Train Loss: 5.775334123979535, Val Loss: 5.75021475976213, Val MAE: 1.630200982093811\n",
      "Epoch 150/2000, Train Loss: 5.775162807682104, Val Loss: 5.750179345373527, Val MAE: 1.6303457021713257\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 151/2000, Train Loss: 5.774745995538277, Val Loss: 5.749823312132717, Val MAE: 1.629660964012146\n",
      "Epoch 152/2000, Train Loss: 5.7745894653755325, Val Loss: 5.749829416082676, Val MAE: 1.628927230834961\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 153/2000, Train Loss: 5.774079182691741, Val Loss: 5.7498308329436405, Val MAE: 1.63128662109375\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 154/2000, Train Loss: 5.773878969644246, Val Loss: 5.749675407297982, Val MAE: 1.6296896934509277\n",
      "Epoch 155/2000, Train Loss: 5.7736486886676985, Val Loss: 5.749225044064345, Val MAE: 1.6280831098556519\n",
      "Epoch 156/2000, Train Loss: 5.773349435705888, Val Loss: 5.749071667236248, Val MAE: 1.630027413368225\n",
      "Epoch 157/2000, Train Loss: 5.772935591245952, Val Loss: 5.749003129883408, Val MAE: 1.6283769607543945\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 158/2000, Train Loss: 5.7727617318170115, Val Loss: 5.748793721741953, Val MAE: 1.6287883520126343\n",
      "Epoch 159/2000, Train Loss: 5.772734050165143, Val Loss: 5.748885880993338, Val MAE: 1.629831075668335\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 160/2000, Train Loss: 5.7720893035855205, Val Loss: 5.748693130508055, Val MAE: 1.629241704940796\n",
      "Epoch 161/2000, Train Loss: 5.771698603504582, Val Loss: 5.748660135982714, Val MAE: 1.6315864324569702\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 162/2000, Train Loss: 5.771267591861257, Val Loss: 5.748254908783217, Val MAE: 1.629982590675354\n",
      "Epoch 163/2000, Train Loss: 5.771304582294665, Val Loss: 5.748299264876813, Val MAE: 1.629982829093933\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 164/2000, Train Loss: 5.7707521350760205, Val Loss: 5.748414286872204, Val MAE: 1.6289335489273071\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 165/2000, Train Loss: 5.7703533925508195, Val Loss: 5.748000204214122, Val MAE: 1.6310534477233887\n",
      "Epoch 166/2000, Train Loss: 5.770308512867543, Val Loss: 5.748217558504004, Val MAE: 1.630954384803772\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 167/2000, Train Loss: 5.770211219787598, Val Loss: 5.747832836543265, Val MAE: 1.6299152374267578\n",
      "Epoch 168/2000, Train Loss: 5.769599871677265, Val Loss: 5.747472003596746, Val MAE: 1.630550742149353\n",
      "Epoch 169/2000, Train Loss: 5.769441703955333, Val Loss: 5.747608348620582, Val MAE: 1.629996657371521\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 170/2000, Train Loss: 5.769315366159406, Val Loss: 5.746890716763697, Val MAE: 1.6302546262741089\n",
      "Epoch 171/2000, Train Loss: 5.7689999111911705, Val Loss: 5.747288268963548, Val MAE: 1.6312196254730225\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 172/2000, Train Loss: 5.769188328793175, Val Loss: 5.746891013352066, Val MAE: 1.6288456916809082\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 173/2000, Train Loss: 5.7683410801385575, Val Loss: 5.746784740713562, Val MAE: 1.631605625152588\n",
      "Epoch 174/2000, Train Loss: 5.7681465274409245, Val Loss: 5.7467626613227285, Val MAE: 1.6300876140594482\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 175/2000, Train Loss: 5.767821056800976, Val Loss: 5.7464027013884165, Val MAE: 1.6296547651290894\n",
      "Epoch 176/2000, Train Loss: 5.767906038384688, Val Loss: 5.74645481252329, Val MAE: 1.630048155784607\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 177/2000, Train Loss: 5.767336869448946, Val Loss: 5.746777847515893, Val MAE: 1.6295264959335327\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 178/2000, Train Loss: 5.766845504442851, Val Loss: 5.746551904572864, Val MAE: 1.630411148071289\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 179/2000, Train Loss: 5.766580276321947, Val Loss: 5.746270740970669, Val MAE: 1.631240963935852\n",
      "Epoch 180/2000, Train Loss: 5.766461080626438, Val Loss: 5.745845414635107, Val MAE: 1.6293152570724487\n",
      "Epoch 181/2000, Train Loss: 5.766408667229769, Val Loss: 5.746088596311729, Val MAE: 1.6312901973724365\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 182/2000, Train Loss: 5.766096468557391, Val Loss: 5.745300106825423, Val MAE: 1.6287270784378052\n",
      "Epoch 183/2000, Train Loss: 5.766012470973165, Val Loss: 5.745725974838068, Val MAE: 1.6306190490722656\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 184/2000, Train Loss: 5.765626285159797, Val Loss: 5.745039011645643, Val MAE: 1.6295312643051147\n",
      "Epoch 185/2000, Train Loss: 5.765379692378797, Val Loss: 5.74531393789516, Val MAE: 1.6287370920181274\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 186/2000, Train Loss: 5.765240335673616, Val Loss: 5.745618442319474, Val MAE: 1.6299740076065063\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 187/2000, Train Loss: 5.764672461308931, Val Loss: 5.745414029644461, Val MAE: 1.6315487623214722\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 188/2000, Train Loss: 5.7647227854059455, Val Loss: 5.7453421556724695, Val MAE: 1.6326234340667725\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 189/2000, Train Loss: 5.764571373922783, Val Loss: 5.7444228164236035, Val MAE: 1.6293271780014038\n",
      "Epoch 190/2000, Train Loss: 5.764220988541319, Val Loss: 5.744805460404326, Val MAE: 1.6294901371002197\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 191/2000, Train Loss: 5.763626577561362, Val Loss: 5.744550915298015, Val MAE: 1.6306735277175903\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 192/2000, Train Loss: 5.763472900056002, Val Loss: 5.744272805377114, Val MAE: 1.6301075220108032\n",
      "Epoch 193/2000, Train Loss: 5.76345271813242, Val Loss: 5.744392829353963, Val MAE: 1.6298201084136963\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 194/2000, Train Loss: 5.7630786634328075, Val Loss: 5.744221577492331, Val MAE: 1.6308397054672241\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 195/2000, Train Loss: 5.762712445175438, Val Loss: 5.7443244545662955, Val MAE: 1.629044771194458\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 196/2000, Train Loss: 5.762611259493911, Val Loss: 5.7438297209680735, Val MAE: 1.6307121515274048\n",
      "Epoch 197/2000, Train Loss: 5.762340438993354, Val Loss: 5.74381659227916, Val MAE: 1.6281208992004395\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 198/2000, Train Loss: 5.76229573132699, Val Loss: 5.743538752266084, Val MAE: 1.6285488605499268\n",
      "Epoch 199/2000, Train Loss: 5.762243394266095, Val Loss: 5.743631272945764, Val MAE: 1.6293829679489136\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 200/2000, Train Loss: 5.762030957037942, Val Loss: 5.743730866761664, Val MAE: 1.6298774480819702\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 201/2000, Train Loss: 5.7617636877193785, Val Loss: 5.743439675618397, Val MAE: 1.6286066770553589\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 202/2000, Train Loss: 5.761273209463086, Val Loss: 5.743450044732476, Val MAE: 1.630050539970398\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 203/2000, Train Loss: 5.761165389889165, Val Loss: 5.743469967963096, Val MAE: 1.6290019750595093\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 204/2000, Train Loss: 5.76094691167798, Val Loss: 5.743337387998304, Val MAE: 1.6295998096466064\n",
      "Epoch 205/2000, Train Loss: 5.760674541456657, Val Loss: 5.743249552546186, Val MAE: 1.6291638612747192\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 206/2000, Train Loss: 5.760525266329448, Val Loss: 5.743190972310193, Val MAE: 1.6286473274230957\n",
      "Epoch 207/2000, Train Loss: 5.760124965717918, Val Loss: 5.742612288381345, Val MAE: 1.6300959587097168\n",
      "Epoch 208/2000, Train Loss: 5.759940295888667, Val Loss: 5.742389057546602, Val MAE: 1.6291245222091675\n",
      "Epoch 209/2000, Train Loss: 5.759739167857588, Val Loss: 5.742326308320541, Val MAE: 1.628724217414856\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 210/2000, Train Loss: 5.759853115207271, Val Loss: 5.742781632273866, Val MAE: 1.630261778831482\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 211/2000, Train Loss: 5.759624004364014, Val Loss: 5.7418577771878665, Val MAE: 1.629287600517273\n",
      "Epoch 212/2000, Train Loss: 5.759125673980043, Val Loss: 5.742234798088893, Val MAE: 1.6285673379898071\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 213/2000, Train Loss: 5.7588136865381605, Val Loss: 5.742281522856335, Val MAE: 1.6300467252731323\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 214/2000, Train Loss: 5.7586071825864025, Val Loss: 5.74188213031945, Val MAE: 1.6276458501815796\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 215/2000, Train Loss: 5.758493587636111, Val Loss: 5.741942194428007, Val MAE: 1.629510521888733\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 216/2000, Train Loss: 5.758740931226496, Val Loss: 5.741846690491259, Val MAE: 1.6305699348449707\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 217/2000, Train Loss: 5.758159982530694, Val Loss: 5.742069060723349, Val MAE: 1.6290438175201416\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 218/2000, Train Loss: 5.758066634337108, Val Loss: 5.74071410978934, Val MAE: 1.6268638372421265\n",
      "Epoch 219/2000, Train Loss: 5.757892907711497, Val Loss: 5.740949721947535, Val MAE: 1.629272222518921\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 220/2000, Train Loss: 5.757723966188598, Val Loss: 5.7415853958278795, Val MAE: 1.63052499294281\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 221/2000, Train Loss: 5.7570445767620155, Val Loss: 5.741508214599371, Val MAE: 1.6302305459976196\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 222/2000, Train Loss: 5.757021287031341, Val Loss: 5.741101028861825, Val MAE: 1.629530668258667\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 223/2000, Train Loss: 5.756790443470604, Val Loss: 5.741351488742568, Val MAE: 1.6297338008880615\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 224/2000, Train Loss: 5.756875273428466, Val Loss: 5.740662994521271, Val MAE: 1.627468466758728\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 225/2000, Train Loss: 5.756571894152122, Val Loss: 5.740730791333905, Val MAE: 1.6284799575805664\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 226/2000, Train Loss: 5.756353978525128, Val Loss: 5.74048272804851, Val MAE: 1.6291728019714355\n",
      "Epoch 227/2000, Train Loss: 5.756379341870024, Val Loss: 5.740704438402502, Val MAE: 1.629135251045227\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 228/2000, Train Loss: 5.75591328269557, Val Loss: 5.740788703315353, Val MAE: 1.6273051500320435\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 229/2000, Train Loss: 5.756041161846697, Val Loss: 5.740026107917146, Val MAE: 1.629837989807129\n",
      "Epoch 230/2000, Train Loss: 5.755900091246555, Val Loss: 5.7406398088185915, Val MAE: 1.628866195678711\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 231/2000, Train Loss: 5.755422294139862, Val Loss: 5.739659475675246, Val MAE: 1.6281241178512573\n",
      "Epoch 232/2000, Train Loss: 5.755070987500642, Val Loss: 5.740686362644412, Val MAE: 1.6311885118484497\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 233/2000, Train Loss: 5.754825504202592, Val Loss: 5.740259562984409, Val MAE: 1.6293891668319702\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 234/2000, Train Loss: 5.7546008846216035, Val Loss: 5.739886772810986, Val MAE: 1.629118800163269\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 235/2000, Train Loss: 5.754515903037891, Val Loss: 5.73956344247097, Val MAE: 1.6298904418945312\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 236/2000, Train Loss: 5.754161188476964, Val Loss: 5.739600159862868, Val MAE: 1.6295607089996338\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 237/2000, Train Loss: 5.753947780843367, Val Loss: 5.739516888024672, Val MAE: 1.6300686597824097\n",
      "Epoch 238/2000, Train Loss: 5.753832996937266, Val Loss: 5.739761391629162, Val MAE: 1.6295613050460815\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 239/2000, Train Loss: 5.753589600847478, Val Loss: 5.738776090940874, Val MAE: 1.6290510892868042\n",
      "Epoch 240/2000, Train Loss: 5.753981262968297, Val Loss: 5.739741491045979, Val MAE: 1.6298097372055054\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 241/2000, Train Loss: 5.75335407047941, Val Loss: 5.739064274318201, Val MAE: 1.629292607307434\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 242/2000, Train Loss: 5.753253313533047, Val Loss: 5.7390765088986635, Val MAE: 1.6299875974655151\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 243/2000, Train Loss: 5.752990611812525, Val Loss: 5.739106447571039, Val MAE: 1.6276835203170776\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 244/2000, Train Loss: 5.753007533257468, Val Loss: 5.7389892998188445, Val MAE: 1.6296496391296387\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 245/2000, Train Loss: 5.752522686071563, Val Loss: 5.738830578195327, Val MAE: 1.6301627159118652\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 246/2000, Train Loss: 5.752493979638083, Val Loss: 5.739034205965974, Val MAE: 1.6309939622879028\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 247/2000, Train Loss: 5.75274960409131, Val Loss: 5.738283103987546, Val MAE: 1.6295918226242065\n",
      "Epoch 248/2000, Train Loss: 5.752251493303399, Val Loss: 5.737765843323786, Val MAE: 1.629492998123169\n",
      "Epoch 249/2000, Train Loss: 5.752372617261452, Val Loss: 5.738589168913425, Val MAE: 1.6298984289169312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 250/2000, Train Loss: 5.751945194445159, Val Loss: 5.738576470215598, Val MAE: 1.6308746337890625\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 251/2000, Train Loss: 5.751714023581722, Val Loss: 5.738538340616319, Val MAE: 1.6284161806106567\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 252/2000, Train Loss: 5.751568102000053, Val Loss: 5.738544615197663, Val MAE: 1.6290276050567627\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 253/2000, Train Loss: 5.751399351839433, Val Loss: 5.738361191734061, Val MAE: 1.6281225681304932\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 254/2000, Train Loss: 5.751407434020126, Val Loss: 5.739075005481053, Val MAE: 1.631088376045227\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 255/2000, Train Loss: 5.750987347803618, Val Loss: 5.738481243942648, Val MAE: 1.6311390399932861\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 256/2000, Train Loss: 5.750728306017424, Val Loss: 5.73762569601224, Val MAE: 1.6285815238952637\n",
      "Epoch 257/2000, Train Loss: 5.750398270916521, Val Loss: 5.737940321669768, Val MAE: 1.6287777423858643\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 258/2000, Train Loss: 5.750620450889855, Val Loss: 5.737715463632101, Val MAE: 1.6283305883407593\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 259/2000, Train Loss: 5.750275933951662, Val Loss: 5.737598798301364, Val MAE: 1.629354476928711\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 260/2000, Train Loss: 5.750074551816573, Val Loss: 5.737241720176007, Val MAE: 1.6279999017715454\n",
      "Epoch 261/2000, Train Loss: 5.749887259382951, Val Loss: 5.737607301638819, Val MAE: 1.6296300888061523\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 262/2000, Train Loss: 5.749699701342666, Val Loss: 5.737658861168965, Val MAE: 1.6295136213302612\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 263/2000, Train Loss: 5.749596106378656, Val Loss: 5.73781114998931, Val MAE: 1.6284667253494263\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 264/2000, Train Loss: 5.749694186344481, Val Loss: 5.73747747690552, Val MAE: 1.628053903579712\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 265/2000, Train Loss: 5.749436980799625, Val Loss: 5.7373910635884275, Val MAE: 1.6274107694625854\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 266/2000, Train Loss: 5.74917370290087, Val Loss: 5.73722106229972, Val MAE: 1.6294746398925781\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 267/2000, Train Loss: 5.748793162797627, Val Loss: 5.737084053054442, Val MAE: 1.6288374662399292\n",
      "Epoch 268/2000, Train Loss: 5.748927328670234, Val Loss: 5.7368826180521975, Val MAE: 1.627557635307312\n",
      "Epoch 269/2000, Train Loss: 5.749134782113527, Val Loss: 5.736913944112843, Val MAE: 1.6295512914657593\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 270/2000, Train Loss: 5.74838415990796, Val Loss: 5.736608919107689, Val MAE: 1.6285001039505005\n",
      "Epoch 271/2000, Train Loss: 5.7484031371903, Val Loss: 5.73724316775062, Val MAE: 1.6289983987808228\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 272/2000, Train Loss: 5.747835565031621, Val Loss: 5.73716017667861, Val MAE: 1.6294634342193604\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 273/2000, Train Loss: 5.74822739760081, Val Loss: 5.736265420758577, Val MAE: 1.6285275220870972\n",
      "Epoch 274/2000, Train Loss: 5.747796423602522, Val Loss: 5.736814128476145, Val MAE: 1.6289920806884766\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 275/2000, Train Loss: 5.7475325580228835, Val Loss: 5.7364333471696565, Val MAE: 1.6277903318405151\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 276/2000, Train Loss: 5.747451386953655, Val Loss: 5.736192817414978, Val MAE: 1.6274231672286987\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 277/2000, Train Loss: 5.747781544400935, Val Loss: 5.736069957234291, Val MAE: 1.627942681312561\n",
      "Epoch 278/2000, Train Loss: 5.7468423696986415, Val Loss: 5.736585944677687, Val MAE: 1.628455400466919\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 279/2000, Train Loss: 5.747034461874711, Val Loss: 5.736559386368145, Val MAE: 1.6291167736053467\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 280/2000, Train Loss: 5.746545625360389, Val Loss: 5.7362102311233, Val MAE: 1.6299831867218018\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 281/2000, Train Loss: 5.746444409353691, Val Loss: 5.736474622280937, Val MAE: 1.6290812492370605\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 282/2000, Train Loss: 5.746356169382731, Val Loss: 5.736294292209206, Val MAE: 1.6298866271972656\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 283/2000, Train Loss: 5.746252507494207, Val Loss: 5.736323656939653, Val MAE: 1.6305540800094604\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 284/2000, Train Loss: 5.745729221586595, Val Loss: 5.736281133737359, Val MAE: 1.6295173168182373\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 285/2000, Train Loss: 5.745798348334798, Val Loss: 5.735653969709487, Val MAE: 1.6291508674621582\n",
      "Epoch 286/2000, Train Loss: 5.745888085741746, Val Loss: 5.736426684169236, Val MAE: 1.6311389207839966\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 287/2000, Train Loss: 5.745551813067052, Val Loss: 5.735939338599697, Val MAE: 1.6296913623809814\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 288/2000, Train Loss: 5.745612893188209, Val Loss: 5.736214469917114, Val MAE: 1.6293057203292847\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 289/2000, Train Loss: 5.745706846839504, Val Loss: 5.735505613787421, Val MAE: 1.628764271736145\n",
      "Epoch 290/2000, Train Loss: 5.745309801478135, Val Loss: 5.735928734324547, Val MAE: 1.6273858547210693\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 291/2000, Train Loss: 5.745240878640559, Val Loss: 5.735423604311227, Val MAE: 1.6283038854599\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 292/2000, Train Loss: 5.744734818475289, Val Loss: 5.735825199854397, Val MAE: 1.6280977725982666\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 293/2000, Train Loss: 5.74460295313283, Val Loss: 5.73566168162756, Val MAE: 1.6279678344726562\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 294/2000, Train Loss: 5.744884350843597, Val Loss: 5.735441211608609, Val MAE: 1.6271575689315796\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 295/2000, Train Loss: 5.744544777953834, Val Loss: 5.735560551198489, Val MAE: 1.6277821063995361\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 296/2000, Train Loss: 5.744021252581947, Val Loss: 5.7351071895060795, Val MAE: 1.6286635398864746\n",
      "Epoch 297/2000, Train Loss: 5.743906802252719, Val Loss: 5.735468857305424, Val MAE: 1.6290639638900757\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 298/2000, Train Loss: 5.743762459671288, Val Loss: 5.735311108901272, Val MAE: 1.6279871463775635\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 299/2000, Train Loss: 5.7440195428697685, Val Loss: 5.7351895753020345, Val MAE: 1.6271398067474365\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 300/2000, Train Loss: 5.743506951290264, Val Loss: 5.735518850749704, Val MAE: 1.6280032396316528\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 301/2000, Train Loss: 5.743377651038923, Val Loss: 5.734994566277707, Val MAE: 1.6263564825057983\n",
      "Epoch 302/2000, Train Loss: 5.743181342618508, Val Loss: 5.735162907478169, Val MAE: 1.629692792892456\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 303/2000, Train Loss: 5.74323195428179, Val Loss: 5.73535149331673, Val MAE: 1.6262348890304565\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 304/2000, Train Loss: 5.743229923541086, Val Loss: 5.73519613716458, Val MAE: 1.6267796754837036\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 305/2000, Train Loss: 5.742761764610023, Val Loss: 5.7347400965594435, Val MAE: 1.629072666168213\n",
      "Epoch 306/2000, Train Loss: 5.742994048093495, Val Loss: 5.734519770491333, Val MAE: 1.6276506185531616\n",
      "Epoch 307/2000, Train Loss: 5.74236215834032, Val Loss: 5.734728977597993, Val MAE: 1.6288270950317383\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 308/2000, Train Loss: 5.742502267946277, Val Loss: 5.734713259655388, Val MAE: 1.6291223764419556\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 309/2000, Train Loss: 5.742025845929196, Val Loss: 5.734677716827889, Val MAE: 1.6287496089935303\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 310/2000, Train Loss: 5.74232961943275, Val Loss: 5.734226662382028, Val MAE: 1.6284313201904297\n",
      "Epoch 311/2000, Train Loss: 5.741953561180516, Val Loss: 5.734749801141842, Val MAE: 1.6282432079315186\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 312/2000, Train Loss: 5.741729456081725, Val Loss: 5.734556110632753, Val MAE: 1.6292980909347534\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 313/2000, Train Loss: 5.7415750497265865, Val Loss: 5.734768570283829, Val MAE: 1.6296167373657227\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 314/2000, Train Loss: 5.741274392395689, Val Loss: 5.735026947150544, Val MAE: 1.6283836364746094\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 315/2000, Train Loss: 5.7412847489641425, Val Loss: 5.734218988313098, Val MAE: 1.6281633377075195\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 316/2000, Train Loss: 5.741339448251222, Val Loss: 5.733684538553966, Val MAE: 1.6267098188400269\n",
      "Epoch 317/2000, Train Loss: 5.741278853332787, Val Loss: 5.734708509277661, Val MAE: 1.6279453039169312\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 318/2000, Train Loss: 5.741016515514307, Val Loss: 5.734198952465765, Val MAE: 1.6286360025405884\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 319/2000, Train Loss: 5.740590253420043, Val Loss: 5.7349191602360685, Val MAE: 1.6285736560821533\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 320/2000, Train Loss: 5.7404766521955795, Val Loss: 5.734347143650986, Val MAE: 1.6276166439056396\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 321/2000, Train Loss: 5.740562343806551, Val Loss: 5.734454906987306, Val MAE: 1.6295721530914307\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 322/2000, Train Loss: 5.7400284545463425, Val Loss: 5.734067527830562, Val MAE: 1.627660870552063\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 323/2000, Train Loss: 5.739879971010643, Val Loss: 5.733870376606769, Val MAE: 1.628391146659851\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 324/2000, Train Loss: 5.739602424596486, Val Loss: 5.734475885960834, Val MAE: 1.6285494565963745\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 325/2000, Train Loss: 5.739879545412566, Val Loss: 5.734692097640922, Val MAE: 1.6295984983444214\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 326/2000, Train Loss: 5.739688401682335, Val Loss: 5.734367462436177, Val MAE: 1.6276211738586426\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 327/2000, Train Loss: 5.739573898022635, Val Loss: 5.7344971143664205, Val MAE: 1.6276969909667969\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 328/2000, Train Loss: 5.739382540970518, Val Loss: 5.733547578016193, Val MAE: 1.6265665292739868\n",
      "Epoch 329/2000, Train Loss: 5.7391485488205625, Val Loss: 5.733833087134377, Val MAE: 1.627480149269104\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 330/2000, Train Loss: 5.739017667477591, Val Loss: 5.734400201324681, Val MAE: 1.6274571418762207\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 331/2000, Train Loss: 5.739041242683143, Val Loss: 5.734000922645465, Val MAE: 1.6286767721176147\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 332/2000, Train Loss: 5.738668918609619, Val Loss: 5.733984347202596, Val MAE: 1.629848837852478\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 333/2000, Train Loss: 5.738968617037723, Val Loss: 5.7339294845590985, Val MAE: 1.6281704902648926\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 334/2000, Train Loss: 5.738556797044319, Val Loss: 5.734262578737138, Val MAE: 1.6291829347610474\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 335/2000, Train Loss: 5.738407321143568, Val Loss: 5.733864365418235, Val MAE: 1.629370093345642\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 336/2000, Train Loss: 5.738709522966753, Val Loss: 5.734702219494613, Val MAE: 1.6293549537658691\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 337/2000, Train Loss: 5.7377230849182395, Val Loss: 5.733792623918244, Val MAE: 1.6295579671859741\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 338/2000, Train Loss: 5.738365573841229, Val Loss: 5.733980613787917, Val MAE: 1.6296337842941284\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 339/2000, Train Loss: 5.738053605221865, Val Loss: 5.7337809161234, Val MAE: 1.628827452659607\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 340/2000, Train Loss: 5.7374411905021, Val Loss: 5.73394523662798, Val MAE: 1.628123164176941\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 341/2000, Train Loss: 5.737299779005218, Val Loss: 5.733562351591647, Val MAE: 1.6284866333007812\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 342/2000, Train Loss: 5.73755717486666, Val Loss: 5.733466449308178, Val MAE: 1.6286416053771973\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 343/2000, Train Loss: 5.73720084261476, Val Loss: 5.733342386021115, Val MAE: 1.6284910440444946\n",
      "Epoch 344/2000, Train Loss: 5.7368284066518145, Val Loss: 5.733833639359583, Val MAE: 1.6291131973266602\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 345/2000, Train Loss: 5.7367303758336785, Val Loss: 5.733590132862853, Val MAE: 1.6278955936431885\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 346/2000, Train Loss: 5.737074812253316, Val Loss: 5.733849892154016, Val MAE: 1.629340410232544\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 347/2000, Train Loss: 5.736426597101646, Val Loss: 5.733649248881393, Val MAE: 1.6287606954574585\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 348/2000, Train Loss: 5.736397994192023, Val Loss: 5.73404170354621, Val MAE: 1.6307761669158936\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 349/2000, Train Loss: 5.736301372971451, Val Loss: 5.733869004730443, Val MAE: 1.6298513412475586\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 350/2000, Train Loss: 5.7361024335811015, Val Loss: 5.733697876964363, Val MAE: 1.6298260688781738\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 351/2000, Train Loss: 5.736082852932444, Val Loss: 5.733815599589202, Val MAE: 1.6295274496078491\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 352/2000, Train Loss: 5.73586662819511, Val Loss: 5.7332629201934955, Val MAE: 1.628208875656128\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 353/2000, Train Loss: 5.735960824447766, Val Loss: 5.7333520679560745, Val MAE: 1.6276626586914062\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 354/2000, Train Loss: 5.7354744578662675, Val Loss: 5.733958107818624, Val MAE: 1.6296343803405762\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 355/2000, Train Loss: 5.735307394412526, Val Loss: 5.733580794052094, Val MAE: 1.6295040845870972\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 356/2000, Train Loss: 5.7352029534808375, Val Loss: 5.733767815355985, Val MAE: 1.628631353378296\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 357/2000, Train Loss: 5.7352786900704364, Val Loss: 5.733638873872776, Val MAE: 1.6287835836410522\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 358/2000, Train Loss: 5.73476501305898, Val Loss: 5.733787357923499, Val MAE: 1.629151463508606\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 359/2000, Train Loss: 5.734898725099731, Val Loss: 5.733657676210322, Val MAE: 1.6291155815124512\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 360/2000, Train Loss: 5.734719303616306, Val Loss: 5.733280124800812, Val MAE: 1.6283754110336304\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 361/2000, Train Loss: 5.73483617682206, Val Loss: 5.733258730727663, Val MAE: 1.6294734477996826\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 362/2000, Train Loss: 5.734235271027214, Val Loss: 5.7333289191718215, Val MAE: 1.6293712854385376\n",
      "EarlyStopping counter: 19 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [12:36<03:04, 184.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363/2000, Train Loss: 5.734109891088385, Val Loss: 5.734061630189458, Val MAE: 1.6306710243225098\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 5.468263626098633\n",
      "Test Mean Absolute Error (MAE): 1.5450461195351375\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 448, 'position': 'DEF', 'window_size': 3, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'large', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 448\n",
      "position DEF\n",
      "window_size 3\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features large\n",
      "stratify_by stdev\n",
      "Running Iteration:  4\n",
      "======= Generating CNN Data for Season: ['2020-21'], Position: DEF =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type DEF = 245.\n",
      "65 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (6124, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (6662, 11).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (6124, 7)\n",
      "Shape of a given window (prior to preprocessing): (3, 11)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.63645621e+00 4.34933809e+01 2.16395112e-02 3.38594705e-02\n",
      " 1.41293279e-01 8.35819756e+00 7.05193483e-02 3.05498982e-03\n",
      " 3.05498982e-03 0.00000000e+00]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[ 2.75034063 43.61774986  0.14896165  0.19046564  0.34832383 10.23164762\n",
      "  0.25602025  0.05518747  0.05518747  1.        ]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building rnn Architecture ======\n",
      "====== Done Building rnn Architecture ======\n",
      "Epoch 1/2000, Train Loss: 10.572160089973597, Val Loss: 8.309813773719897, Val MAE: 1.5237195491790771\n",
      "Epoch 2/2000, Train Loss: 10.38539150586749, Val Loss: 8.150956093931446, Val MAE: 1.497534990310669\n",
      "Epoch 3/2000, Train Loss: 10.197268938291767, Val Loss: 7.9910867889482216, Val MAE: 1.47408127784729\n",
      "Epoch 4/2000, Train Loss: 10.005756388740856, Val Loss: 7.827072434203082, Val MAE: 1.451696753501892\n",
      "Epoch 5/2000, Train Loss: 9.808945379759136, Val Loss: 7.6597028667762554, Val MAE: 1.4299677610397339\n",
      "Epoch 6/2000, Train Loss: 9.60660420467979, Val Loss: 7.487012025208153, Val MAE: 1.4082053899765015\n",
      "Epoch 7/2000, Train Loss: 9.399117803705696, Val Loss: 7.309255968793418, Val MAE: 1.3868366479873657\n",
      "Epoch 8/2000, Train Loss: 9.18582087751901, Val Loss: 7.133043763492007, Val MAE: 1.3655192852020264\n",
      "Epoch 9/2000, Train Loss: 8.970217631860454, Val Loss: 6.949519962372439, Val MAE: 1.3433012962341309\n",
      "Epoch 10/2000, Train Loss: 8.752714577143873, Val Loss: 6.769223731716397, Val MAE: 1.322432041168213\n",
      "Epoch 11/2000, Train Loss: 8.536924102233717, Val Loss: 6.593274318951348, Val MAE: 1.3055064678192139\n",
      "Epoch 12/2000, Train Loss: 8.326280947074995, Val Loss: 6.4176667801822065, Val MAE: 1.2927578687667847\n",
      "Epoch 13/2000, Train Loss: 8.122060028040508, Val Loss: 6.252109020859153, Val MAE: 1.2874966859817505\n",
      "Epoch 14/2000, Train Loss: 7.9269658252473025, Val Loss: 6.099087340968648, Val MAE: 1.2851788997650146\n",
      "Epoch 15/2000, Train Loss: 7.743925634843821, Val Loss: 5.951930542881653, Val MAE: 1.2871215343475342\n",
      "Epoch 16/2000, Train Loss: 7.573450167225339, Val Loss: 5.817995457446101, Val MAE: 1.2919814586639404\n",
      "Epoch 17/2000, Train Loss: 7.416299557091457, Val Loss: 5.697315103173594, Val MAE: 1.3006036281585693\n",
      "Epoch 18/2000, Train Loss: 7.2735153155974075, Val Loss: 5.5899137896808595, Val MAE: 1.3102322816848755\n",
      "Epoch 19/2000, Train Loss: 7.1443695554442685, Val Loss: 5.493490631059094, Val MAE: 1.3223299980163574\n",
      "Epoch 20/2000, Train Loss: 7.029470185361741, Val Loss: 5.407875777127088, Val MAE: 1.3374429941177368\n",
      "Epoch 21/2000, Train Loss: 6.928387672286945, Val Loss: 5.3352976850836455, Val MAE: 1.3543998003005981\n",
      "Epoch 22/2000, Train Loss: 6.840835240541073, Val Loss: 5.27570364541582, Val MAE: 1.372131586074829\n",
      "Epoch 23/2000, Train Loss: 6.766175744275968, Val Loss: 5.223884150949583, Val MAE: 1.3901209831237793\n",
      "Epoch 24/2000, Train Loss: 6.701613333746997, Val Loss: 5.183367413688277, Val MAE: 1.407393217086792\n",
      "Epoch 25/2000, Train Loss: 6.648265967012442, Val Loss: 5.1466589442643365, Val MAE: 1.4246197938919067\n",
      "Epoch 26/2000, Train Loss: 6.602716710362738, Val Loss: 5.119068172885972, Val MAE: 1.4406479597091675\n",
      "Epoch 27/2000, Train Loss: 6.56552655227957, Val Loss: 5.099114069214835, Val MAE: 1.454757809638977\n",
      "Epoch 28/2000, Train Loss: 6.53527821252881, Val Loss: 5.080943659906279, Val MAE: 1.4688771963119507\n",
      "Epoch 29/2000, Train Loss: 6.510132925398132, Val Loss: 5.06769563597809, Val MAE: 1.481102705001831\n",
      "Epoch 30/2000, Train Loss: 6.490172939881724, Val Loss: 5.057357512649341, Val MAE: 1.4917912483215332\n",
      "Epoch 31/2000, Train Loss: 6.473665882279668, Val Loss: 5.050282516327662, Val MAE: 1.501299500465393\n",
      "Epoch 32/2000, Train Loss: 6.460565146977221, Val Loss: 5.043858300805242, Val MAE: 1.5103827714920044\n",
      "Epoch 33/2000, Train Loss: 6.448904341541829, Val Loss: 5.039518810987172, Val MAE: 1.5165408849716187\n",
      "Epoch 34/2000, Train Loss: 6.440208179336506, Val Loss: 5.03561773888701, Val MAE: 1.5224350690841675\n",
      "Epoch 35/2000, Train Loss: 6.4318178372369905, Val Loss: 5.032582351944937, Val MAE: 1.5286041498184204\n",
      "Epoch 36/2000, Train Loss: 6.424934523521698, Val Loss: 5.030052120831394, Val MAE: 1.5333976745605469\n",
      "Epoch 37/2000, Train Loss: 6.41897632131286, Val Loss: 5.027420474143527, Val MAE: 1.5366753339767456\n",
      "Epoch 38/2000, Train Loss: 6.413990921591127, Val Loss: 5.025172167475256, Val MAE: 1.5393821001052856\n",
      "Epoch 39/2000, Train Loss: 6.409400306976403, Val Loss: 5.0227800710279435, Val MAE: 1.5404318571090698\n",
      "Epoch 40/2000, Train Loss: 6.404571335335517, Val Loss: 5.021272313887955, Val MAE: 1.5445224046707153\n",
      "Epoch 41/2000, Train Loss: 6.400400879904834, Val Loss: 5.018945492118184, Val MAE: 1.5453388690948486\n",
      "Epoch 42/2000, Train Loss: 6.396706711486436, Val Loss: 5.01703400077465, Val MAE: 1.5467089414596558\n",
      "Epoch 43/2000, Train Loss: 6.3929855822195965, Val Loss: 5.014599926910665, Val MAE: 1.5464935302734375\n",
      "Epoch 44/2000, Train Loss: 6.389958909475903, Val Loss: 5.012733738892917, Val MAE: 1.5474761724472046\n",
      "Epoch 45/2000, Train Loss: 6.386414496191981, Val Loss: 5.010301549076884, Val MAE: 1.5470963716506958\n",
      "Epoch 46/2000, Train Loss: 6.383079147867218, Val Loss: 5.008411406643905, Val MAE: 1.547715187072754\n",
      "Epoch 47/2000, Train Loss: 6.379779479113973, Val Loss: 5.006198807329428, Val MAE: 1.5474337339401245\n",
      "Epoch 48/2000, Train Loss: 6.376820300094309, Val Loss: 5.004181052178783, Val MAE: 1.5474070310592651\n",
      "Epoch 49/2000, Train Loss: 6.373562202559284, Val Loss: 5.0017812234005445, Val MAE: 1.5463974475860596\n",
      "Epoch 50/2000, Train Loss: 6.370564035959851, Val Loss: 5.000243808703043, Val MAE: 1.5472172498703003\n",
      "Epoch 51/2000, Train Loss: 6.367825870566751, Val Loss: 4.9978757845273085, Val MAE: 1.5465508699417114\n",
      "Epoch 52/2000, Train Loss: 6.365106708521328, Val Loss: 4.996419950887504, Val MAE: 1.5475836992263794\n",
      "Epoch 53/2000, Train Loss: 6.362024122790286, Val Loss: 4.9943024642380305, Val MAE: 1.546830415725708\n",
      "Epoch 54/2000, Train Loss: 6.359578652395106, Val Loss: 4.992227039038007, Val MAE: 1.5462963581085205\n",
      "Epoch 55/2000, Train Loss: 6.356582314023681, Val Loss: 4.989718042285866, Val MAE: 1.544385552406311\n",
      "Epoch 56/2000, Train Loss: 6.354018190626952, Val Loss: 4.98799669629839, Val MAE: 1.54512357711792\n",
      "Epoch 57/2000, Train Loss: 6.351422401808636, Val Loss: 4.98571251097874, Val MAE: 1.5431971549987793\n",
      "Epoch 58/2000, Train Loss: 6.349113769478415, Val Loss: 4.983947706553254, Val MAE: 1.5434595346450806\n",
      "Epoch 59/2000, Train Loss: 6.346593873718769, Val Loss: 4.982349143089893, Val MAE: 1.5432286262512207\n",
      "Epoch 60/2000, Train Loss: 6.3441770867957965, Val Loss: 4.981396893760445, Val MAE: 1.5448917150497437\n",
      "Epoch 61/2000, Train Loss: 6.341627612866853, Val Loss: 4.978502404644691, Val MAE: 1.541506052017212\n",
      "Epoch 62/2000, Train Loss: 6.33931524509208, Val Loss: 4.976610187981348, Val MAE: 1.5409146547317505\n",
      "Epoch 63/2000, Train Loss: 6.336898195050099, Val Loss: 4.975201308182329, Val MAE: 1.5416089296340942\n",
      "Epoch 64/2000, Train Loss: 6.334636462230101, Val Loss: 4.973729923066847, Val MAE: 1.5418775081634521\n",
      "Epoch 65/2000, Train Loss: 6.332678176953852, Val Loss: 4.972628863375695, Val MAE: 1.5427109003067017\n",
      "Epoch 66/2000, Train Loss: 6.330972821560593, Val Loss: 4.970166772171798, Val MAE: 1.540138602256775\n",
      "Epoch 67/2000, Train Loss: 6.328509287424695, Val Loss: 4.969203497655455, Val MAE: 1.5409235954284668\n",
      "Epoch 68/2000, Train Loss: 6.325858513998523, Val Loss: 4.967107871234943, Val MAE: 1.5395623445510864\n",
      "Epoch 69/2000, Train Loss: 6.323800987021745, Val Loss: 4.965329708416405, Val MAE: 1.5382540225982666\n",
      "Epoch 70/2000, Train Loss: 6.3224613702198145, Val Loss: 4.96382253642416, Val MAE: 1.5385693311691284\n",
      "Epoch 71/2000, Train Loss: 6.319774752112306, Val Loss: 4.96268205774988, Val MAE: 1.538989543914795\n",
      "Epoch 72/2000, Train Loss: 6.317981089911632, Val Loss: 4.960975254055531, Val MAE: 1.537631869316101\n",
      "Epoch 73/2000, Train Loss: 6.315701234637865, Val Loss: 4.9596541274578545, Val MAE: 1.5376282930374146\n",
      "Epoch 74/2000, Train Loss: 6.313733290040922, Val Loss: 4.9584764968815245, Val MAE: 1.5378165245056152\n",
      "Epoch 75/2000, Train Loss: 6.311879969303628, Val Loss: 4.956919915983535, Val MAE: 1.5375932455062866\n",
      "Epoch 76/2000, Train Loss: 6.310087468419379, Val Loss: 4.955504280321234, Val MAE: 1.53635573387146\n",
      "Epoch 77/2000, Train Loss: 6.308434901673378, Val Loss: 4.9549603138264615, Val MAE: 1.5381546020507812\n",
      "Epoch 78/2000, Train Loss: 6.306818079221942, Val Loss: 4.953667771789947, Val MAE: 1.538170576095581\n",
      "Epoch 79/2000, Train Loss: 6.3047701275579815, Val Loss: 4.952037960065418, Val MAE: 1.5363514423370361\n",
      "Epoch 80/2000, Train Loss: 6.302597853351498, Val Loss: 4.9503588987065745, Val MAE: 1.534688115119934\n",
      "Epoch 81/2000, Train Loss: 6.301302010068603, Val Loss: 4.948849634989664, Val MAE: 1.5341970920562744\n",
      "Epoch 82/2000, Train Loss: 6.299467908642629, Val Loss: 4.948824845226235, Val MAE: 1.5363281965255737\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 83/2000, Train Loss: 6.297674032723805, Val Loss: 4.946969824228882, Val MAE: 1.534812092781067\n",
      "Epoch 84/2000, Train Loss: 6.29597404300341, Val Loss: 4.945915078060774, Val MAE: 1.5341618061065674\n",
      "Epoch 85/2000, Train Loss: 6.294540175968921, Val Loss: 4.944097828368972, Val MAE: 1.5328385829925537\n",
      "Epoch 86/2000, Train Loss: 6.2928142803857865, Val Loss: 4.943009833688994, Val MAE: 1.5324194431304932\n",
      "Epoch 87/2000, Train Loss: 6.291262362472239, Val Loss: 4.941919316766692, Val MAE: 1.531507968902588\n",
      "Epoch 88/2000, Train Loss: 6.289914457157378, Val Loss: 4.941463154715969, Val MAE: 1.5326266288757324\n",
      "Epoch 89/2000, Train Loss: 6.288780716977952, Val Loss: 4.9399766209823035, Val MAE: 1.5317809581756592\n",
      "Epoch 90/2000, Train Loss: 6.286937822727616, Val Loss: 4.939638377833186, Val MAE: 1.5332459211349487\n",
      "Epoch 91/2000, Train Loss: 6.285319612164907, Val Loss: 4.938372430498482, Val MAE: 1.5316911935806274\n",
      "Epoch 92/2000, Train Loss: 6.284076806911141, Val Loss: 4.937084512219002, Val MAE: 1.53122878074646\n",
      "Epoch 93/2000, Train Loss: 6.282503849251449, Val Loss: 4.936682719507103, Val MAE: 1.5326178073883057\n",
      "Epoch 94/2000, Train Loss: 6.281240679088392, Val Loss: 4.935373526804083, Val MAE: 1.530791997909546\n",
      "Epoch 95/2000, Train Loss: 6.280031932556068, Val Loss: 4.934417308880131, Val MAE: 1.5308969020843506\n",
      "Epoch 96/2000, Train Loss: 6.278136378840396, Val Loss: 4.933787616756645, Val MAE: 1.530852198600769\n",
      "Epoch 97/2000, Train Loss: 6.276784600049175, Val Loss: 4.933293930086934, Val MAE: 1.5316438674926758\n",
      "Epoch 98/2000, Train Loss: 6.275899589755198, Val Loss: 4.932769143810639, Val MAE: 1.5322608947753906\n",
      "Epoch 99/2000, Train Loss: 6.274314756472686, Val Loss: 4.931417264579824, Val MAE: 1.5313687324523926\n",
      "Epoch 100/2000, Train Loss: 6.27295064212873, Val Loss: 4.9299685287032124, Val MAE: 1.5293110609054565\n",
      "Epoch 101/2000, Train Loss: 6.27193773142849, Val Loss: 4.929815298672131, Val MAE: 1.5308283567428589\n",
      "Epoch 102/2000, Train Loss: 6.27055151059687, Val Loss: 4.928624743583975, Val MAE: 1.5293083190917969\n",
      "Epoch 103/2000, Train Loss: 6.269339962745307, Val Loss: 4.927669730417966, Val MAE: 1.5283520221710205\n",
      "Epoch 104/2000, Train Loss: 6.267898685317951, Val Loss: 4.926620907113802, Val MAE: 1.528768539428711\n",
      "Epoch 105/2000, Train Loss: 6.267087605983597, Val Loss: 4.9257460261541235, Val MAE: 1.5277704000473022\n",
      "Epoch 106/2000, Train Loss: 6.265830957988623, Val Loss: 4.925349953515082, Val MAE: 1.5287864208221436\n",
      "Epoch 107/2000, Train Loss: 6.264629086497088, Val Loss: 4.925235906923554, Val MAE: 1.5298597812652588\n",
      "Epoch 108/2000, Train Loss: 6.263677410537847, Val Loss: 4.9243639724665, Val MAE: 1.5279431343078613\n",
      "Epoch 109/2000, Train Loss: 6.262183945462975, Val Loss: 4.923466831896675, Val MAE: 1.5279043912887573\n",
      "Epoch 110/2000, Train Loss: 6.260944684522634, Val Loss: 4.923437257357233, Val MAE: 1.5295804738998413\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 111/2000, Train Loss: 6.25991874599721, Val Loss: 4.921867584201307, Val MAE: 1.527504801750183\n",
      "Epoch 112/2000, Train Loss: 6.25890270507897, Val Loss: 4.92165805767719, Val MAE: 1.5279924869537354\n",
      "Epoch 113/2000, Train Loss: 6.257913744548682, Val Loss: 4.920750922393077, Val MAE: 1.5274333953857422\n",
      "Epoch 114/2000, Train Loss: 6.256663354017728, Val Loss: 4.920928869260214, Val MAE: 1.5290204286575317\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 115/2000, Train Loss: 6.25575651052586, Val Loss: 4.918880250572707, Val MAE: 1.5252532958984375\n",
      "Epoch 116/2000, Train Loss: 6.254793814212662, Val Loss: 4.919017156541423, Val MAE: 1.5269511938095093\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 117/2000, Train Loss: 6.253738731954897, Val Loss: 4.918021851856952, Val MAE: 1.526025414466858\n",
      "Epoch 118/2000, Train Loss: 6.252464194046824, Val Loss: 4.917750228417176, Val MAE: 1.526484489440918\n",
      "Epoch 119/2000, Train Loss: 6.251675035682742, Val Loss: 4.916746944527308, Val MAE: 1.5251286029815674\n",
      "Epoch 120/2000, Train Loss: 6.250552018131246, Val Loss: 4.917009495546358, Val MAE: 1.5273041725158691\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 121/2000, Train Loss: 6.249416708748096, Val Loss: 4.916161667184451, Val MAE: 1.525911808013916\n",
      "Epoch 122/2000, Train Loss: 6.248742054439978, Val Loss: 4.9148249562458615, Val MAE: 1.5238316059112549\n",
      "Epoch 123/2000, Train Loss: 6.2476999150749055, Val Loss: 4.915233150274877, Val MAE: 1.525615930557251\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 124/2000, Train Loss: 6.246563792823094, Val Loss: 4.913778315689345, Val MAE: 1.5238007307052612\n",
      "Epoch 125/2000, Train Loss: 6.245567612898977, Val Loss: 4.913817129343335, Val MAE: 1.5253108739852905\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 126/2000, Train Loss: 6.244648732372928, Val Loss: 4.913661067790973, Val MAE: 1.5255533456802368\n",
      "Epoch 127/2000, Train Loss: 6.2436862813468785, Val Loss: 4.9123139125477016, Val MAE: 1.5236009359359741\n",
      "Epoch 128/2000, Train Loss: 6.242772449879105, Val Loss: 4.911735049731945, Val MAE: 1.5236726999282837\n",
      "Epoch 129/2000, Train Loss: 6.24180277615703, Val Loss: 4.911169922272916, Val MAE: 1.5226774215698242\n",
      "Epoch 130/2000, Train Loss: 6.240837952040569, Val Loss: 4.910588489016248, Val MAE: 1.5226798057556152\n",
      "Epoch 131/2000, Train Loss: 6.2400271243996235, Val Loss: 4.910245607434425, Val MAE: 1.5226284265518188\n",
      "Epoch 132/2000, Train Loss: 6.238978769772601, Val Loss: 4.910212830084871, Val MAE: 1.523720145225525\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 133/2000, Train Loss: 6.23838708143155, Val Loss: 4.9091095439009225, Val MAE: 1.5221319198608398\n",
      "Epoch 134/2000, Train Loss: 6.237605489191917, Val Loss: 4.908759404908664, Val MAE: 1.5212676525115967\n",
      "Epoch 135/2000, Train Loss: 6.236847794815444, Val Loss: 4.907908013177399, Val MAE: 1.5212697982788086\n",
      "Epoch 136/2000, Train Loss: 6.23578609931502, Val Loss: 4.90752241505876, Val MAE: 1.5206199884414673\n",
      "Epoch 137/2000, Train Loss: 6.235179024984301, Val Loss: 4.907833044157654, Val MAE: 1.5226325988769531\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 138/2000, Train Loss: 6.234112967018276, Val Loss: 4.90713926856635, Val MAE: 1.5217703580856323\n",
      "Epoch 139/2000, Train Loss: 6.233059594109448, Val Loss: 4.907083429217639, Val MAE: 1.5225588083267212\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 140/2000, Train Loss: 6.232404233874377, Val Loss: 4.906159564345361, Val MAE: 1.5213618278503418\n",
      "Epoch 141/2000, Train Loss: 6.231501647442001, Val Loss: 4.905655739042077, Val MAE: 1.521079182624817\n",
      "Epoch 142/2000, Train Loss: 6.23059145961772, Val Loss: 4.905056219909711, Val MAE: 1.5196574926376343\n",
      "Epoch 143/2000, Train Loss: 6.230048802288615, Val Loss: 4.905134173596341, Val MAE: 1.5211235284805298\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 144/2000, Train Loss: 6.229481429076261, Val Loss: 4.904075590388621, Val MAE: 1.5200051069259644\n",
      "Epoch 145/2000, Train Loss: 6.228274191911861, Val Loss: 4.903416499814951, Val MAE: 1.5188548564910889\n",
      "Epoch 146/2000, Train Loss: 6.227465415595311, Val Loss: 4.903875792368513, Val MAE: 1.5207011699676514\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 147/2000, Train Loss: 6.226728885788006, Val Loss: 4.902652939906036, Val MAE: 1.5186655521392822\n",
      "Epoch 148/2000, Train Loss: 6.225665271843569, Val Loss: 4.902985072996548, Val MAE: 1.5205206871032715\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 149/2000, Train Loss: 6.225137649282524, Val Loss: 4.9032105474085705, Val MAE: 1.521538257598877\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 150/2000, Train Loss: 6.22431950555944, Val Loss: 4.902256441221562, Val MAE: 1.520405650138855\n",
      "Epoch 151/2000, Train Loss: 6.223521569429012, Val Loss: 4.901797575032877, Val MAE: 1.5198111534118652\n",
      "Epoch 152/2000, Train Loss: 6.222675248196251, Val Loss: 4.901502035191802, Val MAE: 1.5202300548553467\n",
      "Epoch 153/2000, Train Loss: 6.221934537808321, Val Loss: 4.900983912891427, Val MAE: 1.5196912288665771\n",
      "Epoch 154/2000, Train Loss: 6.221267610639747, Val Loss: 4.9001974797376615, Val MAE: 1.5186501741409302\n",
      "Epoch 155/2000, Train Loss: 6.220894435634243, Val Loss: 4.89983981759446, Val MAE: 1.5174617767333984\n",
      "Epoch 156/2000, Train Loss: 6.2199707105219195, Val Loss: 4.899136638994626, Val MAE: 1.5172381401062012\n",
      "Epoch 157/2000, Train Loss: 6.219097304410221, Val Loss: 4.8990915513692554, Val MAE: 1.517899513244629\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 158/2000, Train Loss: 6.218439170066009, Val Loss: 4.898987157667269, Val MAE: 1.5183145999908447\n",
      "Epoch 159/2000, Train Loss: 6.217762340138823, Val Loss: 4.898605698125359, Val MAE: 1.5182526111602783\n",
      "Epoch 160/2000, Train Loss: 6.216773865559755, Val Loss: 4.898192016372614, Val MAE: 1.5180165767669678\n",
      "Epoch 161/2000, Train Loss: 6.21660730911424, Val Loss: 4.898102720854111, Val MAE: 1.5194158554077148\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 162/2000, Train Loss: 6.2153632668577075, Val Loss: 4.897258957145644, Val MAE: 1.5167680978775024\n",
      "Epoch 163/2000, Train Loss: 6.214785495028932, Val Loss: 4.897159810869721, Val MAE: 1.5172806978225708\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 164/2000, Train Loss: 6.214384244881838, Val Loss: 4.8962210180216745, Val MAE: 1.5161566734313965\n",
      "Epoch 165/2000, Train Loss: 6.213578125380413, Val Loss: 4.896929212637988, Val MAE: 1.5183587074279785\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 166/2000, Train Loss: 6.212842771947549, Val Loss: 4.896225931158132, Val MAE: 1.5175929069519043\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 167/2000, Train Loss: 6.2120183693735225, Val Loss: 4.896115833731347, Val MAE: 1.5173100233078003\n",
      "Epoch 168/2000, Train Loss: 6.211252113722698, Val Loss: 4.895397376010727, Val MAE: 1.5170005559921265\n",
      "Epoch 169/2000, Train Loss: 6.210615615633386, Val Loss: 4.8955702487975925, Val MAE: 1.5172219276428223\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 170/2000, Train Loss: 6.210013862461925, Val Loss: 4.894536947956001, Val MAE: 1.5156270265579224\n",
      "Epoch 171/2000, Train Loss: 6.209308143996136, Val Loss: 4.894429050639629, Val MAE: 1.515535593032837\n",
      "Epoch 172/2000, Train Loss: 6.208644658772899, Val Loss: 4.893954008272737, Val MAE: 1.515539526939392\n",
      "Epoch 173/2000, Train Loss: 6.20813939577985, Val Loss: 4.89407626921863, Val MAE: 1.516210675239563\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 174/2000, Train Loss: 6.2072942374485685, Val Loss: 4.89404467144725, Val MAE: 1.5162755250930786\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 175/2000, Train Loss: 6.206735116689159, Val Loss: 4.893274534883595, Val MAE: 1.515651822090149\n",
      "Epoch 176/2000, Train Loss: 6.206100009220789, Val Loss: 4.893119535240971, Val MAE: 1.5160621404647827\n",
      "Epoch 177/2000, Train Loss: 6.205699837174772, Val Loss: 4.892898327070858, Val MAE: 1.516034722328186\n",
      "Epoch 178/2000, Train Loss: 6.204761799550783, Val Loss: 4.892200747342759, Val MAE: 1.515027403831482\n",
      "Epoch 179/2000, Train Loss: 6.204489677830746, Val Loss: 4.892451769823835, Val MAE: 1.5153281688690186\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 180/2000, Train Loss: 6.203821020575441, Val Loss: 4.8919707818748375, Val MAE: 1.515586256980896\n",
      "Epoch 181/2000, Train Loss: 6.203096891176007, Val Loss: 4.891582111655388, Val MAE: 1.51494562625885\n",
      "Epoch 182/2000, Train Loss: 6.2022693549496974, Val Loss: 4.891147702161932, Val MAE: 1.5137990713119507\n",
      "Epoch 183/2000, Train Loss: 6.201994900954397, Val Loss: 4.891090459304229, Val MAE: 1.514319658279419\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 184/2000, Train Loss: 6.201467814141694, Val Loss: 4.890045806183039, Val MAE: 1.5119825601577759\n",
      "Epoch 185/2000, Train Loss: 6.200760087676326, Val Loss: 4.890394409904714, Val MAE: 1.5144037008285522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 186/2000, Train Loss: 6.199741510671261, Val Loss: 4.89020923191557, Val MAE: 1.5148158073425293\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 187/2000, Train Loss: 6.199272625730308, Val Loss: 4.889949440636671, Val MAE: 1.513781189918518\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 188/2000, Train Loss: 6.198830575229719, Val Loss: 4.889784693060276, Val MAE: 1.5150114297866821\n",
      "Epoch 189/2000, Train Loss: 6.19818809937242, Val Loss: 4.889064995555529, Val MAE: 1.5131603479385376\n",
      "Epoch 190/2000, Train Loss: 6.197468078169466, Val Loss: 4.888836456239599, Val MAE: 1.5132089853286743\n",
      "Epoch 191/2000, Train Loss: 6.197012706872829, Val Loss: 4.889083490733992, Val MAE: 1.514312505722046\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 192/2000, Train Loss: 6.196283056465212, Val Loss: 4.8890352279022995, Val MAE: 1.5147502422332764\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 193/2000, Train Loss: 6.195651788262449, Val Loss: 4.887911711691457, Val MAE: 1.5128769874572754\n",
      "Epoch 194/2000, Train Loss: 6.195310350402241, Val Loss: 4.887930186331498, Val MAE: 1.5134954452514648\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 195/2000, Train Loss: 6.194746645137544, Val Loss: 4.887899662871829, Val MAE: 1.5129495859146118\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 196/2000, Train Loss: 6.193913376034132, Val Loss: 4.8878143567305345, Val MAE: 1.514196753501892\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 197/2000, Train Loss: 6.193475118832575, Val Loss: 4.886546811100664, Val MAE: 1.5113965272903442\n",
      "Epoch 198/2000, Train Loss: 6.192773683447587, Val Loss: 4.887002292433272, Val MAE: 1.5126996040344238\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 199/2000, Train Loss: 6.192309594616665, Val Loss: 4.88667365290403, Val MAE: 1.5124282836914062\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 200/2000, Train Loss: 6.191692478835088, Val Loss: 4.886430198859898, Val MAE: 1.512300968170166\n",
      "Epoch 201/2000, Train Loss: 6.191153728400571, Val Loss: 4.886097805797678, Val MAE: 1.5125305652618408\n",
      "Epoch 202/2000, Train Loss: 6.190562475156916, Val Loss: 4.886041863944762, Val MAE: 1.512176275253296\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 203/2000, Train Loss: 6.190115827734781, Val Loss: 4.885002832338308, Val MAE: 1.510284662246704\n",
      "Epoch 204/2000, Train Loss: 6.189466428888802, Val Loss: 4.885533084259057, Val MAE: 1.5122535228729248\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 205/2000, Train Loss: 6.188916412945269, Val Loss: 4.885819224323631, Val MAE: 1.5127886533737183\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 206/2000, Train Loss: 6.188486921886327, Val Loss: 4.884318789460169, Val MAE: 1.5092958211898804\n",
      "Epoch 207/2000, Train Loss: 6.187955666645082, Val Loss: 4.884783555851007, Val MAE: 1.5123708248138428\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 208/2000, Train Loss: 6.187328253030117, Val Loss: 4.884362243538696, Val MAE: 1.5110797882080078\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 209/2000, Train Loss: 6.18684921106143, Val Loss: 4.884064720640567, Val MAE: 1.510878324508667\n",
      "Epoch 210/2000, Train Loss: 6.186359249125557, Val Loss: 4.884262796139056, Val MAE: 1.511435866355896\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 211/2000, Train Loss: 6.18551140016466, Val Loss: 4.88409725560667, Val MAE: 1.5120311975479126\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 212/2000, Train Loss: 6.185136140944885, Val Loss: 4.883196194659717, Val MAE: 1.5105257034301758\n",
      "Epoch 213/2000, Train Loss: 6.184724355470441, Val Loss: 4.883100568665682, Val MAE: 1.5102976560592651\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 214/2000, Train Loss: 6.1841821799978325, Val Loss: 4.882469800802378, Val MAE: 1.5088549852371216\n",
      "Epoch 215/2000, Train Loss: 6.183507394394386, Val Loss: 4.883390972585726, Val MAE: 1.5119471549987793\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 216/2000, Train Loss: 6.183281384940953, Val Loss: 4.882561313549778, Val MAE: 1.5098506212234497\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 217/2000, Train Loss: 6.182691338980297, Val Loss: 4.8823658435559665, Val MAE: 1.5108262300491333\n",
      "Epoch 218/2000, Train Loss: 6.1822581033627415, Val Loss: 4.881523975752492, Val MAE: 1.5085575580596924\n",
      "Epoch 219/2000, Train Loss: 6.181721437406672, Val Loss: 4.882190181395746, Val MAE: 1.510746955871582\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 220/2000, Train Loss: 6.181115152829242, Val Loss: 4.88196900678613, Val MAE: 1.5110386610031128\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 221/2000, Train Loss: 6.180614704174348, Val Loss: 4.8809492314072305, Val MAE: 1.5077459812164307\n",
      "Epoch 222/2000, Train Loss: 6.180059132879791, Val Loss: 4.881111377086795, Val MAE: 1.5101208686828613\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 223/2000, Train Loss: 6.179605473970112, Val Loss: 4.881467371671678, Val MAE: 1.510301113128662\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 224/2000, Train Loss: 6.179356931385241, Val Loss: 4.880459833660011, Val MAE: 1.5091360807418823\n",
      "Epoch 225/2000, Train Loss: 6.178610384167066, Val Loss: 4.880953488723477, Val MAE: 1.5105208158493042\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 226/2000, Train Loss: 6.178279269104849, Val Loss: 4.879822411498085, Val MAE: 1.5076508522033691\n",
      "Epoch 227/2000, Train Loss: 6.177787844956416, Val Loss: 4.8796937538952125, Val MAE: 1.5071165561676025\n",
      "Epoch 228/2000, Train Loss: 6.1771338259414295, Val Loss: 4.879908791500112, Val MAE: 1.5091193914413452\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 229/2000, Train Loss: 6.1765607236825195, Val Loss: 4.880334227375244, Val MAE: 1.509972095489502\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 230/2000, Train Loss: 6.175924312242841, Val Loss: 4.8797291631506186, Val MAE: 1.5085612535476685\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 231/2000, Train Loss: 6.175696787741706, Val Loss: 4.879283995751625, Val MAE: 1.5084608793258667\n",
      "Epoch 232/2000, Train Loss: 6.175545014040622, Val Loss: 4.878513448607426, Val MAE: 1.5067256689071655\n",
      "Epoch 233/2000, Train Loss: 6.174661412331536, Val Loss: 4.878871567763416, Val MAE: 1.5073497295379639\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 234/2000, Train Loss: 6.174283158415903, Val Loss: 4.878555760482822, Val MAE: 1.5073996782302856\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 235/2000, Train Loss: 6.173761217481872, Val Loss: 4.8791210554624715, Val MAE: 1.5090833902359009\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 236/2000, Train Loss: 6.173615493140392, Val Loss: 4.878035922538287, Val MAE: 1.5077396631240845\n",
      "Epoch 237/2000, Train Loss: 6.173627124796944, Val Loss: 4.877673702894664, Val MAE: 1.5077364444732666\n",
      "Epoch 238/2000, Train Loss: 6.1724747458323215, Val Loss: 4.8787408929980565, Val MAE: 1.5092976093292236\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 239/2000, Train Loss: 6.172316348453638, Val Loss: 4.878608373438876, Val MAE: 1.5094871520996094\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 240/2000, Train Loss: 6.1713589533544315, Val Loss: 4.877413058021631, Val MAE: 1.5071746110916138\n",
      "Epoch 241/2000, Train Loss: 6.171227994585962, Val Loss: 4.877312687877448, Val MAE: 1.5075178146362305\n",
      "Epoch 242/2000, Train Loss: 6.1706002618467375, Val Loss: 4.876715629710375, Val MAE: 1.5063412189483643\n",
      "Epoch 243/2000, Train Loss: 6.170239865944986, Val Loss: 4.876823192004298, Val MAE: 1.5074408054351807\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 244/2000, Train Loss: 6.1696400621260965, Val Loss: 4.876439307913804, Val MAE: 1.5059696435928345\n",
      "Epoch 245/2000, Train Loss: 6.169342269791791, Val Loss: 4.8764342072068665, Val MAE: 1.5061684846878052\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 246/2000, Train Loss: 6.168755868135067, Val Loss: 4.87639462528809, Val MAE: 1.507172703742981\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 247/2000, Train Loss: 6.168414044842495, Val Loss: 4.876886662513882, Val MAE: 1.5081791877746582\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 248/2000, Train Loss: 6.168154210066861, Val Loss: 4.875935075637822, Val MAE: 1.507043480873108\n",
      "Epoch 249/2000, Train Loss: 6.167750988904789, Val Loss: 4.876002066466877, Val MAE: 1.5075719356536865\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 250/2000, Train Loss: 6.167061646427144, Val Loss: 4.876167032975718, Val MAE: 1.5081957578659058\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 251/2000, Train Loss: 6.166907469255442, Val Loss: 4.874999284293282, Val MAE: 1.5054889917373657\n",
      "Epoch 252/2000, Train Loss: 6.1661802075246035, Val Loss: 4.874970682463309, Val MAE: 1.505415678024292\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 253/2000, Train Loss: 6.165458959885911, Val Loss: 4.875842017510349, Val MAE: 1.5085569620132446\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 254/2000, Train Loss: 6.165127537587343, Val Loss: 4.875352614971458, Val MAE: 1.507643222808838\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 255/2000, Train Loss: 6.164757794620588, Val Loss: 4.874214055634387, Val MAE: 1.5053588151931763\n",
      "Epoch 256/2000, Train Loss: 6.164447019569101, Val Loss: 4.874894668370448, Val MAE: 1.5066194534301758\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 257/2000, Train Loss: 6.163905310961018, Val Loss: 4.874332291359483, Val MAE: 1.5050970315933228\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 258/2000, Train Loss: 6.163638941270823, Val Loss: 4.873595842628882, Val MAE: 1.50404691696167\n",
      "Epoch 259/2000, Train Loss: 6.162910987135446, Val Loss: 4.87379320151869, Val MAE: 1.5049452781677246\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 260/2000, Train Loss: 6.162715540434185, Val Loss: 4.873677868560464, Val MAE: 1.5034650564193726\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 261/2000, Train Loss: 6.162397680520351, Val Loss: 4.8734804388854425, Val MAE: 1.5053167343139648\n",
      "Epoch 262/2000, Train Loss: 6.16185974586043, Val Loss: 4.873162785223198, Val MAE: 1.5035675764083862\n",
      "Epoch 263/2000, Train Loss: 6.161517911472479, Val Loss: 4.872644371551568, Val MAE: 1.5036541223526\n",
      "Epoch 264/2000, Train Loss: 6.16109778663128, Val Loss: 4.873474321761468, Val MAE: 1.505187749862671\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 265/2000, Train Loss: 6.161580532607609, Val Loss: 4.872332727153695, Val MAE: 1.5026839971542358\n",
      "Epoch 266/2000, Train Loss: 6.160495325865178, Val Loss: 4.8727721619275295, Val MAE: 1.5041478872299194\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 267/2000, Train Loss: 6.159737831073454, Val Loss: 4.8729627923323555, Val MAE: 1.5059641599655151\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 268/2000, Train Loss: 6.159207659480975, Val Loss: 4.8726751446385705, Val MAE: 1.504655122756958\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 269/2000, Train Loss: 6.158826930767281, Val Loss: 4.8720490032475805, Val MAE: 1.5034841299057007\n",
      "Epoch 270/2000, Train Loss: 6.1590661590449365, Val Loss: 4.8719881024648535, Val MAE: 1.5032751560211182\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 271/2000, Train Loss: 6.158067314644599, Val Loss: 4.872241536171785, Val MAE: 1.5041451454162598\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 272/2000, Train Loss: 6.15765514268109, Val Loss: 4.871910919354287, Val MAE: 1.5050150156021118\n",
      "Epoch 273/2000, Train Loss: 6.157298098376583, Val Loss: 4.872135397211225, Val MAE: 1.5051096677780151\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 274/2000, Train Loss: 6.157298670316997, Val Loss: 4.871080629508778, Val MAE: 1.503579020500183\n",
      "Epoch 275/2000, Train Loss: 6.156348736200306, Val Loss: 4.8712294190126135, Val MAE: 1.5038948059082031\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 276/2000, Train Loss: 6.1559518174781696, Val Loss: 4.871322682383382, Val MAE: 1.5038321018218994\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 277/2000, Train Loss: 6.155657530227196, Val Loss: 4.8711755822822544, Val MAE: 1.5042206048965454\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 278/2000, Train Loss: 6.155461944833687, Val Loss: 4.870902493502123, Val MAE: 1.5044845342636108\n",
      "Epoch 279/2000, Train Loss: 6.155008285659832, Val Loss: 4.870728312316262, Val MAE: 1.5043243169784546\n",
      "Epoch 280/2000, Train Loss: 6.154428395670206, Val Loss: 4.871185449735363, Val MAE: 1.5048481225967407\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 281/2000, Train Loss: 6.154128581467098, Val Loss: 4.8707315017096935, Val MAE: 1.504847764968872\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 282/2000, Train Loss: 6.153586754838516, Val Loss: 4.870415506886805, Val MAE: 1.5035308599472046\n",
      "Epoch 283/2000, Train Loss: 6.153296145441789, Val Loss: 4.869613689731003, Val MAE: 1.50264310836792\n",
      "Epoch 284/2000, Train Loss: 6.153189642211407, Val Loss: 4.869319599793583, Val MAE: 1.501478672027588\n",
      "Epoch 285/2000, Train Loss: 6.15235787441856, Val Loss: 4.869843574548504, Val MAE: 1.5033586025238037\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 286/2000, Train Loss: 6.152280736696027, Val Loss: 4.86948679843248, Val MAE: 1.5026785135269165\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 287/2000, Train Loss: 6.152074681358655, Val Loss: 4.8699896122325415, Val MAE: 1.5039106607437134\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 288/2000, Train Loss: 6.1514786434966116, Val Loss: 4.870083683084645, Val MAE: 1.5049233436584473\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 289/2000, Train Loss: 6.151257631719277, Val Loss: 4.869150539543034, Val MAE: 1.5032588243484497\n",
      "Epoch 290/2000, Train Loss: 6.15107446390506, Val Loss: 4.8691896425576555, Val MAE: 1.5033438205718994\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 291/2000, Train Loss: 6.150448920125776, Val Loss: 4.868742468199619, Val MAE: 1.5025131702423096\n",
      "Epoch 292/2000, Train Loss: 6.14996321881577, Val Loss: 4.868972086374652, Val MAE: 1.5030248165130615\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 293/2000, Train Loss: 6.149519394111105, Val Loss: 4.8687945959283905, Val MAE: 1.503281593322754\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 294/2000, Train Loss: 6.149729519471568, Val Loss: 4.8688277491987275, Val MAE: 1.5034961700439453\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 295/2000, Train Loss: 6.149016451109149, Val Loss: 4.868855555883572, Val MAE: 1.5031476020812988\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 296/2000, Train Loss: 6.14861989959125, Val Loss: 4.868446132471177, Val MAE: 1.5031605958938599\n",
      "Epoch 297/2000, Train Loss: 6.148122979002976, Val Loss: 4.868373326985412, Val MAE: 1.5036567449569702\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 298/2000, Train Loss: 6.147881382654248, Val Loss: 4.8685853942514665, Val MAE: 1.5037237405776978\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 299/2000, Train Loss: 6.147268220262184, Val Loss: 4.868369668759734, Val MAE: 1.5041779279708862\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 300/2000, Train Loss: 6.147490344747612, Val Loss: 4.868136672371198, Val MAE: 1.5030101537704468\n",
      "Epoch 301/2000, Train Loss: 6.146466404447265, Val Loss: 4.867792063231802, Val MAE: 1.503156304359436\n",
      "Epoch 302/2000, Train Loss: 6.146313354513322, Val Loss: 4.867083898868792, Val MAE: 1.5014781951904297\n",
      "Epoch 303/2000, Train Loss: 6.146078830940902, Val Loss: 4.867748524359466, Val MAE: 1.5029139518737793\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 304/2000, Train Loss: 6.145847602300036, Val Loss: 4.86827901081848, Val MAE: 1.5043303966522217\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 305/2000, Train Loss: 6.145273548083953, Val Loss: 4.867047580163385, Val MAE: 1.5020169019699097\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 306/2000, Train Loss: 6.144732315454457, Val Loss: 4.8670919335354474, Val MAE: 1.5024396181106567\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 307/2000, Train Loss: 6.144609300581703, Val Loss: 4.866739477487786, Val MAE: 1.5019015073776245\n",
      "Epoch 308/2000, Train Loss: 6.14438363970812, Val Loss: 4.866607192805105, Val MAE: 1.501793384552002\n",
      "Epoch 309/2000, Train Loss: 6.144116249242978, Val Loss: 4.8663724141211855, Val MAE: 1.5010089874267578\n",
      "Epoch 310/2000, Train Loss: 6.143681710314553, Val Loss: 4.866450247790519, Val MAE: 1.50198233127594\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 311/2000, Train Loss: 6.143105263855318, Val Loss: 4.866386179233259, Val MAE: 1.502216100692749\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 312/2000, Train Loss: 6.142936078333128, Val Loss: 4.8660633592745395, Val MAE: 1.5000081062316895\n",
      "Epoch 313/2000, Train Loss: 6.142960476016734, Val Loss: 4.865168611008162, Val MAE: 1.4994487762451172\n",
      "Epoch 314/2000, Train Loss: 6.142101021948944, Val Loss: 4.866552449884962, Val MAE: 1.502622127532959\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 315/2000, Train Loss: 6.142104968337802, Val Loss: 4.865457855287145, Val MAE: 1.4999327659606934\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 316/2000, Train Loss: 6.141546360871798, Val Loss: 4.865468218725311, Val MAE: 1.5001215934753418\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 317/2000, Train Loss: 6.141357243028044, Val Loss: 4.865393731982681, Val MAE: 1.500123143196106\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 318/2000, Train Loss: 6.141534538216208, Val Loss: 4.865744540550068, Val MAE: 1.5015267133712769\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 319/2000, Train Loss: 6.1404017097071595, Val Loss: 4.865334346768309, Val MAE: 1.5010499954223633\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 320/2000, Train Loss: 6.140281542120218, Val Loss: 4.864746676586465, Val MAE: 1.5000375509262085\n",
      "Epoch 321/2000, Train Loss: 6.139902590120268, Val Loss: 4.8649134403388254, Val MAE: 1.5003057718276978\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 322/2000, Train Loss: 6.139622677726429, Val Loss: 4.864706543916637, Val MAE: 1.4993922710418701\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 323/2000, Train Loss: 6.139527952505941, Val Loss: 4.86471084687591, Val MAE: 1.4998691082000732\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 324/2000, Train Loss: 6.13892178192033, Val Loss: 4.864777134024405, Val MAE: 1.5004414319992065\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 325/2000, Train Loss: 6.138824799186305, Val Loss: 4.865102598948632, Val MAE: 1.5007028579711914\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 326/2000, Train Loss: 6.1382809454054055, Val Loss: 4.8647639951463155, Val MAE: 1.5016086101531982\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 327/2000, Train Loss: 6.137948960808836, Val Loss: 4.8638717920659165, Val MAE: 1.499690294265747\n",
      "Epoch 328/2000, Train Loss: 6.137488971192421, Val Loss: 4.863793558677163, Val MAE: 1.4988417625427246\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 329/2000, Train Loss: 6.137313844622668, Val Loss: 4.864067843468087, Val MAE: 1.499543309211731\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 330/2000, Train Loss: 6.136799409713111, Val Loss: 4.863658946557123, Val MAE: 1.4998151063919067\n",
      "Epoch 331/2000, Train Loss: 6.136763255417843, Val Loss: 4.864595975111442, Val MAE: 1.502040982246399\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 332/2000, Train Loss: 6.136321061998193, Val Loss: 4.863648320197082, Val MAE: 1.5001232624053955\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 333/2000, Train Loss: 6.135971195611927, Val Loss: 4.863527272540188, Val MAE: 1.4993985891342163\n",
      "Epoch 334/2000, Train Loss: 6.135955402131226, Val Loss: 4.8644479103245475, Val MAE: 1.502049207687378\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 335/2000, Train Loss: 6.135459564531279, Val Loss: 4.864232341388962, Val MAE: 1.5018616914749146\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 336/2000, Train Loss: 6.134989002999176, Val Loss: 4.8641415337032505, Val MAE: 1.5010923147201538\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 337/2000, Train Loss: 6.134680244599022, Val Loss: 4.863759185644372, Val MAE: 1.5009959936141968\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 338/2000, Train Loss: 6.134406059666683, Val Loss: 4.86276862263905, Val MAE: 1.4983322620391846\n",
      "Epoch 339/2000, Train Loss: 6.1342059473581925, Val Loss: 4.862728364944157, Val MAE: 1.498408317565918\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 340/2000, Train Loss: 6.133690304241022, Val Loss: 4.863123401764588, Val MAE: 1.4993993043899536\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 341/2000, Train Loss: 6.13365188107266, Val Loss: 4.86285698415728, Val MAE: 1.500048041343689\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 342/2000, Train Loss: 6.13292434407073, Val Loss: 4.862680887655457, Val MAE: 1.4988585710525513\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 343/2000, Train Loss: 6.132721376551156, Val Loss: 4.863244996414359, Val MAE: 1.500403881072998\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 344/2000, Train Loss: 6.13230549027715, Val Loss: 4.862846036229053, Val MAE: 1.4992971420288086\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 345/2000, Train Loss: 6.131927886497941, Val Loss: 4.862834016234622, Val MAE: 1.5002617835998535\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 346/2000, Train Loss: 6.131942819294177, Val Loss: 4.8625819905464915, Val MAE: 1.4991635084152222\n",
      "Epoch 347/2000, Train Loss: 6.131469602267828, Val Loss: 4.863027796045904, Val MAE: 1.5006837844848633\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 348/2000, Train Loss: 6.131110979117185, Val Loss: 4.862304593160429, Val MAE: 1.4992820024490356\n",
      "Epoch 349/2000, Train Loss: 6.130809086247495, Val Loss: 4.862326862814297, Val MAE: 1.499608039855957\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 350/2000, Train Loss: 6.130623714019057, Val Loss: 4.862498154001684, Val MAE: 1.5005732774734497\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 351/2000, Train Loss: 6.130713835845694, Val Loss: 4.862011223815354, Val MAE: 1.4990200996398926\n",
      "Epoch 352/2000, Train Loss: 6.129982337132716, Val Loss: 4.862558212615126, Val MAE: 1.5005398988723755\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 353/2000, Train Loss: 6.130040336249608, Val Loss: 4.861648240576476, Val MAE: 1.498402714729309\n",
      "Epoch 354/2000, Train Loss: 6.129452044151496, Val Loss: 4.862512877730072, Val MAE: 1.5004751682281494\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 355/2000, Train Loss: 6.129159464003967, Val Loss: 4.862082144651858, Val MAE: 1.500833511352539\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 356/2000, Train Loss: 6.128997432888379, Val Loss: 4.862422688348597, Val MAE: 1.5008352994918823\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 357/2000, Train Loss: 6.12874910085155, Val Loss: 4.861575947066799, Val MAE: 1.498090147972107\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 358/2000, Train Loss: 6.128252336655297, Val Loss: 4.8621451206796555, Val MAE: 1.5000522136688232\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 359/2000, Train Loss: 6.127813336855817, Val Loss: 4.861421064791959, Val MAE: 1.4993659257888794\n",
      "Epoch 360/2000, Train Loss: 6.1273885619937545, Val Loss: 4.8615275332692445, Val MAE: 1.4995973110198975\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 361/2000, Train Loss: 6.127622026882013, Val Loss: 4.861367690814999, Val MAE: 1.499251127243042\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 362/2000, Train Loss: 6.126794835669182, Val Loss: 4.86105474187026, Val MAE: 1.4987027645111084\n",
      "Epoch 363/2000, Train Loss: 6.126745383561153, Val Loss: 4.860944813655274, Val MAE: 1.4984698295593262\n",
      "Epoch 364/2000, Train Loss: 6.126248053500527, Val Loss: 4.860863120538214, Val MAE: 1.4991798400878906\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 365/2000, Train Loss: 6.125960532582037, Val Loss: 4.860947101218403, Val MAE: 1.499001383781433\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 366/2000, Train Loss: 6.125712845398119, Val Loss: 4.861053418354008, Val MAE: 1.4994070529937744\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 367/2000, Train Loss: 6.125659673167728, Val Loss: 4.86148944779911, Val MAE: 1.5001499652862549\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 368/2000, Train Loss: 6.1251664359813915, Val Loss: 4.860477320831104, Val MAE: 1.497389554977417\n",
      "Epoch 369/2000, Train Loss: 6.124917484716695, Val Loss: 4.860409814662395, Val MAE: 1.4982694387435913\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 370/2000, Train Loss: 6.125237881509881, Val Loss: 4.8605850452244805, Val MAE: 1.4987094402313232\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 371/2000, Train Loss: 6.124376583627717, Val Loss: 4.861116754358533, Val MAE: 1.500477910041809\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 372/2000, Train Loss: 6.124171961311489, Val Loss: 4.860794352722198, Val MAE: 1.4989473819732666\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 373/2000, Train Loss: 6.1240186160291, Val Loss: 4.86088412723881, Val MAE: 1.498821496963501\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 374/2000, Train Loss: 6.123489650134565, Val Loss: 4.860133167746163, Val MAE: 1.4984285831451416\n",
      "Epoch 375/2000, Train Loss: 6.123513174717446, Val Loss: 4.860436194603335, Val MAE: 1.498715877532959\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 376/2000, Train Loss: 6.122761727039834, Val Loss: 4.86076605830149, Val MAE: 1.4994145631790161\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 377/2000, Train Loss: 6.12290150359727, Val Loss: 4.860685556140478, Val MAE: 1.499822974205017\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 378/2000, Train Loss: 6.122386621174059, Val Loss: 4.860484901350579, Val MAE: 1.4995222091674805\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 379/2000, Train Loss: 6.122551913802973, Val Loss: 4.8606232298352525, Val MAE: 1.4991034269332886\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 380/2000, Train Loss: 6.121609584248297, Val Loss: 4.859577056851994, Val MAE: 1.4969298839569092\n",
      "Epoch 381/2000, Train Loss: 6.121644465216639, Val Loss: 4.860154503907111, Val MAE: 1.498619556427002\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 382/2000, Train Loss: 6.121482111542509, Val Loss: 4.8591707302598, Val MAE: 1.4968637228012085\n",
      "Epoch 383/2000, Train Loss: 6.1211535013943825, Val Loss: 4.859768520331323, Val MAE: 1.4986480474472046\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 384/2000, Train Loss: 6.120974067835927, Val Loss: 4.859380234056358, Val MAE: 1.4967280626296997\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 385/2000, Train Loss: 6.120726616032566, Val Loss: 4.859196510010905, Val MAE: 1.4974086284637451\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 386/2000, Train Loss: 6.120344675016535, Val Loss: 4.859363724612648, Val MAE: 1.496971845626831\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 387/2000, Train Loss: 6.11991787458721, Val Loss: 4.859372058570911, Val MAE: 1.4980206489562988\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 388/2000, Train Loss: 6.120290433402867, Val Loss: 4.858989195266332, Val MAE: 1.4964261054992676\n",
      "Epoch 389/2000, Train Loss: 6.119568914851984, Val Loss: 4.858703274012063, Val MAE: 1.4969005584716797\n",
      "Epoch 390/2000, Train Loss: 6.119162809155324, Val Loss: 4.85991881756772, Val MAE: 1.4997906684875488\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 391/2000, Train Loss: 6.1191745040819585, Val Loss: 4.859731862328543, Val MAE: 1.499121904373169\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 392/2000, Train Loss: 6.11879744410845, Val Loss: 4.85882372510576, Val MAE: 1.4969440698623657\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 393/2000, Train Loss: 6.118252942304532, Val Loss: 4.858861274002554, Val MAE: 1.4965163469314575\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 394/2000, Train Loss: 6.118319740982267, Val Loss: 4.858511117910602, Val MAE: 1.4971368312835693\n",
      "Epoch 395/2000, Train Loss: 6.117953983898638, Val Loss: 4.859285111826801, Val MAE: 1.4973347187042236\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 396/2000, Train Loss: 6.117323993249613, Val Loss: 4.858834165088541, Val MAE: 1.4966427087783813\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 397/2000, Train Loss: 6.117503534723848, Val Loss: 4.858504327453325, Val MAE: 1.4969919919967651\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 398/2000, Train Loss: 6.117181821939357, Val Loss: 4.85928319945161, Val MAE: 1.49897038936615\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 399/2000, Train Loss: 6.1168696181595825, Val Loss: 4.858684822456608, Val MAE: 1.497025728225708\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 400/2000, Train Loss: 6.116721842757883, Val Loss: 4.85873800599748, Val MAE: 1.4967893362045288\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 401/2000, Train Loss: 6.116149592201466, Val Loss: 4.858370142703475, Val MAE: 1.4969544410705566\n",
      "Epoch 402/2000, Train Loss: 6.1161405381073255, Val Loss: 4.859027454993614, Val MAE: 1.4986653327941895\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 403/2000, Train Loss: 6.115758508592431, Val Loss: 4.858185350077525, Val MAE: 1.496765375137329\n",
      "Epoch 404/2000, Train Loss: 6.116202755714057, Val Loss: 4.8588653840359495, Val MAE: 1.4983938932418823\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 405/2000, Train Loss: 6.115275852171668, Val Loss: 4.85814219561005, Val MAE: 1.497286081314087\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 406/2000, Train Loss: 6.115640334343316, Val Loss: 4.858731687125312, Val MAE: 1.4983526468276978\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 407/2000, Train Loss: 6.115027980328927, Val Loss: 4.858394093351135, Val MAE: 1.4979714155197144\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 408/2000, Train Loss: 6.114462122296362, Val Loss: 4.8580044169462795, Val MAE: 1.4972082376480103\n",
      "Epoch 409/2000, Train Loss: 6.113990676436067, Val Loss: 4.858398163435138, Val MAE: 1.4985263347625732\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 410/2000, Train Loss: 6.114071342489396, Val Loss: 4.858580503128892, Val MAE: 1.498685598373413\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 411/2000, Train Loss: 6.113781909929418, Val Loss: 4.8580740185226645, Val MAE: 1.4978234767913818\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 412/2000, Train Loss: 6.113508638978995, Val Loss: 4.858618827055186, Val MAE: 1.4988173246383667\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 413/2000, Train Loss: 6.113161091659208, Val Loss: 4.858250187773347, Val MAE: 1.4986974000930786\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 414/2000, Train Loss: 6.113174469649296, Val Loss: 4.858794065067852, Val MAE: 1.499169111251831\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 415/2000, Train Loss: 6.112940992741043, Val Loss: 4.858252931560048, Val MAE: 1.498839020729065\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 416/2000, Train Loss: 6.112527046573459, Val Loss: 4.858063628692721, Val MAE: 1.4982235431671143\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 417/2000, Train Loss: 6.1126008962330065, Val Loss: 4.858544037986222, Val MAE: 1.499429702758789\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 418/2000, Train Loss: 6.111826764579625, Val Loss: 4.857726177240606, Val MAE: 1.4978326559066772\n",
      "Epoch 419/2000, Train Loss: 6.112268686954995, Val Loss: 4.858264794725864, Val MAE: 1.4991455078125\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 420/2000, Train Loss: 6.111627870913688, Val Loss: 4.8574470456731875, Val MAE: 1.4978480339050293\n",
      "Epoch 421/2000, Train Loss: 6.111621459501272, Val Loss: 4.857525298238026, Val MAE: 1.4976811408996582\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 422/2000, Train Loss: 6.111689916277856, Val Loss: 4.857328830195255, Val MAE: 1.4979976415634155\n",
      "Epoch 423/2000, Train Loss: 6.110685024367145, Val Loss: 4.8576307226381115, Val MAE: 1.4985874891281128\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 424/2000, Train Loss: 6.110344360013418, Val Loss: 4.857445471167264, Val MAE: 1.4982768297195435\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 425/2000, Train Loss: 6.110775539947679, Val Loss: 4.85739614882167, Val MAE: 1.4987248182296753\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 426/2000, Train Loss: 6.110307460478468, Val Loss: 4.85793928675175, Val MAE: 1.4990748167037964\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 427/2000, Train Loss: 6.109765109329012, Val Loss: 4.857170845359601, Val MAE: 1.4984631538391113\n",
      "Epoch 428/2000, Train Loss: 6.109266216669056, Val Loss: 4.856914466944573, Val MAE: 1.4979146718978882\n",
      "Epoch 429/2000, Train Loss: 6.109483687303073, Val Loss: 4.857257665669332, Val MAE: 1.498386263847351\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 430/2000, Train Loss: 6.109203518790881, Val Loss: 4.856360205701666, Val MAE: 1.4969911575317383\n",
      "Epoch 431/2000, Train Loss: 6.108925455891194, Val Loss: 4.857000937387065, Val MAE: 1.498213768005371\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 432/2000, Train Loss: 6.108605080099978, Val Loss: 4.857354879905417, Val MAE: 1.4988923072814941\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 433/2000, Train Loss: 6.108128182577625, Val Loss: 4.8575111504686435, Val MAE: 1.4994655847549438\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 434/2000, Train Loss: 6.10778871698723, Val Loss: 4.857488471793872, Val MAE: 1.4994944334030151\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 435/2000, Train Loss: 6.107672446190155, Val Loss: 4.856972824927762, Val MAE: 1.4985979795455933\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 436/2000, Train Loss: 6.107808244657648, Val Loss: 4.856410129392733, Val MAE: 1.4978994131088257\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 437/2000, Train Loss: 6.10751828090636, Val Loss: 4.856222953839982, Val MAE: 1.4973208904266357\n",
      "Epoch 438/2000, Train Loss: 6.106924050138267, Val Loss: 4.856813202862256, Val MAE: 1.4986714124679565\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 439/2000, Train Loss: 6.106708546688682, Val Loss: 4.85692704381588, Val MAE: 1.4985251426696777\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 440/2000, Train Loss: 6.1063379693229445, Val Loss: 4.85683047310138, Val MAE: 1.4983954429626465\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 441/2000, Train Loss: 6.106356738610941, Val Loss: 4.856602459704365, Val MAE: 1.4983645677566528\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 442/2000, Train Loss: 6.106172934397436, Val Loss: 4.857182103900937, Val MAE: 1.499494194984436\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 443/2000, Train Loss: 6.105747105606375, Val Loss: 4.856316715807737, Val MAE: 1.4980278015136719\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 444/2000, Train Loss: 6.105802178052654, Val Loss: 4.856278816705468, Val MAE: 1.4986727237701416\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 445/2000, Train Loss: 6.105675817790784, Val Loss: 4.857199633829906, Val MAE: 1.4999935626983643\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 446/2000, Train Loss: 6.1049934368714736, Val Loss: 4.856397208047169, Val MAE: 1.498611330986023\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 447/2000, Train Loss: 6.104953925695446, Val Loss: 4.856418300077112, Val MAE: 1.498775601387024\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 448/2000, Train Loss: 6.104374007174843, Val Loss: 4.856176787590394, Val MAE: 1.4984062910079956\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 449/2000, Train Loss: 6.104771874816134, Val Loss: 4.855607452122365, Val MAE: 1.497887134552002\n",
      "Epoch 450/2000, Train Loss: 6.1044267572524475, Val Loss: 4.857411864426217, Val MAE: 1.5010461807250977\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 451/2000, Train Loss: 6.104050387437984, Val Loss: 4.856589831468211, Val MAE: 1.4992903470993042\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 452/2000, Train Loss: 6.103974756573706, Val Loss: 4.856380861670643, Val MAE: 1.4988116025924683\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 453/2000, Train Loss: 6.103939955650604, Val Loss: 4.85647266947231, Val MAE: 1.4994124174118042\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 454/2000, Train Loss: 6.10357262090963, Val Loss: 4.856682931142301, Val MAE: 1.4997245073318481\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 455/2000, Train Loss: 6.1031157628320925, Val Loss: 4.855812720099434, Val MAE: 1.4985696077346802\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 456/2000, Train Loss: 6.10284867167803, Val Loss: 4.855618854799081, Val MAE: 1.498321533203125\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 457/2000, Train Loss: 6.102906376635269, Val Loss: 4.855604931139029, Val MAE: 1.4984071254730225\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 458/2000, Train Loss: 6.10271250468542, Val Loss: 4.855266999984148, Val MAE: 1.4973037242889404\n",
      "Epoch 459/2000, Train Loss: 6.102143468883229, Val Loss: 4.856589093945291, Val MAE: 1.5001906156539917\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 460/2000, Train Loss: 6.102097669532755, Val Loss: 4.856176228007633, Val MAE: 1.4988893270492554\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 461/2000, Train Loss: 6.1016419014442, Val Loss: 4.856023936853057, Val MAE: 1.4994796514511108\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 462/2000, Train Loss: 6.10184246470063, Val Loss: 4.8552801447907585, Val MAE: 1.498428463935852\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 463/2000, Train Loss: 6.101399044448979, Val Loss: 4.855869122864244, Val MAE: 1.4993596076965332\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 464/2000, Train Loss: 6.10088647963928, Val Loss: 4.856230677940506, Val MAE: 1.500380277633667\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 465/2000, Train Loss: 6.10082526590025, Val Loss: 4.855636810331898, Val MAE: 1.4988528490066528\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 466/2000, Train Loss: 6.100618552104918, Val Loss: 4.856546083265566, Val MAE: 1.5006701946258545\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 467/2000, Train Loss: 6.100547199037927, Val Loss: 4.856218437294266, Val MAE: 1.5003583431243896\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 468/2000, Train Loss: 6.10001522814468, Val Loss: 4.855284238851913, Val MAE: 1.4990710020065308\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 469/2000, Train Loss: 6.1000734836441, Val Loss: 4.855517803993156, Val MAE: 1.4995335340499878\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 470/2000, Train Loss: 6.099652990409872, Val Loss: 4.855450594363192, Val MAE: 1.4990841150283813\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 471/2000, Train Loss: 6.099469344965969, Val Loss: 4.855659343151923, Val MAE: 1.499469518661499\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 472/2000, Train Loss: 6.099471780401848, Val Loss: 4.855417407156541, Val MAE: 1.4993805885314941\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 473/2000, Train Loss: 6.09906361122871, Val Loss: 4.855814788297927, Val MAE: 1.5000522136688232\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 474/2000, Train Loss: 6.098708629476066, Val Loss: 4.855762958066235, Val MAE: 1.4999597072601318\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 475/2000, Train Loss: 6.0985861605224185, Val Loss: 4.85617192776364, Val MAE: 1.5007613897323608\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 476/2000, Train Loss: 6.098346327150297, Val Loss: 4.855602036104602, Val MAE: 1.4998869895935059\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 477/2000, Train Loss: 6.098041143047513, Val Loss: 4.855203624556374, Val MAE: 1.4993585348129272\n",
      "EarlyStopping counter: 19 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [16:36<00:00, 199.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 478/2000, Train Loss: 6.098246420123241, Val Loss: 4.855934953187859, Val MAE: 1.5003714561462402\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 5.433047771453857\n",
      "Test Mean Absolute Error (MAE): 1.5848207828460326\n",
      "Updating GridSearch Results Log File: gridsearch\\results\\rnn_eval_def_12-12.csv...\n",
      "Logging experiment results to gridsearch\\results\\rnn_eval_def_12-12.csv.\n",
      "======= Done with GridSearch Experiment ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from final_project.rnn.experiment import gridsearch_rnn\n",
    "\n",
    "gridsearch_rnn(experiment_name = \"rnn_eval_big\", verbose = True)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized) — {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
