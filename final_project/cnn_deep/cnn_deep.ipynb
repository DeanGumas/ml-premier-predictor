{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.cnn_deep.model import build_train_cnn, full_cnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from final_project.cnn_deep.config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2178, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 9).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2178, 7)\n",
      "Shape of a given window (prior to preprocessing): (10, 9)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.94211124e+00 4.79057889e+01 0.00000000e+00 1.70261067e-03\n",
      " 1.45289444e-01 9.96594779e+00 2.66742338e-02 1.13507378e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[2.73976222e+00 4.48207323e+01 1.00000000e+00 4.12275610e-02\n",
      " 3.52392425e-01 1.07559860e+01 1.61129510e-01 3.36717298e-02]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building CNN Architecture ======\n",
      "====== Done Building CNN Architecture ======\n",
      "Epoch 1/2000, Train Loss: 11.309990027393454, Val Loss: 7.979194960095458, Val MAE: 1.4797776937484741\n",
      "Epoch 2/2000, Train Loss: 11.304554339783055, Val Loss: 7.975396643746812, Val MAE: 1.4794763326644897\n",
      "Epoch 3/2000, Train Loss: 11.299192334858006, Val Loss: 7.971756492024875, Val MAE: 1.479196548461914\n",
      "Epoch 4/2000, Train Loss: 11.293875945711656, Val Loss: 7.967898703957195, Val MAE: 1.478899359703064\n",
      "Epoch 5/2000, Train Loss: 11.288437082654145, Val Loss: 7.964130787876947, Val MAE: 1.478617548942566\n",
      "Epoch 6/2000, Train Loss: 11.283254852532968, Val Loss: 7.960504362654981, Val MAE: 1.4783512353897095\n",
      "Epoch 7/2000, Train Loss: 11.278013821511113, Val Loss: 7.956758429096626, Val MAE: 1.4780840873718262\n",
      "Epoch 8/2000, Train Loss: 11.272847513326802, Val Loss: 7.953009526740323, Val MAE: 1.477811336517334\n",
      "Epoch 9/2000, Train Loss: 11.267516080377254, Val Loss: 7.949296243092767, Val MAE: 1.4775397777557373\n",
      "Epoch 10/2000, Train Loss: 11.262313015561395, Val Loss: 7.9457089463332755, Val MAE: 1.4772762060165405\n",
      "Epoch 11/2000, Train Loss: 11.257085683289734, Val Loss: 7.941978445598806, Val MAE: 1.4770020246505737\n",
      "Epoch 12/2000, Train Loss: 11.251960569908393, Val Loss: 7.938330641687587, Val MAE: 1.4767342805862427\n",
      "Epoch 13/2000, Train Loss: 11.246583743697954, Val Loss: 7.934531112750237, Val MAE: 1.4764560461044312\n",
      "Epoch 14/2000, Train Loss: 11.241441428010438, Val Loss: 7.930923690126689, Val MAE: 1.4762033224105835\n",
      "Epoch 15/2000, Train Loss: 11.236364752937591, Val Loss: 7.9272793628376075, Val MAE: 1.4759433269500732\n",
      "Epoch 16/2000, Train Loss: 11.231149260785761, Val Loss: 7.923672558954573, Val MAE: 1.4756864309310913\n",
      "Epoch 17/2000, Train Loss: 11.226130776955817, Val Loss: 7.920129181051979, Val MAE: 1.475435495376587\n",
      "Epoch 18/2000, Train Loss: 11.221081355804586, Val Loss: 7.916475785768649, Val MAE: 1.4751739501953125\n",
      "Epoch 19/2000, Train Loss: 11.21594461894379, Val Loss: 7.912930063715389, Val MAE: 1.474926471710205\n",
      "Epoch 20/2000, Train Loss: 11.21090683550247, Val Loss: 7.909273300054776, Val MAE: 1.4746683835983276\n",
      "Epoch 21/2000, Train Loss: 11.205735870211294, Val Loss: 7.90564394213662, Val MAE: 1.4744175672531128\n",
      "Epoch 22/2000, Train Loss: 11.200628941962945, Val Loss: 7.902018181586991, Val MAE: 1.474161982536316\n",
      "Epoch 23/2000, Train Loss: 11.195475242422479, Val Loss: 7.898431330654132, Val MAE: 1.473923683166504\n",
      "Epoch 24/2000, Train Loss: 11.190366092968564, Val Loss: 7.894781900334868, Val MAE: 1.4736785888671875\n",
      "Epoch 25/2000, Train Loss: 11.185246311813918, Val Loss: 7.891132108481215, Val MAE: 1.4734218120574951\n",
      "Epoch 26/2000, Train Loss: 11.18011769043302, Val Loss: 7.8874480974701076, Val MAE: 1.4731724262237549\n",
      "Epoch 27/2000, Train Loss: 11.174876028459733, Val Loss: 7.883858574483845, Val MAE: 1.4729238748550415\n",
      "Epoch 28/2000, Train Loss: 11.169766235277173, Val Loss: 7.880171979628168, Val MAE: 1.4726661443710327\n",
      "Epoch 29/2000, Train Loss: 11.164577821310132, Val Loss: 7.876519392920775, Val MAE: 1.4724210500717163\n",
      "Epoch 30/2000, Train Loss: 11.159433101888677, Val Loss: 7.872860404402688, Val MAE: 1.4721744060516357\n",
      "Epoch 31/2000, Train Loss: 11.15424935494124, Val Loss: 7.869237189494946, Val MAE: 1.4719277620315552\n",
      "Epoch 32/2000, Train Loss: 11.14897195309075, Val Loss: 7.865440572184083, Val MAE: 1.4716607332229614\n",
      "Epoch 33/2000, Train Loss: 11.14375317747619, Val Loss: 7.861788866004428, Val MAE: 1.4714144468307495\n",
      "Epoch 34/2000, Train Loss: 11.138550732325466, Val Loss: 7.858221800803198, Val MAE: 1.4711800813674927\n",
      "Epoch 35/2000, Train Loss: 11.133412246585078, Val Loss: 7.854517908320502, Val MAE: 1.4709420204162598\n",
      "Epoch 36/2000, Train Loss: 11.128197164691741, Val Loss: 7.850861094146967, Val MAE: 1.4707080125808716\n",
      "Epoch 37/2000, Train Loss: 11.122993958396213, Val Loss: 7.847176885524312, Val MAE: 1.4704594612121582\n",
      "Epoch 38/2000, Train Loss: 11.117806116987873, Val Loss: 7.843499764662471, Val MAE: 1.4702073335647583\n",
      "Epoch 39/2000, Train Loss: 11.11254419848997, Val Loss: 7.839772383908968, Val MAE: 1.4699625968933105\n",
      "Epoch 40/2000, Train Loss: 11.107355503879731, Val Loss: 7.836070528562676, Val MAE: 1.469721794128418\n",
      "Epoch 41/2000, Train Loss: 11.102115228661741, Val Loss: 7.83239864631816, Val MAE: 1.469482660293579\n",
      "Epoch 42/2000, Train Loss: 11.096931357837505, Val Loss: 7.828674141465275, Val MAE: 1.4692327976226807\n",
      "Epoch 43/2000, Train Loss: 11.09158931023989, Val Loss: 7.82495860288272, Val MAE: 1.4689861536026\n",
      "Epoch 44/2000, Train Loss: 11.086200593972169, Val Loss: 7.821188309295355, Val MAE: 1.468741774559021\n",
      "Epoch 45/2000, Train Loss: 11.080946318840677, Val Loss: 7.817519635502417, Val MAE: 1.468508005142212\n",
      "Epoch 46/2000, Train Loss: 11.075768806447105, Val Loss: 7.813858298031059, Val MAE: 1.468268871307373\n",
      "Epoch 47/2000, Train Loss: 11.07058993144638, Val Loss: 7.810140340822237, Val MAE: 1.4680308103561401\n",
      "Epoch 48/2000, Train Loss: 11.065155556718942, Val Loss: 7.8064029394607015, Val MAE: 1.4677854776382446\n",
      "Epoch 49/2000, Train Loss: 11.059919944529199, Val Loss: 7.8026250362597604, Val MAE: 1.4675347805023193\n",
      "Epoch 50/2000, Train Loss: 11.054655715186794, Val Loss: 7.799036700671186, Val MAE: 1.4673069715499878\n",
      "Epoch 51/2000, Train Loss: 11.0495075533245, Val Loss: 7.795384654933, Val MAE: 1.4670753479003906\n",
      "Epoch 52/2000, Train Loss: 11.044260239177754, Val Loss: 7.791762552185504, Val MAE: 1.4668371677398682\n",
      "Epoch 53/2000, Train Loss: 11.039106182300728, Val Loss: 7.788030258361418, Val MAE: 1.4665971994400024\n",
      "Epoch 54/2000, Train Loss: 11.033811240411957, Val Loss: 7.78437878473385, Val MAE: 1.4663653373718262\n",
      "Epoch 55/2000, Train Loss: 11.028540462952881, Val Loss: 7.780544509201705, Val MAE: 1.4661211967468262\n",
      "Epoch 56/2000, Train Loss: 11.023237055626316, Val Loss: 7.776871206215373, Val MAE: 1.4659010171890259\n",
      "Epoch 57/2000, Train Loss: 11.017996073048126, Val Loss: 7.773290975203922, Val MAE: 1.465682864189148\n",
      "Epoch 58/2000, Train Loss: 11.012796297088242, Val Loss: 7.769578667783791, Val MAE: 1.4654567241668701\n",
      "Epoch 59/2000, Train Loss: 11.00748792936947, Val Loss: 7.765902057155833, Val MAE: 1.465237021446228\n",
      "Epoch 60/2000, Train Loss: 11.001988105362575, Val Loss: 7.762008821695774, Val MAE: 1.464992642402649\n",
      "Epoch 61/2000, Train Loss: 10.996663187720193, Val Loss: 7.758421583983813, Val MAE: 1.4647767543792725\n",
      "Epoch 62/2000, Train Loss: 10.991449047920298, Val Loss: 7.754744490340084, Val MAE: 1.4645588397979736\n",
      "Epoch 63/2000, Train Loss: 10.986199111908721, Val Loss: 7.750939174610618, Val MAE: 1.4643203020095825\n",
      "Epoch 64/2000, Train Loss: 10.980852754356546, Val Loss: 7.7472755984915, Val MAE: 1.4640998840332031\n",
      "Epoch 65/2000, Train Loss: 10.975616264082898, Val Loss: 7.743660283367242, Val MAE: 1.4638848304748535\n",
      "Epoch 66/2000, Train Loss: 10.970367827458531, Val Loss: 7.739923034276108, Val MAE: 1.4636571407318115\n",
      "Epoch 67/2000, Train Loss: 10.965139356306674, Val Loss: 7.736185211322463, Val MAE: 1.4634228944778442\n",
      "Epoch 68/2000, Train Loss: 10.959862990609942, Val Loss: 7.732453936734447, Val MAE: 1.4632023572921753\n",
      "Epoch 69/2000, Train Loss: 10.95455735336042, Val Loss: 7.728749795305031, Val MAE: 1.4629937410354614\n",
      "Epoch 70/2000, Train Loss: 10.949199015934271, Val Loss: 7.724930664826487, Val MAE: 1.4628046751022339\n",
      "Epoch 71/2000, Train Loss: 10.94384986859588, Val Loss: 7.721188947692648, Val MAE: 1.4626240730285645\n",
      "Epoch 72/2000, Train Loss: 10.938496686367088, Val Loss: 7.717445249630658, Val MAE: 1.4624500274658203\n",
      "Epoch 73/2000, Train Loss: 10.93308339364443, Val Loss: 7.71363808789903, Val MAE: 1.462268590927124\n",
      "Epoch 74/2000, Train Loss: 10.927761622412529, Val Loss: 7.709804130513389, Val MAE: 1.4620797634124756\n",
      "Epoch 75/2000, Train Loss: 10.922261342243546, Val Loss: 7.705856624111399, Val MAE: 1.4619005918502808\n",
      "Epoch 76/2000, Train Loss: 10.916886709037698, Val Loss: 7.702155019457007, Val MAE: 1.4617199897766113\n",
      "Epoch 77/2000, Train Loss: 10.911472236868372, Val Loss: 7.698300980024778, Val MAE: 1.4615403413772583\n",
      "Epoch 78/2000, Train Loss: 10.906140415454153, Val Loss: 7.694602278707264, Val MAE: 1.4613755941390991\n",
      "Epoch 79/2000, Train Loss: 10.900888609253858, Val Loss: 7.690942578104971, Val MAE: 1.4612101316452026\n",
      "Epoch 80/2000, Train Loss: 10.895615837503335, Val Loss: 7.687081830606267, Val MAE: 1.4610182046890259\n",
      "Epoch 81/2000, Train Loss: 10.890219752762507, Val Loss: 7.683379851053427, Val MAE: 1.4608410596847534\n",
      "Epoch 82/2000, Train Loss: 10.884874663049828, Val Loss: 7.679596354906355, Val MAE: 1.4606668949127197\n",
      "Epoch 83/2000, Train Loss: 10.879568171761523, Val Loss: 7.67588906792907, Val MAE: 1.4604969024658203\n",
      "Epoch 84/2000, Train Loss: 10.874246025978124, Val Loss: 7.672019511058524, Val MAE: 1.4603044986724854\n",
      "Epoch 85/2000, Train Loss: 10.86874460169752, Val Loss: 7.668112360135661, Val MAE: 1.460107684135437\n",
      "Epoch 86/2000, Train Loss: 10.863175471404832, Val Loss: 7.664293854307752, Val MAE: 1.4599255323410034\n",
      "Epoch 87/2000, Train Loss: 10.857885884234388, Val Loss: 7.660528851414586, Val MAE: 1.4597433805465698\n",
      "Epoch 88/2000, Train Loss: 10.852551143366535, Val Loss: 7.656761244765004, Val MAE: 1.4595636129379272\n",
      "Epoch 89/2000, Train Loss: 10.847015700539844, Val Loss: 7.652881138436161, Val MAE: 1.459380030632019\n",
      "Epoch 90/2000, Train Loss: 10.84167505017309, Val Loss: 7.649169590001976, Val MAE: 1.4591965675354004\n",
      "Epoch 91/2000, Train Loss: 10.83636163809742, Val Loss: 7.645331279262229, Val MAE: 1.4590181112289429\n",
      "Epoch 92/2000, Train Loss: 10.83095746149139, Val Loss: 7.6414902484631755, Val MAE: 1.458824872970581\n",
      "Epoch 93/2000, Train Loss: 10.825596769216839, Val Loss: 7.637697142819027, Val MAE: 1.4586472511291504\n",
      "Epoch 94/2000, Train Loss: 10.819955398377093, Val Loss: 7.633638913016599, Val MAE: 1.4584521055221558\n",
      "Epoch 95/2000, Train Loss: 10.814571794221257, Val Loss: 7.629943998762079, Val MAE: 1.4582711458206177\n",
      "Epoch 96/2000, Train Loss: 10.809038163719237, Val Loss: 7.626035512594489, Val MAE: 1.4580720663070679\n",
      "Epoch 97/2000, Train Loss: 10.803649339958584, Val Loss: 7.62224533110857, Val MAE: 1.457887887954712\n",
      "Epoch 98/2000, Train Loss: 10.798234180801558, Val Loss: 7.618521026018503, Val MAE: 1.4577043056488037\n",
      "Epoch 99/2000, Train Loss: 10.792752544147175, Val Loss: 7.614590643272475, Val MAE: 1.4574968814849854\n",
      "Epoch 100/2000, Train Loss: 10.787302175822251, Val Loss: 7.610763010275257, Val MAE: 1.4573267698287964\n",
      "Epoch 101/2000, Train Loss: 10.78196547099196, Val Loss: 7.607101271825062, Val MAE: 1.4571497440338135\n",
      "Epoch 102/2000, Train Loss: 10.776741719669582, Val Loss: 7.603289558026973, Val MAE: 1.4569556713104248\n",
      "Epoch 103/2000, Train Loss: 10.771392759778385, Val Loss: 7.599584558935048, Val MAE: 1.4567722082138062\n",
      "Epoch 104/2000, Train Loss: 10.766078443125519, Val Loss: 7.595702639289267, Val MAE: 1.4566195011138916\n",
      "Epoch 105/2000, Train Loss: 10.760584043638197, Val Loss: 7.592038186894612, Val MAE: 1.456436038017273\n",
      "Epoch 106/2000, Train Loss: 10.755271858060603, Val Loss: 7.5881321852204495, Val MAE: 1.4562227725982666\n",
      "Epoch 107/2000, Train Loss: 10.749647661229936, Val Loss: 7.584181234797647, Val MAE: 1.4560226202011108\n",
      "Epoch 108/2000, Train Loss: 10.744240570365918, Val Loss: 7.5803834246152695, Val MAE: 1.4558440446853638\n",
      "Epoch 109/2000, Train Loss: 10.738817782697737, Val Loss: 7.576591053350015, Val MAE: 1.45564866065979\n",
      "Epoch 110/2000, Train Loss: 10.733398949300257, Val Loss: 7.572881221683981, Val MAE: 1.455454707145691\n",
      "Epoch 111/2000, Train Loss: 10.727933692113844, Val Loss: 7.569045513626691, Val MAE: 1.4552775621414185\n",
      "Epoch 112/2000, Train Loss: 10.722445748339577, Val Loss: 7.565186332528656, Val MAE: 1.455073356628418\n",
      "Epoch 113/2000, Train Loss: 10.716812036890833, Val Loss: 7.561176738290636, Val MAE: 1.454862356185913\n",
      "Epoch 114/2000, Train Loss: 10.711322504719037, Val Loss: 7.557422941359314, Val MAE: 1.454663872718811\n",
      "Epoch 115/2000, Train Loss: 10.70585836356218, Val Loss: 7.553574846235213, Val MAE: 1.454453945159912\n",
      "Epoch 116/2000, Train Loss: 10.700380654305267, Val Loss: 7.549691513986201, Val MAE: 1.4542371034622192\n",
      "Epoch 117/2000, Train Loss: 10.694880076392021, Val Loss: 7.545889649459639, Val MAE: 1.4540268182754517\n",
      "Epoch 118/2000, Train Loss: 10.68922206725745, Val Loss: 7.541848356807017, Val MAE: 1.4538191556930542\n",
      "Epoch 119/2000, Train Loss: 10.683711529150024, Val Loss: 7.538103919543393, Val MAE: 1.4536166191101074\n",
      "Epoch 120/2000, Train Loss: 10.67820620481012, Val Loss: 7.534154436388262, Val MAE: 1.4533895254135132\n",
      "Epoch 121/2000, Train Loss: 10.672630943113854, Val Loss: 7.530391379781403, Val MAE: 1.4531816244125366\n",
      "Epoch 122/2000, Train Loss: 10.667131777300664, Val Loss: 7.526546284247626, Val MAE: 1.4529709815979004\n",
      "Epoch 123/2000, Train Loss: 10.661575700105539, Val Loss: 7.522642384093624, Val MAE: 1.452759861946106\n",
      "Epoch 124/2000, Train Loss: 10.656067251601197, Val Loss: 7.518817367760448, Val MAE: 1.452557921409607\n",
      "Epoch 125/2000, Train Loss: 10.650333645935177, Val Loss: 7.5147928600711325, Val MAE: 1.4523241519927979\n",
      "Epoch 126/2000, Train Loss: 10.644728663710183, Val Loss: 7.5109663621866485, Val MAE: 1.4521236419677734\n",
      "Epoch 127/2000, Train Loss: 10.639217346953156, Val Loss: 7.50723274060869, Val MAE: 1.4519234895706177\n",
      "Epoch 128/2000, Train Loss: 10.633551593131841, Val Loss: 7.5030789311874555, Val MAE: 1.4517103433609009\n",
      "Epoch 129/2000, Train Loss: 10.6276421554375, Val Loss: 7.498995676213825, Val MAE: 1.4514760971069336\n",
      "Epoch 130/2000, Train Loss: 10.62198065968983, Val Loss: 7.495088097332297, Val MAE: 1.4512604475021362\n",
      "Epoch 131/2000, Train Loss: 10.616483801054693, Val Loss: 7.491261663999375, Val MAE: 1.451043725013733\n",
      "Epoch 132/2000, Train Loss: 10.610920554468649, Val Loss: 7.487475564397939, Val MAE: 1.4508417844772339\n",
      "Epoch 133/2000, Train Loss: 10.605470342680743, Val Loss: 7.483578205800002, Val MAE: 1.4506131410598755\n",
      "Epoch 134/2000, Train Loss: 10.599863787337139, Val Loss: 7.479627831944743, Val MAE: 1.4503945112228394\n",
      "Epoch 135/2000, Train Loss: 10.59411228911925, Val Loss: 7.475731823022838, Val MAE: 1.4501856565475464\n",
      "Epoch 136/2000, Train Loss: 10.588248238557377, Val Loss: 7.471616113313415, Val MAE: 1.4499455690383911\n",
      "Epoch 137/2000, Train Loss: 10.582650126607481, Val Loss: 7.467764841550374, Val MAE: 1.4497219324111938\n",
      "Epoch 138/2000, Train Loss: 10.577045241309774, Val Loss: 7.46387254399088, Val MAE: 1.4494961500167847\n",
      "Epoch 139/2000, Train Loss: 10.571304541482196, Val Loss: 7.459751458915773, Val MAE: 1.4492605924606323\n",
      "Epoch 140/2000, Train Loss: 10.565512866944122, Val Loss: 7.4557049943855755, Val MAE: 1.4490221738815308\n",
      "Epoch 141/2000, Train Loss: 10.559493631728913, Val Loss: 7.451610612432967, Val MAE: 1.44878089427948\n",
      "Epoch 142/2000, Train Loss: 10.55385647921034, Val Loss: 7.447700959135283, Val MAE: 1.448583722114563\n",
      "Epoch 143/2000, Train Loss: 10.548322992466169, Val Loss: 7.443794970791619, Val MAE: 1.4483494758605957\n",
      "Epoch 144/2000, Train Loss: 10.54268936238102, Val Loss: 7.439983988888898, Val MAE: 1.4481332302093506\n",
      "Epoch 145/2000, Train Loss: 10.537171898761331, Val Loss: 7.436144649814646, Val MAE: 1.4479104280471802\n",
      "Epoch 146/2000, Train Loss: 10.531594354909222, Val Loss: 7.43237684506688, Val MAE: 1.4476888179779053\n",
      "Epoch 147/2000, Train Loss: 10.526021537840274, Val Loss: 7.428425204693466, Val MAE: 1.4474536180496216\n",
      "Epoch 148/2000, Train Loss: 10.520249874997948, Val Loss: 7.424456055777836, Val MAE: 1.4472249746322632\n",
      "Epoch 149/2000, Train Loss: 10.514643308524967, Val Loss: 7.420553256309516, Val MAE: 1.4469842910766602\n",
      "Epoch 150/2000, Train Loss: 10.508969527138934, Val Loss: 7.416674661797447, Val MAE: 1.446753740310669\n",
      "Epoch 151/2000, Train Loss: 10.503129188430476, Val Loss: 7.41254164088551, Val MAE: 1.4464980363845825\n",
      "Epoch 152/2000, Train Loss: 10.497427766023904, Val Loss: 7.408555976220885, Val MAE: 1.4463062286376953\n",
      "Epoch 153/2000, Train Loss: 10.49179247500558, Val Loss: 7.404748777689429, Val MAE: 1.4460783004760742\n",
      "Epoch 154/2000, Train Loss: 10.486154287024844, Val Loss: 7.4008155875444945, Val MAE: 1.445822834968567\n",
      "Epoch 155/2000, Train Loss: 10.48051267555463, Val Loss: 7.396887173194875, Val MAE: 1.4455729722976685\n",
      "Epoch 156/2000, Train Loss: 10.474624461205256, Val Loss: 7.392767599506958, Val MAE: 1.4453052282333374\n",
      "Epoch 157/2000, Train Loss: 10.468635049885409, Val Loss: 7.388612125954918, Val MAE: 1.445028305053711\n",
      "Epoch 158/2000, Train Loss: 10.462844456227819, Val Loss: 7.384651810320111, Val MAE: 1.444779396057129\n",
      "Epoch 159/2000, Train Loss: 10.457097920910245, Val Loss: 7.380706746332549, Val MAE: 1.4445282220840454\n",
      "Epoch 160/2000, Train Loss: 10.451208810910419, Val Loss: 7.376571613818675, Val MAE: 1.4442737102508545\n",
      "Epoch 161/2000, Train Loss: 10.445360674686997, Val Loss: 7.37248852591928, Val MAE: 1.4439998865127563\n",
      "Epoch 162/2000, Train Loss: 10.439421359164852, Val Loss: 7.368427556927677, Val MAE: 1.4437464475631714\n",
      "Epoch 163/2000, Train Loss: 10.433662390746118, Val Loss: 7.364455778312844, Val MAE: 1.4434963464736938\n",
      "Epoch 164/2000, Train Loss: 10.42772296959078, Val Loss: 7.360500878901095, Val MAE: 1.443237066268921\n",
      "Epoch 165/2000, Train Loss: 10.421857185185235, Val Loss: 7.35647502775799, Val MAE: 1.4429770708084106\n",
      "Epoch 166/2000, Train Loss: 10.416118276658556, Val Loss: 7.352502092208948, Val MAE: 1.442733645439148\n",
      "Epoch 167/2000, Train Loss: 10.410237118904007, Val Loss: 7.34839100908186, Val MAE: 1.4424479007720947\n",
      "Epoch 168/2000, Train Loss: 10.404396772756591, Val Loss: 7.3444434400010215, Val MAE: 1.4421881437301636\n",
      "Epoch 169/2000, Train Loss: 10.398442788168719, Val Loss: 7.3402472575438455, Val MAE: 1.4418864250183105\n",
      "Epoch 170/2000, Train Loss: 10.39264462630202, Val Loss: 7.336392534765843, Val MAE: 1.4416347742080688\n",
      "Epoch 171/2000, Train Loss: 10.386956441049085, Val Loss: 7.332423169810224, Val MAE: 1.4414079189300537\n",
      "Epoch 172/2000, Train Loss: 10.381146570217219, Val Loss: 7.328402074661341, Val MAE: 1.4411355257034302\n",
      "Epoch 173/2000, Train Loss: 10.375372435670933, Val Loss: 7.324438822866829, Val MAE: 1.440867304801941\n",
      "Epoch 174/2000, Train Loss: 10.369604716248892, Val Loss: 7.320489854358875, Val MAE: 1.4405940771102905\n",
      "Epoch 175/2000, Train Loss: 10.363717458549417, Val Loss: 7.316393216907441, Val MAE: 1.4403225183486938\n",
      "Epoch 176/2000, Train Loss: 10.357547069674535, Val Loss: 7.312156833996912, Val MAE: 1.4400242567062378\n",
      "Epoch 177/2000, Train Loss: 10.351390469650768, Val Loss: 7.308018775472233, Val MAE: 1.439721703529358\n",
      "Epoch 178/2000, Train Loss: 10.345395475765473, Val Loss: 7.303872897388699, Val MAE: 1.4394313097000122\n",
      "Epoch 179/2000, Train Loss: 10.339485588014218, Val Loss: 7.299950271072957, Val MAE: 1.439164161682129\n",
      "Epoch 180/2000, Train Loss: 10.333647271214335, Val Loss: 7.2959164749327545, Val MAE: 1.4388773441314697\n",
      "Epoch 181/2000, Train Loss: 10.32758643511863, Val Loss: 7.2917508283646795, Val MAE: 1.4385920763015747\n",
      "Epoch 182/2000, Train Loss: 10.321731804347017, Val Loss: 7.287731301596573, Val MAE: 1.4383125305175781\n",
      "Epoch 183/2000, Train Loss: 10.315949911981031, Val Loss: 7.28373152824404, Val MAE: 1.4380289316177368\n",
      "Epoch 184/2000, Train Loss: 10.310140245976202, Val Loss: 7.279709144963606, Val MAE: 1.4377329349517822\n",
      "Epoch 185/2000, Train Loss: 10.304212013756429, Val Loss: 7.275635484451646, Val MAE: 1.4374414682388306\n",
      "Epoch 186/2000, Train Loss: 10.298098071688981, Val Loss: 7.271330735288762, Val MAE: 1.4371259212493896\n",
      "Epoch 187/2000, Train Loss: 10.291955202614508, Val Loss: 7.267214013166256, Val MAE: 1.436820149421692\n",
      "Epoch 188/2000, Train Loss: 10.28607330084219, Val Loss: 7.2632328207764, Val MAE: 1.4365249872207642\n",
      "Epoch 189/2000, Train Loss: 10.280168247669238, Val Loss: 7.2592025791269705, Val MAE: 1.4362431764602661\n",
      "Epoch 190/2000, Train Loss: 10.274260022524924, Val Loss: 7.2550951819430605, Val MAE: 1.435929775238037\n",
      "Epoch 191/2000, Train Loss: 10.268152801927277, Val Loss: 7.2508185939007515, Val MAE: 1.4356117248535156\n",
      "Epoch 192/2000, Train Loss: 10.262089290038658, Val Loss: 7.24679004378147, Val MAE: 1.4353199005126953\n",
      "Epoch 193/2000, Train Loss: 10.256172062640257, Val Loss: 7.24258093043222, Val MAE: 1.4349901676177979\n",
      "Epoch 194/2000, Train Loss: 10.25005848210613, Val Loss: 7.2385415708830765, Val MAE: 1.4346908330917358\n",
      "Epoch 195/2000, Train Loss: 10.243963364459423, Val Loss: 7.234239515883697, Val MAE: 1.4343764781951904\n",
      "Epoch 196/2000, Train Loss: 10.237900838870468, Val Loss: 7.2302244267216675, Val MAE: 1.434073567390442\n",
      "Epoch 197/2000, Train Loss: 10.23195971936778, Val Loss: 7.226073492680852, Val MAE: 1.4337522983551025\n",
      "Epoch 198/2000, Train Loss: 10.225723958424584, Val Loss: 7.221826182399784, Val MAE: 1.433417797088623\n",
      "Epoch 199/2000, Train Loss: 10.21964885775645, Val Loss: 7.217668105205437, Val MAE: 1.4331051111221313\n",
      "Epoch 200/2000, Train Loss: 10.213520026244165, Val Loss: 7.213458354999354, Val MAE: 1.4327839612960815\n",
      "Epoch 201/2000, Train Loss: 10.207433263894734, Val Loss: 7.209286808819921, Val MAE: 1.4324934482574463\n",
      "Epoch 202/2000, Train Loss: 10.201316452621484, Val Loss: 7.205104546181791, Val MAE: 1.4321693181991577\n",
      "Epoch 203/2000, Train Loss: 10.195053569240242, Val Loss: 7.200868185092737, Val MAE: 1.4318428039550781\n",
      "Epoch 204/2000, Train Loss: 10.188840254234636, Val Loss: 7.196419125896048, Val MAE: 1.4314849376678467\n",
      "Epoch 205/2000, Train Loss: 10.182448563774736, Val Loss: 7.192334265040385, Val MAE: 1.431149959564209\n",
      "Epoch 206/2000, Train Loss: 10.176398127015778, Val Loss: 7.188128486651558, Val MAE: 1.4308143854141235\n",
      "Epoch 207/2000, Train Loss: 10.170075546002797, Val Loss: 7.183723246218922, Val MAE: 1.4304720163345337\n",
      "Epoch 208/2000, Train Loss: 10.163574861475904, Val Loss: 7.179446171110009, Val MAE: 1.4301320314407349\n",
      "Epoch 209/2000, Train Loss: 10.157203948813928, Val Loss: 7.175012397464062, Val MAE: 1.4297670125961304\n",
      "Epoch 210/2000, Train Loss: 10.151018024792723, Val Loss: 7.170809472620756, Val MAE: 1.4294488430023193\n",
      "Epoch 211/2000, Train Loss: 10.144799230995119, Val Loss: 7.166617153175511, Val MAE: 1.4291023015975952\n",
      "Epoch 212/2000, Train Loss: 10.138487811341253, Val Loss: 7.162093762798352, Val MAE: 1.4287296533584595\n",
      "Epoch 213/2000, Train Loss: 10.13219921451277, Val Loss: 7.158054693707744, Val MAE: 1.4284100532531738\n",
      "Epoch 214/2000, Train Loss: 10.125726323417865, Val Loss: 7.1535042043406145, Val MAE: 1.4280387163162231\n",
      "Epoch 215/2000, Train Loss: 10.119417364311705, Val Loss: 7.149324217734036, Val MAE: 1.4276986122131348\n",
      "Epoch 216/2000, Train Loss: 10.113347621306243, Val Loss: 7.145105593752216, Val MAE: 1.4273524284362793\n",
      "Epoch 217/2000, Train Loss: 10.10714068111503, Val Loss: 7.140983176922744, Val MAE: 1.4270155429840088\n",
      "Epoch 218/2000, Train Loss: 10.101041997174113, Val Loss: 7.136662955810358, Val MAE: 1.4266555309295654\n",
      "Epoch 219/2000, Train Loss: 10.094800449004063, Val Loss: 7.132638119953173, Val MAE: 1.4263238906860352\n",
      "Epoch 220/2000, Train Loss: 10.088723476517034, Val Loss: 7.128361238284154, Val MAE: 1.4259634017944336\n",
      "Epoch 221/2000, Train Loss: 10.082410288303393, Val Loss: 7.124154807164057, Val MAE: 1.4256196022033691\n",
      "Epoch 222/2000, Train Loss: 10.076151453016701, Val Loss: 7.119858281314373, Val MAE: 1.4252653121948242\n",
      "Epoch 223/2000, Train Loss: 10.069836049667572, Val Loss: 7.115605067602686, Val MAE: 1.4249005317687988\n",
      "Epoch 224/2000, Train Loss: 10.063466695467135, Val Loss: 7.111215050914534, Val MAE: 1.424547791481018\n",
      "Epoch 225/2000, Train Loss: 10.056617644323435, Val Loss: 7.1066415997305965, Val MAE: 1.4241321086883545\n",
      "Epoch 226/2000, Train Loss: 10.050409790134282, Val Loss: 7.102401826980414, Val MAE: 1.4237769842147827\n",
      "Epoch 227/2000, Train Loss: 10.044058782541065, Val Loss: 7.098207683230306, Val MAE: 1.4234306812286377\n",
      "Epoch 228/2000, Train Loss: 10.037968390073493, Val Loss: 7.093877243384853, Val MAE: 1.423067569732666\n",
      "Epoch 229/2000, Train Loss: 10.031511197038077, Val Loss: 7.089515092673603, Val MAE: 1.422692894935608\n",
      "Epoch 230/2000, Train Loss: 10.025261726570367, Val Loss: 7.085236933565623, Val MAE: 1.4223378896713257\n",
      "Epoch 231/2000, Train Loss: 10.019006816475606, Val Loss: 7.081160831256761, Val MAE: 1.421986699104309\n",
      "Epoch 232/2000, Train Loss: 10.01283314285361, Val Loss: 7.076959237348926, Val MAE: 1.4216196537017822\n",
      "Epoch 233/2000, Train Loss: 10.00658431700351, Val Loss: 7.072733239010647, Val MAE: 1.421266794204712\n",
      "Epoch 234/2000, Train Loss: 10.000237844291604, Val Loss: 7.068341904006026, Val MAE: 1.42087984085083\n",
      "Epoch 235/2000, Train Loss: 9.993744153360867, Val Loss: 7.064156347801825, Val MAE: 1.420519232749939\n",
      "Epoch 236/2000, Train Loss: 9.98751308660745, Val Loss: 7.0597576934609325, Val MAE: 1.4201217889785767\n",
      "Epoch 237/2000, Train Loss: 9.98109860948393, Val Loss: 7.055513077088304, Val MAE: 1.41975736618042\n",
      "Epoch 238/2000, Train Loss: 9.97430652612457, Val Loss: 7.050839878699264, Val MAE: 1.4193466901779175\n",
      "Epoch 239/2000, Train Loss: 9.967620757366305, Val Loss: 7.046462535858154, Val MAE: 1.4189351797103882\n",
      "Epoch 240/2000, Train Loss: 9.961273535551408, Val Loss: 7.042091880000389, Val MAE: 1.4185798168182373\n",
      "Epoch 241/2000, Train Loss: 9.95475683271792, Val Loss: 7.037666829151882, Val MAE: 1.4181851148605347\n",
      "Epoch 242/2000, Train Loss: 9.948261081893433, Val Loss: 7.0333028483632445, Val MAE: 1.417793869972229\n",
      "Epoch 243/2000, Train Loss: 9.941806613190126, Val Loss: 7.0291617342010815, Val MAE: 1.4174162149429321\n",
      "Epoch 244/2000, Train Loss: 9.935365606581737, Val Loss: 7.0246569658856135, Val MAE: 1.4170128107070923\n",
      "Epoch 245/2000, Train Loss: 9.928793938409148, Val Loss: 7.020266161771777, Val MAE: 1.4165980815887451\n",
      "Epoch 246/2000, Train Loss: 9.921850085444458, Val Loss: 7.0154813660761794, Val MAE: 1.416138768196106\n",
      "Epoch 247/2000, Train Loss: 9.915245002452371, Val Loss: 7.011239999948858, Val MAE: 1.415756344795227\n",
      "Epoch 248/2000, Train Loss: 9.908959957069243, Val Loss: 7.006882072575726, Val MAE: 1.415361762046814\n",
      "Epoch 249/2000, Train Loss: 9.902515147753094, Val Loss: 7.002686893792303, Val MAE: 1.4149806499481201\n",
      "Epoch 250/2000, Train Loss: 9.896152655159627, Val Loss: 6.998509523386622, Val MAE: 1.4146080017089844\n",
      "Epoch 251/2000, Train Loss: 9.88971190221968, Val Loss: 6.994013334233482, Val MAE: 1.414175271987915\n",
      "Epoch 252/2000, Train Loss: 9.882903225522332, Val Loss: 6.989371191018874, Val MAE: 1.4137413501739502\n",
      "Epoch 253/2000, Train Loss: 9.876136023987103, Val Loss: 6.984822835518164, Val MAE: 1.4133175611495972\n",
      "Epoch 254/2000, Train Loss: 9.869179625964946, Val Loss: 6.980215126390124, Val MAE: 1.4128758907318115\n",
      "Epoch 255/2000, Train Loss: 9.862324094251612, Val Loss: 6.975570116299498, Val MAE: 1.412436842918396\n",
      "Epoch 256/2000, Train Loss: 9.855741281690026, Val Loss: 6.971180729608278, Val MAE: 1.4120405912399292\n",
      "Epoch 257/2000, Train Loss: 9.849349226302783, Val Loss: 6.96697543956138, Val MAE: 1.4116530418395996\n",
      "Epoch 258/2000, Train Loss: 9.843020366618116, Val Loss: 6.962744166212039, Val MAE: 1.4112645387649536\n",
      "Epoch 259/2000, Train Loss: 9.836589253822838, Val Loss: 6.958421423219077, Val MAE: 1.4108545780181885\n",
      "Epoch 260/2000, Train Loss: 9.829752544902973, Val Loss: 6.953899711343619, Val MAE: 1.4104183912277222\n",
      "Epoch 261/2000, Train Loss: 9.823158256721198, Val Loss: 6.9494549817263005, Val MAE: 1.4099873304367065\n",
      "Epoch 262/2000, Train Loss: 9.8166383951205, Val Loss: 6.945130050967674, Val MAE: 1.409583568572998\n",
      "Epoch 263/2000, Train Loss: 9.810215433748576, Val Loss: 6.940688945688643, Val MAE: 1.4091572761535645\n",
      "Epoch 264/2000, Train Loss: 9.803616207586993, Val Loss: 6.93644126250266, Val MAE: 1.408780574798584\n",
      "Epoch 265/2000, Train Loss: 9.797104814797407, Val Loss: 6.931963187563527, Val MAE: 1.408360481262207\n",
      "Epoch 266/2000, Train Loss: 9.790507157395671, Val Loss: 6.927713608023551, Val MAE: 1.4079571962356567\n",
      "Epoch 267/2000, Train Loss: 9.783776085387897, Val Loss: 6.923084256892, Val MAE: 1.407503604888916\n",
      "Epoch 268/2000, Train Loss: 9.776993426829925, Val Loss: 6.918615143524634, Val MAE: 1.407103180885315\n",
      "Epoch 269/2000, Train Loss: 9.770422740585161, Val Loss: 6.914249695918045, Val MAE: 1.4066740274429321\n",
      "Epoch 270/2000, Train Loss: 9.763817273975349, Val Loss: 6.909822963231855, Val MAE: 1.4062445163726807\n",
      "Epoch 271/2000, Train Loss: 9.75714624021057, Val Loss: 6.905436951901999, Val MAE: 1.4058146476745605\n",
      "Epoch 272/2000, Train Loss: 9.75050220995351, Val Loss: 6.900896941501278, Val MAE: 1.4053658246994019\n",
      "Epoch 273/2000, Train Loss: 9.743693241649039, Val Loss: 6.896380765286383, Val MAE: 1.4049220085144043\n",
      "Epoch 274/2000, Train Loss: 9.736780038319946, Val Loss: 6.891656296807635, Val MAE: 1.4044374227523804\n",
      "Epoch 275/2000, Train Loss: 9.730058339754244, Val Loss: 6.88715907045984, Val MAE: 1.4039959907531738\n",
      "Epoch 276/2000, Train Loss: 9.72321015773065, Val Loss: 6.882656022161245, Val MAE: 1.4035710096359253\n",
      "Epoch 277/2000, Train Loss: 9.71608656068302, Val Loss: 6.877839829403538, Val MAE: 1.403092622756958\n",
      "Epoch 278/2000, Train Loss: 9.709297494381643, Val Loss: 6.873397552980496, Val MAE: 1.4026415348052979\n",
      "Epoch 279/2000, Train Loss: 9.702658977067497, Val Loss: 6.868902376267287, Val MAE: 1.4021857976913452\n",
      "Epoch 280/2000, Train Loss: 9.695922622442618, Val Loss: 6.864431717810598, Val MAE: 1.4017295837402344\n",
      "Epoch 281/2000, Train Loss: 9.688936506809199, Val Loss: 6.8597604247229595, Val MAE: 1.4012542963027954\n",
      "Epoch 282/2000, Train Loss: 9.682135898497846, Val Loss: 6.855261133550792, Val MAE: 1.4008036851882935\n",
      "Epoch 283/2000, Train Loss: 9.675318770773139, Val Loss: 6.850808606451159, Val MAE: 1.4003372192382812\n",
      "Epoch 284/2000, Train Loss: 9.66853179872129, Val Loss: 6.846130085958017, Val MAE: 1.3998433351516724\n",
      "Epoch 285/2000, Train Loss: 9.661375022503472, Val Loss: 6.841427035495505, Val MAE: 1.3993630409240723\n",
      "Epoch 286/2000, Train Loss: 9.654549887720583, Val Loss: 6.83683181809144, Val MAE: 1.3988896608352661\n",
      "Epoch 287/2000, Train Loss: 9.647638658883977, Val Loss: 6.832399872844821, Val MAE: 1.3984310626983643\n",
      "Epoch 288/2000, Train Loss: 9.640818121280759, Val Loss: 6.827792753681943, Val MAE: 1.3979395627975464\n",
      "Epoch 289/2000, Train Loss: 9.633571310831865, Val Loss: 6.822997087636241, Val MAE: 1.3974372148513794\n",
      "Epoch 290/2000, Train Loss: 9.626611974421603, Val Loss: 6.818308238148152, Val MAE: 1.3969453573226929\n",
      "Epoch 291/2000, Train Loss: 9.619543367168646, Val Loss: 6.813701848336705, Val MAE: 1.3964529037475586\n",
      "Epoch 292/2000, Train Loss: 9.612597063811446, Val Loss: 6.809018641133029, Val MAE: 1.3959683179855347\n",
      "Epoch 293/2000, Train Loss: 9.605523377982392, Val Loss: 6.8042149757614006, Val MAE: 1.3954572677612305\n",
      "Epoch 294/2000, Train Loss: 9.59842705559247, Val Loss: 6.799634102382907, Val MAE: 1.3949838876724243\n",
      "Epoch 295/2000, Train Loss: 9.591454876194506, Val Loss: 6.795020700742801, Val MAE: 1.3944997787475586\n",
      "Epoch 296/2000, Train Loss: 9.584425442676276, Val Loss: 6.790268937370799, Val MAE: 1.3939868211746216\n",
      "Epoch 297/2000, Train Loss: 9.577275338671322, Val Loss: 6.7854954378226315, Val MAE: 1.3934807777404785\n",
      "Epoch 298/2000, Train Loss: 9.570082701684532, Val Loss: 6.780756119457451, Val MAE: 1.3929606676101685\n",
      "Epoch 299/2000, Train Loss: 9.5626641099055, Val Loss: 6.775915503219978, Val MAE: 1.3924269676208496\n",
      "Epoch 300/2000, Train Loss: 9.555548993083132, Val Loss: 6.771068542242587, Val MAE: 1.3919000625610352\n",
      "Epoch 301/2000, Train Loss: 9.5483933744862, Val Loss: 6.766371463568093, Val MAE: 1.39138662815094\n",
      "Epoch 302/2000, Train Loss: 9.541023069908393, Val Loss: 6.761514389756564, Val MAE: 1.3908638954162598\n",
      "Epoch 303/2000, Train Loss: 9.533890334008822, Val Loss: 6.756720055122901, Val MAE: 1.3903647661209106\n",
      "Epoch 304/2000, Train Loss: 9.526647737357248, Val Loss: 6.751970806127196, Val MAE: 1.3898413181304932\n",
      "Epoch 305/2000, Train Loss: 9.519372284505371, Val Loss: 6.746982760076318, Val MAE: 1.3893020153045654\n",
      "Epoch 306/2000, Train Loss: 9.512109319059041, Val Loss: 6.742251532269759, Val MAE: 1.3887763023376465\n",
      "Epoch 307/2000, Train Loss: 9.504744663997299, Val Loss: 6.737338475325892, Val MAE: 1.3882354497909546\n",
      "Epoch 308/2000, Train Loss: 9.497427916935937, Val Loss: 6.732473123214535, Val MAE: 1.3877005577087402\n",
      "Epoch 309/2000, Train Loss: 9.49006534782754, Val Loss: 6.727591609887711, Val MAE: 1.3871428966522217\n",
      "Epoch 310/2000, Train Loss: 9.482772656052422, Val Loss: 6.72282440935572, Val MAE: 1.3866089582443237\n",
      "Epoch 311/2000, Train Loss: 9.47536533968683, Val Loss: 6.717873562583784, Val MAE: 1.3860607147216797\n",
      "Epoch 312/2000, Train Loss: 9.46783270032469, Val Loss: 6.712824764364474, Val MAE: 1.3854644298553467\n",
      "Epoch 313/2000, Train Loss: 9.460494957922402, Val Loss: 6.707906294312026, Val MAE: 1.3849183320999146\n",
      "Epoch 314/2000, Train Loss: 9.452909796825622, Val Loss: 6.702894256297532, Val MAE: 1.384344220161438\n",
      "Epoch 315/2000, Train Loss: 9.445503968170113, Val Loss: 6.698118114290205, Val MAE: 1.3838011026382446\n",
      "Epoch 316/2000, Train Loss: 9.43821701133121, Val Loss: 6.693245818056502, Val MAE: 1.383244514465332\n",
      "Epoch 317/2000, Train Loss: 9.430841182583766, Val Loss: 6.6884232984067085, Val MAE: 1.3827190399169922\n",
      "Epoch 318/2000, Train Loss: 9.423346939027402, Val Loss: 6.683498417700196, Val MAE: 1.3821642398834229\n",
      "Epoch 319/2000, Train Loss: 9.415759670660016, Val Loss: 6.678416255089614, Val MAE: 1.3815761804580688\n",
      "Epoch 320/2000, Train Loss: 9.408327938547117, Val Loss: 6.673360953446444, Val MAE: 1.3809800148010254\n",
      "Epoch 321/2000, Train Loss: 9.400809003391801, Val Loss: 6.668586286942701, Val MAE: 1.3804442882537842\n",
      "Epoch 322/2000, Train Loss: 9.39339304453869, Val Loss: 6.6636227376691926, Val MAE: 1.3798625469207764\n",
      "Epoch 323/2000, Train Loss: 9.38583584545927, Val Loss: 6.658665181146012, Val MAE: 1.379282832145691\n",
      "Epoch 324/2000, Train Loss: 9.378365027830121, Val Loss: 6.653649910720619, Val MAE: 1.3786840438842773\n",
      "Epoch 325/2000, Train Loss: 9.370771827638242, Val Loss: 6.648726749433591, Val MAE: 1.3781358003616333\n",
      "Epoch 326/2000, Train Loss: 9.363309271436028, Val Loss: 6.6437519585629845, Val MAE: 1.377614140510559\n",
      "Epoch 327/2000, Train Loss: 9.355626952071644, Val Loss: 6.638945172498892, Val MAE: 1.3770475387573242\n",
      "Epoch 328/2000, Train Loss: 9.348071251012234, Val Loss: 6.633711572286782, Val MAE: 1.3764270544052124\n",
      "Epoch 329/2000, Train Loss: 9.340445918709552, Val Loss: 6.628624276212744, Val MAE: 1.3758289813995361\n",
      "Epoch 330/2000, Train Loss: 9.332428372995391, Val Loss: 6.623511099493181, Val MAE: 1.3752079010009766\n",
      "Epoch 331/2000, Train Loss: 9.324906346555247, Val Loss: 6.618555658746947, Val MAE: 1.3746140003204346\n",
      "Epoch 332/2000, Train Loss: 9.317487801478917, Val Loss: 6.613432463117548, Val MAE: 1.3740049600601196\n",
      "Epoch 333/2000, Train Loss: 9.309676421227953, Val Loss: 6.608429601211268, Val MAE: 1.3734183311462402\n",
      "Epoch 334/2000, Train Loss: 9.302110603558663, Val Loss: 6.6034444626118685, Val MAE: 1.372824788093567\n",
      "Epoch 335/2000, Train Loss: 9.2943453650076, Val Loss: 6.5982135220825135, Val MAE: 1.372165322303772\n",
      "Epoch 336/2000, Train Loss: 9.286763189595872, Val Loss: 6.593240772966329, Val MAE: 1.371559500694275\n",
      "Epoch 337/2000, Train Loss: 9.279188983525946, Val Loss: 6.588342007054939, Val MAE: 1.3709808588027954\n",
      "Epoch 338/2000, Train Loss: 9.27161077296219, Val Loss: 6.583353506283717, Val MAE: 1.3703725337982178\n",
      "Epoch 339/2000, Train Loss: 9.264031633014055, Val Loss: 6.578377609559007, Val MAE: 1.3697744607925415\n",
      "Epoch 340/2000, Train Loss: 9.256314082748247, Val Loss: 6.573246271411578, Val MAE: 1.3691380023956299\n",
      "Epoch 341/2000, Train Loss: 9.248523896644341, Val Loss: 6.568207130091148, Val MAE: 1.3685346841812134\n",
      "Epoch 342/2000, Train Loss: 9.240482470192097, Val Loss: 6.56272428140447, Val MAE: 1.367855191230774\n",
      "Epoch 343/2000, Train Loss: 9.232662616021548, Val Loss: 6.557805653413137, Val MAE: 1.3672629594802856\n",
      "Epoch 344/2000, Train Loss: 9.224975394012404, Val Loss: 6.552766842294384, Val MAE: 1.366642713546753\n",
      "Epoch 345/2000, Train Loss: 9.217255114765138, Val Loss: 6.547496949458444, Val MAE: 1.3659944534301758\n",
      "Epoch 346/2000, Train Loss: 9.20935159024322, Val Loss: 6.542440760565234, Val MAE: 1.3653916120529175\n",
      "Epoch 347/2000, Train Loss: 9.20185918695411, Val Loss: 6.537342012666904, Val MAE: 1.3647538423538208\n",
      "Epoch 348/2000, Train Loss: 9.194208502211549, Val Loss: 6.532467164703317, Val MAE: 1.3641526699066162\n",
      "Epoch 349/2000, Train Loss: 9.186278892195727, Val Loss: 6.527152120946227, Val MAE: 1.3634814023971558\n",
      "Epoch 350/2000, Train Loss: 9.178485960660389, Val Loss: 6.522122247476836, Val MAE: 1.3628685474395752\n",
      "Epoch 351/2000, Train Loss: 9.170898374338604, Val Loss: 6.517095840823006, Val MAE: 1.3622430562973022\n",
      "Epoch 352/2000, Train Loss: 9.16329898785759, Val Loss: 6.512057087671113, Val MAE: 1.3616042137145996\n",
      "Epoch 353/2000, Train Loss: 9.155672882722804, Val Loss: 6.506933325998955, Val MAE: 1.360959768295288\n",
      "Epoch 354/2000, Train Loss: 9.147462010662567, Val Loss: 6.501507035662999, Val MAE: 1.3602585792541504\n",
      "Epoch 355/2000, Train Loss: 9.13970314610209, Val Loss: 6.496514577680343, Val MAE: 1.3596543073654175\n",
      "Epoch 356/2000, Train Loss: 9.132055048875849, Val Loss: 6.491468298408362, Val MAE: 1.3590238094329834\n",
      "Epoch 357/2000, Train Loss: 9.124435950738963, Val Loss: 6.486428094393498, Val MAE: 1.3584035634994507\n",
      "Epoch 358/2000, Train Loss: 9.116695227005552, Val Loss: 6.481392634196862, Val MAE: 1.3577821254730225\n",
      "Epoch 359/2000, Train Loss: 9.108796903756033, Val Loss: 6.476130007730948, Val MAE: 1.3571208715438843\n",
      "Epoch 360/2000, Train Loss: 9.100596350598819, Val Loss: 6.470728570397373, Val MAE: 1.3564507961273193\n",
      "Epoch 361/2000, Train Loss: 9.092815484160008, Val Loss: 6.4657047729771415, Val MAE: 1.3558248281478882\n",
      "Epoch 362/2000, Train Loss: 9.085128256571647, Val Loss: 6.4607306671572164, Val MAE: 1.3552027940750122\n",
      "Epoch 363/2000, Train Loss: 9.077498919506342, Val Loss: 6.455551967212746, Val MAE: 1.3545525074005127\n",
      "Epoch 364/2000, Train Loss: 9.069426875962481, Val Loss: 6.450289774370623, Val MAE: 1.3539117574691772\n",
      "Epoch 365/2000, Train Loss: 9.0617774912794, Val Loss: 6.445244251889688, Val MAE: 1.3532617092132568\n",
      "Epoch 366/2000, Train Loss: 9.053896911616631, Val Loss: 6.440220244900063, Val MAE: 1.3526273965835571\n",
      "Epoch 367/2000, Train Loss: 9.046260503450533, Val Loss: 6.435265009085069, Val MAE: 1.3520005941390991\n",
      "Epoch 368/2000, Train Loss: 9.03832887087523, Val Loss: 6.42997742210691, Val MAE: 1.351332426071167\n",
      "Epoch 369/2000, Train Loss: 9.030473246403306, Val Loss: 6.424898466286627, Val MAE: 1.3506951332092285\n",
      "Epoch 370/2000, Train Loss: 9.022581758648855, Val Loss: 6.419539606316133, Val MAE: 1.3500165939331055\n",
      "Epoch 371/2000, Train Loss: 9.014622226148239, Val Loss: 6.414563344244485, Val MAE: 1.3493866920471191\n",
      "Epoch 372/2000, Train Loss: 9.006791140099583, Val Loss: 6.409475580133027, Val MAE: 1.3487517833709717\n",
      "Epoch 373/2000, Train Loss: 8.99886863785193, Val Loss: 6.40414868857678, Val MAE: 1.3480768203735352\n",
      "Epoch 374/2000, Train Loss: 8.990914095024609, Val Loss: 6.399022506090167, Val MAE: 1.3474451303482056\n",
      "Epoch 375/2000, Train Loss: 8.982952184870538, Val Loss: 6.3938660515925365, Val MAE: 1.3468002080917358\n",
      "Epoch 376/2000, Train Loss: 8.974946653298952, Val Loss: 6.388727581977576, Val MAE: 1.3461753129959106\n",
      "Epoch 377/2000, Train Loss: 8.967036668298396, Val Loss: 6.383542063859132, Val MAE: 1.345542550086975\n",
      "Epoch 378/2000, Train Loss: 8.958717349166989, Val Loss: 6.37789824247226, Val MAE: 1.3448535203933716\n",
      "Epoch 379/2000, Train Loss: 8.950140050968356, Val Loss: 6.3724741861224174, Val MAE: 1.3441736698150635\n",
      "Epoch 380/2000, Train Loss: 8.94212215629736, Val Loss: 6.367368216664942, Val MAE: 1.3435428142547607\n",
      "Epoch 381/2000, Train Loss: 8.93419129465374, Val Loss: 6.362151023765674, Val MAE: 1.3428820371627808\n",
      "Epoch 382/2000, Train Loss: 8.92569195315265, Val Loss: 6.356535398429847, Val MAE: 1.3421602249145508\n",
      "Epoch 383/2000, Train Loss: 8.917623130096102, Val Loss: 6.35145512037986, Val MAE: 1.3415273427963257\n",
      "Epoch 384/2000, Train Loss: 8.909329889635679, Val Loss: 6.346110863862811, Val MAE: 1.3408678770065308\n",
      "Epoch 385/2000, Train Loss: 8.901422092955496, Val Loss: 6.34098587556331, Val MAE: 1.3402271270751953\n",
      "Epoch 386/2000, Train Loss: 8.893194636019084, Val Loss: 6.335671250884597, Val MAE: 1.3395460844039917\n",
      "Epoch 387/2000, Train Loss: 8.885111465097031, Val Loss: 6.330448984468842, Val MAE: 1.3389232158660889\n",
      "Epoch 388/2000, Train Loss: 8.87690715261629, Val Loss: 6.32496315715415, Val MAE: 1.3382165431976318\n",
      "Epoch 389/2000, Train Loss: 8.868736725190129, Val Loss: 6.319890584823516, Val MAE: 1.3375879526138306\n",
      "Epoch 390/2000, Train Loss: 8.860945415943164, Val Loss: 6.314648312417505, Val MAE: 1.3369113206863403\n",
      "Epoch 391/2000, Train Loss: 8.852693404287518, Val Loss: 6.309341759993149, Val MAE: 1.3362318277359009\n",
      "Epoch 392/2000, Train Loss: 8.844754283030207, Val Loss: 6.304318110148112, Val MAE: 1.335587739944458\n",
      "Epoch 393/2000, Train Loss: 8.836936812690936, Val Loss: 6.299122364933158, Val MAE: 1.3349609375\n",
      "Epoch 394/2000, Train Loss: 8.82897295083345, Val Loss: 6.293907860183232, Val MAE: 1.3342790603637695\n",
      "Epoch 395/2000, Train Loss: 8.820925324643829, Val Loss: 6.28877823687083, Val MAE: 1.3336278200149536\n",
      "Epoch 396/2000, Train Loss: 8.813169419114564, Val Loss: 6.283836767856065, Val MAE: 1.3329967260360718\n",
      "Epoch 397/2000, Train Loss: 8.805135487394288, Val Loss: 6.27855240224718, Val MAE: 1.332327127456665\n",
      "Epoch 398/2000, Train Loss: 8.796739634504185, Val Loss: 6.2729485342161615, Val MAE: 1.3315843343734741\n",
      "Epoch 399/2000, Train Loss: 8.78858160489063, Val Loss: 6.2677310293389334, Val MAE: 1.3309129476547241\n",
      "Epoch 400/2000, Train Loss: 8.780519448278847, Val Loss: 6.262663062198742, Val MAE: 1.330255389213562\n",
      "Epoch 401/2000, Train Loss: 8.77219944057896, Val Loss: 6.257082202030463, Val MAE: 1.3295178413391113\n",
      "Epoch 402/2000, Train Loss: 8.764028810952643, Val Loss: 6.251886334229965, Val MAE: 1.3288358449935913\n",
      "Epoch 403/2000, Train Loss: 8.755822204018532, Val Loss: 6.2467887688931585, Val MAE: 1.3281702995300293\n",
      "Epoch 404/2000, Train Loss: 8.747484631173883, Val Loss: 6.241282786818238, Val MAE: 1.3274503946304321\n",
      "Epoch 405/2000, Train Loss: 8.739444183856946, Val Loss: 6.235954220928587, Val MAE: 1.326824426651001\n",
      "Epoch 406/2000, Train Loss: 8.731303431314537, Val Loss: 6.23090788411933, Val MAE: 1.326158046722412\n",
      "Epoch 407/2000, Train Loss: 8.723292106771247, Val Loss: 6.225527036565918, Val MAE: 1.325435757637024\n",
      "Epoch 408/2000, Train Loss: 8.715335622592574, Val Loss: 6.2204836568921, Val MAE: 1.324784278869629\n",
      "Epoch 409/2000, Train Loss: 8.707309349871556, Val Loss: 6.21511366832498, Val MAE: 1.3240766525268555\n",
      "Epoch 410/2000, Train Loss: 8.69906910933124, Val Loss: 6.20998463068191, Val MAE: 1.3233981132507324\n",
      "Epoch 411/2000, Train Loss: 8.691015766488967, Val Loss: 6.204582733452857, Val MAE: 1.3226864337921143\n",
      "Epoch 412/2000, Train Loss: 8.682608060253996, Val Loss: 6.199352908899655, Val MAE: 1.3219882249832153\n",
      "Epoch 413/2000, Train Loss: 8.674446702747375, Val Loss: 6.194011008262903, Val MAE: 1.3212835788726807\n",
      "Epoch 414/2000, Train Loss: 8.666210806128015, Val Loss: 6.1885807420756365, Val MAE: 1.320560336112976\n",
      "Epoch 415/2000, Train Loss: 8.65776068558001, Val Loss: 6.183343793471923, Val MAE: 1.3198719024658203\n",
      "Epoch 416/2000, Train Loss: 8.649031310297211, Val Loss: 6.177477563749831, Val MAE: 1.319064736366272\n",
      "Epoch 417/2000, Train Loss: 8.640172148945737, Val Loss: 6.171817338419658, Val MAE: 1.3183248043060303\n",
      "Epoch 418/2000, Train Loss: 8.631860977030023, Val Loss: 6.1666096251625735, Val MAE: 1.3176277875900269\n",
      "Epoch 419/2000, Train Loss: 8.62333451250972, Val Loss: 6.161001335829496, Val MAE: 1.3168888092041016\n",
      "Epoch 420/2000, Train Loss: 8.61492360589587, Val Loss: 6.155717572992718, Val MAE: 1.3161853551864624\n",
      "Epoch 421/2000, Train Loss: 8.6067725410467, Val Loss: 6.1504573874913895, Val MAE: 1.3154910802841187\n",
      "Epoch 422/2000, Train Loss: 8.598679957635317, Val Loss: 6.145074676118187, Val MAE: 1.3147732019424438\n",
      "Epoch 423/2000, Train Loss: 8.589850521729257, Val Loss: 6.139591704543915, Val MAE: 1.3140435218811035\n",
      "Epoch 424/2000, Train Loss: 8.581702445115017, Val Loss: 6.134172199673213, Val MAE: 1.313323974609375\n",
      "Epoch 425/2000, Train Loss: 8.57324727468611, Val Loss: 6.128815063760356, Val MAE: 1.312606692314148\n",
      "Epoch 426/2000, Train Loss: 8.565016859726814, Val Loss: 6.123554548755422, Val MAE: 1.311903715133667\n",
      "Epoch 427/2000, Train Loss: 8.556854298631784, Val Loss: 6.118185725734309, Val MAE: 1.3111810684204102\n",
      "Epoch 428/2000, Train Loss: 8.548393901443333, Val Loss: 6.112765517146201, Val MAE: 1.3104661703109741\n",
      "Epoch 429/2000, Train Loss: 8.540154443702162, Val Loss: 6.107490922772401, Val MAE: 1.3097480535507202\n",
      "Epoch 430/2000, Train Loss: 8.53166466067994, Val Loss: 6.102035856038869, Val MAE: 1.3090039491653442\n",
      "Epoch 431/2000, Train Loss: 8.523259310194184, Val Loss: 6.096721384815267, Val MAE: 1.3082983493804932\n",
      "Epoch 432/2000, Train Loss: 8.514720485288528, Val Loss: 6.091191031206567, Val MAE: 1.3075463771820068\n",
      "Epoch 433/2000, Train Loss: 8.50630872223567, Val Loss: 6.085863083876199, Val MAE: 1.3068724870681763\n",
      "Epoch 434/2000, Train Loss: 8.497762272398855, Val Loss: 6.080197890129712, Val MAE: 1.3060799837112427\n",
      "Epoch 435/2000, Train Loss: 8.48898747335544, Val Loss: 6.074760626511531, Val MAE: 1.3053416013717651\n",
      "Epoch 436/2000, Train Loss: 8.480440643852846, Val Loss: 6.069238805878269, Val MAE: 1.3045997619628906\n",
      "Epoch 437/2000, Train Loss: 8.472166329668763, Val Loss: 6.063851252587529, Val MAE: 1.3038705587387085\n",
      "Epoch 438/2000, Train Loss: 8.463913214650127, Val Loss: 6.058688816126134, Val MAE: 1.3031821250915527\n",
      "Epoch 439/2000, Train Loss: 8.455648942968217, Val Loss: 6.053281077628468, Val MAE: 1.3024672269821167\n",
      "Epoch 440/2000, Train Loss: 8.446773898106377, Val Loss: 6.047550228728099, Val MAE: 1.3017536401748657\n",
      "Epoch 441/2000, Train Loss: 8.438308062633375, Val Loss: 6.04234816506505, Val MAE: 1.3010571002960205\n",
      "Epoch 442/2000, Train Loss: 8.430098197389505, Val Loss: 6.03691819855893, Val MAE: 1.3003093004226685\n",
      "Epoch 443/2000, Train Loss: 8.421574900563161, Val Loss: 6.031338398366629, Val MAE: 1.299544334411621\n",
      "Epoch 444/2000, Train Loss: 8.412653188065693, Val Loss: 6.0257176494141955, Val MAE: 1.2987511157989502\n",
      "Epoch 445/2000, Train Loss: 8.40417698555729, Val Loss: 6.02015258359748, Val MAE: 1.2979984283447266\n",
      "Epoch 446/2000, Train Loss: 8.395722809894222, Val Loss: 6.014897650896429, Val MAE: 1.297269582748413\n",
      "Epoch 447/2000, Train Loss: 8.387322940990073, Val Loss: 6.009491030193932, Val MAE: 1.2965272665023804\n",
      "Epoch 448/2000, Train Loss: 8.378845190178957, Val Loss: 6.004258014771852, Val MAE: 1.2958253622055054\n",
      "Epoch 449/2000, Train Loss: 8.37058566819859, Val Loss: 5.998651085290555, Val MAE: 1.2950377464294434\n",
      "Epoch 450/2000, Train Loss: 8.362162204688872, Val Loss: 5.993302056837726, Val MAE: 1.2942901849746704\n",
      "Epoch 451/2000, Train Loss: 8.353563952929516, Val Loss: 5.987877480363523, Val MAE: 1.2935426235198975\n",
      "Epoch 452/2000, Train Loss: 8.344910566975448, Val Loss: 5.98259800663134, Val MAE: 1.292816162109375\n",
      "Epoch 453/2000, Train Loss: 8.33657454737635, Val Loss: 5.976884138080719, Val MAE: 1.2920182943344116\n",
      "Epoch 454/2000, Train Loss: 8.3278868810249, Val Loss: 5.97134739741146, Val MAE: 1.2912615537643433\n",
      "Epoch 455/2000, Train Loss: 8.319182460991064, Val Loss: 5.965945158361851, Val MAE: 1.2905193567276\n",
      "Epoch 456/2000, Train Loss: 8.310680490574114, Val Loss: 5.960436100063023, Val MAE: 1.2897534370422363\n",
      "Epoch 457/2000, Train Loss: 8.301649609333639, Val Loss: 5.954490671118906, Val MAE: 1.2889273166656494\n",
      "Epoch 458/2000, Train Loss: 8.29296580566073, Val Loss: 5.949057797510345, Val MAE: 1.2881882190704346\n",
      "Epoch 459/2000, Train Loss: 8.284457586112893, Val Loss: 5.943676060317336, Val MAE: 1.2874583005905151\n",
      "Epoch 460/2000, Train Loss: 8.276192140177056, Val Loss: 5.93847473220111, Val MAE: 1.2867498397827148\n",
      "Epoch 461/2000, Train Loss: 8.267738111429033, Val Loss: 5.933110324541728, Val MAE: 1.2860198020935059\n",
      "Epoch 462/2000, Train Loss: 8.259202043649372, Val Loss: 5.927609946632439, Val MAE: 1.2852565050125122\n",
      "Epoch 463/2000, Train Loss: 8.250241141237446, Val Loss: 5.921703887240843, Val MAE: 1.2844431400299072\n",
      "Epoch 464/2000, Train Loss: 8.24083302545473, Val Loss: 5.91580511432913, Val MAE: 1.2835928201675415\n",
      "Epoch 465/2000, Train Loss: 8.232069436268947, Val Loss: 5.910565672935666, Val MAE: 1.2828725576400757\n",
      "Epoch 466/2000, Train Loss: 8.223675550416392, Val Loss: 5.904949374577484, Val MAE: 1.2820948362350464\n",
      "Epoch 467/2000, Train Loss: 8.215086874928378, Val Loss: 5.899597229996512, Val MAE: 1.2813676595687866\n",
      "Epoch 468/2000, Train Loss: 8.206587802191583, Val Loss: 5.8940920039944285, Val MAE: 1.2806085348129272\n",
      "Epoch 469/2000, Train Loss: 8.197999191730517, Val Loss: 5.8887258020137345, Val MAE: 1.2798908948898315\n",
      "Epoch 470/2000, Train Loss: 8.18937904740046, Val Loss: 5.883179886847198, Val MAE: 1.2791264057159424\n",
      "Epoch 471/2000, Train Loss: 8.180752386726938, Val Loss: 5.877766906933204, Val MAE: 1.2783888578414917\n",
      "Epoch 472/2000, Train Loss: 8.1719164439185, Val Loss: 5.872000517676005, Val MAE: 1.2776070833206177\n",
      "Epoch 473/2000, Train Loss: 8.162540956239061, Val Loss: 5.86609699066292, Val MAE: 1.27681303024292\n",
      "Epoch 474/2000, Train Loss: 8.153931190740671, Val Loss: 5.860597907221532, Val MAE: 1.2760528326034546\n",
      "Epoch 475/2000, Train Loss: 8.145102419274425, Val Loss: 5.855022371600609, Val MAE: 1.2753098011016846\n",
      "Epoch 476/2000, Train Loss: 8.136524106708592, Val Loss: 5.849689320978281, Val MAE: 1.2745866775512695\n",
      "Epoch 477/2000, Train Loss: 8.127903272059285, Val Loss: 5.844086587570003, Val MAE: 1.2738358974456787\n",
      "Epoch 478/2000, Train Loss: 8.11927356660459, Val Loss: 5.838762772110131, Val MAE: 1.2731093168258667\n",
      "Epoch 479/2000, Train Loss: 8.110557338934793, Val Loss: 5.832992609268104, Val MAE: 1.2723369598388672\n",
      "Epoch 480/2000, Train Loss: 8.101727527070809, Val Loss: 5.827523720814838, Val MAE: 1.271613597869873\n",
      "Epoch 481/2000, Train Loss: 8.093190702000944, Val Loss: 5.822043092186386, Val MAE: 1.2708944082260132\n",
      "Epoch 482/2000, Train Loss: 8.084557201672641, Val Loss: 5.816576523085435, Val MAE: 1.270193099975586\n",
      "Epoch 483/2000, Train Loss: 8.07592742640217, Val Loss: 5.811268707277539, Val MAE: 1.2694916725158691\n",
      "Epoch 484/2000, Train Loss: 8.06725870391322, Val Loss: 5.805543665097909, Val MAE: 1.2687259912490845\n",
      "Epoch 485/2000, Train Loss: 8.057992584061884, Val Loss: 5.799715718660537, Val MAE: 1.2679593563079834\n",
      "Epoch 486/2000, Train Loss: 8.049116661321726, Val Loss: 5.794206693372479, Val MAE: 1.267242670059204\n",
      "Epoch 487/2000, Train Loss: 8.040143805295926, Val Loss: 5.788462572846864, Val MAE: 1.2664844989776611\n",
      "Epoch 488/2000, Train Loss: 8.031414589904214, Val Loss: 5.783152622830223, Val MAE: 1.2657790184020996\n",
      "Epoch 489/2000, Train Loss: 8.022773821886146, Val Loss: 5.77768378875277, Val MAE: 1.2650724649429321\n",
      "Epoch 490/2000, Train Loss: 8.014209317900647, Val Loss: 5.7721596609499, Val MAE: 1.2643251419067383\n",
      "Epoch 491/2000, Train Loss: 8.005472800289041, Val Loss: 5.766621696767775, Val MAE: 1.2635828256607056\n",
      "Epoch 492/2000, Train Loss: 7.996726692187805, Val Loss: 5.761101176066173, Val MAE: 1.2628326416015625\n",
      "Epoch 493/2000, Train Loss: 7.987963615319519, Val Loss: 5.755603749633909, Val MAE: 1.2621053457260132\n",
      "Epoch 494/2000, Train Loss: 7.979385314186724, Val Loss: 5.750079971968054, Val MAE: 1.2613554000854492\n",
      "Epoch 495/2000, Train Loss: 7.970718953985134, Val Loss: 5.744546597511381, Val MAE: 1.260608434677124\n",
      "Epoch 496/2000, Train Loss: 7.961911841115497, Val Loss: 5.739363637009451, Val MAE: 1.2599111795425415\n",
      "Epoch 497/2000, Train Loss: 7.953409011204045, Val Loss: 5.733814666212142, Val MAE: 1.2591516971588135\n",
      "Epoch 498/2000, Train Loss: 7.9447861459402604, Val Loss: 5.728429475668315, Val MAE: 1.2584151029586792\n",
      "Epoch 499/2000, Train Loss: 7.936108904770124, Val Loss: 5.7230936802118215, Val MAE: 1.2576886415481567\n",
      "Epoch 500/2000, Train Loss: 7.92763613613087, Val Loss: 5.717367216327169, Val MAE: 1.25688898563385\n",
      "Epoch 501/2000, Train Loss: 7.9190063967533675, Val Loss: 5.712320023405928, Val MAE: 1.2562110424041748\n",
      "Epoch 502/2000, Train Loss: 7.910514480424188, Val Loss: 5.706730499152128, Val MAE: 1.2554442882537842\n",
      "Epoch 503/2000, Train Loss: 7.901338008934176, Val Loss: 5.700879507156106, Val MAE: 1.2546511888504028\n",
      "Epoch 504/2000, Train Loss: 7.892310835827903, Val Loss: 5.695163199600873, Val MAE: 1.2538716793060303\n",
      "Epoch 505/2000, Train Loss: 7.883158733989816, Val Loss: 5.689478481896558, Val MAE: 1.253077507019043\n",
      "Epoch 506/2000, Train Loss: 7.874331234769777, Val Loss: 5.684015378020368, Val MAE: 1.2523274421691895\n",
      "Epoch 507/2000, Train Loss: 7.865337368943576, Val Loss: 5.678335604588459, Val MAE: 1.2515281438827515\n",
      "Epoch 508/2000, Train Loss: 7.856690543676129, Val Loss: 5.672856301284051, Val MAE: 1.2507588863372803\n",
      "Epoch 509/2000, Train Loss: 7.848341232156977, Val Loss: 5.667937614070671, Val MAE: 1.2501187324523926\n",
      "Epoch 510/2000, Train Loss: 7.83948930032993, Val Loss: 5.661913647368416, Val MAE: 1.249280571937561\n",
      "Epoch 511/2000, Train Loss: 7.830687814076866, Val Loss: 5.656721645982953, Val MAE: 1.2485758066177368\n",
      "Epoch 512/2000, Train Loss: 7.822255205996509, Val Loss: 5.6513111125174404, Val MAE: 1.2478289604187012\n",
      "Epoch 513/2000, Train Loss: 7.813458001929773, Val Loss: 5.645867328849193, Val MAE: 1.247098684310913\n",
      "Epoch 514/2000, Train Loss: 7.804700736508541, Val Loss: 5.640266150659, Val MAE: 1.246309518814087\n",
      "Epoch 515/2000, Train Loss: 7.795924022305588, Val Loss: 5.634890711367936, Val MAE: 1.2455649375915527\n",
      "Epoch 516/2000, Train Loss: 7.787274215711126, Val Loss: 5.62943717823238, Val MAE: 1.2448194026947021\n",
      "Epoch 517/2000, Train Loss: 7.778647760519186, Val Loss: 5.624158123732956, Val MAE: 1.244081974029541\n",
      "Epoch 518/2000, Train Loss: 7.769788533774627, Val Loss: 5.61856744541107, Val MAE: 1.2433022260665894\n",
      "Epoch 519/2000, Train Loss: 7.760842074842051, Val Loss: 5.612710575954066, Val MAE: 1.2424870729446411\n",
      "Epoch 520/2000, Train Loss: 7.7519532350965665, Val Loss: 5.6073934761857664, Val MAE: 1.2417879104614258\n",
      "Epoch 521/2000, Train Loss: 7.742636770806196, Val Loss: 5.601623064138599, Val MAE: 1.2409621477127075\n",
      "Epoch 522/2000, Train Loss: 7.734056868530844, Val Loss: 5.596324394871523, Val MAE: 1.2402323484420776\n",
      "Epoch 523/2000, Train Loss: 7.725384597659297, Val Loss: 5.590826979259381, Val MAE: 1.239488959312439\n",
      "Epoch 524/2000, Train Loss: 7.716359758897802, Val Loss: 5.584976874533537, Val MAE: 1.23866868019104\n",
      "Epoch 525/2000, Train Loss: 7.707487059300896, Val Loss: 5.57982387823296, Val MAE: 1.2379642724990845\n",
      "Epoch 526/2000, Train Loss: 7.699202869779048, Val Loss: 5.574468439387846, Val MAE: 1.2372255325317383\n",
      "Epoch 527/2000, Train Loss: 7.690883990568973, Val Loss: 5.569353227901297, Val MAE: 1.236504077911377\n",
      "Epoch 528/2000, Train Loss: 7.682053862235476, Val Loss: 5.563973132843102, Val MAE: 1.235779047012329\n",
      "Epoch 529/2000, Train Loss: 7.673595059247522, Val Loss: 5.558486068255461, Val MAE: 1.2350053787231445\n",
      "Epoch 530/2000, Train Loss: 7.665082931518555, Val Loss: 5.553270493246414, Val MAE: 1.234265923500061\n",
      "Epoch 531/2000, Train Loss: 7.656342925484962, Val Loss: 5.54768847501627, Val MAE: 1.2334747314453125\n",
      "Epoch 532/2000, Train Loss: 7.647755857283909, Val Loss: 5.5426049959686425, Val MAE: 1.2327747344970703\n",
      "Epoch 533/2000, Train Loss: 7.639333777048287, Val Loss: 5.53735336478766, Val MAE: 1.2320367097854614\n",
      "Epoch 534/2000, Train Loss: 7.630650139597568, Val Loss: 5.531731093627912, Val MAE: 1.2312737703323364\n",
      "Epoch 535/2000, Train Loss: 7.622065761345969, Val Loss: 5.526569392250196, Val MAE: 1.2305549383163452\n",
      "Epoch 536/2000, Train Loss: 7.61307845184472, Val Loss: 5.520898887783558, Val MAE: 1.2297301292419434\n",
      "Epoch 537/2000, Train Loss: 7.604469636626623, Val Loss: 5.515434107842209, Val MAE: 1.228947639465332\n",
      "Epoch 538/2000, Train Loss: 7.595926876931034, Val Loss: 5.510367295884334, Val MAE: 1.2282209396362305\n",
      "Epoch 539/2000, Train Loss: 7.58728997086214, Val Loss: 5.504770934125324, Val MAE: 1.227454423904419\n",
      "Epoch 540/2000, Train Loss: 7.57818763341621, Val Loss: 5.499133814971994, Val MAE: 1.2266716957092285\n",
      "Epoch 541/2000, Train Loss: 7.56959969911858, Val Loss: 5.494016366921835, Val MAE: 1.2259459495544434\n",
      "Epoch 542/2000, Train Loss: 7.561181452456577, Val Loss: 5.488711648917682, Val MAE: 1.2252171039581299\n",
      "Epoch 543/2000, Train Loss: 7.552793867243149, Val Loss: 5.483442439427515, Val MAE: 1.224448323249817\n",
      "Epoch 544/2000, Train Loss: 7.544428970395949, Val Loss: 5.478266824231492, Val MAE: 1.2237125635147095\n",
      "Epoch 545/2000, Train Loss: 7.5360907243678055, Val Loss: 5.473043448249768, Val MAE: 1.2229617834091187\n",
      "Epoch 546/2000, Train Loss: 7.527668725059901, Val Loss: 5.467844213974906, Val MAE: 1.2221907377243042\n",
      "Epoch 547/2000, Train Loss: 7.519258755417584, Val Loss: 5.46263591718969, Val MAE: 1.2214351892471313\n",
      "Epoch 548/2000, Train Loss: 7.510772392641922, Val Loss: 5.457268219810349, Val MAE: 1.220652461051941\n",
      "Epoch 549/2000, Train Loss: 7.501553514632346, Val Loss: 5.451658497039262, Val MAE: 1.219862937927246\n",
      "Epoch 550/2000, Train Loss: 7.4926276299027315, Val Loss: 5.446094315982348, Val MAE: 1.2190407514572144\n",
      "Epoch 551/2000, Train Loss: 7.484368468270101, Val Loss: 5.4411360438208325, Val MAE: 1.2183433771133423\n",
      "Epoch 552/2000, Train Loss: 7.4761229071783575, Val Loss: 5.43600960032897, Val MAE: 1.217613935470581\n",
      "Epoch 553/2000, Train Loss: 7.467920370295341, Val Loss: 5.430731460230576, Val MAE: 1.216850996017456\n",
      "Epoch 554/2000, Train Loss: 7.459281930766779, Val Loss: 5.425305915623904, Val MAE: 1.2160553932189941\n",
      "Epoch 555/2000, Train Loss: 7.450837056461995, Val Loss: 5.42025139462035, Val MAE: 1.215335726737976\n",
      "Epoch 556/2000, Train Loss: 7.442068208584361, Val Loss: 5.414935605422602, Val MAE: 1.2145699262619019\n",
      "Epoch 557/2000, Train Loss: 7.433782577514648, Val Loss: 5.4096936854626145, Val MAE: 1.2138391733169556\n",
      "Epoch 558/2000, Train Loss: 7.42511732263168, Val Loss: 5.404228468199034, Val MAE: 1.213011622428894\n",
      "Epoch 559/2000, Train Loss: 7.416688237658901, Val Loss: 5.39920470987757, Val MAE: 1.21229887008667\n",
      "Epoch 560/2000, Train Loss: 7.4084604831642, Val Loss: 5.394086290258277, Val MAE: 1.2115530967712402\n",
      "Epoch 561/2000, Train Loss: 7.400017209897361, Val Loss: 5.388507103215198, Val MAE: 1.2107313871383667\n",
      "Epoch 562/2000, Train Loss: 7.391453739633426, Val Loss: 5.383583984094429, Val MAE: 1.2100080251693726\n",
      "Epoch 563/2000, Train Loss: 7.383195549761077, Val Loss: 5.3786198492724075, Val MAE: 1.209304928779602\n",
      "Epoch 564/2000, Train Loss: 7.374662263159075, Val Loss: 5.3731536238647255, Val MAE: 1.2084910869598389\n",
      "Epoch 565/2000, Train Loss: 7.366124154154252, Val Loss: 5.367971326316799, Val MAE: 1.2077102661132812\n",
      "Epoch 566/2000, Train Loss: 7.357837589222258, Val Loss: 5.362846669447315, Val MAE: 1.206952452659607\n",
      "Epoch 567/2000, Train Loss: 7.34929916053406, Val Loss: 5.357436419822074, Val MAE: 1.2061375379562378\n",
      "Epoch 568/2000, Train Loss: 7.340841896634392, Val Loss: 5.352348116214748, Val MAE: 1.2053791284561157\n",
      "Epoch 569/2000, Train Loss: 7.332381970052972, Val Loss: 5.347207991490224, Val MAE: 1.2046109437942505\n",
      "Epoch 570/2000, Train Loss: 7.323273605377923, Val Loss: 5.341488853385588, Val MAE: 1.2037272453308105\n",
      "Epoch 571/2000, Train Loss: 7.314764244136126, Val Loss: 5.336356980205925, Val MAE: 1.2029509544372559\n",
      "Epoch 572/2000, Train Loss: 7.306536508610765, Val Loss: 5.331249123751312, Val MAE: 1.2021803855895996\n",
      "Epoch 573/2000, Train Loss: 7.298431948733962, Val Loss: 5.326165681808918, Val MAE: 1.2014086246490479\n",
      "Epoch 574/2000, Train Loss: 7.290311366784591, Val Loss: 5.321359323804175, Val MAE: 1.2007054090499878\n",
      "Epoch 575/2000, Train Loss: 7.282276312758138, Val Loss: 5.316422659098296, Val MAE: 1.2000035047531128\n",
      "Epoch 576/2000, Train Loss: 7.273899258368659, Val Loss: 5.311083711482383, Val MAE: 1.1992260217666626\n",
      "Epoch 577/2000, Train Loss: 7.265571069968327, Val Loss: 5.306062943481647, Val MAE: 1.1984978914260864\n",
      "Epoch 578/2000, Train Loss: 7.257366849972565, Val Loss: 5.300990454261904, Val MAE: 1.197752594947815\n",
      "Epoch 579/2000, Train Loss: 7.249160533390254, Val Loss: 5.29576369196445, Val MAE: 1.1969863176345825\n",
      "Epoch 580/2000, Train Loss: 7.240761667025071, Val Loss: 5.29066947469706, Val MAE: 1.1962486505508423\n",
      "Epoch 581/2000, Train Loss: 7.232352848543949, Val Loss: 5.285829393545518, Val MAE: 1.195528507232666\n",
      "Epoch 582/2000, Train Loss: 7.224100741675463, Val Loss: 5.280524935016224, Val MAE: 1.1947572231292725\n",
      "Epoch 583/2000, Train Loss: 7.215884514838782, Val Loss: 5.27534080588737, Val MAE: 1.1940261125564575\n",
      "Epoch 584/2000, Train Loss: 7.207503375322696, Val Loss: 5.2704019087727545, Val MAE: 1.1933448314666748\n",
      "Epoch 585/2000, Train Loss: 7.198920405413171, Val Loss: 5.265086630678123, Val MAE: 1.1925889253616333\n",
      "Epoch 586/2000, Train Loss: 7.190628831918452, Val Loss: 5.26008335210987, Val MAE: 1.1918874979019165\n",
      "Epoch 587/2000, Train Loss: 7.182562587227725, Val Loss: 5.255329001748615, Val MAE: 1.1912128925323486\n",
      "Epoch 588/2000, Train Loss: 7.174635367302367, Val Loss: 5.250052981857244, Val MAE: 1.1905031204223633\n",
      "Epoch 589/2000, Train Loss: 7.1662588878280475, Val Loss: 5.245225928649978, Val MAE: 1.1898510456085205\n",
      "Epoch 590/2000, Train Loss: 7.157987244322594, Val Loss: 5.239839195903088, Val MAE: 1.1890780925750732\n",
      "Epoch 591/2000, Train Loss: 7.149686535137492, Val Loss: 5.235042241591591, Val MAE: 1.1884225606918335\n",
      "Epoch 592/2000, Train Loss: 7.141298289529909, Val Loss: 5.229894502997936, Val MAE: 1.1876838207244873\n",
      "Epoch 593/2000, Train Loss: 7.133084801541476, Val Loss: 5.2248795977651, Val MAE: 1.1869726181030273\n",
      "Epoch 594/2000, Train Loss: 7.124547319756981, Val Loss: 5.219573974750332, Val MAE: 1.186226487159729\n",
      "Epoch 595/2000, Train Loss: 7.116363944947812, Val Loss: 5.214523784366546, Val MAE: 1.1855502128601074\n",
      "Epoch 596/2000, Train Loss: 7.1082408344896155, Val Loss: 5.209698596321516, Val MAE: 1.1849919557571411\n",
      "Epoch 597/2000, Train Loss: 7.100196059706058, Val Loss: 5.204787898090508, Val MAE: 1.1843316555023193\n",
      "Epoch 598/2000, Train Loss: 7.091975522673632, Val Loss: 5.19980250954091, Val MAE: 1.1836615800857544\n",
      "Epoch 599/2000, Train Loss: 7.083770924536933, Val Loss: 5.194696063447643, Val MAE: 1.1829702854156494\n",
      "Epoch 600/2000, Train Loss: 7.07548985475683, Val Loss: 5.1896866931034635, Val MAE: 1.1822803020477295\n",
      "Epoch 601/2000, Train Loss: 7.06735567053469, Val Loss: 5.184839134871423, Val MAE: 1.1816214323043823\n",
      "Epoch 602/2000, Train Loss: 7.0593602175645636, Val Loss: 5.179794069124503, Val MAE: 1.1809483766555786\n",
      "Epoch 603/2000, Train Loss: 7.051143086830651, Val Loss: 5.174866403941367, Val MAE: 1.180268406867981\n",
      "Epoch 604/2000, Train Loss: 7.042344404271166, Val Loss: 5.169336319385885, Val MAE: 1.1795159578323364\n",
      "Epoch 605/2000, Train Loss: 7.033696404845685, Val Loss: 5.164376182666233, Val MAE: 1.178859829902649\n",
      "Epoch 606/2000, Train Loss: 7.025749489223343, Val Loss: 5.159347296700822, Val MAE: 1.1781635284423828\n",
      "Epoch 607/2000, Train Loss: 7.017207525077737, Val Loss: 5.154373239705691, Val MAE: 1.1775102615356445\n",
      "Epoch 608/2000, Train Loss: 7.00911143491867, Val Loss: 5.149019563090694, Val MAE: 1.1767628192901611\n",
      "Epoch 609/2000, Train Loss: 7.000942027629259, Val Loss: 5.144225359983272, Val MAE: 1.1761399507522583\n",
      "Epoch 610/2000, Train Loss: 6.993014381009964, Val Loss: 5.139420264516328, Val MAE: 1.1754963397979736\n",
      "Epoch 611/2000, Train Loss: 6.984970268332828, Val Loss: 5.134675314445216, Val MAE: 1.174883246421814\n",
      "Epoch 612/2000, Train Loss: 6.976923906181862, Val Loss: 5.129910557602977, Val MAE: 1.1742621660232544\n",
      "Epoch 613/2000, Train Loss: 6.969237871735404, Val Loss: 5.124766921392969, Val MAE: 1.1735520362854004\n",
      "Epoch 614/2000, Train Loss: 6.961235835474106, Val Loss: 5.120245897783352, Val MAE: 1.1729549169540405\n",
      "Epoch 615/2000, Train Loss: 6.953355500111342, Val Loss: 5.115536017407168, Val MAE: 1.1723390817642212\n",
      "Epoch 616/2000, Train Loss: 6.945581043659246, Val Loss: 5.110739967018903, Val MAE: 1.1716907024383545\n",
      "Epoch 617/2000, Train Loss: 6.937770260291613, Val Loss: 5.10607334037756, Val MAE: 1.1710840463638306\n",
      "Epoch 618/2000, Train Loss: 6.92963343886615, Val Loss: 5.100853199142594, Val MAE: 1.1703822612762451\n",
      "Epoch 619/2000, Train Loss: 6.921526212588115, Val Loss: 5.096192095911986, Val MAE: 1.1698075532913208\n",
      "Epoch 620/2000, Train Loss: 6.9134954856636, Val Loss: 5.091064356049305, Val MAE: 1.1691536903381348\n",
      "Epoch 621/2000, Train Loss: 6.905514280063314, Val Loss: 5.086393257441956, Val MAE: 1.1685490608215332\n",
      "Epoch 622/2000, Train Loss: 6.8973230164434165, Val Loss: 5.081529980488457, Val MAE: 1.1679238080978394\n",
      "Epoch 623/2000, Train Loss: 6.889484106535426, Val Loss: 5.0767759045065795, Val MAE: 1.1673181056976318\n",
      "Epoch 624/2000, Train Loss: 6.881733595534532, Val Loss: 5.0721179481327265, Val MAE: 1.166745662689209\n",
      "Epoch 625/2000, Train Loss: 6.874159127613312, Val Loss: 5.067237153414402, Val MAE: 1.1661341190338135\n",
      "Epoch 626/2000, Train Loss: 6.866180473947302, Val Loss: 5.062666763244448, Val MAE: 1.165584683418274\n",
      "Epoch 627/2000, Train Loss: 6.858478331528662, Val Loss: 5.057822177163115, Val MAE: 1.1649892330169678\n",
      "Epoch 628/2000, Train Loss: 6.850509482873211, Val Loss: 5.053334421258386, Val MAE: 1.1645047664642334\n",
      "Epoch 629/2000, Train Loss: 6.842746739082515, Val Loss: 5.0482425837030815, Val MAE: 1.163912057876587\n",
      "Epoch 630/2000, Train Loss: 6.834682556350865, Val Loss: 5.0434872583259605, Val MAE: 1.1633599996566772\n",
      "Epoch 631/2000, Train Loss: 6.8269006835875015, Val Loss: 5.0390676714043625, Val MAE: 1.1628613471984863\n",
      "Epoch 632/2000, Train Loss: 6.8192993021234525, Val Loss: 5.034323578280908, Val MAE: 1.162293791770935\n",
      "Epoch 633/2000, Train Loss: 6.810976355756501, Val Loss: 5.029330734586393, Val MAE: 1.161699891090393\n",
      "Epoch 634/2000, Train Loss: 6.803455693487444, Val Loss: 5.0245566321620805, Val MAE: 1.1612569093704224\n",
      "Epoch 635/2000, Train Loss: 6.7955310287974, Val Loss: 5.019958922515313, Val MAE: 1.160754919052124\n",
      "Epoch 636/2000, Train Loss: 6.7878650443602275, Val Loss: 5.015593154594168, Val MAE: 1.160274624824524\n",
      "Epoch 637/2000, Train Loss: 6.780820661250577, Val Loss: 5.011357846097635, Val MAE: 1.1598098278045654\n",
      "Epoch 638/2000, Train Loss: 6.773580614401844, Val Loss: 5.006653352128762, Val MAE: 1.1592804193496704\n",
      "Epoch 639/2000, Train Loss: 6.76602384518954, Val Loss: 5.002374167783975, Val MAE: 1.1588134765625\n",
      "Epoch 640/2000, Train Loss: 6.758717303343013, Val Loss: 4.9978289569619, Val MAE: 1.1583054065704346\n",
      "Epoch 641/2000, Train Loss: 6.75095072402597, Val Loss: 4.993010459269758, Val MAE: 1.1578551530838013\n",
      "Epoch 642/2000, Train Loss: 6.743153049867722, Val Loss: 4.98844936035305, Val MAE: 1.1573487520217896\n",
      "Epoch 643/2000, Train Loss: 6.735363946666769, Val Loss: 4.983784563121226, Val MAE: 1.1568171977996826\n",
      "Epoch 644/2000, Train Loss: 6.727941718190769, Val Loss: 4.979503112353451, Val MAE: 1.156347632408142\n",
      "Epoch 645/2000, Train Loss: 6.720755364333225, Val Loss: 4.975090984580619, Val MAE: 1.155860424041748\n",
      "Epoch 646/2000, Train Loss: 6.713042132661049, Val Loss: 4.970553809387592, Val MAE: 1.1553411483764648\n",
      "Epoch 647/2000, Train Loss: 6.705816047565428, Val Loss: 4.9662312863612765, Val MAE: 1.154870867729187\n",
      "Epoch 648/2000, Train Loss: 6.698650238108524, Val Loss: 4.961882379385936, Val MAE: 1.1543910503387451\n",
      "Epoch 649/2000, Train Loss: 6.691740019645036, Val Loss: 4.958027621868763, Val MAE: 1.1539984941482544\n",
      "Epoch 650/2000, Train Loss: 6.685017587881472, Val Loss: 4.953767214463772, Val MAE: 1.1535743474960327\n",
      "Epoch 651/2000, Train Loss: 6.677733244362771, Val Loss: 4.949624702542483, Val MAE: 1.1531662940979004\n",
      "Epoch 652/2000, Train Loss: 6.670687317569245, Val Loss: 4.945163751913754, Val MAE: 1.1527011394500732\n",
      "Epoch 653/2000, Train Loss: 6.663402974971697, Val Loss: 4.940955144677076, Val MAE: 1.1522836685180664\n",
      "Epoch 654/2000, Train Loss: 6.656235364595553, Val Loss: 4.936762256376647, Val MAE: 1.1518645286560059\n",
      "Epoch 655/2000, Train Loss: 6.648839115724549, Val Loss: 4.931915951547054, Val MAE: 1.1513584852218628\n",
      "Epoch 656/2000, Train Loss: 6.6413975419380735, Val Loss: 4.927827548618252, Val MAE: 1.1509549617767334\n",
      "Epoch 657/2000, Train Loss: 6.63436042010354, Val Loss: 4.923712621803756, Val MAE: 1.1505645513534546\n",
      "Epoch 658/2000, Train Loss: 6.62734583350314, Val Loss: 4.919435901691516, Val MAE: 1.150134563446045\n",
      "Epoch 659/2000, Train Loss: 6.620133375600794, Val Loss: 4.9150809499493855, Val MAE: 1.1497178077697754\n",
      "Epoch 660/2000, Train Loss: 6.612783966774874, Val Loss: 4.910899659853664, Val MAE: 1.1493052244186401\n",
      "Epoch 661/2000, Train Loss: 6.605767712578201, Val Loss: 4.906654804485204, Val MAE: 1.1488982439041138\n",
      "Epoch 662/2000, Train Loss: 6.598848805968416, Val Loss: 4.9025076213150145, Val MAE: 1.1484949588775635\n",
      "Epoch 663/2000, Train Loss: 6.59172488672313, Val Loss: 4.898534929399958, Val MAE: 1.148129940032959\n",
      "Epoch 664/2000, Train Loss: 6.584552361701096, Val Loss: 4.893869761891059, Val MAE: 1.1476887464523315\n",
      "Epoch 665/2000, Train Loss: 6.57729607634537, Val Loss: 4.889645970367768, Val MAE: 1.1473149061203003\n",
      "Epoch 666/2000, Train Loss: 6.5702292919158936, Val Loss: 4.885737263810661, Val MAE: 1.1469911336898804\n",
      "Epoch 667/2000, Train Loss: 6.5633983522793065, Val Loss: 4.881606800760235, Val MAE: 1.1466714143753052\n",
      "Epoch 668/2000, Train Loss: 6.556488018326008, Val Loss: 4.877508591058421, Val MAE: 1.146400809288025\n",
      "Epoch 669/2000, Train Loss: 6.549646267838858, Val Loss: 4.8734611232303555, Val MAE: 1.1460697650909424\n",
      "Epoch 670/2000, Train Loss: 6.542624860502164, Val Loss: 4.869242208175831, Val MAE: 1.1457278728485107\n",
      "Epoch 671/2000, Train Loss: 6.535636050838762, Val Loss: 4.865209137994024, Val MAE: 1.145422339439392\n",
      "Epoch 672/2000, Train Loss: 6.528362932099195, Val Loss: 4.860757622475157, Val MAE: 1.145056962966919\n",
      "Epoch 673/2000, Train Loss: 6.521269454389764, Val Loss: 4.856847777207558, Val MAE: 1.1447539329528809\n",
      "Epoch 674/2000, Train Loss: 6.514452451383826, Val Loss: 4.852788580262715, Val MAE: 1.1444129943847656\n",
      "Epoch 675/2000, Train Loss: 6.507829489904149, Val Loss: 4.848599267885223, Val MAE: 1.144071102142334\n",
      "Epoch 676/2000, Train Loss: 6.500856260427633, Val Loss: 4.844820324499328, Val MAE: 1.1437894105911255\n",
      "Epoch 677/2000, Train Loss: 6.494184749851849, Val Loss: 4.840733123486777, Val MAE: 1.143495798110962\n",
      "Epoch 678/2000, Train Loss: 6.487379708267783, Val Loss: 4.836785479490687, Val MAE: 1.1432418823242188\n",
      "Epoch 679/2000, Train Loss: 6.48061555261359, Val Loss: 4.832856813892051, Val MAE: 1.1429671049118042\n",
      "Epoch 680/2000, Train Loss: 6.4738536909478315, Val Loss: 4.828846911555743, Val MAE: 1.142699956893921\n",
      "Epoch 681/2000, Train Loss: 6.467209739729692, Val Loss: 4.824706579892485, Val MAE: 1.1424113512039185\n",
      "Epoch 682/2000, Train Loss: 6.460656360582145, Val Loss: 4.821286240941635, Val MAE: 1.142238974571228\n",
      "Epoch 683/2000, Train Loss: 6.454173578607578, Val Loss: 4.817526518405827, Val MAE: 1.1420352458953857\n",
      "Epoch 684/2000, Train Loss: 6.447655501678097, Val Loss: 4.813554535739057, Val MAE: 1.1417967081069946\n",
      "Epoch 685/2000, Train Loss: 6.441143280118936, Val Loss: 4.8097183598054425, Val MAE: 1.1415953636169434\n",
      "Epoch 686/2000, Train Loss: 6.434569916103105, Val Loss: 4.805943253881357, Val MAE: 1.1414130926132202\n",
      "Epoch 687/2000, Train Loss: 6.428049081573248, Val Loss: 4.802155383753481, Val MAE: 1.1412218809127808\n",
      "Epoch 688/2000, Train Loss: 6.421599682472797, Val Loss: 4.798161078314926, Val MAE: 1.1410531997680664\n",
      "Epoch 689/2000, Train Loss: 6.414937066585523, Val Loss: 4.794458591421177, Val MAE: 1.140871524810791\n",
      "Epoch 690/2000, Train Loss: 6.40836148496351, Val Loss: 4.790584734787006, Val MAE: 1.1406879425048828\n",
      "Epoch 691/2000, Train Loss: 6.402023878342991, Val Loss: 4.786979301179851, Val MAE: 1.140546202659607\n",
      "Epoch 692/2000, Train Loss: 6.395648920631789, Val Loss: 4.783246113017604, Val MAE: 1.140366792678833\n",
      "Epoch 693/2000, Train Loss: 6.389251544555088, Val Loss: 4.779573681490781, Val MAE: 1.1401914358139038\n",
      "Epoch 694/2000, Train Loss: 6.382938821971498, Val Loss: 4.775800237945608, Val MAE: 1.1400338411331177\n",
      "Epoch 695/2000, Train Loss: 6.376589393780774, Val Loss: 4.7720633072023455, Val MAE: 1.1398625373840332\n",
      "Epoch 696/2000, Train Loss: 6.370201855293488, Val Loss: 4.768455122907956, Val MAE: 1.1397022008895874\n",
      "Epoch 697/2000, Train Loss: 6.363683723250343, Val Loss: 4.7645697976252785, Val MAE: 1.1395180225372314\n",
      "Epoch 698/2000, Train Loss: 6.357400468656686, Val Loss: 4.7607977117537645, Val MAE: 1.1393967866897583\n",
      "Epoch 699/2000, Train Loss: 6.3503880374331185, Val Loss: 4.756983204340344, Val MAE: 1.1392799615859985\n",
      "Epoch 700/2000, Train Loss: 6.343769518335971, Val Loss: 4.753175527659488, Val MAE: 1.1391210556030273\n",
      "Epoch 701/2000, Train Loss: 6.33860566761118, Val Loss: 4.750312489240959, Val MAE: 1.139068603515625\n",
      "Epoch 702/2000, Train Loss: 6.33302675692042, Val Loss: 4.746756652591599, Val MAE: 1.138973593711853\n",
      "Epoch 703/2000, Train Loss: 6.326678298378884, Val Loss: 4.743228261441261, Val MAE: 1.1388721466064453\n",
      "Epoch 704/2000, Train Loss: 6.320419781572547, Val Loss: 4.739244381731024, Val MAE: 1.1387664079666138\n",
      "Epoch 705/2000, Train Loss: 6.3143669223636625, Val Loss: 4.736094158042121, Val MAE: 1.1386998891830444\n",
      "Epoch 706/2000, Train Loss: 6.3095847676681025, Val Loss: 4.73361383559736, Val MAE: 1.1386901140213013\n",
      "Epoch 707/2000, Train Loss: 6.304370568062698, Val Loss: 4.7303752896954885, Val MAE: 1.1386150121688843\n",
      "Epoch 708/2000, Train Loss: 6.299068929624632, Val Loss: 4.727524935001054, Val MAE: 1.1385616064071655\n",
      "Epoch 709/2000, Train Loss: 6.293772077062944, Val Loss: 4.724144038004247, Val MAE: 1.1384567022323608\n",
      "Epoch 710/2000, Train Loss: 6.288127411900788, Val Loss: 4.7209761583892345, Val MAE: 1.1383975744247437\n",
      "Epoch 711/2000, Train Loss: 6.28259503699595, Val Loss: 4.717750659991089, Val MAE: 1.138329029083252\n",
      "Epoch 712/2000, Train Loss: 6.277041102999272, Val Loss: 4.714545161857664, Val MAE: 1.1382567882537842\n",
      "Epoch 713/2000, Train Loss: 6.271486980375629, Val Loss: 4.7114031163623205, Val MAE: 1.1382094621658325\n",
      "Epoch 714/2000, Train Loss: 6.266063731843707, Val Loss: 4.708039786716974, Val MAE: 1.1381347179412842\n",
      "Epoch 715/2000, Train Loss: 6.260110760558042, Val Loss: 4.704644146920727, Val MAE: 1.138075828552246\n",
      "Epoch 716/2000, Train Loss: 6.254484828288395, Val Loss: 4.701583202498722, Val MAE: 1.1380311250686646\n",
      "Epoch 717/2000, Train Loss: 6.248524955951852, Val Loss: 4.6980372497560206, Val MAE: 1.1379451751708984\n",
      "Epoch 718/2000, Train Loss: 6.242868284725362, Val Loss: 4.694626482317711, Val MAE: 1.1378825902938843\n",
      "Epoch 719/2000, Train Loss: 6.237322485484125, Val Loss: 4.6915583676683745, Val MAE: 1.1378710269927979\n",
      "Epoch 720/2000, Train Loss: 6.231741032111292, Val Loss: 4.688527778713955, Val MAE: 1.1378238201141357\n",
      "Epoch 721/2000, Train Loss: 6.226376006458182, Val Loss: 4.685207217290729, Val MAE: 1.137755274772644\n",
      "Epoch 722/2000, Train Loss: 6.220783014828267, Val Loss: 4.682240615573686, Val MAE: 1.137734055519104\n",
      "Epoch 723/2000, Train Loss: 6.215596189687549, Val Loss: 4.678940950364277, Val MAE: 1.1376906633377075\n",
      "Epoch 724/2000, Train Loss: 6.210007342624706, Val Loss: 4.676009719003294, Val MAE: 1.137673258781433\n",
      "Epoch 725/2000, Train Loss: 6.204718690580585, Val Loss: 4.672910653866895, Val MAE: 1.1376396417617798\n",
      "Epoch 726/2000, Train Loss: 6.199076933998399, Val Loss: 4.669587816925602, Val MAE: 1.1375868320465088\n",
      "Epoch 727/2000, Train Loss: 6.193802001881711, Val Loss: 4.666643589543733, Val MAE: 1.1375634670257568\n",
      "Epoch 728/2000, Train Loss: 6.188225168147809, Val Loss: 4.663352796301112, Val MAE: 1.1375285387039185\n",
      "Epoch 729/2000, Train Loss: 6.182543360870826, Val Loss: 4.660265743651905, Val MAE: 1.137513518333435\n",
      "Epoch 730/2000, Train Loss: 6.1772703604653545, Val Loss: 4.657293466696734, Val MAE: 1.1375160217285156\n",
      "Epoch 731/2000, Train Loss: 6.171840147183578, Val Loss: 4.654250700248254, Val MAE: 1.137498378753662\n",
      "Epoch 732/2000, Train Loss: 6.166704731314863, Val Loss: 4.651367524531376, Val MAE: 1.137494683265686\n",
      "Epoch 733/2000, Train Loss: 6.161818876281357, Val Loss: 4.648512859152513, Val MAE: 1.137516975402832\n",
      "Epoch 734/2000, Train Loss: 6.157262709334655, Val Loss: 4.646068789400496, Val MAE: 1.1375914812088013\n",
      "Epoch 735/2000, Train Loss: 6.152446265339665, Val Loss: 4.643161458176401, Val MAE: 1.1376398801803589\n",
      "Epoch 736/2000, Train Loss: 6.147230753846548, Val Loss: 4.640412470795684, Val MAE: 1.1376938819885254\n",
      "Epoch 737/2000, Train Loss: 6.1418990000919695, Val Loss: 4.637175213689874, Val MAE: 1.1377570629119873\n",
      "Epoch 738/2000, Train Loss: 6.13683609622596, Val Loss: 4.634465313884052, Val MAE: 1.1378288269042969\n",
      "Epoch 739/2000, Train Loss: 6.131926423115961, Val Loss: 4.6316940819592896, Val MAE: 1.13792884349823\n",
      "Epoch 740/2000, Train Loss: 6.126914305917558, Val Loss: 4.62884340085559, Val MAE: 1.138044834136963\n",
      "Epoch 741/2000, Train Loss: 6.122194027565943, Val Loss: 4.626065834229057, Val MAE: 1.1381311416625977\n",
      "Epoch 742/2000, Train Loss: 6.117140616715977, Val Loss: 4.6234710556933205, Val MAE: 1.1382720470428467\n",
      "Epoch 743/2000, Train Loss: 6.112189419927909, Val Loss: 4.620662136120839, Val MAE: 1.1383835077285767\n",
      "Epoch 744/2000, Train Loss: 6.107395371576181, Val Loss: 4.617866126072031, Val MAE: 1.1384776830673218\n",
      "Epoch 745/2000, Train Loss: 6.102409782350156, Val Loss: 4.61520053197269, Val MAE: 1.138596534729004\n",
      "Epoch 746/2000, Train Loss: 6.096895516941588, Val Loss: 4.611938765829613, Val MAE: 1.1387012004852295\n",
      "Epoch 747/2000, Train Loss: 6.0919911501792585, Val Loss: 4.609306333422124, Val MAE: 1.1388492584228516\n",
      "Epoch 748/2000, Train Loss: 6.087250460096157, Val Loss: 4.60664621031432, Val MAE: 1.138945460319519\n",
      "Epoch 749/2000, Train Loss: 6.082586217782055, Val Loss: 4.604037809774682, Val MAE: 1.139086127281189\n",
      "Epoch 750/2000, Train Loss: 6.077824819943462, Val Loss: 4.601337488713834, Val MAE: 1.1392252445220947\n",
      "Epoch 751/2000, Train Loss: 6.073151104535774, Val Loss: 4.598782214448527, Val MAE: 1.1393613815307617\n",
      "Epoch 752/2000, Train Loss: 6.068506926716583, Val Loss: 4.596183748442579, Val MAE: 1.1394898891448975\n",
      "Epoch 753/2000, Train Loss: 6.063771581129053, Val Loss: 4.593516220061763, Val MAE: 1.1396386623382568\n",
      "Epoch 754/2000, Train Loss: 6.058996658499081, Val Loss: 4.591101000800326, Val MAE: 1.1397578716278076\n",
      "Epoch 755/2000, Train Loss: 6.054531594613413, Val Loss: 4.588422270055424, Val MAE: 1.139873743057251\n",
      "Epoch 756/2000, Train Loss: 6.0498894880788745, Val Loss: 4.585902739349787, Val MAE: 1.1399983167648315\n",
      "Epoch 757/2000, Train Loss: 6.045300466501025, Val Loss: 4.583359402602723, Val MAE: 1.1401339769363403\n",
      "Epoch 758/2000, Train Loss: 6.04075860605225, Val Loss: 4.580883718771977, Val MAE: 1.14026939868927\n",
      "Epoch 759/2000, Train Loss: 6.035790614516427, Val Loss: 4.578084303076203, Val MAE: 1.1404087543487549\n",
      "Epoch 760/2000, Train Loss: 6.030561195632702, Val Loss: 4.575331650610577, Val MAE: 1.1405922174453735\n",
      "Epoch 761/2000, Train Loss: 6.026055365382416, Val Loss: 4.572828355904769, Val MAE: 1.140761375427246\n",
      "Epoch 762/2000, Train Loss: 6.021689075017682, Val Loss: 4.570435939121756, Val MAE: 1.1409294605255127\n",
      "Epoch 763/2000, Train Loss: 6.017388330048965, Val Loss: 4.568081657769712, Val MAE: 1.141098141670227\n",
      "Epoch 764/2000, Train Loss: 6.0132377453601675, Val Loss: 4.565862819411465, Val MAE: 1.1412655115127563\n",
      "Epoch 765/2000, Train Loss: 6.009050443652267, Val Loss: 4.563380060589931, Val MAE: 1.1414330005645752\n",
      "Epoch 766/2000, Train Loss: 6.004244512690931, Val Loss: 4.560744532694419, Val MAE: 1.1416295766830444\n",
      "Epoch 767/2000, Train Loss: 5.999843446401278, Val Loss: 4.558493122606127, Val MAE: 1.141794204711914\n",
      "Epoch 768/2000, Train Loss: 5.995622552129296, Val Loss: 4.556183916565265, Val MAE: 1.1420273780822754\n",
      "Epoch 769/2000, Train Loss: 5.991594329453109, Val Loss: 4.553936414175608, Val MAE: 1.1422287225723267\n",
      "Epoch 770/2000, Train Loss: 5.98741887251784, Val Loss: 4.5516873508121245, Val MAE: 1.1424366235733032\n",
      "Epoch 771/2000, Train Loss: 5.98286936993532, Val Loss: 4.549397838397606, Val MAE: 1.1426392793655396\n",
      "Epoch 772/2000, Train Loss: 5.979654886002845, Val Loss: 4.547701902187488, Val MAE: 1.1428196430206299\n",
      "Epoch 773/2000, Train Loss: 5.975907145861345, Val Loss: 4.545555684168463, Val MAE: 1.1430206298828125\n",
      "Epoch 774/2000, Train Loss: 5.971948578884002, Val Loss: 4.5433880646687905, Val MAE: 1.1432228088378906\n",
      "Epoch 775/2000, Train Loss: 5.967979557544877, Val Loss: 4.541298005246633, Val MAE: 1.1434276103973389\n",
      "Epoch 776/2000, Train Loss: 5.9639764479281565, Val Loss: 4.539113996181268, Val MAE: 1.1436197757720947\n",
      "Epoch 777/2000, Train Loss: 5.959314309862586, Val Loss: 4.536600663479384, Val MAE: 1.143851399421692\n",
      "Epoch 778/2000, Train Loss: 5.955531591279272, Val Loss: 4.534701123437634, Val MAE: 1.1440629959106445\n",
      "Epoch 779/2000, Train Loss: 5.951868348300178, Val Loss: 4.532718053762172, Val MAE: 1.1442639827728271\n",
      "Epoch 780/2000, Train Loss: 5.9481996351768744, Val Loss: 4.530789525726357, Val MAE: 1.144465684890747\n",
      "Epoch 781/2000, Train Loss: 5.944195571816098, Val Loss: 4.528538995132119, Val MAE: 1.1446696519851685\n",
      "Epoch 782/2000, Train Loss: 5.940227255573883, Val Loss: 4.526419079589012, Val MAE: 1.1449027061462402\n",
      "Epoch 783/2000, Train Loss: 5.936598321019022, Val Loss: 4.524425720074424, Val MAE: 1.1451151371002197\n",
      "Epoch 784/2000, Train Loss: 5.932812221987571, Val Loss: 4.522520041358364, Val MAE: 1.1453166007995605\n",
      "Epoch 785/2000, Train Loss: 5.929279710240255, Val Loss: 4.520549455320311, Val MAE: 1.1455267667770386\n",
      "Epoch 786/2000, Train Loss: 5.925686273113614, Val Loss: 4.518565699458122, Val MAE: 1.1457412242889404\n",
      "Epoch 787/2000, Train Loss: 5.922224043916429, Val Loss: 4.516886477603636, Val MAE: 1.145939588546753\n",
      "Epoch 788/2000, Train Loss: 5.918778995269919, Val Loss: 4.514960422635347, Val MAE: 1.1461482048034668\n",
      "Epoch 789/2000, Train Loss: 5.914937712838394, Val Loss: 4.512970210105046, Val MAE: 1.1463590860366821\n",
      "Epoch 790/2000, Train Loss: 5.911300938698505, Val Loss: 4.511094377770483, Val MAE: 1.1465739011764526\n",
      "Epoch 791/2000, Train Loss: 5.907523659573703, Val Loss: 4.509057161248885, Val MAE: 1.1467849016189575\n",
      "Epoch 792/2000, Train Loss: 5.904054655113756, Val Loss: 4.507177980328063, Val MAE: 1.1469990015029907\n",
      "Epoch 793/2000, Train Loss: 5.900738943757579, Val Loss: 4.5054220983859254, Val MAE: 1.147230863571167\n",
      "Epoch 794/2000, Train Loss: 5.896737278716612, Val Loss: 4.50343780183141, Val MAE: 1.1474616527557373\n",
      "Epoch 795/2000, Train Loss: 5.89335329870166, Val Loss: 4.501721544795342, Val MAE: 1.1476720571517944\n",
      "Epoch 796/2000, Train Loss: 5.889980429815428, Val Loss: 4.4998570759117875, Val MAE: 1.1479010581970215\n",
      "Epoch 797/2000, Train Loss: 5.88650677654189, Val Loss: 4.498064926192836, Val MAE: 1.1481409072875977\n",
      "Epoch 798/2000, Train Loss: 5.883125045858382, Val Loss: 4.496279876549972, Val MAE: 1.1483533382415771\n",
      "Epoch 799/2000, Train Loss: 5.879759180082035, Val Loss: 4.494537554922942, Val MAE: 1.1485700607299805\n",
      "Epoch 800/2000, Train Loss: 5.876361170721164, Val Loss: 4.492796007584076, Val MAE: 1.1488159894943237\n",
      "Epoch 801/2000, Train Loss: 5.8729871243656895, Val Loss: 4.4911187455477615, Val MAE: 1.1490267515182495\n",
      "Epoch 802/2000, Train Loss: 5.869790177402928, Val Loss: 4.4893824372776185, Val MAE: 1.1492537260055542\n",
      "Epoch 803/2000, Train Loss: 5.866383454775476, Val Loss: 4.487779843191373, Val MAE: 1.1494739055633545\n",
      "Epoch 804/2000, Train Loss: 5.863221226541933, Val Loss: 4.485915817338806, Val MAE: 1.1497019529342651\n",
      "Epoch 805/2000, Train Loss: 5.859279066091767, Val Loss: 4.483900098739242, Val MAE: 1.149954080581665\n",
      "Epoch 806/2000, Train Loss: 5.856073014709849, Val Loss: 4.4825501114549535, Val MAE: 1.1501703262329102\n",
      "Epoch 807/2000, Train Loss: 5.853023502272489, Val Loss: 4.480794209027075, Val MAE: 1.1504029035568237\n",
      "Epoch 808/2000, Train Loss: 5.849663497505247, Val Loss: 4.479112484651777, Val MAE: 1.1506553888320923\n",
      "Epoch 809/2000, Train Loss: 5.846559848316746, Val Loss: 4.47758718576148, Val MAE: 1.150898814201355\n",
      "Epoch 810/2000, Train Loss: 5.843491557235836, Val Loss: 4.475978088201032, Val MAE: 1.1511379480361938\n",
      "Epoch 811/2000, Train Loss: 5.840488722469431, Val Loss: 4.47436165794361, Val MAE: 1.1513769626617432\n",
      "Epoch 812/2000, Train Loss: 5.836907181534306, Val Loss: 4.472599555674504, Val MAE: 1.151633858680725\n",
      "Epoch 813/2000, Train Loss: 5.833858980961411, Val Loss: 4.471032614562962, Val MAE: 1.151878833770752\n",
      "Epoch 814/2000, Train Loss: 5.830771884568582, Val Loss: 4.46955639140865, Val MAE: 1.1521306037902832\n",
      "Epoch 815/2000, Train Loss: 5.827890984167733, Val Loss: 4.467976820426951, Val MAE: 1.152381181716919\n",
      "Epoch 816/2000, Train Loss: 5.825053174670512, Val Loss: 4.466712148356679, Val MAE: 1.1526132822036743\n",
      "Epoch 817/2000, Train Loss: 5.822389341181786, Val Loss: 4.465152165738379, Val MAE: 1.1528671979904175\n",
      "Epoch 818/2000, Train Loss: 5.819387260353696, Val Loss: 4.46388444788754, Val MAE: 1.1530827283859253\n",
      "Epoch 819/2000, Train Loss: 5.816636136095163, Val Loss: 4.4625020376213635, Val MAE: 1.15340256690979\n",
      "Epoch 820/2000, Train Loss: 5.813503730815491, Val Loss: 4.460756011216624, Val MAE: 1.1536706686019897\n",
      "Epoch 821/2000, Train Loss: 5.810475274827477, Val Loss: 4.459410401549492, Val MAE: 1.153899908065796\n",
      "Epoch 822/2000, Train Loss: 5.807675434000407, Val Loss: 4.4579536643533695, Val MAE: 1.1541416645050049\n",
      "Epoch 823/2000, Train Loss: 5.804881249500697, Val Loss: 4.456643383315689, Val MAE: 1.1543980836868286\n",
      "Epoch 824/2000, Train Loss: 5.802225083700787, Val Loss: 4.455154094403727, Val MAE: 1.1546472311019897\n",
      "Epoch 825/2000, Train Loss: 5.79934664883661, Val Loss: 4.453852128161973, Val MAE: 1.1548839807510376\n",
      "Epoch 826/2000, Train Loss: 5.796631639442652, Val Loss: 4.452510471180552, Val MAE: 1.1551367044448853\n",
      "Epoch 827/2000, Train Loss: 5.793943072629979, Val Loss: 4.451161342918001, Val MAE: 1.1553692817687988\n",
      "Epoch 828/2000, Train Loss: 5.7914216986359, Val Loss: 4.449972760679016, Val MAE: 1.1556167602539062\n",
      "Epoch 829/2000, Train Loss: 5.788838399739793, Val Loss: 4.4487202899463405, Val MAE: 1.155856966972351\n",
      "Epoch 830/2000, Train Loss: 5.786312945361442, Val Loss: 4.447311895023528, Val MAE: 1.1561102867126465\n",
      "Epoch 831/2000, Train Loss: 5.783339845222914, Val Loss: 4.446004365491975, Val MAE: 1.156383752822876\n",
      "Epoch 832/2000, Train Loss: 5.780901001403094, Val Loss: 4.4447289595749595, Val MAE: 1.1566588878631592\n",
      "Epoch 833/2000, Train Loss: 5.778245690832272, Val Loss: 4.443498566671199, Val MAE: 1.15695321559906\n",
      "Epoch 834/2000, Train Loss: 5.774954937131096, Val Loss: 4.441939289305728, Val MAE: 1.157285213470459\n",
      "Epoch 835/2000, Train Loss: 5.772352210445077, Val Loss: 4.440768157029608, Val MAE: 1.1575623750686646\n",
      "Epoch 836/2000, Train Loss: 5.770163159660542, Val Loss: 4.439728507060591, Val MAE: 1.1578333377838135\n",
      "Epoch 837/2000, Train Loss: 5.767369224947068, Val Loss: 4.438367592284942, Val MAE: 1.1582081317901611\n",
      "Epoch 838/2000, Train Loss: 5.7646861678910515, Val Loss: 4.437006653088506, Val MAE: 1.1585588455200195\n",
      "Epoch 839/2000, Train Loss: 5.76156698560194, Val Loss: 4.435592262760677, Val MAE: 1.1589035987854004\n",
      "Epoch 840/2000, Train Loss: 5.759277534152435, Val Loss: 4.434545233213083, Val MAE: 1.1592028141021729\n",
      "Epoch 841/2000, Train Loss: 5.757057015311886, Val Loss: 4.433511532158465, Val MAE: 1.159500241279602\n",
      "Epoch 842/2000, Train Loss: 5.754892562630815, Val Loss: 4.432379457235403, Val MAE: 1.1598360538482666\n",
      "Epoch 843/2000, Train Loss: 5.75258738336672, Val Loss: 4.431453073615426, Val MAE: 1.1601072549819946\n",
      "Epoch 844/2000, Train Loss: 5.750537533934886, Val Loss: 4.4302582149184095, Val MAE: 1.1604417562484741\n",
      "Epoch 845/2000, Train Loss: 5.748144958394924, Val Loss: 4.429261036341389, Val MAE: 1.1607505083084106\n",
      "Epoch 846/2000, Train Loss: 5.745773633677576, Val Loss: 4.428189380374644, Val MAE: 1.161048412322998\n",
      "Epoch 847/2000, Train Loss: 5.74362598008559, Val Loss: 4.427135555172624, Val MAE: 1.161379337310791\n",
      "Epoch 848/2000, Train Loss: 5.741236482879115, Val Loss: 4.426145493396902, Val MAE: 1.1617063283920288\n",
      "Epoch 849/2000, Train Loss: 5.738954064998538, Val Loss: 4.42482396992645, Val MAE: 1.1620763540267944\n",
      "Epoch 850/2000, Train Loss: 5.735799349042443, Val Loss: 4.423418833543589, Val MAE: 1.162480354309082\n",
      "Epoch 851/2000, Train Loss: 5.733274087080276, Val Loss: 4.422470823314612, Val MAE: 1.162788987159729\n",
      "Epoch 852/2000, Train Loss: 5.731337462498134, Val Loss: 4.421582949168242, Val MAE: 1.1630760431289673\n",
      "Epoch 853/2000, Train Loss: 5.729234620115128, Val Loss: 4.420530568108567, Val MAE: 1.163371205329895\n",
      "Epoch 854/2000, Train Loss: 5.727361249477369, Val Loss: 4.419711102427797, Val MAE: 1.1636483669281006\n",
      "Epoch 855/2000, Train Loss: 5.7254440414366226, Val Loss: 4.41886342697114, Val MAE: 1.1639384031295776\n",
      "Epoch 856/2000, Train Loss: 5.723679837868106, Val Loss: 4.41798724469368, Val MAE: 1.1642338037490845\n",
      "Epoch 857/2000, Train Loss: 5.721865591411472, Val Loss: 4.417262806550407, Val MAE: 1.1645095348358154\n",
      "Epoch 858/2000, Train Loss: 5.720137524716382, Val Loss: 4.416514696355339, Val MAE: 1.1647826433181763\n",
      "Epoch 859/2000, Train Loss: 5.718457230868024, Val Loss: 4.41568477965455, Val MAE: 1.1650806665420532\n",
      "Epoch 860/2000, Train Loss: 5.7166478162483925, Val Loss: 4.414802537334932, Val MAE: 1.1654081344604492\n",
      "Epoch 861/2000, Train Loss: 5.7147776977280405, Val Loss: 4.41399970550325, Val MAE: 1.1657110452651978\n",
      "Epoch 862/2000, Train Loss: 5.7129967773221315, Val Loss: 4.413163741840771, Val MAE: 1.1660418510437012\n",
      "Epoch 863/2000, Train Loss: 5.7111947778234615, Val Loss: 4.412355396297534, Val MAE: 1.166358470916748\n",
      "Epoch 864/2000, Train Loss: 5.709586119921457, Val Loss: 4.411728157589699, Val MAE: 1.1666384935379028\n",
      "Epoch 865/2000, Train Loss: 5.707941834900569, Val Loss: 4.410978679212968, Val MAE: 1.1669563055038452\n",
      "Epoch 866/2000, Train Loss: 5.706033819179267, Val Loss: 4.41004040601185, Val MAE: 1.167341947555542\n",
      "Epoch 867/2000, Train Loss: 5.7040575123660835, Val Loss: 4.409203621036969, Val MAE: 1.1676807403564453\n",
      "Epoch 868/2000, Train Loss: 5.7023530470878585, Val Loss: 4.40841361543516, Val MAE: 1.168000340461731\n",
      "Epoch 869/2000, Train Loss: 5.700531635386942, Val Loss: 4.407630870905747, Val MAE: 1.1683329343795776\n",
      "Epoch 870/2000, Train Loss: 5.698850507111334, Val Loss: 4.40685427645473, Val MAE: 1.1686736345291138\n",
      "Epoch 871/2000, Train Loss: 5.697143590592092, Val Loss: 4.406160564240706, Val MAE: 1.1689682006835938\n",
      "Epoch 872/2000, Train Loss: 5.6954982663837495, Val Loss: 4.405409631992246, Val MAE: 1.1692956686019897\n",
      "Epoch 873/2000, Train Loss: 5.693621730606265, Val Loss: 4.404595322964025, Val MAE: 1.1696434020996094\n",
      "Epoch 874/2000, Train Loss: 5.691991241041472, Val Loss: 4.403788569655169, Val MAE: 1.1699881553649902\n",
      "Epoch 875/2000, Train Loss: 5.690156858908405, Val Loss: 4.403164596226368, Val MAE: 1.1702967882156372\n",
      "Epoch 876/2000, Train Loss: 5.6887438248918505, Val Loss: 4.402457580295903, Val MAE: 1.1706087589263916\n",
      "Epoch 877/2000, Train Loss: 5.687337511601203, Val Loss: 4.401806664574254, Val MAE: 1.1709263324737549\n",
      "Epoch 878/2000, Train Loss: 5.685822615106467, Val Loss: 4.401250552691452, Val MAE: 1.171198844909668\n",
      "Epoch 879/2000, Train Loss: 5.684379043682317, Val Loss: 4.400598147377238, Val MAE: 1.1714932918548584\n",
      "Epoch 880/2000, Train Loss: 5.682796463904999, Val Loss: 4.399848261517515, Val MAE: 1.1718336343765259\n",
      "Epoch 881/2000, Train Loss: 5.68117306570925, Val Loss: 4.399178708517538, Val MAE: 1.172173261642456\n",
      "Epoch 882/2000, Train Loss: 5.679634751656126, Val Loss: 4.398544567482764, Val MAE: 1.1725273132324219\n",
      "Epoch 883/2000, Train Loss: 5.677869411042997, Val Loss: 4.397745381233593, Val MAE: 1.1729031801223755\n",
      "Epoch 884/2000, Train Loss: 5.676127601897289, Val Loss: 4.397018705357101, Val MAE: 1.1732417345046997\n",
      "Epoch 885/2000, Train Loss: 5.674024238787277, Val Loss: 4.39616772106404, Val MAE: 1.1736747026443481\n",
      "Epoch 886/2000, Train Loss: 5.672463350102608, Val Loss: 4.3955483295140905, Val MAE: 1.173992395401001\n",
      "Epoch 887/2000, Train Loss: 5.6708832620272585, Val Loss: 4.394889871633469, Val MAE: 1.174330234527588\n",
      "Epoch 888/2000, Train Loss: 5.669066986502054, Val Loss: 4.3942346502296825, Val MAE: 1.1747159957885742\n",
      "Epoch 889/2000, Train Loss: 5.667594641726028, Val Loss: 4.393666543569919, Val MAE: 1.1750127077102661\n",
      "Epoch 890/2000, Train Loss: 5.666278018239833, Val Loss: 4.393084905148895, Val MAE: 1.1753265857696533\n",
      "Epoch 891/2000, Train Loss: 5.664916267633066, Val Loss: 4.392473223630909, Val MAE: 1.1756373643875122\n",
      "Epoch 892/2000, Train Loss: 5.663514335888224, Val Loss: 4.391984972559117, Val MAE: 1.175910472869873\n",
      "Epoch 893/2000, Train Loss: 5.662073452368541, Val Loss: 4.391383934160275, Val MAE: 1.176242709159851\n",
      "Epoch 894/2000, Train Loss: 5.660525942369482, Val Loss: 4.3907631514677865, Val MAE: 1.176573395729065\n",
      "Epoch 895/2000, Train Loss: 5.65882147813736, Val Loss: 4.390117302087245, Val MAE: 1.176938772201538\n",
      "Epoch 896/2000, Train Loss: 5.657487997212759, Val Loss: 4.38954470572775, Val MAE: 1.1772645711898804\n",
      "Epoch 897/2000, Train Loss: 5.656198580811808, Val Loss: 4.389066368185387, Val MAE: 1.1775460243225098\n",
      "Epoch 898/2000, Train Loss: 5.6549596191382445, Val Loss: 4.3885589801127445, Val MAE: 1.17784583568573\n",
      "Epoch 899/2000, Train Loss: 5.653734164007369, Val Loss: 4.388046107045165, Val MAE: 1.1781401634216309\n",
      "Epoch 900/2000, Train Loss: 5.652279111412869, Val Loss: 4.387464959191175, Val MAE: 1.1784824132919312\n",
      "Epoch 901/2000, Train Loss: 5.650797329157266, Val Loss: 4.386814920676385, Val MAE: 1.1788493394851685\n",
      "Epoch 902/2000, Train Loss: 5.649317975628208, Val Loss: 4.386358559729011, Val MAE: 1.1791445016860962\n",
      "Epoch 903/2000, Train Loss: 5.648086848385434, Val Loss: 4.385834284140183, Val MAE: 1.1794662475585938\n",
      "Epoch 904/2000, Train Loss: 5.646627807022071, Val Loss: 4.3852396671165215, Val MAE: 1.179835557937622\n",
      "Epoch 905/2000, Train Loss: 5.6453027338393955, Val Loss: 4.3847435628206615, Val MAE: 1.1801639795303345\n",
      "Epoch 906/2000, Train Loss: 5.644240627794668, Val Loss: 4.3842812473038295, Val MAE: 1.180457592010498\n",
      "Epoch 907/2000, Train Loss: 5.642996637014071, Val Loss: 4.38384949015873, Val MAE: 1.180798888206482\n",
      "Epoch 908/2000, Train Loss: 5.641763416355746, Val Loss: 4.383326237353387, Val MAE: 1.1811281442642212\n",
      "Epoch 909/2000, Train Loss: 5.640390805260812, Val Loss: 4.382832309761429, Val MAE: 1.1814558506011963\n",
      "Epoch 910/2000, Train Loss: 5.639139728129561, Val Loss: 4.382319169115644, Val MAE: 1.1818339824676514\n",
      "Epoch 911/2000, Train Loss: 5.637921980502267, Val Loss: 4.381882120084923, Val MAE: 1.182101845741272\n",
      "Epoch 912/2000, Train Loss: 5.6366848982812465, Val Loss: 4.381376144269834, Val MAE: 1.1824350357055664\n",
      "Epoch 913/2000, Train Loss: 5.635499651086126, Val Loss: 4.380921672751104, Val MAE: 1.1827365159988403\n",
      "Epoch 914/2000, Train Loss: 5.6343202605820295, Val Loss: 4.3805237357211, Val MAE: 1.1830055713653564\n",
      "Epoch 915/2000, Train Loss: 5.63328123021095, Val Loss: 4.380072603670058, Val MAE: 1.1833012104034424\n",
      "Epoch 916/2000, Train Loss: 5.632150466212431, Val Loss: 4.37966975655306, Val MAE: 1.1835860013961792\n",
      "Epoch 917/2000, Train Loss: 5.631037845812424, Val Loss: 4.379239308585723, Val MAE: 1.1838840246200562\n",
      "Epoch 918/2000, Train Loss: 5.629841755112333, Val Loss: 4.37881017887109, Val MAE: 1.1841812133789062\n",
      "Epoch 919/2000, Train Loss: 5.628758207684188, Val Loss: 4.378406318451638, Val MAE: 1.1844961643218994\n",
      "Epoch 920/2000, Train Loss: 5.627545678485388, Val Loss: 4.37797908862499, Val MAE: 1.1848152875900269\n",
      "Epoch 921/2000, Train Loss: 5.626386294312857, Val Loss: 4.377578870341316, Val MAE: 1.185123324394226\n",
      "Epoch 922/2000, Train Loss: 5.625219954622331, Val Loss: 4.377111117434394, Val MAE: 1.1854604482650757\n",
      "Epoch 923/2000, Train Loss: 5.624031351062698, Val Loss: 4.376743407688431, Val MAE: 1.1857454776763916\n",
      "Epoch 924/2000, Train Loss: 5.622968145540091, Val Loss: 4.376348861276701, Val MAE: 1.186057209968567\n",
      "Epoch 925/2000, Train Loss: 5.621800145939249, Val Loss: 4.375899336164868, Val MAE: 1.1864120960235596\n",
      "Epoch 926/2000, Train Loss: 5.620646683920564, Val Loss: 4.3754810688161365, Val MAE: 1.1867573261260986\n",
      "Epoch 927/2000, Train Loss: 5.619453267438422, Val Loss: 4.3751426337706345, Val MAE: 1.187048316001892\n",
      "Epoch 928/2000, Train Loss: 5.618473488901409, Val Loss: 4.374786072513005, Val MAE: 1.187343955039978\n",
      "Epoch 929/2000, Train Loss: 5.617313518910995, Val Loss: 4.374378626189522, Val MAE: 1.1876870393753052\n",
      "Epoch 930/2000, Train Loss: 5.616624263538028, Val Loss: 4.374101867721424, Val MAE: 1.1878890991210938\n",
      "Epoch 931/2000, Train Loss: 5.615859718525465, Val Loss: 4.373778001279445, Val MAE: 1.1881630420684814\n",
      "Epoch 932/2000, Train Loss: 5.614908266620778, Val Loss: 4.373420308063831, Val MAE: 1.1884688138961792\n",
      "Epoch 933/2000, Train Loss: 5.613957539735458, Val Loss: 4.373076218205529, Val MAE: 1.1887532472610474\n",
      "Epoch 934/2000, Train Loss: 5.6129162847158875, Val Loss: 4.372750791132048, Val MAE: 1.1890501976013184\n",
      "Epoch 935/2000, Train Loss: 5.612095843239247, Val Loss: 4.372467559089397, Val MAE: 1.1892927885055542\n",
      "Epoch 936/2000, Train Loss: 5.6108964778912975, Val Loss: 4.372113104587471, Val MAE: 1.1897284984588623\n",
      "Epoch 937/2000, Train Loss: 5.609890320090586, Val Loss: 4.37179979843398, Val MAE: 1.1900012493133545\n",
      "Epoch 938/2000, Train Loss: 5.608981961207159, Val Loss: 4.371471538085927, Val MAE: 1.1903043985366821\n",
      "Epoch 939/2000, Train Loss: 5.60772038785546, Val Loss: 4.371044255215843, Val MAE: 1.190752625465393\n",
      "Epoch 940/2000, Train Loss: 5.606763340939597, Val Loss: 4.370790374141421, Val MAE: 1.1909797191619873\n",
      "Epoch 941/2000, Train Loss: 5.605992893997853, Val Loss: 4.370480422289656, Val MAE: 1.1912837028503418\n",
      "Epoch 942/2000, Train Loss: 5.605179010054995, Val Loss: 4.3702369893255, Val MAE: 1.1915256977081299\n",
      "Epoch 943/2000, Train Loss: 5.604196834861767, Val Loss: 4.369897429987385, Val MAE: 1.1918386220932007\n",
      "Epoch 944/2000, Train Loss: 5.603428710176495, Val Loss: 4.369647251760906, Val MAE: 1.1920883655548096\n",
      "Epoch 945/2000, Train Loss: 5.6027268336827225, Val Loss: 4.369384350000066, Val MAE: 1.1923383474349976\n",
      "Epoch 946/2000, Train Loss: 5.601707787298003, Val Loss: 4.369083847944532, Val MAE: 1.1926887035369873\n",
      "Epoch 947/2000, Train Loss: 5.600928450755879, Val Loss: 4.368827045614924, Val MAE: 1.1929327249526978\n",
      "Epoch 948/2000, Train Loss: 5.600110904846846, Val Loss: 4.368557216287464, Val MAE: 1.1932028532028198\n",
      "Epoch 949/2000, Train Loss: 5.599355730363247, Val Loss: 4.368324492750941, Val MAE: 1.193439245223999\n",
      "Epoch 950/2000, Train Loss: 5.598868911400112, Val Loss: 4.368079906769164, Val MAE: 1.1937180757522583\n",
      "Epoch 951/2000, Train Loss: 5.598003391914546, Val Loss: 4.367833543948091, Val MAE: 1.1939551830291748\n",
      "Epoch 952/2000, Train Loss: 5.597225424280032, Val Loss: 4.36757101969676, Val MAE: 1.194254994392395\n",
      "Epoch 953/2000, Train Loss: 5.596358666658494, Val Loss: 4.367276191536908, Val MAE: 1.1945356130599976\n",
      "Epoch 954/2000, Train Loss: 5.595587100217616, Val Loss: 4.367041340735447, Val MAE: 1.194804072380066\n",
      "Epoch 955/2000, Train Loss: 5.594851494579345, Val Loss: 4.3667867894712336, Val MAE: 1.1950892210006714\n",
      "Epoch 956/2000, Train Loss: 5.5939623070953415, Val Loss: 4.366517290869006, Val MAE: 1.1953973770141602\n",
      "Epoch 957/2000, Train Loss: 5.593233361445053, Val Loss: 4.366288992799483, Val MAE: 1.1956268548965454\n",
      "Epoch 958/2000, Train Loss: 5.592296648058639, Val Loss: 4.3660103624132836, Val MAE: 1.1959871053695679\n",
      "Epoch 959/2000, Train Loss: 5.591578888105354, Val Loss: 4.3657904114909805, Val MAE: 1.196236252784729\n",
      "Epoch 960/2000, Train Loss: 5.590806791265372, Val Loss: 4.365574598231831, Val MAE: 1.1964844465255737\n",
      "Epoch 961/2000, Train Loss: 5.590201444438602, Val Loss: 4.365351778822573, Val MAE: 1.1967272758483887\n",
      "Epoch 962/2000, Train Loss: 5.5895761565745525, Val Loss: 4.365141897917062, Val MAE: 1.1969656944274902\n",
      "Epoch 963/2000, Train Loss: 5.588769415983358, Val Loss: 4.3648924493440635, Val MAE: 1.1972614526748657\n",
      "Epoch 964/2000, Train Loss: 5.588134967964637, Val Loss: 4.364681821967568, Val MAE: 1.1975274085998535\n",
      "Epoch 965/2000, Train Loss: 5.587578238480958, Val Loss: 4.364449634496961, Val MAE: 1.1976940631866455\n",
      "Epoch 966/2000, Train Loss: 5.586966779321292, Val Loss: 4.364244897858248, Val MAE: 1.1979676485061646\n",
      "Epoch 967/2000, Train Loss: 5.586223858102064, Val Loss: 4.364035137514541, Val MAE: 1.1982065439224243\n",
      "Epoch 968/2000, Train Loss: 5.585604962432254, Val Loss: 4.363815947135424, Val MAE: 1.1984566450119019\n",
      "Epoch 969/2000, Train Loss: 5.584894756259115, Val Loss: 4.363591751778448, Val MAE: 1.1987342834472656\n",
      "Epoch 970/2000, Train Loss: 5.5842110141298145, Val Loss: 4.363408142989417, Val MAE: 1.198966145515442\n",
      "Epoch 971/2000, Train Loss: 5.583592960085028, Val Loss: 4.3631963754727225, Val MAE: 1.1992205381393433\n",
      "Epoch 972/2000, Train Loss: 5.582913173342272, Val Loss: 4.362979216685703, Val MAE: 1.1994565725326538\n",
      "Epoch 973/2000, Train Loss: 5.582040654329726, Val Loss: 4.362735096418911, Val MAE: 1.1998103857040405\n",
      "Epoch 974/2000, Train Loss: 5.58125371828838, Val Loss: 4.362534758317712, Val MAE: 1.2001515626907349\n",
      "Epoch 975/2000, Train Loss: 5.5803392746757465, Val Loss: 4.362292744987854, Val MAE: 1.200499176979065\n",
      "Epoch 976/2000, Train Loss: 5.579599564979303, Val Loss: 4.362109389736712, Val MAE: 1.2007434368133545\n",
      "Epoch 977/2000, Train Loss: 5.579064829394151, Val Loss: 4.361906191196527, Val MAE: 1.2008967399597168\n",
      "Epoch 978/2000, Train Loss: 5.57859621330654, Val Loss: 4.361706471506943, Val MAE: 1.201143503189087\n",
      "Epoch 979/2000, Train Loss: 5.577984541940812, Val Loss: 4.361492340145884, Val MAE: 1.2013882398605347\n",
      "Epoch 980/2000, Train Loss: 5.577439587499348, Val Loss: 4.36132009113627, Val MAE: 1.20168936252594\n",
      "Epoch 981/2000, Train Loss: 5.576747037319237, Val Loss: 4.3611290837226955, Val MAE: 1.2018816471099854\n",
      "Epoch 982/2000, Train Loss: 5.576218305251528, Val Loss: 4.360942509767037, Val MAE: 1.2020940780639648\n",
      "Epoch 983/2000, Train Loss: 5.575684122659859, Val Loss: 4.360759708157799, Val MAE: 1.2023365497589111\n",
      "Epoch 984/2000, Train Loss: 5.5751742513243014, Val Loss: 4.360583376693162, Val MAE: 1.202527403831482\n",
      "Epoch 985/2000, Train Loss: 5.574392435517215, Val Loss: 4.360392379109656, Val MAE: 1.202933430671692\n",
      "Epoch 986/2000, Train Loss: 5.573573822164312, Val Loss: 4.360203387923875, Val MAE: 1.2031692266464233\n",
      "Epoch 987/2000, Train Loss: 5.573016153296889, Val Loss: 4.360008599394345, Val MAE: 1.2034399509429932\n",
      "Epoch 988/2000, Train Loss: 5.5723586863549, Val Loss: 4.359836388627688, Val MAE: 1.203691840171814\n",
      "Epoch 989/2000, Train Loss: 5.57170554665433, Val Loss: 4.359689943495769, Val MAE: 1.2039662599563599\n",
      "Epoch 990/2000, Train Loss: 5.5710467131759005, Val Loss: 4.359518929859539, Val MAE: 1.204235553741455\n",
      "Epoch 991/2000, Train Loss: 5.570377777756283, Val Loss: 4.359374755191373, Val MAE: 1.2045849561691284\n",
      "Epoch 992/2000, Train Loss: 5.569729483955178, Val Loss: 4.35921874502422, Val MAE: 1.204798936843872\n",
      "Epoch 993/2000, Train Loss: 5.569229790147586, Val Loss: 4.3590734031017835, Val MAE: 1.2050395011901855\n",
      "Epoch 994/2000, Train Loss: 5.568781128623951, Val Loss: 4.358914512150863, Val MAE: 1.2053033113479614\n",
      "Epoch 995/2000, Train Loss: 5.56816236686409, Val Loss: 4.358754628369803, Val MAE: 1.2055245637893677\n",
      "Epoch 996/2000, Train Loss: 5.567618364848882, Val Loss: 4.358595803054469, Val MAE: 1.2057757377624512\n",
      "Epoch 997/2000, Train Loss: 5.567085998850568, Val Loss: 4.358428148720582, Val MAE: 1.2059929370880127\n",
      "Epoch 998/2000, Train Loss: 5.56658061267009, Val Loss: 4.358286725608884, Val MAE: 1.2062269449234009\n",
      "Epoch 999/2000, Train Loss: 5.566043860245607, Val Loss: 4.3581312744366425, Val MAE: 1.2064523696899414\n",
      "Epoch 1000/2000, Train Loss: 5.5655250515841095, Val Loss: 4.357965577712601, Val MAE: 1.2066985368728638\n",
      "Epoch 1001/2000, Train Loss: 5.565046513601324, Val Loss: 4.357801201842255, Val MAE: 1.2069003582000732\n",
      "Epoch 1002/2000, Train Loss: 5.564553374735502, Val Loss: 4.357674934319011, Val MAE: 1.2071722745895386\n",
      "Epoch 1003/2000, Train Loss: 5.564001482102131, Val Loss: 4.357507350913307, Val MAE: 1.2073851823806763\n",
      "Epoch 1004/2000, Train Loss: 5.563383887383895, Val Loss: 4.357343078723496, Val MAE: 1.2076520919799805\n",
      "Epoch 1005/2000, Train Loss: 5.562863298396866, Val Loss: 4.357223120667376, Val MAE: 1.2079333066940308\n",
      "Epoch 1006/2000, Train Loss: 5.562305801455077, Val Loss: 4.357074398510494, Val MAE: 1.2081639766693115\n",
      "Epoch 1007/2000, Train Loss: 5.561766129016132, Val Loss: 4.356910933007125, Val MAE: 1.2084044218063354\n",
      "Epoch 1008/2000, Train Loss: 5.561319964600061, Val Loss: 4.356751849874854, Val MAE: 1.208574891090393\n",
      "Epoch 1009/2000, Train Loss: 5.560966995688571, Val Loss: 4.35660828257332, Val MAE: 1.208883285522461\n",
      "Epoch 1010/2000, Train Loss: 5.560305135670393, Val Loss: 4.356442448043743, Val MAE: 1.209023118019104\n",
      "Epoch 1011/2000, Train Loss: 5.560131950794999, Val Loss: 4.3562651844477065, Val MAE: 1.209159016609192\n",
      "Epoch 1012/2000, Train Loss: 5.559502000927739, Val Loss: 4.35609173635776, Val MAE: 1.2094775438308716\n",
      "Epoch 1013/2000, Train Loss: 5.558911150965009, Val Loss: 4.35595782047792, Val MAE: 1.2097214460372925\n",
      "Epoch 1014/2000, Train Loss: 5.5584050520161945, Val Loss: 4.355816842538414, Val MAE: 1.2099425792694092\n",
      "Epoch 1015/2000, Train Loss: 5.557916537165502, Val Loss: 4.355667189740249, Val MAE: 1.2101571559906006\n",
      "Epoch 1016/2000, Train Loss: 5.557522511147486, Val Loss: 4.355503397666522, Val MAE: 1.21035635471344\n",
      "Epoch 1017/2000, Train Loss: 5.5570120810765, Val Loss: 4.355378780638178, Val MAE: 1.2105696201324463\n",
      "Epoch 1018/2000, Train Loss: 5.556536614243958, Val Loss: 4.355233276387056, Val MAE: 1.210797667503357\n",
      "Epoch 1019/2000, Train Loss: 5.555968229930598, Val Loss: 4.3551111635827535, Val MAE: 1.211073637008667\n",
      "Epoch 1020/2000, Train Loss: 5.555551604064132, Val Loss: 4.354984984432792, Val MAE: 1.2113442420959473\n",
      "Epoch 1021/2000, Train Loss: 5.555000016590735, Val Loss: 4.354850205315931, Val MAE: 1.2115812301635742\n",
      "Epoch 1022/2000, Train Loss: 5.554519620588901, Val Loss: 4.354685765971337, Val MAE: 1.2117843627929688\n",
      "Epoch 1023/2000, Train Loss: 5.554005074612622, Val Loss: 4.354581223656465, Val MAE: 1.2120411396026611\n",
      "Epoch 1024/2000, Train Loss: 5.553567580797371, Val Loss: 4.35438516057088, Val MAE: 1.212220549583435\n",
      "Epoch 1025/2000, Train Loss: 5.553131531730771, Val Loss: 4.354229520727788, Val MAE: 1.2124234437942505\n",
      "Epoch 1026/2000, Train Loss: 5.552654837669337, Val Loss: 4.354098118600008, Val MAE: 1.2126530408859253\n",
      "Epoch 1027/2000, Train Loss: 5.552250397256496, Val Loss: 4.353942833146131, Val MAE: 1.212850570678711\n",
      "Epoch 1028/2000, Train Loss: 5.551779741058112, Val Loss: 4.353821994985143, Val MAE: 1.2131048440933228\n",
      "Epoch 1029/2000, Train Loss: 5.551201391145703, Val Loss: 4.353695834773752, Val MAE: 1.2133700847625732\n",
      "Epoch 1030/2000, Train Loss: 5.550769584134291, Val Loss: 4.353537302356851, Val MAE: 1.213579773902893\n",
      "Epoch 1031/2000, Train Loss: 5.550301155322427, Val Loss: 4.353401768757953, Val MAE: 1.2137877941131592\n",
      "Epoch 1032/2000, Train Loss: 5.549841639776126, Val Loss: 4.3532604707992295, Val MAE: 1.2140244245529175\n",
      "Epoch 1033/2000, Train Loss: 5.549335339883933, Val Loss: 4.353122058292633, Val MAE: 1.2143288850784302\n",
      "Epoch 1034/2000, Train Loss: 5.548716913333363, Val Loss: 4.352991239206346, Val MAE: 1.214556336402893\n",
      "Epoch 1035/2000, Train Loss: 5.548367573206956, Val Loss: 4.352803464646677, Val MAE: 1.2147116661071777\n",
      "Epoch 1036/2000, Train Loss: 5.5479683686257015, Val Loss: 4.352771082621168, Val MAE: 1.2150402069091797\n",
      "Epoch 1037/2000, Train Loss: 5.547396484384671, Val Loss: 4.352657978226607, Val MAE: 1.215247631072998\n",
      "Epoch 1038/2000, Train Loss: 5.546964802167578, Val Loss: 4.352484585576364, Val MAE: 1.2154422998428345\n",
      "Epoch 1039/2000, Train Loss: 5.546491593168976, Val Loss: 4.352348502238725, Val MAE: 1.2156633138656616\n",
      "Epoch 1040/2000, Train Loss: 5.546104350811606, Val Loss: 4.352199849298408, Val MAE: 1.2158564329147339\n",
      "Epoch 1041/2000, Train Loss: 5.5457154733714376, Val Loss: 4.352119303900782, Val MAE: 1.2161191701889038\n",
      "Epoch 1042/2000, Train Loss: 5.545263378370943, Val Loss: 4.351908525686946, Val MAE: 1.2162529230117798\n",
      "Epoch 1043/2000, Train Loss: 5.544894459281064, Val Loss: 4.351790837564312, Val MAE: 1.216492772102356\n",
      "Epoch 1044/2000, Train Loss: 5.5444629329973, Val Loss: 4.351625102295263, Val MAE: 1.2166402339935303\n",
      "Epoch 1045/2000, Train Loss: 5.544006834919571, Val Loss: 4.3515337381444805, Val MAE: 1.2169716358184814\n",
      "Epoch 1046/2000, Train Loss: 5.543455480971314, Val Loss: 4.3514275242381535, Val MAE: 1.2172155380249023\n",
      "Epoch 1047/2000, Train Loss: 5.542963839271185, Val Loss: 4.351285522048538, Val MAE: 1.2174490690231323\n",
      "Epoch 1048/2000, Train Loss: 5.542564313643715, Val Loss: 4.351125078608056, Val MAE: 1.2176270484924316\n",
      "Epoch 1049/2000, Train Loss: 5.54213722484904, Val Loss: 4.351037117288456, Val MAE: 1.2179019451141357\n",
      "Epoch 1050/2000, Train Loss: 5.5416420959517385, Val Loss: 4.3509361026019935, Val MAE: 1.218153476715088\n",
      "Epoch 1051/2000, Train Loss: 5.541176038564658, Val Loss: 4.350808616694029, Val MAE: 1.218366026878357\n",
      "Epoch 1052/2000, Train Loss: 5.540719375967422, Val Loss: 4.3506973646883225, Val MAE: 1.2185953855514526\n",
      "Epoch 1053/2000, Train Loss: 5.540283557219364, Val Loss: 4.350483995323648, Val MAE: 1.2188220024108887\n",
      "Epoch 1054/2000, Train Loss: 5.539782559071986, Val Loss: 4.350572372885706, Val MAE: 1.219232439994812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1055/2000, Train Loss: 5.53915153539125, Val Loss: 4.350462869054697, Val MAE: 1.2194751501083374\n",
      "Epoch 1056/2000, Train Loss: 5.538662337289772, Val Loss: 4.350367202958814, Val MAE: 1.2197452783584595\n",
      "Epoch 1057/2000, Train Loss: 5.538284335227308, Val Loss: 4.350245251644168, Val MAE: 1.2199243307113647\n",
      "Epoch 1058/2000, Train Loss: 5.537882669481584, Val Loss: 4.350136882939317, Val MAE: 1.2201076745986938\n",
      "Epoch 1059/2000, Train Loss: 5.537448607048081, Val Loss: 4.349988041704034, Val MAE: 1.2203121185302734\n",
      "Epoch 1060/2000, Train Loss: 5.537041045202294, Val Loss: 4.349871419635308, Val MAE: 1.220537781715393\n",
      "Epoch 1061/2000, Train Loss: 5.536563456709411, Val Loss: 4.349783883508932, Val MAE: 1.220798134803772\n",
      "Epoch 1062/2000, Train Loss: 5.536109806036986, Val Loss: 4.3496776532482455, Val MAE: 1.2210050821304321\n",
      "Epoch 1063/2000, Train Loss: 5.535704293600668, Val Loss: 4.349559257713121, Val MAE: 1.221204161643982\n",
      "Epoch 1064/2000, Train Loss: 5.535395796325017, Val Loss: 4.349441330450344, Val MAE: 1.2214809656143188\n",
      "Epoch 1065/2000, Train Loss: 5.534737567633809, Val Loss: 4.349382491614501, Val MAE: 1.2217503786087036\n",
      "Epoch 1066/2000, Train Loss: 5.534383645853647, Val Loss: 4.349305112092748, Val MAE: 1.2220149040222168\n",
      "Epoch 1067/2000, Train Loss: 5.53391007030624, Val Loss: 4.3490990002111, Val MAE: 1.222136378288269\n",
      "Epoch 1068/2000, Train Loss: 5.533542389105337, Val Loss: 4.348987395615056, Val MAE: 1.2223410606384277\n",
      "Epoch 1069/2000, Train Loss: 5.5331667023776285, Val Loss: 4.34889455896777, Val MAE: 1.22255539894104\n",
      "Epoch 1070/2000, Train Loss: 5.5327586668329936, Val Loss: 4.34876710477176, Val MAE: 1.2227767705917358\n",
      "Epoch 1071/2000, Train Loss: 5.532379939663986, Val Loss: 4.348631894430733, Val MAE: 1.22295081615448\n",
      "Epoch 1072/2000, Train Loss: 5.531942494062477, Val Loss: 4.348498678106714, Val MAE: 1.2231416702270508\n",
      "Epoch 1073/2000, Train Loss: 5.531599137042874, Val Loss: 4.34835599748468, Val MAE: 1.2233185768127441\n",
      "Epoch 1074/2000, Train Loss: 5.531228480491549, Val Loss: 4.3481820443460535, Val MAE: 1.2235029935836792\n",
      "Epoch 1075/2000, Train Loss: 5.530851836904721, Val Loss: 4.348069450675367, Val MAE: 1.2237110137939453\n",
      "Epoch 1076/2000, Train Loss: 5.5304584529210175, Val Loss: 4.347959466728272, Val MAE: 1.2239060401916504\n",
      "Epoch 1077/2000, Train Loss: 5.530040146594114, Val Loss: 4.347859457175474, Val MAE: 1.224119782447815\n",
      "Epoch 1078/2000, Train Loss: 5.5296466785734415, Val Loss: 4.34776621328348, Val MAE: 1.224379539489746\n",
      "Epoch 1079/2000, Train Loss: 5.529184173496205, Val Loss: 4.347582973768045, Val MAE: 1.2245407104492188\n",
      "Epoch 1080/2000, Train Loss: 5.528800819481777, Val Loss: 4.347461643905656, Val MAE: 1.2247284650802612\n",
      "Epoch 1081/2000, Train Loss: 5.528452621421279, Val Loss: 4.347362999178402, Val MAE: 1.2249082326889038\n",
      "Epoch 1082/2000, Train Loss: 5.5280656740185625, Val Loss: 4.3471958783899876, Val MAE: 1.225063443183899\n",
      "Epoch 1083/2000, Train Loss: 5.5277929858372845, Val Loss: 4.346985331335449, Val MAE: 1.2252005338668823\n",
      "Epoch 1084/2000, Train Loss: 5.527432856107837, Val Loss: 4.346892689131536, Val MAE: 1.2254276275634766\n",
      "Epoch 1085/2000, Train Loss: 5.527044638722566, Val Loss: 4.346713711415325, Val MAE: 1.2255648374557495\n",
      "Epoch 1086/2000, Train Loss: 5.526657341515962, Val Loss: 4.3466652892966255, Val MAE: 1.225792407989502\n",
      "Epoch 1087/2000, Train Loss: 5.526274599617292, Val Loss: 4.346504108020448, Val MAE: 1.225977897644043\n",
      "Epoch 1088/2000, Train Loss: 5.525902685516059, Val Loss: 4.3464097348184465, Val MAE: 1.226193904876709\n",
      "Epoch 1089/2000, Train Loss: 5.525487685538305, Val Loss: 4.346278713965738, Val MAE: 1.2263911962509155\n",
      "Epoch 1090/2000, Train Loss: 5.525155661072635, Val Loss: 4.346044478874218, Val MAE: 1.2264987230300903\n",
      "Epoch 1091/2000, Train Loss: 5.524721468394904, Val Loss: 4.345923182740807, Val MAE: 1.226758360862732\n",
      "Epoch 1092/2000, Train Loss: 5.524314136475371, Val Loss: 4.345795217203396, Val MAE: 1.2269469499588013\n",
      "Epoch 1093/2000, Train Loss: 5.523904137232003, Val Loss: 4.345761695265904, Val MAE: 1.2272037267684937\n",
      "Epoch 1094/2000, Train Loss: 5.5235121960573, Val Loss: 4.345591824041965, Val MAE: 1.2273802757263184\n",
      "Epoch 1095/2000, Train Loss: 5.523203278480565, Val Loss: 4.34543178931617, Val MAE: 1.2275416851043701\n",
      "Epoch 1096/2000, Train Loss: 5.522952488915597, Val Loss: 4.345246440232605, Val MAE: 1.2276500463485718\n",
      "Epoch 1097/2000, Train Loss: 5.522442328204603, Val Loss: 4.345271041717481, Val MAE: 1.227948546409607\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1098/2000, Train Loss: 5.5220146887759896, Val Loss: 4.345169853523105, Val MAE: 1.2281432151794434\n",
      "Epoch 1099/2000, Train Loss: 5.521709198637774, Val Loss: 4.345036562988619, Val MAE: 1.2283036708831787\n",
      "Epoch 1100/2000, Train Loss: 5.521339863957183, Val Loss: 4.344926366361009, Val MAE: 1.2285090684890747\n",
      "Epoch 1101/2000, Train Loss: 5.521088193991627, Val Loss: 4.344856784600127, Val MAE: 1.2287054061889648\n",
      "Epoch 1102/2000, Train Loss: 5.520588135942468, Val Loss: 4.3447398570240345, Val MAE: 1.2289127111434937\n",
      "Epoch 1103/2000, Train Loss: 5.52023831693319, Val Loss: 4.344652476520823, Val MAE: 1.229105830192566\n",
      "Epoch 1104/2000, Train Loss: 5.519845378330898, Val Loss: 4.3446645158368185, Val MAE: 1.2294062376022339\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1105/2000, Train Loss: 5.519435444948454, Val Loss: 4.34457944107351, Val MAE: 1.2295900583267212\n",
      "Epoch 1106/2000, Train Loss: 5.519182508663157, Val Loss: 4.344518386807527, Val MAE: 1.2298095226287842\n",
      "Epoch 1107/2000, Train Loss: 5.51874760789695, Val Loss: 4.344308447955294, Val MAE: 1.2299327850341797\n",
      "Epoch 1108/2000, Train Loss: 5.518496118172097, Val Loss: 4.344249254857769, Val MAE: 1.2301548719406128\n",
      "Epoch 1109/2000, Train Loss: 5.518064357933127, Val Loss: 4.344139507896191, Val MAE: 1.2303380966186523\n",
      "Epoch 1110/2000, Train Loss: 5.517704654409435, Val Loss: 4.343996181889429, Val MAE: 1.2305169105529785\n",
      "Epoch 1111/2000, Train Loss: 5.517326595630735, Val Loss: 4.343840350539566, Val MAE: 1.2306429147720337\n",
      "Epoch 1112/2000, Train Loss: 5.516989896896291, Val Loss: 4.343738486309041, Val MAE: 1.230819821357727\n",
      "Epoch 1113/2000, Train Loss: 5.5165856896250185, Val Loss: 4.343813211692346, Val MAE: 1.231173038482666\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1114/2000, Train Loss: 5.516213715355407, Val Loss: 4.343637516369691, Val MAE: 1.2313051223754883\n",
      "Epoch 1115/2000, Train Loss: 5.5158385129503085, Val Loss: 4.343580850220478, Val MAE: 1.2315315008163452\n",
      "Epoch 1116/2000, Train Loss: 5.515479126698142, Val Loss: 4.343348680288942, Val MAE: 1.2317113876342773\n",
      "Epoch 1117/2000, Train Loss: 5.515094573430821, Val Loss: 4.343222670646401, Val MAE: 1.2318719625473022\n",
      "Epoch 1118/2000, Train Loss: 5.514889371216692, Val Loss: 4.343179662968661, Val MAE: 1.2321016788482666\n",
      "Epoch 1119/2000, Train Loss: 5.514463208952858, Val Loss: 4.342999556741199, Val MAE: 1.2322131395339966\n",
      "Epoch 1120/2000, Train Loss: 5.514158447521525, Val Loss: 4.342832605903213, Val MAE: 1.232328176498413\n",
      "Epoch 1121/2000, Train Loss: 5.513753271465554, Val Loss: 4.3427787283116634, Val MAE: 1.23255455493927\n",
      "Epoch 1122/2000, Train Loss: 5.513423366219317, Val Loss: 4.3426981572363825, Val MAE: 1.232782006263733\n",
      "Epoch 1123/2000, Train Loss: 5.513063298372694, Val Loss: 4.342546306530366, Val MAE: 1.2329286336898804\n",
      "Epoch 1124/2000, Train Loss: 5.512706683968977, Val Loss: 4.342447046875148, Val MAE: 1.2331665754318237\n",
      "Epoch 1125/2000, Train Loss: 5.512303320414935, Val Loss: 4.342244025178858, Val MAE: 1.2332595586776733\n",
      "Epoch 1126/2000, Train Loss: 5.511920005036591, Val Loss: 4.342181716731808, Val MAE: 1.2334825992584229\n",
      "Epoch 1127/2000, Train Loss: 5.511575964516299, Val Loss: 4.3421046475502285, Val MAE: 1.233696699142456\n",
      "Epoch 1128/2000, Train Loss: 5.51123163220291, Val Loss: 4.34207222421427, Val MAE: 1.2339189052581787\n",
      "Epoch 1129/2000, Train Loss: 5.510845690175263, Val Loss: 4.3419729355085, Val MAE: 1.234128713607788\n",
      "Epoch 1130/2000, Train Loss: 5.510526144932287, Val Loss: 4.342105153961493, Val MAE: 1.2344743013381958\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1131/2000, Train Loss: 5.510138416439062, Val Loss: 4.341961565103617, Val MAE: 1.2346168756484985\n",
      "Epoch 1132/2000, Train Loss: 5.509816858586209, Val Loss: 4.341963917003558, Val MAE: 1.2348856925964355\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1133/2000, Train Loss: 5.509412789144865, Val Loss: 4.341820022181885, Val MAE: 1.235041618347168\n",
      "Epoch 1134/2000, Train Loss: 5.509127670512556, Val Loss: 4.3417503939421325, Val MAE: 1.2352628707885742\n",
      "Epoch 1135/2000, Train Loss: 5.5087526640542395, Val Loss: 4.341607330927441, Val MAE: 1.235398292541504\n",
      "Epoch 1136/2000, Train Loss: 5.508471546976503, Val Loss: 4.341508672956948, Val MAE: 1.2355763912200928\n",
      "Epoch 1137/2000, Train Loss: 5.508044375272325, Val Loss: 4.341519770469215, Val MAE: 1.2358275651931763\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1138/2000, Train Loss: 5.507626787786969, Val Loss: 4.34142544035976, Val MAE: 1.2360609769821167\n",
      "Epoch 1139/2000, Train Loss: 5.50745577154777, Val Loss: 4.341276996012207, Val MAE: 1.2362060546875\n",
      "Epoch 1140/2000, Train Loss: 5.507106612192382, Val Loss: 4.341252280677761, Val MAE: 1.2364376783370972\n",
      "Epoch 1141/2000, Train Loss: 5.506683436366214, Val Loss: 4.34105992417212, Val MAE: 1.2365525960922241\n",
      "Epoch 1142/2000, Train Loss: 5.506422150386105, Val Loss: 4.340927025783169, Val MAE: 1.2367074489593506\n",
      "Epoch 1143/2000, Train Loss: 5.506121044647234, Val Loss: 4.340773969210751, Val MAE: 1.2368420362472534\n",
      "Epoch 1144/2000, Train Loss: 5.505800551102844, Val Loss: 4.340667346730694, Val MAE: 1.237021803855896\n",
      "Epoch 1145/2000, Train Loss: 5.505451446390375, Val Loss: 4.340515780771101, Val MAE: 1.237173080444336\n",
      "Epoch 1146/2000, Train Loss: 5.505135954263244, Val Loss: 4.340375049794848, Val MAE: 1.2373627424240112\n",
      "Epoch 1147/2000, Train Loss: 5.504810446696051, Val Loss: 4.340481633471476, Val MAE: 1.2376883029937744\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1148/2000, Train Loss: 5.504485539452706, Val Loss: 4.340319316518737, Val MAE: 1.2378473281860352\n",
      "Epoch 1149/2000, Train Loss: 5.50406051016821, Val Loss: 4.340216713371846, Val MAE: 1.238038182258606\n",
      "Epoch 1150/2000, Train Loss: 5.503754700587804, Val Loss: 4.340027378721012, Val MAE: 1.238185167312622\n",
      "Epoch 1151/2000, Train Loss: 5.503360486904656, Val Loss: 4.340009448858532, Val MAE: 1.2384346723556519\n",
      "Epoch 1152/2000, Train Loss: 5.5030746965810025, Val Loss: 4.339975862325849, Val MAE: 1.2386393547058105\n",
      "Epoch 1153/2000, Train Loss: 5.502695985027994, Val Loss: 4.339868272377832, Val MAE: 1.2388206720352173\n",
      "Epoch 1154/2000, Train Loss: 5.502388818308641, Val Loss: 4.33972244853372, Val MAE: 1.23898446559906\n",
      "Epoch 1155/2000, Train Loss: 5.502063990662139, Val Loss: 4.339588354930684, Val MAE: 1.239137053489685\n",
      "Epoch 1156/2000, Train Loss: 5.501743472868492, Val Loss: 4.339439766719803, Val MAE: 1.2393035888671875\n",
      "Epoch 1157/2000, Train Loss: 5.5014750622735935, Val Loss: 4.33930540821171, Val MAE: 1.2394386529922485\n",
      "Epoch 1158/2000, Train Loss: 5.501152009867399, Val Loss: 4.339138669252127, Val MAE: 1.239585280418396\n",
      "Epoch 1159/2000, Train Loss: 5.5008332123384225, Val Loss: 4.339034550026193, Val MAE: 1.2397565841674805\n",
      "Epoch 1160/2000, Train Loss: 5.500491253857308, Val Loss: 4.338915381285253, Val MAE: 1.239935040473938\n",
      "Epoch 1161/2000, Train Loss: 5.500094581877757, Val Loss: 4.338824030723389, Val MAE: 1.2402032613754272\n",
      "Epoch 1162/2000, Train Loss: 5.499785782580442, Val Loss: 4.338573479974592, Val MAE: 1.240289568901062\n",
      "Epoch 1163/2000, Train Loss: 5.499421531808172, Val Loss: 4.338516512056729, Val MAE: 1.240531086921692\n",
      "Epoch 1164/2000, Train Loss: 5.499166655094129, Val Loss: 4.338294632122055, Val MAE: 1.2406164407730103\n",
      "Epoch 1165/2000, Train Loss: 5.49878450563285, Val Loss: 4.33817404653575, Val MAE: 1.2407735586166382\n",
      "Epoch 1166/2000, Train Loss: 5.498437009577446, Val Loss: 4.338115558545063, Val MAE: 1.2410074472427368\n",
      "Epoch 1167/2000, Train Loss: 5.498135288494425, Val Loss: 4.337999250645842, Val MAE: 1.2411705255508423\n",
      "Epoch 1168/2000, Train Loss: 5.497837691522798, Val Loss: 4.337828012831039, Val MAE: 1.2413060665130615\n",
      "Epoch 1169/2000, Train Loss: 5.497511725343892, Val Loss: 4.3376200345655285, Val MAE: 1.2414414882659912\n",
      "Epoch 1170/2000, Train Loss: 5.497225175967641, Val Loss: 4.3372983504388785, Val MAE: 1.241481065750122\n",
      "Epoch 1171/2000, Train Loss: 5.496950144805421, Val Loss: 4.337239005691833, Val MAE: 1.241684913635254\n",
      "Epoch 1172/2000, Train Loss: 5.496545161453312, Val Loss: 4.337076803491459, Val MAE: 1.2418239116668701\n",
      "Epoch 1173/2000, Train Loss: 5.496253163877776, Val Loss: 4.3369551712798105, Val MAE: 1.2419878244400024\n",
      "Epoch 1174/2000, Train Loss: 5.4959744799434675, Val Loss: 4.336623085108963, Val MAE: 1.242035984992981\n",
      "Epoch 1175/2000, Train Loss: 5.495610836701535, Val Loss: 4.336562027345907, Val MAE: 1.2422446012496948\n",
      "Epoch 1176/2000, Train Loss: 5.495319318659778, Val Loss: 4.3363658953693, Val MAE: 1.2423502206802368\n",
      "Epoch 1177/2000, Train Loss: 5.494976685311233, Val Loss: 4.33628442149173, Val MAE: 1.242536187171936\n",
      "Epoch 1178/2000, Train Loss: 5.49471965111362, Val Loss: 4.3361535077562205, Val MAE: 1.2427006959915161\n",
      "Epoch 1179/2000, Train Loss: 5.494364751854478, Val Loss: 4.335930169836895, Val MAE: 1.2428163290023804\n",
      "Epoch 1180/2000, Train Loss: 5.49420003920747, Val Loss: 4.335937170744748, Val MAE: 1.2431156635284424\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1181/2000, Train Loss: 5.4937396856626375, Val Loss: 4.335794852828389, Val MAE: 1.243262767791748\n",
      "Epoch 1182/2000, Train Loss: 5.493471642924173, Val Loss: 4.3357305090908, Val MAE: 1.243486762046814\n",
      "Epoch 1183/2000, Train Loss: 5.493121719955468, Val Loss: 4.33562110165755, Val MAE: 1.2436552047729492\n",
      "Epoch 1184/2000, Train Loss: 5.492765317655019, Val Loss: 4.335341522701689, Val MAE: 1.2437317371368408\n",
      "Epoch 1185/2000, Train Loss: 5.492455455984531, Val Loss: 4.335252475436475, Val MAE: 1.243914246559143\n",
      "Epoch 1186/2000, Train Loss: 5.492119665841707, Val Loss: 4.3351582973121525, Val MAE: 1.2440823316574097\n",
      "Epoch 1187/2000, Train Loss: 5.491955652623764, Val Loss: 4.334980348155305, Val MAE: 1.2442196607589722\n",
      "Epoch 1188/2000, Train Loss: 5.49150473562864, Val Loss: 4.334853157354099, Val MAE: 1.2443732023239136\n",
      "Epoch 1189/2000, Train Loss: 5.491198308010369, Val Loss: 4.334700573732456, Val MAE: 1.2445157766342163\n",
      "Epoch 1190/2000, Train Loss: 5.490894380486142, Val Loss: 4.334633910078723, Val MAE: 1.2447015047073364\n",
      "Epoch 1191/2000, Train Loss: 5.490633811819107, Val Loss: 4.334571112262773, Val MAE: 1.2449166774749756\n",
      "Epoch 1192/2000, Train Loss: 5.490262822502303, Val Loss: 4.334381849231484, Val MAE: 1.2450201511383057\n",
      "Epoch 1193/2000, Train Loss: 5.489961403562991, Val Loss: 4.334272054253935, Val MAE: 1.245186686515808\n",
      "Epoch 1194/2000, Train Loss: 5.489676679887006, Val Loss: 4.334129709582608, Val MAE: 1.2453259229660034\n",
      "Epoch 1195/2000, Train Loss: 5.4893365921147526, Val Loss: 4.334052977751235, Val MAE: 1.245513677597046\n",
      "Epoch 1196/2000, Train Loss: 5.489031864588792, Val Loss: 4.3338974885575405, Val MAE: 1.2456731796264648\n",
      "Epoch 1197/2000, Train Loss: 5.488825109265687, Val Loss: 4.333977892142427, Val MAE: 1.2459651231765747\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1198/2000, Train Loss: 5.48833453897009, Val Loss: 4.333815857788196, Val MAE: 1.2460917234420776\n",
      "Epoch 1199/2000, Train Loss: 5.488058666543172, Val Loss: 4.333593046316156, Val MAE: 1.246176838874817\n",
      "Epoch 1200/2000, Train Loss: 5.487749379436237, Val Loss: 4.333501256478799, Val MAE: 1.2463527917861938\n",
      "Epoch 1201/2000, Train Loss: 5.487410510386022, Val Loss: 4.333378676727817, Val MAE: 1.2465158700942993\n",
      "Epoch 1202/2000, Train Loss: 5.48706863055735, Val Loss: 4.3333552966045366, Val MAE: 1.246760368347168\n",
      "Epoch 1203/2000, Train Loss: 5.486799761583206, Val Loss: 4.333261614178752, Val MAE: 1.2469500303268433\n",
      "Epoch 1204/2000, Train Loss: 5.486407954943208, Val Loss: 4.333099409964707, Val MAE: 1.2471033334732056\n",
      "Epoch 1205/2000, Train Loss: 5.48609502787895, Val Loss: 4.3330107412561105, Val MAE: 1.2472811937332153\n",
      "Epoch 1206/2000, Train Loss: 5.485775554793859, Val Loss: 4.332922029126066, Val MAE: 1.2474675178527832\n",
      "Epoch 1207/2000, Train Loss: 5.485425587935306, Val Loss: 4.332705660452982, Val MAE: 1.2475736141204834\n",
      "Epoch 1208/2000, Train Loss: 5.485133547800798, Val Loss: 4.332520303915481, Val MAE: 1.2476887702941895\n",
      "Epoch 1209/2000, Train Loss: 5.484891468993215, Val Loss: 4.332386941221115, Val MAE: 1.2478580474853516\n",
      "Epoch 1210/2000, Train Loss: 5.484521267752566, Val Loss: 4.332271623618163, Val MAE: 1.2480303049087524\n",
      "Epoch 1211/2000, Train Loss: 5.484169592537485, Val Loss: 4.332255397643055, Val MAE: 1.248280644416809\n",
      "Epoch 1212/2000, Train Loss: 5.483726314003122, Val Loss: 4.332202619253784, Val MAE: 1.2485301494598389\n",
      "Epoch 1213/2000, Train Loss: 5.483352751423156, Val Loss: 4.332248115646946, Val MAE: 1.2487908601760864\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1214/2000, Train Loss: 5.48306584767358, Val Loss: 4.332110650092363, Val MAE: 1.2489477396011353\n",
      "Epoch 1215/2000, Train Loss: 5.482758762407903, Val Loss: 4.332049205181029, Val MAE: 1.2491769790649414\n",
      "Epoch 1216/2000, Train Loss: 5.482438486749408, Val Loss: 4.331916126514877, Val MAE: 1.2493008375167847\n",
      "Epoch 1217/2000, Train Loss: 5.482143146199481, Val Loss: 4.331757646973606, Val MAE: 1.2494497299194336\n",
      "Epoch 1218/2000, Train Loss: 5.481727055937935, Val Loss: 4.3316975104177855, Val MAE: 1.2497496604919434\n",
      "Epoch 1219/2000, Train Loss: 5.481471393019101, Val Loss: 4.331490574803976, Val MAE: 1.2498599290847778\n",
      "Epoch 1220/2000, Train Loss: 5.481101907173668, Val Loss: 4.331400512843518, Val MAE: 1.2500386238098145\n",
      "Epoch 1221/2000, Train Loss: 5.480793655383605, Val Loss: 4.331252857586285, Val MAE: 1.250172734260559\n",
      "Epoch 1222/2000, Train Loss: 5.480558740785825, Val Loss: 4.331237551751169, Val MAE: 1.2503712177276611\n",
      "Epoch 1223/2000, Train Loss: 5.480167944234172, Val Loss: 4.331000969122659, Val MAE: 1.250464916229248\n",
      "Epoch 1224/2000, Train Loss: 5.47996850690678, Val Loss: 4.330766175298003, Val MAE: 1.2505598068237305\n",
      "Epoch 1225/2000, Train Loss: 5.4795512980120415, Val Loss: 4.330695235024433, Val MAE: 1.2508361339569092\n",
      "Epoch 1226/2000, Train Loss: 5.479217980934566, Val Loss: 4.330516614261511, Val MAE: 1.2509881258010864\n",
      "Epoch 1227/2000, Train Loss: 5.4789158447112385, Val Loss: 4.3303734679949715, Val MAE: 1.2511231899261475\n",
      "Epoch 1228/2000, Train Loss: 5.478611263209683, Val Loss: 4.330220726206227, Val MAE: 1.2512381076812744\n",
      "Epoch 1229/2000, Train Loss: 5.478221452924279, Val Loss: 4.329660603645686, Val MAE: 1.2511788606643677\n",
      "Epoch 1230/2000, Train Loss: 5.478014262692419, Val Loss: 4.329537053587469, Val MAE: 1.251381754875183\n",
      "Epoch 1231/2000, Train Loss: 5.477689238680692, Val Loss: 4.329397133033018, Val MAE: 1.2515010833740234\n",
      "Epoch 1232/2000, Train Loss: 5.477377101895218, Val Loss: 4.329340960266622, Val MAE: 1.251711130142212\n",
      "Epoch 1233/2000, Train Loss: 5.477082899970681, Val Loss: 4.329204093329273, Val MAE: 1.251844048500061\n",
      "Epoch 1234/2000, Train Loss: 5.476783661872102, Val Loss: 4.329065864378805, Val MAE: 1.2519700527191162\n",
      "Epoch 1235/2000, Train Loss: 5.476515395778947, Val Loss: 4.32894935403187, Val MAE: 1.2521296739578247\n",
      "Epoch 1236/2000, Train Loss: 5.476218157365058, Val Loss: 4.328900877117842, Val MAE: 1.2523112297058105\n",
      "Epoch 1237/2000, Train Loss: 5.475929744855476, Val Loss: 4.328872878057463, Val MAE: 1.2525262832641602\n",
      "Epoch 1238/2000, Train Loss: 5.475661765778214, Val Loss: 4.328790346277995, Val MAE: 1.2526639699935913\n",
      "Epoch 1239/2000, Train Loss: 5.475356210877482, Val Loss: 4.328667128670054, Val MAE: 1.252805233001709\n",
      "Epoch 1240/2000, Train Loss: 5.475075485181883, Val Loss: 4.328520751059861, Val MAE: 1.2529200315475464\n",
      "Epoch 1241/2000, Train Loss: 5.474789977631591, Val Loss: 4.3285059894997255, Val MAE: 1.2531516551971436\n",
      "Epoch 1242/2000, Train Loss: 5.4744356574952695, Val Loss: 4.32834794864327, Val MAE: 1.2532559633255005\n",
      "Epoch 1243/2000, Train Loss: 5.474145724695298, Val Loss: 4.328228459406543, Val MAE: 1.2534388303756714\n",
      "Epoch 1244/2000, Train Loss: 5.47379908044699, Val Loss: 4.328174488125621, Val MAE: 1.2536308765411377\n",
      "Epoch 1245/2000, Train Loss: 5.4735116623865085, Val Loss: 4.328064734065855, Val MAE: 1.253800392150879\n",
      "Epoch 1246/2000, Train Loss: 5.473233280985292, Val Loss: 4.3279987952059455, Val MAE: 1.253974199295044\n",
      "Epoch 1247/2000, Train Loss: 5.472915092980062, Val Loss: 4.32784216812065, Val MAE: 1.2541061639785767\n",
      "Epoch 1248/2000, Train Loss: 5.472838309923312, Val Loss: 4.327955436706543, Val MAE: 1.254399299621582\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1249/2000, Train Loss: 5.472294768646894, Val Loss: 4.3276729666434965, Val MAE: 1.2544325590133667\n",
      "Epoch 1250/2000, Train Loss: 5.472017707253767, Val Loss: 4.327626221479328, Val MAE: 1.2546252012252808\n",
      "Epoch 1251/2000, Train Loss: 5.471728974357596, Val Loss: 4.327477114103936, Val MAE: 1.2547357082366943\n",
      "Epoch 1252/2000, Train Loss: 5.47143140925725, Val Loss: 4.3274252173465655, Val MAE: 1.2549114227294922\n",
      "Epoch 1253/2000, Train Loss: 5.471137551175265, Val Loss: 4.327311229336638, Val MAE: 1.2550616264343262\n",
      "Epoch 1254/2000, Train Loss: 5.470830944417606, Val Loss: 4.327267544991798, Val MAE: 1.255271077156067\n",
      "Epoch 1255/2000, Train Loss: 5.470538723969748, Val Loss: 4.3271143751101455, Val MAE: 1.2553789615631104\n",
      "Epoch 1256/2000, Train Loss: 5.4702690064256165, Val Loss: 4.327009065986217, Val MAE: 1.2555259466171265\n",
      "Epoch 1257/2000, Train Loss: 5.470028811051582, Val Loss: 4.326941566424327, Val MAE: 1.255702257156372\n",
      "Epoch 1258/2000, Train Loss: 5.4696699858827635, Val Loss: 4.326791236918789, Val MAE: 1.2558143138885498\n",
      "Epoch 1259/2000, Train Loss: 5.4694211525775716, Val Loss: 4.326601884493956, Val MAE: 1.2559101581573486\n",
      "Epoch 1260/2000, Train Loss: 5.469110034066202, Val Loss: 4.326572133949748, Val MAE: 1.2561315298080444\n",
      "Epoch 1261/2000, Train Loss: 5.468790803517269, Val Loss: 4.3264390618146, Val MAE: 1.256244421005249\n",
      "Epoch 1262/2000, Train Loss: 5.46853784912276, Val Loss: 4.3263696607504345, Val MAE: 1.2564457654953003\n",
      "Epoch 1263/2000, Train Loss: 5.468233153154251, Val Loss: 4.3262507346702055, Val MAE: 1.2565909624099731\n",
      "Epoch 1264/2000, Train Loss: 5.467896896293866, Val Loss: 4.326235859727, Val MAE: 1.2568596601486206\n",
      "Epoch 1265/2000, Train Loss: 5.467555308118811, Val Loss: 4.32612792468957, Val MAE: 1.2570027112960815\n",
      "Epoch 1266/2000, Train Loss: 5.467301004762397, Val Loss: 4.325930371706013, Val MAE: 1.2571110725402832\n",
      "Epoch 1267/2000, Train Loss: 5.466990833907343, Val Loss: 4.3258376639437035, Val MAE: 1.2572604417800903\n",
      "Epoch 1268/2000, Train Loss: 5.466703798953159, Val Loss: 4.32572661154979, Val MAE: 1.2574106454849243\n",
      "Epoch 1269/2000, Train Loss: 5.466470862327611, Val Loss: 4.325552472660133, Val MAE: 1.25751793384552\n",
      "Epoch 1270/2000, Train Loss: 5.46613232226528, Val Loss: 4.325417249327576, Val MAE: 1.2576539516448975\n",
      "Epoch 1271/2000, Train Loss: 5.465885197316615, Val Loss: 4.325242831041147, Val MAE: 1.2577581405639648\n",
      "Epoch 1272/2000, Train Loss: 5.465601610133131, Val Loss: 4.325093963688558, Val MAE: 1.2578812837600708\n",
      "Epoch 1273/2000, Train Loss: 5.465331535815449, Val Loss: 4.325238507428954, Val MAE: 1.2581886053085327\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1274/2000, Train Loss: 5.464947592318872, Val Loss: 4.325110448413604, Val MAE: 1.2583603858947754\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1275/2000, Train Loss: 5.4646532210796, Val Loss: 4.325030635975234, Val MAE: 1.2585338354110718\n",
      "Epoch 1276/2000, Train Loss: 5.464397790465451, Val Loss: 4.324889459373715, Val MAE: 1.2586592435836792\n",
      "Epoch 1277/2000, Train Loss: 5.464136413776559, Val Loss: 4.324695789344139, Val MAE: 1.2587672472000122\n",
      "Epoch 1278/2000, Train Loss: 5.463794076367748, Val Loss: 4.3244566451456095, Val MAE: 1.2588335275650024\n",
      "Epoch 1279/2000, Train Loss: 5.463515064831643, Val Loss: 4.324382784835121, Val MAE: 1.2589874267578125\n",
      "Epoch 1280/2000, Train Loss: 5.463219330667892, Val Loss: 4.324270013927876, Val MAE: 1.2591485977172852\n",
      "Epoch 1281/2000, Train Loss: 5.462970168049361, Val Loss: 4.324108899310902, Val MAE: 1.2592766284942627\n",
      "Epoch 1282/2000, Train Loss: 5.462639667965506, Val Loss: 4.3240537139276665, Val MAE: 1.2594527006149292\n",
      "Epoch 1283/2000, Train Loss: 5.462390077560443, Val Loss: 4.323946913318323, Val MAE: 1.2596186399459839\n",
      "Epoch 1284/2000, Train Loss: 5.462080542829219, Val Loss: 4.32378542683683, Val MAE: 1.2597371339797974\n",
      "Epoch 1285/2000, Train Loss: 5.461796492756622, Val Loss: 4.323653018904162, Val MAE: 1.2598826885223389\n",
      "Epoch 1286/2000, Train Loss: 5.461517435340167, Val Loss: 4.323489002655219, Val MAE: 1.260008692741394\n",
      "Epoch 1287/2000, Train Loss: 5.461348594583327, Val Loss: 4.3234737852269465, Val MAE: 1.2602365016937256\n",
      "Epoch 1288/2000, Train Loss: 5.460958445127966, Val Loss: 4.323232465734084, Val MAE: 1.2603020668029785\n",
      "Epoch 1289/2000, Train Loss: 5.4607393743467405, Val Loss: 4.323090975201345, Val MAE: 1.2604353427886963\n",
      "Epoch 1290/2000, Train Loss: 5.460376023130372, Val Loss: 4.322968929737538, Val MAE: 1.260597825050354\n",
      "Epoch 1291/2000, Train Loss: 5.460127390677025, Val Loss: 4.322849412544354, Val MAE: 1.2607489824295044\n",
      "Epoch 1292/2000, Train Loss: 5.459786641988293, Val Loss: 4.3228186678443405, Val MAE: 1.2609803676605225\n",
      "Epoch 1293/2000, Train Loss: 5.459543962002544, Val Loss: 4.322681578872977, Val MAE: 1.2611039876937866\n",
      "Epoch 1294/2000, Train Loss: 5.459252463444463, Val Loss: 4.322502488085815, Val MAE: 1.2612049579620361\n",
      "Epoch 1295/2000, Train Loss: 5.458951027642546, Val Loss: 4.322321573920078, Val MAE: 1.26134192943573\n",
      "Epoch 1296/2000, Train Loss: 5.4585337259468165, Val Loss: 4.322224107149753, Val MAE: 1.2615727186203003\n",
      "Epoch 1297/2000, Train Loss: 5.458215200767944, Val Loss: 4.322221546680541, Val MAE: 1.2618495225906372\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1298/2000, Train Loss: 5.457937858405984, Val Loss: 4.322165907288457, Val MAE: 1.2620271444320679\n",
      "Epoch 1299/2000, Train Loss: 5.457654331478054, Val Loss: 4.32205832412651, Val MAE: 1.262178897857666\n",
      "Epoch 1300/2000, Train Loss: 5.4573792399469525, Val Loss: 4.321867835051841, Val MAE: 1.2622947692871094\n",
      "Epoch 1301/2000, Train Loss: 5.457104074415662, Val Loss: 4.321783210155932, Val MAE: 1.2624603509902954\n",
      "Epoch 1302/2000, Train Loss: 5.456715067863092, Val Loss: 4.321687305577703, Val MAE: 1.2626664638519287\n",
      "Epoch 1303/2000, Train Loss: 5.456448205361686, Val Loss: 4.321508784994886, Val MAE: 1.262782096862793\n",
      "Epoch 1304/2000, Train Loss: 5.456099588476272, Val Loss: 4.321328524238354, Val MAE: 1.2629106044769287\n",
      "Epoch 1305/2000, Train Loss: 5.455794574689939, Val Loss: 4.321257619385246, Val MAE: 1.2630891799926758\n",
      "Epoch 1306/2000, Train Loss: 5.455584420056871, Val Loss: 4.321045220448627, Val MAE: 1.263157844543457\n",
      "Epoch 1307/2000, Train Loss: 5.45531395779757, Val Loss: 4.321043713619043, Val MAE: 1.2633936405181885\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1308/2000, Train Loss: 5.454955205530533, Val Loss: 4.320789207450979, Val MAE: 1.2634594440460205\n",
      "Epoch 1309/2000, Train Loss: 5.454679953779984, Val Loss: 4.320709127952924, Val MAE: 1.2636250257492065\n",
      "Epoch 1310/2000, Train Loss: 5.454433818456721, Val Loss: 4.320551342369469, Val MAE: 1.263768196105957\n",
      "Epoch 1311/2000, Train Loss: 5.454159380028289, Val Loss: 4.320411840584632, Val MAE: 1.2638968229293823\n",
      "Epoch 1312/2000, Train Loss: 5.453897906028536, Val Loss: 4.320394509372948, Val MAE: 1.2640973329544067\n",
      "Epoch 1313/2000, Train Loss: 5.453538226263014, Val Loss: 4.320245207275625, Val MAE: 1.2642418146133423\n",
      "Epoch 1314/2000, Train Loss: 5.453269316700059, Val Loss: 4.32009905916345, Val MAE: 1.2643611431121826\n",
      "Epoch 1315/2000, Train Loss: 5.452984855997023, Val Loss: 4.319910379378377, Val MAE: 1.264474630355835\n",
      "Epoch 1316/2000, Train Loss: 5.452709971463625, Val Loss: 4.319888371873546, Val MAE: 1.2647022008895874\n",
      "Epoch 1317/2000, Train Loss: 5.452385424079836, Val Loss: 4.3197009134131505, Val MAE: 1.2648324966430664\n",
      "Epoch 1318/2000, Train Loss: 5.452120291461439, Val Loss: 4.319546002478481, Val MAE: 1.2649562358856201\n",
      "Epoch 1319/2000, Train Loss: 5.4517857325430406, Val Loss: 4.319501104044753, Val MAE: 1.265161395072937\n",
      "Epoch 1320/2000, Train Loss: 5.451499291403617, Val Loss: 4.319512119164338, Val MAE: 1.2653944492340088\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1321/2000, Train Loss: 5.451217020543615, Val Loss: 4.3193471967234265, Val MAE: 1.265520691871643\n",
      "Epoch 1322/2000, Train Loss: 5.4509543926221165, Val Loss: 4.3192545790930055, Val MAE: 1.2656866312026978\n",
      "Epoch 1323/2000, Train Loss: 5.450662787730534, Val Loss: 4.318615581028096, Val MAE: 1.2656172513961792\n",
      "Epoch 1324/2000, Train Loss: 5.450381564647657, Val Loss: 4.318605148980209, Val MAE: 1.2658116817474365\n",
      "Epoch 1325/2000, Train Loss: 5.4501089132252885, Val Loss: 4.3185170914273, Val MAE: 1.265993356704712\n",
      "Epoch 1326/2000, Train Loss: 5.449787559449765, Val Loss: 4.318479321722512, Val MAE: 1.266169548034668\n",
      "Epoch 1327/2000, Train Loss: 5.449532514429316, Val Loss: 4.318446454123871, Val MAE: 1.266370177268982\n",
      "Epoch 1328/2000, Train Loss: 5.4492648075597705, Val Loss: 4.318322128755552, Val MAE: 1.2665003538131714\n",
      "Epoch 1329/2000, Train Loss: 5.448967676638813, Val Loss: 4.318085659839012, Val MAE: 1.2665624618530273\n",
      "Epoch 1330/2000, Train Loss: 5.448690936271561, Val Loss: 4.317968144502726, Val MAE: 1.266711950302124\n",
      "Epoch 1331/2000, Train Loss: 5.448424877131041, Val Loss: 4.317942313970746, Val MAE: 1.2669110298156738\n",
      "Epoch 1332/2000, Train Loss: 5.448203316809793, Val Loss: 4.31784304705289, Val MAE: 1.2670665979385376\n",
      "Epoch 1333/2000, Train Loss: 5.447983277011401, Val Loss: 4.3177973015217095, Val MAE: 1.2672218084335327\n",
      "Epoch 1334/2000, Train Loss: 5.447711064923386, Val Loss: 4.317575751003381, Val MAE: 1.2673019170761108\n",
      "Epoch 1335/2000, Train Loss: 5.447412573231178, Val Loss: 4.317524695906553, Val MAE: 1.2674710750579834\n",
      "Epoch 1336/2000, Train Loss: 5.447184158189806, Val Loss: 4.3173373572461236, Val MAE: 1.2675774097442627\n",
      "Epoch 1337/2000, Train Loss: 5.446933082782906, Val Loss: 4.317280257486545, Val MAE: 1.2677114009857178\n",
      "Epoch 1338/2000, Train Loss: 5.446678587640875, Val Loss: 4.317043626576931, Val MAE: 1.2677780389785767\n",
      "Epoch 1339/2000, Train Loss: 5.446496469554217, Val Loss: 4.31693749588889, Val MAE: 1.2679166793823242\n",
      "Epoch 1340/2000, Train Loss: 5.446137410520275, Val Loss: 4.317071987299232, Val MAE: 1.268231749534607\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1341/2000, Train Loss: 5.4458576991758925, Val Loss: 4.316978790819108, Val MAE: 1.268351674079895\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1342/2000, Train Loss: 5.445581063465469, Val Loss: 4.316871418501879, Val MAE: 1.2684977054595947\n",
      "Epoch 1343/2000, Train Loss: 5.44534961332955, Val Loss: 4.316747738058503, Val MAE: 1.2686116695404053\n",
      "Epoch 1344/2000, Train Loss: 5.445113534302496, Val Loss: 4.316676441291431, Val MAE: 1.2687675952911377\n",
      "Epoch 1345/2000, Train Loss: 5.444832481199791, Val Loss: 4.3166409293005055, Val MAE: 1.2689751386642456\n",
      "Epoch 1346/2000, Train Loss: 5.4445879863874405, Val Loss: 4.316542129339399, Val MAE: 1.269110083580017\n",
      "Epoch 1347/2000, Train Loss: 5.444298639312363, Val Loss: 4.316428252960647, Val MAE: 1.269248604774475\n",
      "Epoch 1348/2000, Train Loss: 5.444011313901769, Val Loss: 4.3163199257072025, Val MAE: 1.269407868385315\n",
      "Epoch 1349/2000, Train Loss: 5.443805247870697, Val Loss: 4.316206729949058, Val MAE: 1.2695579528808594\n",
      "Epoch 1350/2000, Train Loss: 5.443483314920048, Val Loss: 4.316121859929046, Val MAE: 1.269709825515747\n",
      "Epoch 1351/2000, Train Loss: 5.4432444304646275, Val Loss: 4.316011027642736, Val MAE: 1.2698416709899902\n",
      "Epoch 1352/2000, Train Loss: 5.443005799503297, Val Loss: 4.315949347722638, Val MAE: 1.2700237035751343\n",
      "Epoch 1353/2000, Train Loss: 5.442735252997805, Val Loss: 4.315860522518287, Val MAE: 1.270172119140625\n",
      "Epoch 1354/2000, Train Loss: 5.44246038537641, Val Loss: 4.315724201943423, Val MAE: 1.2702959775924683\n",
      "Epoch 1355/2000, Train Loss: 5.442214979210435, Val Loss: 4.315631284907058, Val MAE: 1.2704603672027588\n",
      "Epoch 1356/2000, Train Loss: 5.4419587487921515, Val Loss: 4.3155559524491025, Val MAE: 1.270627737045288\n",
      "Epoch 1357/2000, Train Loss: 5.441665393513934, Val Loss: 4.315295838517649, Val MAE: 1.2706631422042847\n",
      "Epoch 1358/2000, Train Loss: 5.441450313779754, Val Loss: 4.315258869743562, Val MAE: 1.2708740234375\n",
      "Epoch 1359/2000, Train Loss: 5.441157122298075, Val Loss: 4.315136046157227, Val MAE: 1.2710071802139282\n",
      "Epoch 1360/2000, Train Loss: 5.4410670488933315, Val Loss: 4.315245932823903, Val MAE: 1.27131986618042\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1361/2000, Train Loss: 5.4405814273495015, Val Loss: 4.315047277120857, Val MAE: 1.2713897228240967\n",
      "Epoch 1362/2000, Train Loss: 5.440234538638461, Val Loss: 4.314916681505001, Val MAE: 1.2715833187103271\n",
      "Epoch 1363/2000, Train Loss: 5.439985642753042, Val Loss: 4.314839504578629, Val MAE: 1.2717515230178833\n",
      "Epoch 1364/2000, Train Loss: 5.439666133681995, Val Loss: 4.314680194747341, Val MAE: 1.2718396186828613\n",
      "Epoch 1365/2000, Train Loss: 5.439431175054886, Val Loss: 4.314560879095717, Val MAE: 1.2719827890396118\n",
      "Epoch 1366/2000, Train Loss: 5.439243575526102, Val Loss: 4.314428668301384, Val MAE: 1.2720988988876343\n",
      "Epoch 1367/2000, Train Loss: 5.438927633714006, Val Loss: 4.314370505968193, Val MAE: 1.2723612785339355\n",
      "Epoch 1368/2000, Train Loss: 5.438586945600471, Val Loss: 4.3141793859031825, Val MAE: 1.2725096940994263\n",
      "Epoch 1369/2000, Train Loss: 5.43830121670052, Val Loss: 4.3140936561398675, Val MAE: 1.2726590633392334\n",
      "Epoch 1370/2000, Train Loss: 5.43806453056157, Val Loss: 4.313948594785488, Val MAE: 1.2727771997451782\n",
      "Epoch 1371/2000, Train Loss: 5.437820728804318, Val Loss: 4.313806010259165, Val MAE: 1.2728885412216187\n",
      "Epoch 1372/2000, Train Loss: 5.437523403273265, Val Loss: 4.313819695297663, Val MAE: 1.273105263710022\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1373/2000, Train Loss: 5.437235206226105, Val Loss: 4.3136605714624, Val MAE: 1.2732348442077637\n",
      "Epoch 1374/2000, Train Loss: 5.43694318438097, Val Loss: 4.31357419988862, Val MAE: 1.273419737815857\n",
      "Epoch 1375/2000, Train Loss: 5.43670672973307, Val Loss: 4.313420178372044, Val MAE: 1.2735495567321777\n",
      "Epoch 1376/2000, Train Loss: 5.436414435761582, Val Loss: 4.313277635783763, Val MAE: 1.2736668586730957\n",
      "Epoch 1377/2000, Train Loss: 5.436144137066352, Val Loss: 4.313165156878867, Val MAE: 1.2738206386566162\n",
      "Epoch 1378/2000, Train Loss: 5.435912500956501, Val Loss: 4.313037053851394, Val MAE: 1.273947834968567\n",
      "Epoch 1379/2000, Train Loss: 5.435577928183045, Val Loss: 4.31292011219639, Val MAE: 1.2741119861602783\n",
      "Epoch 1380/2000, Train Loss: 5.4352964390644605, Val Loss: 4.3129178379838535, Val MAE: 1.274381399154663\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1381/2000, Train Loss: 5.435002556085214, Val Loss: 4.312786310857481, Val MAE: 1.2745155096054077\n",
      "Epoch 1382/2000, Train Loss: 5.434674021623762, Val Loss: 4.312814112155287, Val MAE: 1.2747386693954468\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1383/2000, Train Loss: 5.434402352748163, Val Loss: 4.312661418385871, Val MAE: 1.2748981714248657\n",
      "Epoch 1384/2000, Train Loss: 5.43411589933446, Val Loss: 4.312395936344658, Val MAE: 1.2749650478363037\n",
      "Epoch 1385/2000, Train Loss: 5.433933647857999, Val Loss: 4.312147980525687, Val MAE: 1.2750099897384644\n",
      "Epoch 1386/2000, Train Loss: 5.433546428189449, Val Loss: 4.312137649550631, Val MAE: 1.2752331495285034\n",
      "Epoch 1387/2000, Train Loss: 5.433340069098331, Val Loss: 4.312098566489714, Val MAE: 1.275413990020752\n",
      "Epoch 1388/2000, Train Loss: 5.433095955811499, Val Loss: 4.311908049089414, Val MAE: 1.2755249738693237\n",
      "Epoch 1389/2000, Train Loss: 5.432807188510151, Val Loss: 4.3117474292044164, Val MAE: 1.2756447792053223\n",
      "Epoch 1390/2000, Train Loss: 5.432770715674819, Val Loss: 4.311486445219667, Val MAE: 1.2757102251052856\n",
      "Epoch 1391/2000, Train Loss: 5.432201560313541, Val Loss: 4.311623338012545, Val MAE: 1.276016354560852\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1392/2000, Train Loss: 5.432033703591262, Val Loss: 4.311578606485247, Val MAE: 1.2762117385864258\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1393/2000, Train Loss: 5.431699452645693, Val Loss: 4.3113896473705235, Val MAE: 1.276324987411499\n",
      "Epoch 1394/2000, Train Loss: 5.431470357788242, Val Loss: 4.311186914709774, Val MAE: 1.2764090299606323\n",
      "Epoch 1395/2000, Train Loss: 5.431194457546598, Val Loss: 4.31110908603883, Val MAE: 1.2765666246414185\n",
      "Epoch 1396/2000, Train Loss: 5.430904639585528, Val Loss: 4.311012000664397, Val MAE: 1.2767322063446045\n",
      "Epoch 1397/2000, Train Loss: 5.430675338099416, Val Loss: 4.310914675408118, Val MAE: 1.27688467502594\n",
      "Epoch 1398/2000, Train Loss: 5.430445418156998, Val Loss: 4.310820436585057, Val MAE: 1.2770321369171143\n",
      "Epoch 1399/2000, Train Loss: 5.430197324470127, Val Loss: 4.31063314190319, Val MAE: 1.2771309614181519\n",
      "Epoch 1400/2000, Train Loss: 5.429849330051083, Val Loss: 4.310619450300126, Val MAE: 1.2773716449737549\n",
      "Epoch 1401/2000, Train Loss: 5.429612583749566, Val Loss: 4.310404352387329, Val MAE: 1.2775503396987915\n",
      "Epoch 1402/2000, Train Loss: 5.429260620832816, Val Loss: 4.310401408535403, Val MAE: 1.2777950763702393\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1403/2000, Train Loss: 5.4289972439942975, Val Loss: 4.310220867220883, Val MAE: 1.2779102325439453\n",
      "Epoch 1404/2000, Train Loss: 5.428813625237499, Val Loss: 4.310215725649048, Val MAE: 1.278110146522522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1405/2000, Train Loss: 5.4285008315921015, Val Loss: 4.310001384124562, Val MAE: 1.2781883478164673\n",
      "Epoch 1406/2000, Train Loss: 5.428161258072637, Val Loss: 4.310023340341207, Val MAE: 1.2784074544906616\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1407/2000, Train Loss: 5.427935206016029, Val Loss: 4.310189369347718, Val MAE: 1.278771162033081\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1408/2000, Train Loss: 5.427559074880366, Val Loss: 4.310340851558759, Val MAE: 1.279060959815979\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1409/2000, Train Loss: 5.427346408460144, Val Loss: 4.3102677961459035, Val MAE: 1.2792150974273682\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1410/2000, Train Loss: 5.427069231054154, Val Loss: 4.3100564630584675, Val MAE: 1.2792961597442627\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1411/2000, Train Loss: 5.426841374864444, Val Loss: 4.310142271985879, Val MAE: 1.2795467376708984\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1412/2000, Train Loss: 5.426600163886774, Val Loss: 4.309997349104902, Val MAE: 1.279649019241333\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1413/2000, Train Loss: 5.426359247538303, Val Loss: 4.3098167152152405, Val MAE: 1.279752254486084\n",
      "Epoch 1414/2000, Train Loss: 5.426177469504977, Val Loss: 4.309765518691625, Val MAE: 1.2799086570739746\n",
      "Epoch 1415/2000, Train Loss: 5.425940177369974, Val Loss: 4.309547637295616, Val MAE: 1.2799797058105469\n",
      "Epoch 1416/2000, Train Loss: 5.425620485393566, Val Loss: 4.309563537747473, Val MAE: 1.280184268951416\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1417/2000, Train Loss: 5.425430249498341, Val Loss: 4.309455345275703, Val MAE: 1.2803168296813965\n",
      "Epoch 1418/2000, Train Loss: 5.425168527792936, Val Loss: 4.309248126707636, Val MAE: 1.2804028987884521\n",
      "Epoch 1419/2000, Train Loss: 5.424923014622211, Val Loss: 4.309181268848814, Val MAE: 1.2805558443069458\n",
      "Epoch 1420/2000, Train Loss: 5.424659187250223, Val Loss: 4.309049062269765, Val MAE: 1.2806758880615234\n",
      "Epoch 1421/2000, Train Loss: 5.4244314542798655, Val Loss: 4.3089539040182085, Val MAE: 1.2808501720428467\n",
      "Epoch 1422/2000, Train Loss: 5.424174587365804, Val Loss: 4.308866481617227, Val MAE: 1.2809937000274658\n",
      "Epoch 1423/2000, Train Loss: 5.42393290643201, Val Loss: 4.308786442889287, Val MAE: 1.2811367511749268\n",
      "Epoch 1424/2000, Train Loss: 5.42370426859387, Val Loss: 4.308553999461032, Val MAE: 1.281209111213684\n",
      "Epoch 1425/2000, Train Loss: 5.423406897022646, Val Loss: 4.3083586481374665, Val MAE: 1.281300663948059\n",
      "Epoch 1426/2000, Train Loss: 5.42321987419902, Val Loss: 4.308184333830266, Val MAE: 1.2813827991485596\n",
      "Epoch 1427/2000, Train Loss: 5.422911349771176, Val Loss: 4.308247772422996, Val MAE: 1.28165864944458\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1428/2000, Train Loss: 5.422690139848059, Val Loss: 4.3080917783685635, Val MAE: 1.2817658185958862\n",
      "Epoch 1429/2000, Train Loss: 5.422378313708789, Val Loss: 4.308045687398932, Val MAE: 1.2819489240646362\n",
      "Epoch 1430/2000, Train Loss: 5.422195986540939, Val Loss: 4.308017860540936, Val MAE: 1.2821297645568848\n",
      "Epoch 1431/2000, Train Loss: 5.421895159946031, Val Loss: 4.307878823027955, Val MAE: 1.2822742462158203\n",
      "Epoch 1432/2000, Train Loss: 5.42165562171088, Val Loss: 4.307793728174927, Val MAE: 1.2824032306671143\n",
      "Epoch 1433/2000, Train Loss: 5.421395261648478, Val Loss: 4.30763373182969, Val MAE: 1.2825145721435547\n",
      "Epoch 1434/2000, Train Loss: 5.421120713355947, Val Loss: 4.307579090479795, Val MAE: 1.282698392868042\n",
      "Epoch 1435/2000, Train Loss: 5.420865225531568, Val Loss: 4.307546074661586, Val MAE: 1.2828993797302246\n",
      "Epoch 1436/2000, Train Loss: 5.4205969990508605, Val Loss: 4.307398316009087, Val MAE: 1.2830357551574707\n",
      "Epoch 1437/2000, Train Loss: 5.420325401978634, Val Loss: 4.30737549257171, Val MAE: 1.2832401990890503\n",
      "Epoch 1438/2000, Train Loss: 5.420122750463798, Val Loss: 4.307217131890692, Val MAE: 1.2833269834518433\n",
      "Epoch 1439/2000, Train Loss: 5.419866400464277, Val Loss: 4.307113790861121, Val MAE: 1.283469319343567\n",
      "Epoch 1440/2000, Train Loss: 5.419632895083955, Val Loss: 4.306915296077191, Val MAE: 1.2835683822631836\n",
      "Epoch 1441/2000, Train Loss: 5.419377569289364, Val Loss: 4.306860449467156, Val MAE: 1.2837402820587158\n",
      "Epoch 1442/2000, Train Loss: 5.419130563364014, Val Loss: 4.306695394634127, Val MAE: 1.2838610410690308\n",
      "Epoch 1443/2000, Train Loss: 5.418880788077616, Val Loss: 4.3065698565797765, Val MAE: 1.2839511632919312\n",
      "Epoch 1444/2000, Train Loss: 5.4186701318961035, Val Loss: 4.306544914049608, Val MAE: 1.2841559648513794\n",
      "Epoch 1445/2000, Train Loss: 5.418394984395567, Val Loss: 4.306389419989543, Val MAE: 1.284284234046936\n",
      "Epoch 1446/2000, Train Loss: 5.418155967724305, Val Loss: 4.306374407364978, Val MAE: 1.2845392227172852\n",
      "Epoch 1447/2000, Train Loss: 5.417707036489071, Val Loss: 4.306418551987893, Val MAE: 1.284807801246643\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1448/2000, Train Loss: 5.417487956432396, Val Loss: 4.306290047367414, Val MAE: 1.2849268913269043\n",
      "Epoch 1449/2000, Train Loss: 5.417242002439527, Val Loss: 4.306260649940452, Val MAE: 1.285114049911499\n",
      "Epoch 1450/2000, Train Loss: 5.416972831286804, Val Loss: 4.306195057284188, Val MAE: 1.2852803468704224\n",
      "Epoch 1451/2000, Train Loss: 5.416735402135507, Val Loss: 4.306043770788489, Val MAE: 1.2854129076004028\n",
      "Epoch 1452/2000, Train Loss: 5.416457882909433, Val Loss: 4.30603246518352, Val MAE: 1.285623550415039\n",
      "Epoch 1453/2000, Train Loss: 5.416107341437927, Val Loss: 4.306116455108733, Val MAE: 1.2859069108963013\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1454/2000, Train Loss: 5.415918493047705, Val Loss: 4.306036115310214, Val MAE: 1.2860419750213623\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1455/2000, Train Loss: 5.415788276891068, Val Loss: 4.306090827863495, Val MAE: 1.2862967252731323\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1456/2000, Train Loss: 5.415396061404818, Val Loss: 4.3059859415969335, Val MAE: 1.2864389419555664\n",
      "Epoch 1457/2000, Train Loss: 5.415188287981959, Val Loss: 4.3058831834041325, Val MAE: 1.2865692377090454\n",
      "Epoch 1458/2000, Train Loss: 5.414943579020627, Val Loss: 4.305942424903582, Val MAE: 1.2867679595947266\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1459/2000, Train Loss: 5.414707163427252, Val Loss: 4.305887712511393, Val MAE: 1.286953330039978\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1460/2000, Train Loss: 5.4144610733770175, Val Loss: 4.305883832772573, Val MAE: 1.2870984077453613\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1461/2000, Train Loss: 5.41429090406891, Val Loss: 4.305935079772193, Val MAE: 1.2873526811599731\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1462/2000, Train Loss: 5.413939718895137, Val Loss: 4.305728941256398, Val MAE: 1.2874133586883545\n",
      "Epoch 1463/2000, Train Loss: 5.413735688755553, Val Loss: 4.305643468504553, Val MAE: 1.2875807285308838\n",
      "Epoch 1464/2000, Train Loss: 5.413397549467042, Val Loss: 4.305472380298752, Val MAE: 1.2877895832061768\n",
      "Epoch 1465/2000, Train Loss: 5.413087180549753, Val Loss: 4.305514495745973, Val MAE: 1.2880195379257202\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1466/2000, Train Loss: 5.412908519578984, Val Loss: 4.305411071914273, Val MAE: 1.2881377935409546\n",
      "Epoch 1467/2000, Train Loss: 5.412704877659981, Val Loss: 4.3052918026307685, Val MAE: 1.2882623672485352\n",
      "Epoch 1468/2000, Train Loss: 5.41258249267959, Val Loss: 4.305216164309699, Val MAE: 1.2884029150009155\n",
      "Epoch 1469/2000, Train Loss: 5.4122175534318275, Val Loss: 4.3051020641987385, Val MAE: 1.2885444164276123\n",
      "Epoch 1470/2000, Train Loss: 5.411963414848316, Val Loss: 4.304960656434566, Val MAE: 1.2886457443237305\n",
      "Epoch 1471/2000, Train Loss: 5.411833214499463, Val Loss: 4.304757388875828, Val MAE: 1.2887345552444458\n",
      "Epoch 1472/2000, Train Loss: 5.411515864980946, Val Loss: 4.3046324331078445, Val MAE: 1.288856029510498\n",
      "Epoch 1473/2000, Train Loss: 5.411331898569131, Val Loss: 4.304424604088873, Val MAE: 1.288928508758545\n",
      "Epoch 1474/2000, Train Loss: 5.41102624497436, Val Loss: 4.304324437328824, Val MAE: 1.289080262184143\n",
      "Epoch 1475/2000, Train Loss: 5.410754814534775, Val Loss: 4.304327440275265, Val MAE: 1.2893292903900146\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1476/2000, Train Loss: 5.410539092818586, Val Loss: 4.3043343043944855, Val MAE: 1.2895281314849854\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1477/2000, Train Loss: 5.410297173605694, Val Loss: 4.304234074727372, Val MAE: 1.2896549701690674\n",
      "Epoch 1478/2000, Train Loss: 5.409961518185001, Val Loss: 4.3042190585990205, Val MAE: 1.2899367809295654\n",
      "Epoch 1479/2000, Train Loss: 5.4097393128131745, Val Loss: 4.304050264943827, Val MAE: 1.2900153398513794\n",
      "Epoch 1480/2000, Train Loss: 5.409485770090136, Val Loss: 4.303865154651371, Val MAE: 1.2901486158370972\n",
      "Epoch 1481/2000, Train Loss: 5.40922657822298, Val Loss: 4.303768326892509, Val MAE: 1.2902841567993164\n",
      "Epoch 1482/2000, Train Loss: 5.408946104243095, Val Loss: 4.3036619034167884, Val MAE: 1.2904659509658813\n",
      "Epoch 1483/2000, Train Loss: 5.408725797130425, Val Loss: 4.303472748613572, Val MAE: 1.290542483329773\n",
      "Epoch 1484/2000, Train Loss: 5.408508625119785, Val Loss: 4.303387810759716, Val MAE: 1.290704369544983\n",
      "Epoch 1485/2000, Train Loss: 5.408252179157715, Val Loss: 4.3034002231450765, Val MAE: 1.2909183502197266\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1486/2000, Train Loss: 5.407985752718683, Val Loss: 4.303282721716541, Val MAE: 1.291040301322937\n",
      "Epoch 1487/2000, Train Loss: 5.407700461270099, Val Loss: 4.3032958357989255, Val MAE: 1.2912895679473877\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1488/2000, Train Loss: 5.407436603317023, Val Loss: 4.303304733765555, Val MAE: 1.2915091514587402\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1489/2000, Train Loss: 5.407238152395358, Val Loss: 4.303079121069866, Val MAE: 1.2915617227554321\n",
      "Epoch 1490/2000, Train Loss: 5.406989643614676, Val Loss: 4.303026905159156, Val MAE: 1.291710376739502\n",
      "Epoch 1491/2000, Train Loss: 5.406745760781507, Val Loss: 4.3030314046117635, Val MAE: 1.2919172048568726\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1492/2000, Train Loss: 5.406521844789502, Val Loss: 4.302927232849169, Val MAE: 1.2920472621917725\n",
      "Epoch 1493/2000, Train Loss: 5.406277318873011, Val Loss: 4.302807525162761, Val MAE: 1.2921842336654663\n",
      "Epoch 1494/2000, Train Loss: 5.406040541281381, Val Loss: 4.302745816930457, Val MAE: 1.2923071384429932\n",
      "Epoch 1495/2000, Train Loss: 5.40586270891746, Val Loss: 4.30274144474182, Val MAE: 1.2925093173980713\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1496/2000, Train Loss: 5.405583037606872, Val Loss: 4.302525966030521, Val MAE: 1.292579174041748\n",
      "Epoch 1497/2000, Train Loss: 5.405376991550561, Val Loss: 4.302461383860927, Val MAE: 1.2927296161651611\n",
      "Epoch 1498/2000, Train Loss: 5.405169020576894, Val Loss: 4.302375137376356, Val MAE: 1.2928712368011475\n",
      "Epoch 1499/2000, Train Loss: 5.404876331085348, Val Loss: 4.302208935046518, Val MAE: 1.2929834127426147\n",
      "Epoch 1500/2000, Train Loss: 5.404644588935486, Val Loss: 4.302288520967101, Val MAE: 1.2932274341583252\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1501/2000, Train Loss: 5.404403892908007, Val Loss: 4.302175963555907, Val MAE: 1.293358564376831\n",
      "Epoch 1502/2000, Train Loss: 5.404161280663263, Val Loss: 4.302060213604489, Val MAE: 1.293481469154358\n",
      "Epoch 1503/2000, Train Loss: 5.403958270032767, Val Loss: 4.3019216713470385, Val MAE: 1.293601155281067\n",
      "Epoch 1504/2000, Train Loss: 5.403756555826541, Val Loss: 4.301705573284411, Val MAE: 1.2936985492706299\n",
      "Epoch 1505/2000, Train Loss: 5.403543727295168, Val Loss: 4.301526135246496, Val MAE: 1.2938239574432373\n",
      "Epoch 1506/2000, Train Loss: 5.403268209485666, Val Loss: 4.3015267357095945, Val MAE: 1.294007420539856\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1507/2000, Train Loss: 5.403049001641653, Val Loss: 4.301335839739254, Val MAE: 1.2940964698791504\n",
      "Epoch 1508/2000, Train Loss: 5.402748893835987, Val Loss: 4.301230258850364, Val MAE: 1.2942235469818115\n",
      "Epoch 1509/2000, Train Loss: 5.402551164493174, Val Loss: 4.301043043292321, Val MAE: 1.2943168878555298\n",
      "Epoch 1510/2000, Train Loss: 5.402248655206142, Val Loss: 4.301016448303923, Val MAE: 1.2945446968078613\n",
      "Epoch 1511/2000, Train Loss: 5.401987624056812, Val Loss: 4.300827264047421, Val MAE: 1.2946504354476929\n",
      "Epoch 1512/2000, Train Loss: 5.401733599800774, Val Loss: 4.300826293068963, Val MAE: 1.2948495149612427\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1513/2000, Train Loss: 5.4015997169356265, Val Loss: 4.300802332726684, Val MAE: 1.295019507408142\n",
      "Epoch 1514/2000, Train Loss: 5.401266179850991, Val Loss: 4.300637960876967, Val MAE: 1.2951176166534424\n",
      "Epoch 1515/2000, Train Loss: 5.401000944237906, Val Loss: 4.3005697441664905, Val MAE: 1.2953124046325684\n",
      "Epoch 1516/2000, Train Loss: 5.400763780947966, Val Loss: 4.300501524905363, Val MAE: 1.2954918146133423\n",
      "Epoch 1517/2000, Train Loss: 5.400517365861795, Val Loss: 4.300555717690034, Val MAE: 1.2957208156585693\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1518/2000, Train Loss: 5.400266447021697, Val Loss: 4.300471149103061, Val MAE: 1.2958898544311523\n",
      "Epoch 1519/2000, Train Loss: 5.400040946587014, Val Loss: 4.300394007026612, Val MAE: 1.296045184135437\n",
      "Epoch 1520/2000, Train Loss: 5.3998093886094605, Val Loss: 4.30027467413797, Val MAE: 1.2961753606796265\n",
      "Epoch 1521/2000, Train Loss: 5.399550105952157, Val Loss: 4.300165626299274, Val MAE: 1.2962960004806519\n",
      "Epoch 1522/2000, Train Loss: 5.399320154777741, Val Loss: 4.300085750638365, Val MAE: 1.296462059020996\n",
      "Epoch 1523/2000, Train Loss: 5.399075638671375, Val Loss: 4.2999800564738, Val MAE: 1.2965933084487915\n",
      "Epoch 1524/2000, Train Loss: 5.3988888981561765, Val Loss: 4.2997796306336244, Val MAE: 1.2966973781585693\n",
      "Epoch 1525/2000, Train Loss: 5.398568070065027, Val Loss: 4.299784933648131, Val MAE: 1.296889305114746\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1526/2000, Train Loss: 5.398284486217451, Val Loss: 4.299766428876031, Val MAE: 1.2971209287643433\n",
      "Epoch 1527/2000, Train Loss: 5.398026888158504, Val Loss: 4.299714560148952, Val MAE: 1.2972784042358398\n",
      "Epoch 1528/2000, Train Loss: 5.397754945918662, Val Loss: 4.299465691600297, Val MAE: 1.2973419427871704\n",
      "Epoch 1529/2000, Train Loss: 5.397526476200955, Val Loss: 4.299260247210125, Val MAE: 1.297538161277771\n",
      "Epoch 1530/2000, Train Loss: 5.39725988436717, Val Loss: 4.29932754215625, Val MAE: 1.2977733612060547\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1531/2000, Train Loss: 5.396959636215115, Val Loss: 4.299239276644883, Val MAE: 1.2979234457015991\n",
      "Epoch 1532/2000, Train Loss: 5.39676738708235, Val Loss: 4.299080144634118, Val MAE: 1.2980598211288452\n",
      "Epoch 1533/2000, Train Loss: 5.396463060899755, Val Loss: 4.299040221375925, Val MAE: 1.2982380390167236\n",
      "Epoch 1534/2000, Train Loss: 5.396276083895457, Val Loss: 4.299287913632286, Val MAE: 1.2985897064208984\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1535/2000, Train Loss: 5.396045642040449, Val Loss: 4.299328806848677, Val MAE: 1.2988109588623047\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1536/2000, Train Loss: 5.3957505923305025, Val Loss: 4.299189312447298, Val MAE: 1.298933982849121\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1537/2000, Train Loss: 5.39552947176786, Val Loss: 4.299074535769922, Val MAE: 1.2990460395812988\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1538/2000, Train Loss: 5.395347522963972, Val Loss: 4.299379852696045, Val MAE: 1.2993724346160889\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1539/2000, Train Loss: 5.395102846083143, Val Loss: 4.2992207609989626, Val MAE: 1.2994709014892578\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1540/2000, Train Loss: 5.394866384879661, Val Loss: 4.299068893008941, Val MAE: 1.2995561361312866\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1541/2000, Train Loss: 5.39471508721805, Val Loss: 4.298906856439672, Val MAE: 1.299646019935608\n",
      "Epoch 1542/2000, Train Loss: 5.39446403753367, Val Loss: 4.298862733376455, Val MAE: 1.2998145818710327\n",
      "Epoch 1543/2000, Train Loss: 5.394252797184794, Val Loss: 4.298901338848445, Val MAE: 1.300022006034851\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1544/2000, Train Loss: 5.393958274734188, Val Loss: 4.29888616982881, Val MAE: 1.3002091646194458\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1545/2000, Train Loss: 5.393736964094565, Val Loss: 4.298864565037929, Val MAE: 1.3003895282745361\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1546/2000, Train Loss: 5.393532873315856, Val Loss: 4.2986796601264325, Val MAE: 1.3004707098007202\n",
      "Epoch 1547/2000, Train Loss: 5.393267234174212, Val Loss: 4.298568086661734, Val MAE: 1.300595998764038\n",
      "Epoch 1548/2000, Train Loss: 5.3930709942268695, Val Loss: 4.298477791504817, Val MAE: 1.300722360610962\n",
      "Epoch 1549/2000, Train Loss: 5.392874998294759, Val Loss: 4.298329613552437, Val MAE: 1.3008111715316772\n",
      "Epoch 1550/2000, Train Loss: 5.3926375244806595, Val Loss: 4.298258868665308, Val MAE: 1.3009618520736694\n",
      "Epoch 1551/2000, Train Loss: 5.392373129283768, Val Loss: 4.2981909589053275, Val MAE: 1.301110029220581\n",
      "Epoch 1552/2000, Train Loss: 5.39222331947172, Val Loss: 4.298015207052231, Val MAE: 1.3012218475341797\n",
      "Epoch 1553/2000, Train Loss: 5.391943567070686, Val Loss: 4.298024078612929, Val MAE: 1.301439642906189\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1554/2000, Train Loss: 5.391708247189217, Val Loss: 4.2979167549459785, Val MAE: 1.3015400171279907\n",
      "Epoch 1555/2000, Train Loss: 5.391488349753869, Val Loss: 4.297842105726401, Val MAE: 1.3017044067382812\n",
      "Epoch 1556/2000, Train Loss: 5.3912707955319314, Val Loss: 4.2976684563063285, Val MAE: 1.3017784357070923\n",
      "Epoch 1557/2000, Train Loss: 5.391071835471762, Val Loss: 4.297651996054091, Val MAE: 1.3019683361053467\n",
      "Epoch 1558/2000, Train Loss: 5.390808358393295, Val Loss: 4.297585582853975, Val MAE: 1.3021429777145386\n",
      "Epoch 1559/2000, Train Loss: 5.39064004194346, Val Loss: 4.2974369495972855, Val MAE: 1.3022993803024292\n",
      "Epoch 1560/2000, Train Loss: 5.390286554621646, Val Loss: 4.297407501117066, Val MAE: 1.3024693727493286\n",
      "Epoch 1561/2000, Train Loss: 5.390046362199947, Val Loss: 4.297320134669274, Val MAE: 1.3026257753372192\n",
      "Epoch 1562/2000, Train Loss: 5.389868264265254, Val Loss: 4.297324214204474, Val MAE: 1.302794337272644\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1563/2000, Train Loss: 5.389652776213685, Val Loss: 4.297292289755366, Val MAE: 1.3029608726501465\n",
      "Epoch 1564/2000, Train Loss: 5.389403798185905, Val Loss: 4.29724547182386, Val MAE: 1.303108811378479\n",
      "Epoch 1565/2000, Train Loss: 5.389182367116538, Val Loss: 4.297091233663194, Val MAE: 1.3032126426696777\n",
      "Epoch 1566/2000, Train Loss: 5.389014558933455, Val Loss: 4.296981782003029, Val MAE: 1.3033560514450073\n",
      "Epoch 1567/2000, Train Loss: 5.388782609829478, Val Loss: 4.2969405738217334, Val MAE: 1.3034987449645996\n",
      "Epoch 1568/2000, Train Loss: 5.38848614692688, Val Loss: 4.296918944262706, Val MAE: 1.3036892414093018\n",
      "Epoch 1569/2000, Train Loss: 5.388294602075938, Val Loss: 4.29688560683448, Val MAE: 1.3038530349731445\n",
      "Epoch 1570/2000, Train Loss: 5.38811766822327, Val Loss: 4.296679988328938, Val MAE: 1.303925633430481\n",
      "Epoch 1571/2000, Train Loss: 5.387831085371711, Val Loss: 4.296752330043295, Val MAE: 1.3041331768035889\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1572/2000, Train Loss: 5.3876040715509195, Val Loss: 4.2967443409266775, Val MAE: 1.3043811321258545\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1573/2000, Train Loss: 5.387408301908401, Val Loss: 4.296698204112483, Val MAE: 1.3045482635498047\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1574/2000, Train Loss: 5.387095891369301, Val Loss: 4.296477498342325, Val MAE: 1.3046064376831055\n",
      "Epoch 1575/2000, Train Loss: 5.386926254202349, Val Loss: 4.296415129102565, Val MAE: 1.3047815561294556\n",
      "Epoch 1576/2000, Train Loss: 5.386669610980055, Val Loss: 4.296355527236655, Val MAE: 1.3049176931381226\n",
      "Epoch 1577/2000, Train Loss: 5.386507724776469, Val Loss: 4.296314782723114, Val MAE: 1.3050816059112549\n",
      "Epoch 1578/2000, Train Loss: 5.386225003348126, Val Loss: 4.296239227472662, Val MAE: 1.3052247762680054\n",
      "Epoch 1579/2000, Train Loss: 5.3860049539023365, Val Loss: 4.296137651492346, Val MAE: 1.3053569793701172\n",
      "Epoch 1580/2000, Train Loss: 5.385856254982316, Val Loss: 4.295993797011204, Val MAE: 1.3054717779159546\n",
      "Epoch 1581/2000, Train Loss: 5.385543465614319, Val Loss: 4.296089328315344, Val MAE: 1.3057478666305542\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1582/2000, Train Loss: 5.385329633346772, Val Loss: 4.296073279831861, Val MAE: 1.3059030771255493\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1583/2000, Train Loss: 5.385078200869925, Val Loss: 4.296038112648436, Val MAE: 1.3061034679412842\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1584/2000, Train Loss: 5.384930144978016, Val Loss: 4.296018621097277, Val MAE: 1.3062560558319092\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1585/2000, Train Loss: 5.384561618293131, Val Loss: 4.295660869965145, Val MAE: 1.3062843084335327\n",
      "Epoch 1586/2000, Train Loss: 5.384371487473177, Val Loss: 4.295457483089722, Val MAE: 1.3063654899597168\n",
      "Epoch 1587/2000, Train Loss: 5.384166081870402, Val Loss: 4.295433224012723, Val MAE: 1.3065216541290283\n",
      "Epoch 1588/2000, Train Loss: 5.38397574201575, Val Loss: 4.295243948510101, Val MAE: 1.3066147565841675\n",
      "Epoch 1589/2000, Train Loss: 5.383753800122489, Val Loss: 4.295190446481511, Val MAE: 1.3067567348480225\n",
      "Epoch 1590/2000, Train Loss: 5.383545576503236, Val Loss: 4.295177564186019, Val MAE: 1.3069301843643188\n",
      "Epoch 1591/2000, Train Loss: 5.383268790758344, Val Loss: 4.295139600349976, Val MAE: 1.307127594947815\n",
      "Epoch 1592/2000, Train Loss: 5.383031677472145, Val Loss: 4.295148823081373, Val MAE: 1.3073341846466064\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1593/2000, Train Loss: 5.382813346599826, Val Loss: 4.295032592507096, Val MAE: 1.3074394464492798\n",
      "Epoch 1594/2000, Train Loss: 5.38258364122483, Val Loss: 4.294978507373247, Val MAE: 1.3075965642929077\n",
      "Epoch 1595/2000, Train Loss: 5.382413336760169, Val Loss: 4.2949761214557, Val MAE: 1.3077718019485474\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1596/2000, Train Loss: 5.382248717797528, Val Loss: 4.294897476281669, Val MAE: 1.3079049587249756\n",
      "Epoch 1597/2000, Train Loss: 5.382136833649158, Val Loss: 4.295158388203866, Val MAE: 1.3082177639007568\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1598/2000, Train Loss: 5.381713534433644, Val Loss: 4.295002804360948, Val MAE: 1.3083436489105225\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1599/2000, Train Loss: 5.381487474426651, Val Loss: 4.294892187403129, Val MAE: 1.3084633350372314\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1600/2000, Train Loss: 5.381257956187922, Val Loss: 4.294825703885641, Val MAE: 1.3086127042770386\n",
      "Epoch 1601/2000, Train Loss: 5.381062969425911, Val Loss: 4.294692268940779, Val MAE: 1.308709740638733\n",
      "Epoch 1602/2000, Train Loss: 5.3808557819836595, Val Loss: 4.294590916477882, Val MAE: 1.3088574409484863\n",
      "Epoch 1603/2000, Train Loss: 5.3806033513847265, Val Loss: 4.294612965793223, Val MAE: 1.30906081199646\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1604/2000, Train Loss: 5.380416853007586, Val Loss: 4.294531272982692, Val MAE: 1.3091964721679688\n",
      "Epoch 1605/2000, Train Loss: 5.380203245954469, Val Loss: 4.294432470121899, Val MAE: 1.3093339204788208\n",
      "Epoch 1606/2000, Train Loss: 5.380009727526381, Val Loss: 4.294358779879303, Val MAE: 1.309476375579834\n",
      "Epoch 1607/2000, Train Loss: 5.379756795076796, Val Loss: 4.294328689602044, Val MAE: 1.3096492290496826\n",
      "Epoch 1608/2000, Train Loss: 5.379525762144331, Val Loss: 4.294144062013239, Val MAE: 1.3097156286239624\n",
      "Epoch 1609/2000, Train Loss: 5.379340898972034, Val Loss: 4.294096245529415, Val MAE: 1.3098809719085693\n",
      "Epoch 1610/2000, Train Loss: 5.37911002163582, Val Loss: 4.293964191116729, Val MAE: 1.3099746704101562\n",
      "Epoch 1611/2000, Train Loss: 5.378882175302729, Val Loss: 4.29403197521025, Val MAE: 1.3102011680603027\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1612/2000, Train Loss: 5.378648706036221, Val Loss: 4.293992983516272, Val MAE: 1.3104041814804077\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1613/2000, Train Loss: 5.378413625514823, Val Loss: 4.293922022871069, Val MAE: 1.3105559349060059\n",
      "Epoch 1614/2000, Train Loss: 5.378313380951721, Val Loss: 4.293767274392618, Val MAE: 1.3106608390808105\n",
      "Epoch 1615/2000, Train Loss: 5.377958956077206, Val Loss: 4.293762599186854, Val MAE: 1.3108493089675903\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1616/2000, Train Loss: 5.377809298763781, Val Loss: 4.2934688592547765, Val MAE: 1.3109022378921509\n",
      "Epoch 1617/2000, Train Loss: 5.377539888373032, Val Loss: 4.293214890978358, Val MAE: 1.3109546899795532\n",
      "Epoch 1618/2000, Train Loss: 5.377279677368735, Val Loss: 4.293319873632611, Val MAE: 1.3112207651138306\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1619/2000, Train Loss: 5.377003562269084, Val Loss: 4.2932494599271465, Val MAE: 1.311466097831726\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1620/2000, Train Loss: 5.376787437291673, Val Loss: 4.293152263814265, Val MAE: 1.3115849494934082\n",
      "Epoch 1621/2000, Train Loss: 5.376526261640599, Val Loss: 4.293126638354482, Val MAE: 1.3117660284042358\n",
      "Epoch 1622/2000, Train Loss: 5.376366673476192, Val Loss: 4.293006570811745, Val MAE: 1.3118664026260376\n",
      "Epoch 1623/2000, Train Loss: 5.3761359064515775, Val Loss: 4.292946517923931, Val MAE: 1.3120269775390625\n",
      "Epoch 1624/2000, Train Loss: 5.375970574883328, Val Loss: 4.292729341742155, Val MAE: 1.3121064901351929\n",
      "Epoch 1625/2000, Train Loss: 5.3757624482959745, Val Loss: 4.292514802528931, Val MAE: 1.3121705055236816\n",
      "Epoch 1626/2000, Train Loss: 5.375482158616255, Val Loss: 4.2924743430034535, Val MAE: 1.312333106994629\n",
      "Epoch 1627/2000, Train Loss: 5.375292798007334, Val Loss: 4.29238275200934, Val MAE: 1.312447190284729\n",
      "Epoch 1628/2000, Train Loss: 5.375113889877213, Val Loss: 4.2922836931170645, Val MAE: 1.3125368356704712\n",
      "Epoch 1629/2000, Train Loss: 5.37488892018888, Val Loss: 4.292233555408211, Val MAE: 1.3126881122589111\n",
      "Epoch 1630/2000, Train Loss: 5.374691551077571, Val Loss: 4.2922042623028025, Val MAE: 1.3128552436828613\n",
      "Epoch 1631/2000, Train Loss: 5.374466303916134, Val Loss: 4.292175189492939, Val MAE: 1.313043236732483\n",
      "Epoch 1632/2000, Train Loss: 5.374206416692451, Val Loss: 4.292208667703577, Val MAE: 1.3132578134536743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1633/2000, Train Loss: 5.37410570939134, Val Loss: 4.292218693258526, Val MAE: 1.3134088516235352\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1634/2000, Train Loss: 5.373747733379489, Val Loss: 4.292227107044813, Val MAE: 1.3136407136917114\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1635/2000, Train Loss: 5.373525811431932, Val Loss: 4.291975886972101, Val MAE: 1.3137058019638062\n",
      "Epoch 1636/2000, Train Loss: 5.373308213750211, Val Loss: 4.291861758548934, Val MAE: 1.3137823343276978\n",
      "Epoch 1637/2000, Train Loss: 5.37313672086564, Val Loss: 4.291690581124108, Val MAE: 1.3138494491577148\n",
      "Epoch 1638/2000, Train Loss: 5.372958021305281, Val Loss: 4.2916415413489215, Val MAE: 1.3139688968658447\n",
      "Epoch 1639/2000, Train Loss: 5.372803947994378, Val Loss: 4.291494242028073, Val MAE: 1.3140928745269775\n",
      "Epoch 1640/2000, Train Loss: 5.3725256890105015, Val Loss: 4.291467000127913, Val MAE: 1.3142377138137817\n",
      "Epoch 1641/2000, Train Loss: 5.372347971206522, Val Loss: 4.291414250258927, Val MAE: 1.3144053220748901\n",
      "Epoch 1642/2000, Train Loss: 5.372109966977338, Val Loss: 4.291393652128744, Val MAE: 1.3145681619644165\n",
      "Epoch 1643/2000, Train Loss: 5.371923822322613, Val Loss: 4.291351863178047, Val MAE: 1.314720869064331\n",
      "Epoch 1644/2000, Train Loss: 5.3716847005877035, Val Loss: 4.291351577585882, Val MAE: 1.3149052858352661\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1645/2000, Train Loss: 5.371488389749423, Val Loss: 4.291258553422249, Val MAE: 1.3150502443313599\n",
      "Epoch 1646/2000, Train Loss: 5.371276773826194, Val Loss: 4.291247537544182, Val MAE: 1.3151934146881104\n",
      "Epoch 1647/2000, Train Loss: 5.371060905144107, Val Loss: 4.291159574378718, Val MAE: 1.3153092861175537\n",
      "Epoch 1648/2000, Train Loss: 5.370931584358959, Val Loss: 4.290979155334266, Val MAE: 1.3154067993164062\n",
      "Epoch 1649/2000, Train Loss: 5.3706564481098455, Val Loss: 4.290956824158763, Val MAE: 1.3155837059020996\n",
      "Epoch 1650/2000, Train Loss: 5.370437939336027, Val Loss: 4.290923762052983, Val MAE: 1.315726637840271\n",
      "Epoch 1651/2000, Train Loss: 5.370243209386579, Val Loss: 4.290867412466187, Val MAE: 1.3158838748931885\n",
      "Epoch 1652/2000, Train Loss: 5.370037038931144, Val Loss: 4.29091509180563, Val MAE: 1.3161317110061646\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1653/2000, Train Loss: 5.369805543417585, Val Loss: 4.290874375308956, Val MAE: 1.3162997961044312\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1654/2000, Train Loss: 5.369599504515459, Val Loss: 4.290775783239185, Val MAE: 1.3164019584655762\n",
      "Epoch 1655/2000, Train Loss: 5.369536576888491, Val Loss: 4.2903758403417225, Val MAE: 1.3164176940917969\n",
      "Epoch 1656/2000, Train Loss: 5.369232350155455, Val Loss: 4.290337932109833, Val MAE: 1.3165783882141113\n",
      "Epoch 1657/2000, Train Loss: 5.369020204648213, Val Loss: 4.290289588098053, Val MAE: 1.316692590713501\n",
      "Epoch 1658/2000, Train Loss: 5.3688986816104975, Val Loss: 4.29015071778684, Val MAE: 1.316801905632019\n",
      "Epoch 1659/2000, Train Loss: 5.36866523962404, Val Loss: 4.290158623215315, Val MAE: 1.3169512748718262\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1660/2000, Train Loss: 5.368440996280141, Val Loss: 4.290110411890992, Val MAE: 1.3170949220657349\n",
      "Epoch 1661/2000, Train Loss: 5.368303450704922, Val Loss: 4.289874359025611, Val MAE: 1.3171663284301758\n",
      "Epoch 1662/2000, Train Loss: 5.36800252926331, Val Loss: 4.28985266502913, Val MAE: 1.3173123598098755\n",
      "Epoch 1663/2000, Train Loss: 5.3678027351216855, Val Loss: 4.289877768438141, Val MAE: 1.3174842596054077\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1664/2000, Train Loss: 5.367605960685266, Val Loss: 4.289812189045253, Val MAE: 1.3176127672195435\n",
      "Epoch 1665/2000, Train Loss: 5.367414746846126, Val Loss: 4.289668229893521, Val MAE: 1.3177073001861572\n",
      "Epoch 1666/2000, Train Loss: 5.367218288356168, Val Loss: 4.289601596035399, Val MAE: 1.3178330659866333\n",
      "Epoch 1667/2000, Train Loss: 5.366996391701066, Val Loss: 4.289632823875358, Val MAE: 1.3180184364318848\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1668/2000, Train Loss: 5.366820986296941, Val Loss: 4.289558540217511, Val MAE: 1.3181613683700562\n",
      "Epoch 1669/2000, Train Loss: 5.366628977372754, Val Loss: 4.2895660196338685, Val MAE: 1.3183099031448364\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1670/2000, Train Loss: 5.366469044581218, Val Loss: 4.2894802695459076, Val MAE: 1.3184819221496582\n",
      "Epoch 1671/2000, Train Loss: 5.366221506770427, Val Loss: 4.2894655130736465, Val MAE: 1.3186534643173218\n",
      "Epoch 1672/2000, Train Loss: 5.365970207822862, Val Loss: 4.289547584695859, Val MAE: 1.3189561367034912\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1673/2000, Train Loss: 5.365749086865025, Val Loss: 4.289483659922539, Val MAE: 1.3190860748291016\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1674/2000, Train Loss: 5.365591165114211, Val Loss: 4.289504921275216, Val MAE: 1.3192561864852905\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1675/2000, Train Loss: 5.365375634381626, Val Loss: 4.289438138163842, Val MAE: 1.3194236755371094\n",
      "Epoch 1676/2000, Train Loss: 5.365203690510272, Val Loss: 4.289332276206832, Val MAE: 1.3195239305496216\n",
      "Epoch 1677/2000, Train Loss: 5.364989286651849, Val Loss: 4.289338661528922, Val MAE: 1.3197171688079834\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1678/2000, Train Loss: 5.3647244171168245, Val Loss: 4.28934579929253, Val MAE: 1.3199421167373657\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1679/2000, Train Loss: 5.3644986939783585, Val Loss: 4.2892827216032385, Val MAE: 1.3200905323028564\n",
      "Epoch 1680/2000, Train Loss: 5.364308892099794, Val Loss: 4.289204700465675, Val MAE: 1.3202139139175415\n",
      "Epoch 1681/2000, Train Loss: 5.364154353342636, Val Loss: 4.2892638876899944, Val MAE: 1.32047700881958\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1682/2000, Train Loss: 5.363914584964746, Val Loss: 4.289102565114563, Val MAE: 1.3205718994140625\n",
      "Epoch 1683/2000, Train Loss: 5.363726355983947, Val Loss: 4.289337302086589, Val MAE: 1.3208866119384766\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1684/2000, Train Loss: 5.3634698111999795, Val Loss: 4.289285280817264, Val MAE: 1.3210387229919434\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1685/2000, Train Loss: 5.363314659844694, Val Loss: 4.289250188260465, Val MAE: 1.3212029933929443\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1686/2000, Train Loss: 5.3630809066076175, Val Loss: 4.289163900173462, Val MAE: 1.321354866027832\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1687/2000, Train Loss: 5.362866636743411, Val Loss: 4.289128003786276, Val MAE: 1.3214757442474365\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1688/2000, Train Loss: 5.362696808976428, Val Loss: 4.289072571976765, Val MAE: 1.3216251134872437\n",
      "Epoch 1689/2000, Train Loss: 5.362571260300515, Val Loss: 4.289065035211073, Val MAE: 1.3217600584030151\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1690/2000, Train Loss: 5.362290981965207, Val Loss: 4.2887587622479275, Val MAE: 1.3217930793762207\n",
      "Epoch 1691/2000, Train Loss: 5.362066480187283, Val Loss: 4.288678802455868, Val MAE: 1.3219184875488281\n",
      "Epoch 1692/2000, Train Loss: 5.361932797023268, Val Loss: 4.288641594524856, Val MAE: 1.3221336603164673\n",
      "Epoch 1693/2000, Train Loss: 5.361700387901151, Val Loss: 4.2884536524076715, Val MAE: 1.322206974029541\n",
      "Epoch 1694/2000, Train Loss: 5.361467932390163, Val Loss: 4.288433571334358, Val MAE: 1.3223716020584106\n",
      "Epoch 1695/2000, Train Loss: 5.3612518994930936, Val Loss: 4.288422063019898, Val MAE: 1.3225550651550293\n",
      "Epoch 1696/2000, Train Loss: 5.36112656719785, Val Loss: 4.288494536871308, Val MAE: 1.3227382898330688\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1697/2000, Train Loss: 5.360890641413315, Val Loss: 4.2884312651715835, Val MAE: 1.3228975534439087\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1698/2000, Train Loss: 5.360679222622453, Val Loss: 4.288343142711365, Val MAE: 1.3230212926864624\n",
      "Epoch 1699/2000, Train Loss: 5.3605118639151135, Val Loss: 4.288336277840374, Val MAE: 1.3232253789901733\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1700/2000, Train Loss: 5.360298502859013, Val Loss: 4.28826334702002, Val MAE: 1.3233221769332886\n",
      "Epoch 1701/2000, Train Loss: 5.360084041604385, Val Loss: 4.288228371095013, Val MAE: 1.323476791381836\n",
      "Epoch 1702/2000, Train Loss: 5.359893049911106, Val Loss: 4.288165156825169, Val MAE: 1.3236325979232788\n",
      "Epoch 1703/2000, Train Loss: 5.359681036468601, Val Loss: 4.288124748124733, Val MAE: 1.3237751722335815\n",
      "Epoch 1704/2000, Train Loss: 5.359498187643875, Val Loss: 4.288052239826134, Val MAE: 1.323914885520935\n",
      "Epoch 1705/2000, Train Loss: 5.3592842783250045, Val Loss: 4.287990429648408, Val MAE: 1.3240702152252197\n",
      "Epoch 1706/2000, Train Loss: 5.35908412338233, Val Loss: 4.2879657189319795, Val MAE: 1.3242449760437012\n",
      "Epoch 1707/2000, Train Loss: 5.358912974380991, Val Loss: 4.287929957192223, Val MAE: 1.3243831396102905\n",
      "Epoch 1708/2000, Train Loss: 5.35869164530089, Val Loss: 4.287884045452685, Val MAE: 1.3245279788970947\n",
      "Epoch 1709/2000, Train Loss: 5.358490313375703, Val Loss: 4.2877991847626795, Val MAE: 1.3246450424194336\n",
      "Epoch 1710/2000, Train Loss: 5.358329770438571, Val Loss: 4.287819015845522, Val MAE: 1.3248330354690552\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1711/2000, Train Loss: 5.3581615632484185, Val Loss: 4.2877214120315, Val MAE: 1.3249499797821045\n",
      "Epoch 1712/2000, Train Loss: 5.357884988026016, Val Loss: 4.287617694955689, Val MAE: 1.3250895738601685\n",
      "Epoch 1713/2000, Train Loss: 5.357763341065734, Val Loss: 4.2874865800679265, Val MAE: 1.325181484222412\n",
      "Epoch 1714/2000, Train Loss: 5.357565183721355, Val Loss: 4.287498253691304, Val MAE: 1.3253403902053833\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1715/2000, Train Loss: 5.357373614021099, Val Loss: 4.287365471564971, Val MAE: 1.3254592418670654\n",
      "Epoch 1716/2000, Train Loss: 5.35721021386186, Val Loss: 4.287635024066444, Val MAE: 1.325753092765808\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1717/2000, Train Loss: 5.35697962006615, Val Loss: 4.287576345548973, Val MAE: 1.3258765935897827\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1718/2000, Train Loss: 5.35678873930632, Val Loss: 4.287466320905599, Val MAE: 1.3259754180908203\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1719/2000, Train Loss: 5.356591022191293, Val Loss: 4.287367294930123, Val MAE: 1.3261003494262695\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1720/2000, Train Loss: 5.356392924759578, Val Loss: 4.287353177677404, Val MAE: 1.32628333568573\n",
      "Epoch 1721/2000, Train Loss: 5.356175159329371, Val Loss: 4.287347775939349, Val MAE: 1.326472520828247\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1722/2000, Train Loss: 5.356016257251852, Val Loss: 4.287591022145641, Val MAE: 1.3267704248428345\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1723/2000, Train Loss: 5.355773328432985, Val Loss: 4.287638403220219, Val MAE: 1.3269646167755127\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1724/2000, Train Loss: 5.35561632877579, Val Loss: 4.287698195565929, Val MAE: 1.3271989822387695\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1725/2000, Train Loss: 5.355402243909151, Val Loss: 4.287658678626155, Val MAE: 1.3273452520370483\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1726/2000, Train Loss: 5.3552066130496785, Val Loss: 4.287594744670498, Val MAE: 1.3274770975112915\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1727/2000, Train Loss: 5.355052948369995, Val Loss: 4.28748799618837, Val MAE: 1.3275808095932007\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1728/2000, Train Loss: 5.354904198190909, Val Loss: 4.287555835698102, Val MAE: 1.3277904987335205\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1729/2000, Train Loss: 5.354646895493435, Val Loss: 4.287483304685301, Val MAE: 1.3279457092285156\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1730/2000, Train Loss: 5.354473831129148, Val Loss: 4.2874098624463555, Val MAE: 1.3280612230300903\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1731/2000, Train Loss: 5.354276424637079, Val Loss: 4.287471969600197, Val MAE: 1.328270435333252\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1732/2000, Train Loss: 5.35409446941709, Val Loss: 4.287386623239732, Val MAE: 1.3283957242965698\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1733/2000, Train Loss: 5.353884847227385, Val Loss: 4.287318709385287, Val MAE: 1.328493356704712\n",
      "Epoch 1734/2000, Train Loss: 5.353692014578166, Val Loss: 4.287396659292616, Val MAE: 1.3287684917449951\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1735/2000, Train Loss: 5.3534694856117, Val Loss: 4.287385465513479, Val MAE: 1.3289583921432495\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1736/2000, Train Loss: 5.353197682247891, Val Loss: 4.287403702655354, Val MAE: 1.3292171955108643\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1737/2000, Train Loss: 5.353026961434092, Val Loss: 4.287403340919598, Val MAE: 1.3293769359588623\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1738/2000, Train Loss: 5.352822238309149, Val Loss: 4.287375595521283, Val MAE: 1.3295143842697144\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1739/2000, Train Loss: 5.352651741872897, Val Loss: 4.287359054883321, Val MAE: 1.3296912908554077\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1740/2000, Train Loss: 5.352466122036605, Val Loss: 4.287257786860337, Val MAE: 1.329795002937317\n",
      "Epoch 1741/2000, Train Loss: 5.3522563241015355, Val Loss: 4.2872384387093625, Val MAE: 1.3299319744110107\n",
      "Epoch 1742/2000, Train Loss: 5.352038150353941, Val Loss: 4.287345514990188, Val MAE: 1.3301936388015747\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1743/2000, Train Loss: 5.3519024549984895, Val Loss: 4.287363406747311, Val MAE: 1.330376148223877\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1744/2000, Train Loss: 5.351670617618352, Val Loss: 4.287230712175369, Val MAE: 1.3304643630981445\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1745/2000, Train Loss: 5.351506464566902, Val Loss: 4.2871411603313305, Val MAE: 1.3305647373199463\n",
      "Epoch 1746/2000, Train Loss: 5.3513072790482115, Val Loss: 4.287088413684217, Val MAE: 1.3306933641433716\n",
      "Epoch 1747/2000, Train Loss: 5.351137547522737, Val Loss: 4.286916930611069, Val MAE: 1.3307832479476929\n",
      "Epoch 1748/2000, Train Loss: 5.350929473008083, Val Loss: 4.28684575450313, Val MAE: 1.3309121131896973\n",
      "Epoch 1749/2000, Train Loss: 5.350748977776437, Val Loss: 4.2867775429476485, Val MAE: 1.331036925315857\n",
      "Epoch 1750/2000, Train Loss: 5.350552657093161, Val Loss: 4.286728483814377, Val MAE: 1.3311889171600342\n",
      "Epoch 1751/2000, Train Loss: 5.350365232565846, Val Loss: 4.2869206423963515, Val MAE: 1.331440806388855\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1752/2000, Train Loss: 5.350312435310828, Val Loss: 4.287007781460479, Val MAE: 1.3316993713378906\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1753/2000, Train Loss: 5.349928782808242, Val Loss: 4.286998511622618, Val MAE: 1.3318732976913452\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1754/2000, Train Loss: 5.349771544267532, Val Loss: 4.286923427920084, Val MAE: 1.331979751586914\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1755/2000, Train Loss: 5.349546463738737, Val Loss: 4.286896689518078, Val MAE: 1.332161784172058\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1756/2000, Train Loss: 5.349339484703522, Val Loss: 4.286948154772724, Val MAE: 1.3323545455932617\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1757/2000, Train Loss: 5.349169674790036, Val Loss: 4.286891977878304, Val MAE: 1.3324775695800781\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1758/2000, Train Loss: 5.34897145800955, Val Loss: 4.286816903572899, Val MAE: 1.332604169845581\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1759/2000, Train Loss: 5.348936854770143, Val Loss: 4.286652313051997, Val MAE: 1.332686424255371\n",
      "Epoch 1760/2000, Train Loss: 5.34865283519727, Val Loss: 4.286642741083025, Val MAE: 1.332830548286438\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1761/2000, Train Loss: 5.348474178195883, Val Loss: 4.286521411854942, Val MAE: 1.3329622745513916\n",
      "Epoch 1762/2000, Train Loss: 5.348236665245896, Val Loss: 4.286531844708296, Val MAE: 1.333127737045288\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1763/2000, Train Loss: 5.348056521226017, Val Loss: 4.286535267786936, Val MAE: 1.3332864046096802\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1764/2000, Train Loss: 5.347893450077908, Val Loss: 4.286423731226105, Val MAE: 1.3334194421768188\n",
      "Epoch 1765/2000, Train Loss: 5.347703459156842, Val Loss: 4.286521584923203, Val MAE: 1.3336461782455444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1766/2000, Train Loss: 5.347476097053373, Val Loss: 4.286384431628494, Val MAE: 1.3337199687957764\n",
      "Epoch 1767/2000, Train Loss: 5.347323936716815, Val Loss: 4.286285019914309, Val MAE: 1.3338277339935303\n",
      "Epoch 1768/2000, Train Loss: 5.347138875732184, Val Loss: 4.286269432386836, Val MAE: 1.3339778184890747\n",
      "Epoch 1769/2000, Train Loss: 5.347006782912613, Val Loss: 4.286281982386434, Val MAE: 1.334165334701538\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1770/2000, Train Loss: 5.346827630505733, Val Loss: 4.286053329733041, Val MAE: 1.3342093229293823\n",
      "Epoch 1771/2000, Train Loss: 5.346637899157781, Val Loss: 4.286293258323326, Val MAE: 1.3345072269439697\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1772/2000, Train Loss: 5.346439100743828, Val Loss: 4.286180838754585, Val MAE: 1.334619402885437\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1773/2000, Train Loss: 5.346286447111418, Val Loss: 4.2861797071791985, Val MAE: 1.3347686529159546\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1774/2000, Train Loss: 5.346066440695347, Val Loss: 4.286117388321473, Val MAE: 1.334918737411499\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1775/2000, Train Loss: 5.345966818459134, Val Loss: 4.286384962566264, Val MAE: 1.335237979888916\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1776/2000, Train Loss: 5.345632611868348, Val Loss: 4.286164691587826, Val MAE: 1.3353196382522583\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1777/2000, Train Loss: 5.345441609760528, Val Loss: 4.286230849199467, Val MAE: 1.3355127573013306\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1778/2000, Train Loss: 5.345235781241412, Val Loss: 4.28613934130282, Val MAE: 1.3356270790100098\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1779/2000, Train Loss: 5.3450558330637055, Val Loss: 4.286081854180173, Val MAE: 1.3357737064361572\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1780/2000, Train Loss: 5.344888292125161, Val Loss: 4.2862610758424875, Val MAE: 1.336019515991211\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1781/2000, Train Loss: 5.344707331214792, Val Loss: 4.286259556246233, Val MAE: 1.3362369537353516\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1782/2000, Train Loss: 5.344484397662039, Val Loss: 4.286075587047113, Val MAE: 1.3362928628921509\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1783/2000, Train Loss: 5.344278487697965, Val Loss: 4.286283297232679, Val MAE: 1.3366162776947021\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1784/2000, Train Loss: 5.344131056120541, Val Loss: 4.286194549406971, Val MAE: 1.336708664894104\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1785/2000, Train Loss: 5.3439595325502705, Val Loss: 4.286136256655057, Val MAE: 1.3368914127349854\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1786/2000, Train Loss: 5.343774108579397, Val Loss: 4.28609266026063, Val MAE: 1.3370130062103271\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 1787/2000, Train Loss: 5.343570146099454, Val Loss: 4.286085623341638, Val MAE: 1.3371771574020386\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 1788/2000, Train Loss: 5.343412283569714, Val Loss: 4.286029242798015, Val MAE: 1.3373196125030518\n",
      "Epoch 1789/2000, Train Loss: 5.343244745932205, Val Loss: 4.285999287329279, Val MAE: 1.3374520540237427\n",
      "Epoch 1790/2000, Train Loss: 5.343037058522288, Val Loss: 4.285932046416644, Val MAE: 1.3375710248947144\n",
      "Epoch 1791/2000, Train Loss: 5.342925537395031, Val Loss: 4.2859262956692294, Val MAE: 1.3377280235290527\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1792/2000, Train Loss: 5.342690662735152, Val Loss: 4.2861163351986855, Val MAE: 1.3379924297332764\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1793/2000, Train Loss: 5.342436827521614, Val Loss: 4.286005258184296, Val MAE: 1.338186502456665\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1794/2000, Train Loss: 5.342204473505898, Val Loss: 4.285908900698026, Val MAE: 1.3383164405822754\n",
      "Epoch 1795/2000, Train Loss: 5.342071408205397, Val Loss: 4.2858732072888195, Val MAE: 1.338436484336853\n",
      "Epoch 1796/2000, Train Loss: 5.341905535682571, Val Loss: 4.285841329274951, Val MAE: 1.3385913372039795\n",
      "Epoch 1797/2000, Train Loss: 5.341739110753243, Val Loss: 4.28573318488963, Val MAE: 1.33867347240448\n",
      "Epoch 1798/2000, Train Loss: 5.341576704182974, Val Loss: 4.285715035409541, Val MAE: 1.338808298110962\n",
      "Epoch 1799/2000, Train Loss: 5.341411257310888, Val Loss: 4.285700115480939, Val MAE: 1.338930368423462\n",
      "Epoch 1800/2000, Train Loss: 5.341199329974313, Val Loss: 4.285919744017962, Val MAE: 1.3391923904418945\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1801/2000, Train Loss: 5.341024805900645, Val Loss: 4.285838006477098, Val MAE: 1.3392857313156128\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1802/2000, Train Loss: 5.3408729862683275, Val Loss: 4.285725738363223, Val MAE: 1.3393893241882324\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1803/2000, Train Loss: 5.340719299178786, Val Loss: 4.285750249592033, Val MAE: 1.339582085609436\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1804/2000, Train Loss: 5.340572215465599, Val Loss: 4.285659005298271, Val MAE: 1.3396719694137573\n",
      "Epoch 1805/2000, Train Loss: 5.340350688898619, Val Loss: 4.285609182285833, Val MAE: 1.339816927909851\n",
      "Epoch 1806/2000, Train Loss: 5.340193569939148, Val Loss: 4.285569576127035, Val MAE: 1.3399912118911743\n",
      "Epoch 1807/2000, Train Loss: 5.3399680632162205, Val Loss: 4.285385303373809, Val MAE: 1.3400218486785889\n",
      "Epoch 1808/2000, Train Loss: 5.339860039212495, Val Loss: 4.285335077036608, Val MAE: 1.3401672840118408\n",
      "Epoch 1809/2000, Train Loss: 5.339648874613126, Val Loss: 4.285322621035147, Val MAE: 1.3403164148330688\n",
      "Epoch 1810/2000, Train Loss: 5.339463863283536, Val Loss: 4.285361351059364, Val MAE: 1.340524673461914\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1811/2000, Train Loss: 5.33933887934071, Val Loss: 4.285378960580439, Val MAE: 1.3406578302383423\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1812/2000, Train Loss: 5.339195651122821, Val Loss: 4.2852827529917965, Val MAE: 1.340753197669983\n",
      "Epoch 1813/2000, Train Loss: 5.339005074151407, Val Loss: 4.285241840336774, Val MAE: 1.340889573097229\n",
      "Epoch 1814/2000, Train Loss: 5.338832628336414, Val Loss: 4.285136640957884, Val MAE: 1.3409777879714966\n",
      "Epoch 1815/2000, Train Loss: 5.33871776562957, Val Loss: 4.284951677569398, Val MAE: 1.3410558700561523\n",
      "Epoch 1816/2000, Train Loss: 5.338560575848901, Val Loss: 4.2848131017105, Val MAE: 1.3411504030227661\n",
      "Epoch 1817/2000, Train Loss: 5.338329736230526, Val Loss: 4.284870127514676, Val MAE: 1.3413279056549072\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1818/2000, Train Loss: 5.338146376721387, Val Loss: 4.284751598389299, Val MAE: 1.3414157629013062\n",
      "Epoch 1819/2000, Train Loss: 5.337986836195364, Val Loss: 4.284766663329021, Val MAE: 1.341599941253662\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1820/2000, Train Loss: 5.337810809452336, Val Loss: 4.284611480864319, Val MAE: 1.341687798500061\n",
      "Epoch 1821/2000, Train Loss: 5.337628738975004, Val Loss: 4.284639657483445, Val MAE: 1.341861367225647\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1822/2000, Train Loss: 5.337557139151182, Val Loss: 4.284703983433611, Val MAE: 1.3420374393463135\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1823/2000, Train Loss: 5.337252626181021, Val Loss: 4.28452489504943, Val MAE: 1.342079997062683\n",
      "Epoch 1824/2000, Train Loss: 5.337134880506304, Val Loss: 4.284399140444962, Val MAE: 1.342195987701416\n",
      "Epoch 1825/2000, Train Loss: 5.336908301771524, Val Loss: 4.284287218118573, Val MAE: 1.342325210571289\n",
      "Epoch 1826/2000, Train Loss: 5.336732676918161, Val Loss: 4.2842530522529065, Val MAE: 1.3425155878067017\n",
      "Epoch 1827/2000, Train Loss: 5.3365417337640775, Val Loss: 4.284305841965718, Val MAE: 1.3426963090896606\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1828/2000, Train Loss: 5.336363318557858, Val Loss: 4.284490417118545, Val MAE: 1.3429757356643677\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1829/2000, Train Loss: 5.336168572980789, Val Loss: 4.284503380242769, Val MAE: 1.3431345224380493\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1830/2000, Train Loss: 5.3360238171962795, Val Loss: 4.284429547190666, Val MAE: 1.3432334661483765\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1831/2000, Train Loss: 5.3358876441551635, Val Loss: 4.284580532521815, Val MAE: 1.3434330224990845\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1832/2000, Train Loss: 5.335762663524999, Val Loss: 4.284534055021432, Val MAE: 1.3435381650924683\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1833/2000, Train Loss: 5.335596448592202, Val Loss: 4.2845262248236855, Val MAE: 1.3436825275421143\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1834/2000, Train Loss: 5.335409536376572, Val Loss: 4.284448497359817, Val MAE: 1.3437740802764893\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1835/2000, Train Loss: 5.3352057967282684, Val Loss: 4.284259384628888, Val MAE: 1.3438304662704468\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1836/2000, Train Loss: 5.3350836654163185, Val Loss: 4.28419058360495, Val MAE: 1.3440266847610474\n",
      "Epoch 1837/2000, Train Loss: 5.334866754908272, Val Loss: 4.284138000790064, Val MAE: 1.3442177772521973\n",
      "Epoch 1838/2000, Train Loss: 5.334704175196274, Val Loss: 4.284078135817975, Val MAE: 1.3443166017532349\n",
      "Epoch 1839/2000, Train Loss: 5.334549884342366, Val Loss: 4.283993752341012, Val MAE: 1.3444019556045532\n",
      "Epoch 1840/2000, Train Loss: 5.3344301076696, Val Loss: 4.283941422899564, Val MAE: 1.3445228338241577\n",
      "Epoch 1841/2000, Train Loss: 5.33425669662666, Val Loss: 4.283909239854898, Val MAE: 1.3446518182754517\n",
      "Epoch 1842/2000, Train Loss: 5.334084186836636, Val Loss: 4.283847897439389, Val MAE: 1.3447232246398926\n",
      "Epoch 1843/2000, Train Loss: 5.333930222738924, Val Loss: 4.283834355720528, Val MAE: 1.3448771238327026\n",
      "Epoch 1844/2000, Train Loss: 5.333794080522987, Val Loss: 4.283715849712088, Val MAE: 1.3449335098266602\n",
      "Epoch 1845/2000, Train Loss: 5.333591360011822, Val Loss: 4.2838111350665224, Val MAE: 1.3451472520828247\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1846/2000, Train Loss: 5.333556160307525, Val Loss: 4.283583831089037, Val MAE: 1.3451827764511108\n",
      "Epoch 1847/2000, Train Loss: 5.333298982212584, Val Loss: 4.283521579648998, Val MAE: 1.3453153371810913\n",
      "Epoch 1848/2000, Train Loss: 5.3331511834761285, Val Loss: 4.283427236396987, Val MAE: 1.3454079627990723\n",
      "Epoch 1849/2000, Train Loss: 5.332994606944496, Val Loss: 4.283378208166845, Val MAE: 1.3455458879470825\n",
      "Epoch 1850/2000, Train Loss: 5.332819202202902, Val Loss: 4.283326988875329, Val MAE: 1.3456445932388306\n",
      "Epoch 1851/2000, Train Loss: 5.332725910275198, Val Loss: 4.2833642315757166, Val MAE: 1.3457832336425781\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1852/2000, Train Loss: 5.332505171086971, Val Loss: 4.283342579893164, Val MAE: 1.345920443534851\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1853/2000, Train Loss: 5.332326993183487, Val Loss: 4.283341460748836, Val MAE: 1.3460873365402222\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1854/2000, Train Loss: 5.332106163274851, Val Loss: 4.283433667335425, Val MAE: 1.3463126420974731\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1855/2000, Train Loss: 5.331999148667881, Val Loss: 4.283131063521445, Val MAE: 1.3463144302368164\n",
      "Epoch 1856/2000, Train Loss: 5.331796575064499, Val Loss: 4.283169725510451, Val MAE: 1.3464716672897339\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1857/2000, Train Loss: 5.331631990751126, Val Loss: 4.28315788926305, Val MAE: 1.3466026782989502\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1858/2000, Train Loss: 5.331493146333977, Val Loss: 4.282889165722572, Val MAE: 1.3465955257415771\n",
      "Epoch 1859/2000, Train Loss: 5.331362144274756, Val Loss: 4.282763117014825, Val MAE: 1.3466426134109497\n",
      "Epoch 1860/2000, Train Loss: 5.331242777070091, Val Loss: 4.2828016703193255, Val MAE: 1.3468139171600342\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1861/2000, Train Loss: 5.3310618983602005, Val Loss: 4.282902992657713, Val MAE: 1.3470449447631836\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1862/2000, Train Loss: 5.330900871437537, Val Loss: 4.282871045293034, Val MAE: 1.3471713066101074\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1863/2000, Train Loss: 5.330880958092938, Val Loss: 4.283069880024807, Val MAE: 1.3473891019821167\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1864/2000, Train Loss: 5.330568816472141, Val Loss: 4.282997906825564, Val MAE: 1.3475223779678345\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1865/2000, Train Loss: 5.33038312856939, Val Loss: 4.282925245998142, Val MAE: 1.3477320671081543\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1866/2000, Train Loss: 5.330192927358303, Val Loss: 4.282862806857169, Val MAE: 1.3478264808654785\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1867/2000, Train Loss: 5.330100253108139, Val Loss: 4.282839967941379, Val MAE: 1.3479347229003906\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1868/2000, Train Loss: 5.329938272162458, Val Loss: 4.28272748557297, Val MAE: 1.3480141162872314\n",
      "Epoch 1869/2000, Train Loss: 5.329805650502768, Val Loss: 4.282729305018176, Val MAE: 1.348145842552185\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1870/2000, Train Loss: 5.329658525387136, Val Loss: 4.282791537314922, Val MAE: 1.3483272790908813\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1871/2000, Train Loss: 5.329474351800362, Val Loss: 4.282688449014414, Val MAE: 1.3484172821044922\n",
      "Epoch 1872/2000, Train Loss: 5.329329968614623, Val Loss: 4.28271395773501, Val MAE: 1.348535418510437\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1873/2000, Train Loss: 5.3292007922382325, Val Loss: 4.2825677451518205, Val MAE: 1.348597526550293\n",
      "Epoch 1874/2000, Train Loss: 5.328999902454441, Val Loss: 4.282510293738262, Val MAE: 1.348698377609253\n",
      "Epoch 1875/2000, Train Loss: 5.328867463351412, Val Loss: 4.282321917446884, Val MAE: 1.3487560749053955\n",
      "Epoch 1876/2000, Train Loss: 5.328716772022932, Val Loss: 4.282342344280836, Val MAE: 1.3489071130752563\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1877/2000, Train Loss: 5.328704435209774, Val Loss: 4.282654582756059, Val MAE: 1.3491930961608887\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1878/2000, Train Loss: 5.328390281345236, Val Loss: 4.282545470036902, Val MAE: 1.349281907081604\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1879/2000, Train Loss: 5.328277170727852, Val Loss: 4.282458339510737, Val MAE: 1.3493664264678955\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1880/2000, Train Loss: 5.328111734851474, Val Loss: 4.282440946955939, Val MAE: 1.3494997024536133\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1881/2000, Train Loss: 5.327922674497464, Val Loss: 4.282307527543188, Val MAE: 1.3495514392852783\n",
      "Epoch 1882/2000, Train Loss: 5.327864339961649, Val Loss: 4.282205547808527, Val MAE: 1.3495935201644897\n",
      "Epoch 1883/2000, Train Loss: 5.327755841212972, Val Loss: 4.282268543098424, Val MAE: 1.3497556447982788\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1884/2000, Train Loss: 5.327591394671412, Val Loss: 4.282090007641294, Val MAE: 1.3498075008392334\n",
      "Epoch 1885/2000, Train Loss: 5.327427027749941, Val Loss: 4.282178057099248, Val MAE: 1.3499947786331177\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1886/2000, Train Loss: 5.327284427037152, Val Loss: 4.282062277063593, Val MAE: 1.3500804901123047\n",
      "Epoch 1887/2000, Train Loss: 5.327092016728917, Val Loss: 4.282083953071285, Val MAE: 1.3502254486083984\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1888/2000, Train Loss: 5.326975234771109, Val Loss: 4.28211929929686, Val MAE: 1.3503719568252563\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1889/2000, Train Loss: 5.3267424523179505, Val Loss: 4.282107732666505, Val MAE: 1.3506383895874023\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1890/2000, Train Loss: 5.326645325665913, Val Loss: 4.282144277321326, Val MAE: 1.3507903814315796\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1891/2000, Train Loss: 5.326444427792256, Val Loss: 4.282124789367925, Val MAE: 1.3509312868118286\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1892/2000, Train Loss: 5.32628572166152, Val Loss: 4.28219604250547, Val MAE: 1.3510922193527222\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1893/2000, Train Loss: 5.326166785823387, Val Loss: 4.282199345918389, Val MAE: 1.3512307405471802\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1894/2000, Train Loss: 5.325996933252131, Val Loss: 4.282115166922948, Val MAE: 1.351324200630188\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1895/2000, Train Loss: 5.325880844395916, Val Loss: 4.282079259581394, Val MAE: 1.3514348268508911\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1896/2000, Train Loss: 5.325668876517024, Val Loss: 4.2821411473525535, Val MAE: 1.3516203165054321\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1897/2000, Train Loss: 5.325478773407185, Val Loss: 4.28185467360256, Val MAE: 1.3516173362731934\n",
      "Epoch 1898/2000, Train Loss: 5.325414182987302, Val Loss: 4.281787629304706, Val MAE: 1.3517107963562012\n",
      "Epoch 1899/2000, Train Loss: 5.325237792641993, Val Loss: 4.281792038867065, Val MAE: 1.351874828338623\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1900/2000, Train Loss: 5.3251214878235515, Val Loss: 4.281731804534122, Val MAE: 1.35196852684021\n",
      "Epoch 1901/2000, Train Loss: 5.324942316540318, Val Loss: 4.281718922252054, Val MAE: 1.3520902395248413\n",
      "Epoch 1902/2000, Train Loss: 5.324773763229434, Val Loss: 4.281984697940113, Val MAE: 1.3524365425109863\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1903/2000, Train Loss: 5.324676135772848, Val Loss: 4.28202793042402, Val MAE: 1.3526060581207275\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1904/2000, Train Loss: 5.324427654934747, Val Loss: 4.282208379539283, Val MAE: 1.3528090715408325\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1905/2000, Train Loss: 5.32434835490682, Val Loss: 4.282260004464571, Val MAE: 1.352946162223816\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1906/2000, Train Loss: 5.324243974016163, Val Loss: 4.282240505691047, Val MAE: 1.3530290126800537\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1907/2000, Train Loss: 5.324208528872771, Val Loss: 4.282599905589679, Val MAE: 1.3533440828323364\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1908/2000, Train Loss: 5.323978023112471, Val Loss: 4.2825185012709985, Val MAE: 1.353489637374878\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1909/2000, Train Loss: 5.323801029490978, Val Loss: 4.2824391485603, Val MAE: 1.3535616397857666\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1910/2000, Train Loss: 5.323684170168015, Val Loss: 4.282356519983695, Val MAE: 1.3536314964294434\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1911/2000, Train Loss: 5.323575979421226, Val Loss: 4.28224503008632, Val MAE: 1.3536930084228516\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1912/2000, Train Loss: 5.323440460258639, Val Loss: 4.282204695220466, Val MAE: 1.3537715673446655\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1913/2000, Train Loss: 5.323313971205546, Val Loss: 4.282171283541499, Val MAE: 1.353868842124939\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1914/2000, Train Loss: 5.323180059003012, Val Loss: 4.282207769101804, Val MAE: 1.3540047407150269\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1915/2000, Train Loss: 5.323065229585501, Val Loss: 4.2820951927621085, Val MAE: 1.354082465171814\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1916/2000, Train Loss: 5.322907550751512, Val Loss: 4.28222698032319, Val MAE: 1.354245901107788\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1917/2000, Train Loss: 5.32278812917272, Val Loss: 4.282217261222032, Val MAE: 1.3543537855148315\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 1918/2000, Train Loss: 5.322698468351141, Val Loss: 4.282068169922442, Val MAE: 1.3543742895126343\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 1919/2000, Train Loss: 5.322568935090778, Val Loss: 4.282034090456662, Val MAE: 1.3544965982437134\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 1920/2000, Train Loss: 5.3224215745553956, Val Loss: 4.282176918757928, Val MAE: 1.35474693775177\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 1921/2000, Train Loss: 5.32230065653737, Val Loss: 4.282062612809576, Val MAE: 1.3548691272735596\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.537674903869629\n",
      "Test Mean Absolute Error (MAE): 1.6349790034668188\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(CNNModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (conv1d): Conv1d(10, 64, kernel_size=(3,), stride=(1,))\n",
       "   (convolutional_stack): Sequential(\n",
       "     (0): Conv1d(10, 10, kernel_size=(3,), stride=(1,))\n",
       "     (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "     (2): Conv1d(10, 64, kernel_size=(2,), stride=(1,))\n",
       "     (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "   )\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=65, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'test_mae': 1.6349790034668188})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 229\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_cnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=10,\n",
    "                kernel_size=3,\n",
    "                num_filters=64,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = NUM_FEATURES_DICT['GK']['large'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpremier.cnn.experiment import gridsearch_cnn\n",
    "\n",
    "#gridsearch_cnn(epochs=100, verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized)  {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
