{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(os.path.join(os.getcwd(), '..','..'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from final_project.cnn.preprocess import generate_cnn_data, split_preprocess_cnn_data, preprocess_cnn_data\n",
    "from final_project.cnn_deep.model import build_train_cnn, full_cnn_pipeline\n",
    "from final_project.cnn.evaluate import gridsearch_analysis\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "\n",
    "from final_project.cnn_deep.config import STANDARD_CAT_FEATURES, STANDARD_NUM_FEATURES, NUM_FEATURES_DICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Generating CNN Data for Season: ['2020-21', '2021-22'], Position: GK =======\n",
      "Dropping Players with Avg. Playtime < 1e-06...\n",
      "\n",
      "Total players of type GK = 163.\n",
      "82 players dropped due to low average playtime.\n",
      "Generated windowed dataframe for CNN of shape: (2178, 7).\n",
      "Generated combined features dataframe for preprocessing of shape: (2988, 9).\n",
      "\n",
      "========== EDA ==========\n",
      "========== Done Generating CNN Data ==========\n",
      "\n",
      "========== Splitting CNN Data ==========\n",
      "\n",
      "=== Stratifying Split by : Stdev ===\n",
      "Shape of windowed_df: (2178, 7)\n",
      "Shape of a given window (prior to preprocessing): (10, 9)\n",
      "stdev Distribution of Players:\n",
      "\n",
      "========== Preprocessing CNN Data ==========\n",
      "\n",
      "Mean of Standard Scaler:\n",
      "[1.94211124e+00 4.79057889e+01 0.00000000e+00 1.70261067e-03\n",
      " 1.45289444e-01 9.96594779e+00 2.66742338e-02 1.13507378e-03]\n",
      "\n",
      "Standard Deviation of Standard Scaler:\n",
      "[2.73976222e+00 4.48207323e+01 1.00000000e+00 4.12275610e-02\n",
      " 3.52392425e-01 1.07559860e+01 1.61129510e-01 3.36717298e-02]\n",
      "Transforming features using StandardScaler + OHE Pipeline.\n",
      "========== Done Preprocessing CNN Data ==========\n",
      "\n",
      "========== Done Splitting CNN Data ==========\n",
      "\n",
      "====== Building CNN Architecture ======\n",
      "====== Done Building CNN Architecture ======\n",
      "Epoch 1/2000, Train Loss: 11.309990027393454, Val Loss: 7.979194960095458, Val MAE: 1.4797776937484741\n",
      "Epoch 2/2000, Train Loss: 11.304554339783055, Val Loss: 7.975396643746812, Val MAE: 1.4794763326644897\n",
      "Epoch 3/2000, Train Loss: 11.299192334858006, Val Loss: 7.971756492024875, Val MAE: 1.479196548461914\n",
      "Epoch 4/2000, Train Loss: 11.293875945711656, Val Loss: 7.967898703957195, Val MAE: 1.478899359703064\n",
      "Epoch 5/2000, Train Loss: 11.288437082654145, Val Loss: 7.964130787876947, Val MAE: 1.478617548942566\n",
      "Epoch 6/2000, Train Loss: 11.283254852532968, Val Loss: 7.960504362654981, Val MAE: 1.4783512353897095\n",
      "Epoch 7/2000, Train Loss: 11.278013821511113, Val Loss: 7.956758429096626, Val MAE: 1.4780840873718262\n",
      "Epoch 8/2000, Train Loss: 11.272847513326802, Val Loss: 7.953009526740323, Val MAE: 1.477811336517334\n",
      "Epoch 9/2000, Train Loss: 11.267516080377254, Val Loss: 7.949296243092767, Val MAE: 1.4775397777557373\n",
      "Epoch 10/2000, Train Loss: 11.262313015561395, Val Loss: 7.9457089463332755, Val MAE: 1.4772762060165405\n",
      "Epoch 11/2000, Train Loss: 11.257085683289734, Val Loss: 7.941978445598806, Val MAE: 1.4770020246505737\n",
      "Epoch 12/2000, Train Loss: 11.251960569908393, Val Loss: 7.938330641687587, Val MAE: 1.4767342805862427\n",
      "Epoch 13/2000, Train Loss: 11.246583743697954, Val Loss: 7.934531112750237, Val MAE: 1.4764560461044312\n",
      "Epoch 14/2000, Train Loss: 11.241441428010438, Val Loss: 7.930923690126689, Val MAE: 1.4762033224105835\n",
      "Epoch 15/2000, Train Loss: 11.236364752937591, Val Loss: 7.9272793628376075, Val MAE: 1.4759433269500732\n",
      "Epoch 16/2000, Train Loss: 11.231149260785761, Val Loss: 7.923672558954573, Val MAE: 1.4756864309310913\n",
      "Epoch 17/2000, Train Loss: 11.226130776955817, Val Loss: 7.920129181051979, Val MAE: 1.475435495376587\n",
      "Epoch 18/2000, Train Loss: 11.221081355804586, Val Loss: 7.916475785768649, Val MAE: 1.4751739501953125\n",
      "Epoch 19/2000, Train Loss: 11.21594461894379, Val Loss: 7.912930063715389, Val MAE: 1.474926471710205\n",
      "Epoch 20/2000, Train Loss: 11.21090683550247, Val Loss: 7.909273300054776, Val MAE: 1.4746683835983276\n",
      "Epoch 21/2000, Train Loss: 11.205735870211294, Val Loss: 7.90564394213662, Val MAE: 1.4744175672531128\n",
      "Epoch 22/2000, Train Loss: 11.200628941962945, Val Loss: 7.902018181586991, Val MAE: 1.474161982536316\n",
      "Epoch 23/2000, Train Loss: 11.195475242422479, Val Loss: 7.898431330654132, Val MAE: 1.473923683166504\n",
      "Epoch 24/2000, Train Loss: 11.190366092968564, Val Loss: 7.894781900334868, Val MAE: 1.4736785888671875\n",
      "Epoch 25/2000, Train Loss: 11.185246311813918, Val Loss: 7.891132108481215, Val MAE: 1.4734218120574951\n",
      "Epoch 26/2000, Train Loss: 11.18011769043302, Val Loss: 7.8874480974701076, Val MAE: 1.4731724262237549\n",
      "Epoch 27/2000, Train Loss: 11.174876028459733, Val Loss: 7.883858574483845, Val MAE: 1.4729238748550415\n",
      "Epoch 28/2000, Train Loss: 11.169766235277173, Val Loss: 7.880171979628168, Val MAE: 1.4726661443710327\n",
      "Epoch 29/2000, Train Loss: 11.164577821310132, Val Loss: 7.876519392920775, Val MAE: 1.4724210500717163\n",
      "Epoch 30/2000, Train Loss: 11.159433101888677, Val Loss: 7.872860404402688, Val MAE: 1.4721744060516357\n",
      "Epoch 31/2000, Train Loss: 11.15424935494124, Val Loss: 7.869237189494946, Val MAE: 1.4719277620315552\n",
      "Epoch 32/2000, Train Loss: 11.14897195309075, Val Loss: 7.865440572184083, Val MAE: 1.4716607332229614\n",
      "Epoch 33/2000, Train Loss: 11.14375317747619, Val Loss: 7.861788866004428, Val MAE: 1.4714144468307495\n",
      "Epoch 34/2000, Train Loss: 11.138550732325466, Val Loss: 7.858221800803198, Val MAE: 1.4711800813674927\n",
      "Epoch 35/2000, Train Loss: 11.133412246585078, Val Loss: 7.854517908320502, Val MAE: 1.4709420204162598\n",
      "Epoch 36/2000, Train Loss: 11.128197164691741, Val Loss: 7.850861094146967, Val MAE: 1.4707080125808716\n",
      "Epoch 37/2000, Train Loss: 11.122993958396213, Val Loss: 7.847176885524312, Val MAE: 1.4704594612121582\n",
      "Epoch 38/2000, Train Loss: 11.117806116987873, Val Loss: 7.843499764662471, Val MAE: 1.4702073335647583\n",
      "Epoch 39/2000, Train Loss: 11.11254419848997, Val Loss: 7.839772383908968, Val MAE: 1.4699625968933105\n",
      "Epoch 40/2000, Train Loss: 11.107355503879731, Val Loss: 7.836070528562676, Val MAE: 1.469721794128418\n",
      "Epoch 41/2000, Train Loss: 11.102115228661741, Val Loss: 7.83239864631816, Val MAE: 1.469482660293579\n",
      "Epoch 42/2000, Train Loss: 11.096931357837505, Val Loss: 7.828674141465275, Val MAE: 1.4692327976226807\n",
      "Epoch 43/2000, Train Loss: 11.09158931023989, Val Loss: 7.82495860288272, Val MAE: 1.4689861536026\n",
      "Epoch 44/2000, Train Loss: 11.086200593972169, Val Loss: 7.821188309295355, Val MAE: 1.468741774559021\n",
      "Epoch 45/2000, Train Loss: 11.080946318840677, Val Loss: 7.817519635502417, Val MAE: 1.468508005142212\n",
      "Epoch 46/2000, Train Loss: 11.075768806447105, Val Loss: 7.813858298031059, Val MAE: 1.468268871307373\n",
      "Epoch 47/2000, Train Loss: 11.07058993144638, Val Loss: 7.810140340822237, Val MAE: 1.4680308103561401\n",
      "Epoch 48/2000, Train Loss: 11.065155556718942, Val Loss: 7.8064029394607015, Val MAE: 1.4677854776382446\n",
      "Epoch 49/2000, Train Loss: 11.059919944529199, Val Loss: 7.8026250362597604, Val MAE: 1.4675347805023193\n",
      "Epoch 50/2000, Train Loss: 11.054655715186794, Val Loss: 7.799036700671186, Val MAE: 1.4673069715499878\n",
      "Epoch 51/2000, Train Loss: 11.0495075533245, Val Loss: 7.795384654933, Val MAE: 1.4670753479003906\n",
      "Epoch 52/2000, Train Loss: 11.044260239177754, Val Loss: 7.791762552185504, Val MAE: 1.4668371677398682\n",
      "Epoch 53/2000, Train Loss: 11.039106182300728, Val Loss: 7.788030258361418, Val MAE: 1.4665971994400024\n",
      "Epoch 54/2000, Train Loss: 11.033811240411957, Val Loss: 7.78437878473385, Val MAE: 1.4663653373718262\n",
      "Epoch 55/2000, Train Loss: 11.028540462952881, Val Loss: 7.780544509201705, Val MAE: 1.4661211967468262\n",
      "Epoch 56/2000, Train Loss: 11.023237055626316, Val Loss: 7.776871206215373, Val MAE: 1.4659010171890259\n",
      "Epoch 57/2000, Train Loss: 11.017996073048126, Val Loss: 7.773290975203922, Val MAE: 1.465682864189148\n",
      "Epoch 58/2000, Train Loss: 11.012796297088242, Val Loss: 7.769578667783791, Val MAE: 1.4654567241668701\n",
      "Epoch 59/2000, Train Loss: 11.00748792936947, Val Loss: 7.765902057155833, Val MAE: 1.465237021446228\n",
      "Epoch 60/2000, Train Loss: 11.001988105362575, Val Loss: 7.762008821695774, Val MAE: 1.464992642402649\n",
      "Epoch 61/2000, Train Loss: 10.996663187720193, Val Loss: 7.758421583983813, Val MAE: 1.4647767543792725\n",
      "Epoch 62/2000, Train Loss: 10.991449047920298, Val Loss: 7.754744490340084, Val MAE: 1.4645588397979736\n",
      "Epoch 63/2000, Train Loss: 10.986199111908721, Val Loss: 7.750939174610618, Val MAE: 1.4643203020095825\n",
      "Epoch 64/2000, Train Loss: 10.980852754356546, Val Loss: 7.7472755984915, Val MAE: 1.4640998840332031\n",
      "Epoch 65/2000, Train Loss: 10.975616264082898, Val Loss: 7.743660283367242, Val MAE: 1.4638848304748535\n",
      "Epoch 66/2000, Train Loss: 10.970367827458531, Val Loss: 7.739923034276108, Val MAE: 1.4636571407318115\n",
      "Epoch 67/2000, Train Loss: 10.965139356306674, Val Loss: 7.736185211322463, Val MAE: 1.4634228944778442\n",
      "Epoch 68/2000, Train Loss: 10.959862990609942, Val Loss: 7.732453936734447, Val MAE: 1.4632023572921753\n",
      "Epoch 69/2000, Train Loss: 10.95455735336042, Val Loss: 7.728749795305031, Val MAE: 1.4629937410354614\n",
      "Epoch 70/2000, Train Loss: 10.949199015934271, Val Loss: 7.724930664826487, Val MAE: 1.4628046751022339\n",
      "Epoch 71/2000, Train Loss: 10.94384986859588, Val Loss: 7.721188947692648, Val MAE: 1.4626240730285645\n",
      "Epoch 72/2000, Train Loss: 10.938496686367088, Val Loss: 7.717445249630658, Val MAE: 1.4624500274658203\n",
      "Epoch 73/2000, Train Loss: 10.93308339364443, Val Loss: 7.71363808789903, Val MAE: 1.462268590927124\n",
      "Epoch 74/2000, Train Loss: 10.927761622412529, Val Loss: 7.709804130513389, Val MAE: 1.4620797634124756\n",
      "Epoch 75/2000, Train Loss: 10.922261342243546, Val Loss: 7.705856624111399, Val MAE: 1.4619005918502808\n",
      "Epoch 76/2000, Train Loss: 10.916886709037698, Val Loss: 7.702155019457007, Val MAE: 1.4617199897766113\n",
      "Epoch 77/2000, Train Loss: 10.911472236868372, Val Loss: 7.698300980024778, Val MAE: 1.4615403413772583\n",
      "Epoch 78/2000, Train Loss: 10.906140415454153, Val Loss: 7.694602278707264, Val MAE: 1.4613755941390991\n",
      "Epoch 79/2000, Train Loss: 10.900888609253858, Val Loss: 7.690942578104971, Val MAE: 1.4612101316452026\n",
      "Epoch 80/2000, Train Loss: 10.895615837503335, Val Loss: 7.687081830606267, Val MAE: 1.4610182046890259\n",
      "Epoch 81/2000, Train Loss: 10.890219752762507, Val Loss: 7.683379851053427, Val MAE: 1.4608410596847534\n",
      "Epoch 82/2000, Train Loss: 10.884874663049828, Val Loss: 7.679596354906355, Val MAE: 1.4606668949127197\n",
      "Epoch 83/2000, Train Loss: 10.879568171761523, Val Loss: 7.67588906792907, Val MAE: 1.4604969024658203\n",
      "Epoch 84/2000, Train Loss: 10.874246025978124, Val Loss: 7.672019511058524, Val MAE: 1.4603044986724854\n",
      "Epoch 85/2000, Train Loss: 10.86874460169752, Val Loss: 7.668112360135661, Val MAE: 1.460107684135437\n",
      "Epoch 86/2000, Train Loss: 10.863175471404832, Val Loss: 7.664293854307752, Val MAE: 1.4599255323410034\n",
      "Epoch 87/2000, Train Loss: 10.857885884234388, Val Loss: 7.660528851414586, Val MAE: 1.4597433805465698\n",
      "Epoch 88/2000, Train Loss: 10.852551143366535, Val Loss: 7.656761244765004, Val MAE: 1.4595636129379272\n",
      "Epoch 89/2000, Train Loss: 10.847015700539844, Val Loss: 7.652881138436161, Val MAE: 1.459380030632019\n",
      "Epoch 90/2000, Train Loss: 10.84167505017309, Val Loss: 7.649169590001976, Val MAE: 1.4591965675354004\n",
      "Epoch 91/2000, Train Loss: 10.83636163809742, Val Loss: 7.645331279262229, Val MAE: 1.4590181112289429\n",
      "Epoch 92/2000, Train Loss: 10.83095746149139, Val Loss: 7.6414902484631755, Val MAE: 1.458824872970581\n",
      "Epoch 93/2000, Train Loss: 10.825596769216839, Val Loss: 7.637697142819027, Val MAE: 1.4586472511291504\n",
      "Epoch 94/2000, Train Loss: 10.819955398377093, Val Loss: 7.633638913016599, Val MAE: 1.4584521055221558\n",
      "Epoch 95/2000, Train Loss: 10.814571794221257, Val Loss: 7.629943998762079, Val MAE: 1.4582711458206177\n",
      "Epoch 96/2000, Train Loss: 10.809038163719237, Val Loss: 7.626035512594489, Val MAE: 1.4580720663070679\n",
      "Epoch 97/2000, Train Loss: 10.803649339958584, Val Loss: 7.62224533110857, Val MAE: 1.457887887954712\n",
      "Epoch 98/2000, Train Loss: 10.798234180801558, Val Loss: 7.618521026018503, Val MAE: 1.4577043056488037\n",
      "Epoch 99/2000, Train Loss: 10.792752544147175, Val Loss: 7.614590643272475, Val MAE: 1.4574968814849854\n",
      "Epoch 100/2000, Train Loss: 10.787302175822251, Val Loss: 7.610763010275257, Val MAE: 1.4573267698287964\n",
      "Epoch 101/2000, Train Loss: 10.78196547099196, Val Loss: 7.607101271825062, Val MAE: 1.4571497440338135\n",
      "Epoch 102/2000, Train Loss: 10.776741719669582, Val Loss: 7.603289558026973, Val MAE: 1.4569556713104248\n",
      "Epoch 103/2000, Train Loss: 10.771392759778385, Val Loss: 7.599584558935048, Val MAE: 1.4567722082138062\n",
      "Epoch 104/2000, Train Loss: 10.766078443125519, Val Loss: 7.595702639289267, Val MAE: 1.4566195011138916\n",
      "Epoch 105/2000, Train Loss: 10.760584043638197, Val Loss: 7.592038186894612, Val MAE: 1.456436038017273\n",
      "Epoch 106/2000, Train Loss: 10.755271858060603, Val Loss: 7.5881321852204495, Val MAE: 1.4562227725982666\n",
      "Epoch 107/2000, Train Loss: 10.749647661229936, Val Loss: 7.584181234797647, Val MAE: 1.4560226202011108\n",
      "Epoch 108/2000, Train Loss: 10.744240570365918, Val Loss: 7.5803834246152695, Val MAE: 1.4558440446853638\n",
      "Epoch 109/2000, Train Loss: 10.738817782697737, Val Loss: 7.576591053350015, Val MAE: 1.45564866065979\n",
      "Epoch 110/2000, Train Loss: 10.733398949300257, Val Loss: 7.572881221683981, Val MAE: 1.455454707145691\n",
      "Epoch 111/2000, Train Loss: 10.727933692113844, Val Loss: 7.569045513626691, Val MAE: 1.4552775621414185\n",
      "Epoch 112/2000, Train Loss: 10.722445748339577, Val Loss: 7.565186332528656, Val MAE: 1.455073356628418\n",
      "Epoch 113/2000, Train Loss: 10.716812036890833, Val Loss: 7.561176738290636, Val MAE: 1.454862356185913\n",
      "Epoch 114/2000, Train Loss: 10.711322504719037, Val Loss: 7.557422941359314, Val MAE: 1.454663872718811\n",
      "Epoch 115/2000, Train Loss: 10.70585836356218, Val Loss: 7.553574846235213, Val MAE: 1.454453945159912\n",
      "Epoch 116/2000, Train Loss: 10.700380654305267, Val Loss: 7.549691513986201, Val MAE: 1.4542371034622192\n",
      "Epoch 117/2000, Train Loss: 10.694880076392021, Val Loss: 7.545889649459639, Val MAE: 1.4540268182754517\n",
      "Epoch 118/2000, Train Loss: 10.68922206725745, Val Loss: 7.541848356807017, Val MAE: 1.4538191556930542\n",
      "Epoch 119/2000, Train Loss: 10.683711529150024, Val Loss: 7.538103919543393, Val MAE: 1.4536166191101074\n",
      "Epoch 120/2000, Train Loss: 10.67820620481012, Val Loss: 7.534154436388262, Val MAE: 1.4533895254135132\n",
      "Epoch 121/2000, Train Loss: 10.672630943113854, Val Loss: 7.530391379781403, Val MAE: 1.4531816244125366\n",
      "Epoch 122/2000, Train Loss: 10.667131777300664, Val Loss: 7.526546284247626, Val MAE: 1.4529709815979004\n",
      "Epoch 123/2000, Train Loss: 10.661575700105539, Val Loss: 7.522642384093624, Val MAE: 1.452759861946106\n",
      "Epoch 124/2000, Train Loss: 10.656067251601197, Val Loss: 7.518817367760448, Val MAE: 1.452557921409607\n",
      "Epoch 125/2000, Train Loss: 10.650333645935177, Val Loss: 7.5147928600711325, Val MAE: 1.4523241519927979\n",
      "Epoch 126/2000, Train Loss: 10.644728663710183, Val Loss: 7.5109663621866485, Val MAE: 1.4521236419677734\n",
      "Epoch 127/2000, Train Loss: 10.639217346953156, Val Loss: 7.50723274060869, Val MAE: 1.4519234895706177\n",
      "Epoch 128/2000, Train Loss: 10.633551593131841, Val Loss: 7.5030789311874555, Val MAE: 1.4517103433609009\n",
      "Epoch 129/2000, Train Loss: 10.6276421554375, Val Loss: 7.498995676213825, Val MAE: 1.4514760971069336\n",
      "Epoch 130/2000, Train Loss: 10.62198065968983, Val Loss: 7.495088097332297, Val MAE: 1.4512604475021362\n",
      "Epoch 131/2000, Train Loss: 10.616483801054693, Val Loss: 7.491261663999375, Val MAE: 1.451043725013733\n",
      "Epoch 132/2000, Train Loss: 10.610920554468649, Val Loss: 7.487475564397939, Val MAE: 1.4508417844772339\n",
      "Epoch 133/2000, Train Loss: 10.605470342680743, Val Loss: 7.483578205800002, Val MAE: 1.4506131410598755\n",
      "Epoch 134/2000, Train Loss: 10.599863787337139, Val Loss: 7.479627831944743, Val MAE: 1.4503945112228394\n",
      "Epoch 135/2000, Train Loss: 10.59411228911925, Val Loss: 7.475731823022838, Val MAE: 1.4501856565475464\n",
      "Epoch 136/2000, Train Loss: 10.588248238557377, Val Loss: 7.471616113313415, Val MAE: 1.4499455690383911\n",
      "Epoch 137/2000, Train Loss: 10.582650126607481, Val Loss: 7.467764841550374, Val MAE: 1.4497219324111938\n",
      "Epoch 138/2000, Train Loss: 10.577045241309774, Val Loss: 7.46387254399088, Val MAE: 1.4494961500167847\n",
      "Epoch 139/2000, Train Loss: 10.571304541482196, Val Loss: 7.459751458915773, Val MAE: 1.4492605924606323\n",
      "Epoch 140/2000, Train Loss: 10.565512866944122, Val Loss: 7.4557049943855755, Val MAE: 1.4490221738815308\n",
      "Epoch 141/2000, Train Loss: 10.559493631728913, Val Loss: 7.451610612432967, Val MAE: 1.44878089427948\n",
      "Epoch 142/2000, Train Loss: 10.55385647921034, Val Loss: 7.447700959135283, Val MAE: 1.448583722114563\n",
      "Epoch 143/2000, Train Loss: 10.548322992466169, Val Loss: 7.443794970791619, Val MAE: 1.4483494758605957\n",
      "Epoch 144/2000, Train Loss: 10.54268936238102, Val Loss: 7.439983988888898, Val MAE: 1.4481332302093506\n",
      "Epoch 145/2000, Train Loss: 10.537171898761331, Val Loss: 7.436144649814646, Val MAE: 1.4479104280471802\n",
      "Epoch 146/2000, Train Loss: 10.531594354909222, Val Loss: 7.43237684506688, Val MAE: 1.4476888179779053\n",
      "Epoch 147/2000, Train Loss: 10.526021537840274, Val Loss: 7.428425204693466, Val MAE: 1.4474536180496216\n",
      "Epoch 148/2000, Train Loss: 10.520249874997948, Val Loss: 7.424456055777836, Val MAE: 1.4472249746322632\n",
      "Epoch 149/2000, Train Loss: 10.514643308524967, Val Loss: 7.420553256309516, Val MAE: 1.4469842910766602\n",
      "Epoch 150/2000, Train Loss: 10.508969527138934, Val Loss: 7.416674661797447, Val MAE: 1.446753740310669\n",
      "Epoch 151/2000, Train Loss: 10.503129188430476, Val Loss: 7.41254164088551, Val MAE: 1.4464980363845825\n",
      "Epoch 152/2000, Train Loss: 10.497427766023904, Val Loss: 7.408555976220885, Val MAE: 1.4463062286376953\n",
      "Epoch 153/2000, Train Loss: 10.49179247500558, Val Loss: 7.404748777689429, Val MAE: 1.4460783004760742\n",
      "Epoch 154/2000, Train Loss: 10.486154287024844, Val Loss: 7.4008155875444945, Val MAE: 1.445822834968567\n",
      "Epoch 155/2000, Train Loss: 10.48051267555463, Val Loss: 7.396887173194875, Val MAE: 1.4455729722976685\n",
      "Epoch 156/2000, Train Loss: 10.474624461205256, Val Loss: 7.392767599506958, Val MAE: 1.4453052282333374\n",
      "Epoch 157/2000, Train Loss: 10.468635049885409, Val Loss: 7.388612125954918, Val MAE: 1.445028305053711\n",
      "Epoch 158/2000, Train Loss: 10.462844456227819, Val Loss: 7.384651810320111, Val MAE: 1.444779396057129\n",
      "Epoch 159/2000, Train Loss: 10.457097920910245, Val Loss: 7.380706746332549, Val MAE: 1.4445282220840454\n",
      "Epoch 160/2000, Train Loss: 10.451208810910419, Val Loss: 7.376571613818675, Val MAE: 1.4442737102508545\n",
      "Epoch 161/2000, Train Loss: 10.445360674686997, Val Loss: 7.37248852591928, Val MAE: 1.4439998865127563\n",
      "Epoch 162/2000, Train Loss: 10.439421359164852, Val Loss: 7.368427556927677, Val MAE: 1.4437464475631714\n",
      "Epoch 163/2000, Train Loss: 10.433662390746118, Val Loss: 7.364455778312844, Val MAE: 1.4434963464736938\n",
      "Epoch 164/2000, Train Loss: 10.42772296959078, Val Loss: 7.360500878901095, Val MAE: 1.443237066268921\n",
      "Epoch 165/2000, Train Loss: 10.421857185185235, Val Loss: 7.35647502775799, Val MAE: 1.4429770708084106\n",
      "Epoch 166/2000, Train Loss: 10.416118276658556, Val Loss: 7.352502092208948, Val MAE: 1.442733645439148\n",
      "Epoch 167/2000, Train Loss: 10.410237118904007, Val Loss: 7.34839100908186, Val MAE: 1.4424479007720947\n",
      "Epoch 168/2000, Train Loss: 10.404396772756591, Val Loss: 7.3444434400010215, Val MAE: 1.4421881437301636\n",
      "Epoch 169/2000, Train Loss: 10.398442788168719, Val Loss: 7.3402472575438455, Val MAE: 1.4418864250183105\n",
      "Epoch 170/2000, Train Loss: 10.39264462630202, Val Loss: 7.336392534765843, Val MAE: 1.4416347742080688\n",
      "Epoch 171/2000, Train Loss: 10.386956441049085, Val Loss: 7.332423169810224, Val MAE: 1.4414079189300537\n",
      "Epoch 172/2000, Train Loss: 10.381146570217219, Val Loss: 7.328402074661341, Val MAE: 1.4411355257034302\n",
      "Epoch 173/2000, Train Loss: 10.375372435670933, Val Loss: 7.324438822866829, Val MAE: 1.440867304801941\n",
      "Epoch 174/2000, Train Loss: 10.369604716248892, Val Loss: 7.320489854358875, Val MAE: 1.4405940771102905\n",
      "Epoch 175/2000, Train Loss: 10.363717458549417, Val Loss: 7.316393216907441, Val MAE: 1.4403225183486938\n",
      "Epoch 176/2000, Train Loss: 10.357547069674535, Val Loss: 7.312156833996912, Val MAE: 1.4400242567062378\n",
      "Epoch 177/2000, Train Loss: 10.351390469650768, Val Loss: 7.308018775472233, Val MAE: 1.439721703529358\n",
      "Epoch 178/2000, Train Loss: 10.345395475765473, Val Loss: 7.303872897388699, Val MAE: 1.4394313097000122\n",
      "Epoch 179/2000, Train Loss: 10.339485588014218, Val Loss: 7.299950271072957, Val MAE: 1.439164161682129\n",
      "Epoch 180/2000, Train Loss: 10.333647271214335, Val Loss: 7.2959164749327545, Val MAE: 1.4388773441314697\n",
      "Epoch 181/2000, Train Loss: 10.32758643511863, Val Loss: 7.2917508283646795, Val MAE: 1.4385920763015747\n",
      "Epoch 182/2000, Train Loss: 10.321731804347017, Val Loss: 7.287731301596573, Val MAE: 1.4383125305175781\n",
      "Epoch 183/2000, Train Loss: 10.315949911981031, Val Loss: 7.28373152824404, Val MAE: 1.4380289316177368\n",
      "Epoch 184/2000, Train Loss: 10.310140245976202, Val Loss: 7.279709144963606, Val MAE: 1.4377329349517822\n",
      "Epoch 185/2000, Train Loss: 10.304212013756429, Val Loss: 7.275635484451646, Val MAE: 1.4374414682388306\n",
      "Epoch 186/2000, Train Loss: 10.298098071688981, Val Loss: 7.271330735288762, Val MAE: 1.4371259212493896\n",
      "Epoch 187/2000, Train Loss: 10.291955202614508, Val Loss: 7.267214013166256, Val MAE: 1.436820149421692\n",
      "Epoch 188/2000, Train Loss: 10.28607330084219, Val Loss: 7.2632328207764, Val MAE: 1.4365249872207642\n",
      "Epoch 189/2000, Train Loss: 10.280168247669238, Val Loss: 7.2592025791269705, Val MAE: 1.4362431764602661\n",
      "Epoch 190/2000, Train Loss: 10.274260022524924, Val Loss: 7.2550951819430605, Val MAE: 1.435929775238037\n",
      "Epoch 191/2000, Train Loss: 10.268152801927277, Val Loss: 7.2508185939007515, Val MAE: 1.4356117248535156\n",
      "Epoch 192/2000, Train Loss: 10.262089290038658, Val Loss: 7.24679004378147, Val MAE: 1.4353199005126953\n",
      "Epoch 193/2000, Train Loss: 10.256172062640257, Val Loss: 7.24258093043222, Val MAE: 1.4349901676177979\n",
      "Epoch 194/2000, Train Loss: 10.25005848210613, Val Loss: 7.2385415708830765, Val MAE: 1.4346908330917358\n",
      "Epoch 195/2000, Train Loss: 10.243963364459423, Val Loss: 7.234239515883697, Val MAE: 1.4343764781951904\n",
      "Epoch 196/2000, Train Loss: 10.237900838870468, Val Loss: 7.2302244267216675, Val MAE: 1.434073567390442\n",
      "Epoch 197/2000, Train Loss: 10.23195971936778, Val Loss: 7.226073492680852, Val MAE: 1.4337522983551025\n",
      "Epoch 198/2000, Train Loss: 10.225723958424584, Val Loss: 7.221826182399784, Val MAE: 1.433417797088623\n",
      "Epoch 199/2000, Train Loss: 10.21964885775645, Val Loss: 7.217668105205437, Val MAE: 1.4331051111221313\n",
      "Epoch 200/2000, Train Loss: 10.213520026244165, Val Loss: 7.213458354999354, Val MAE: 1.4327839612960815\n",
      "Epoch 201/2000, Train Loss: 10.207433263894734, Val Loss: 7.209286808819921, Val MAE: 1.4324934482574463\n",
      "Epoch 202/2000, Train Loss: 10.201316452621484, Val Loss: 7.205104546181791, Val MAE: 1.4321693181991577\n",
      "Epoch 203/2000, Train Loss: 10.195053569240242, Val Loss: 7.200868185092737, Val MAE: 1.4318428039550781\n",
      "Epoch 204/2000, Train Loss: 10.188840254234636, Val Loss: 7.196419125896048, Val MAE: 1.4314849376678467\n",
      "Epoch 205/2000, Train Loss: 10.182448563774736, Val Loss: 7.192334265040385, Val MAE: 1.431149959564209\n",
      "Epoch 206/2000, Train Loss: 10.176398127015778, Val Loss: 7.188128486651558, Val MAE: 1.4308143854141235\n",
      "Epoch 207/2000, Train Loss: 10.170075546002797, Val Loss: 7.183723246218922, Val MAE: 1.4304720163345337\n",
      "Epoch 208/2000, Train Loss: 10.163574861475904, Val Loss: 7.179446171110009, Val MAE: 1.4301320314407349\n",
      "Epoch 209/2000, Train Loss: 10.157203948813928, Val Loss: 7.175012397464062, Val MAE: 1.4297670125961304\n",
      "Epoch 210/2000, Train Loss: 10.151018024792723, Val Loss: 7.170809472620756, Val MAE: 1.4294488430023193\n",
      "Epoch 211/2000, Train Loss: 10.144799230995119, Val Loss: 7.166617153175511, Val MAE: 1.4291023015975952\n",
      "Epoch 212/2000, Train Loss: 10.138487811341253, Val Loss: 7.162093762798352, Val MAE: 1.4287296533584595\n",
      "Epoch 213/2000, Train Loss: 10.13219921451277, Val Loss: 7.158054693707744, Val MAE: 1.4284100532531738\n",
      "Epoch 214/2000, Train Loss: 10.125726323417865, Val Loss: 7.1535042043406145, Val MAE: 1.4280387163162231\n",
      "Epoch 215/2000, Train Loss: 10.119417364311705, Val Loss: 7.149324217734036, Val MAE: 1.4276986122131348\n",
      "Epoch 216/2000, Train Loss: 10.113347621306243, Val Loss: 7.145105593752216, Val MAE: 1.4273524284362793\n",
      "Epoch 217/2000, Train Loss: 10.10714068111503, Val Loss: 7.140983176922744, Val MAE: 1.4270155429840088\n",
      "Epoch 218/2000, Train Loss: 10.101041997174113, Val Loss: 7.136662955810358, Val MAE: 1.4266555309295654\n",
      "Epoch 219/2000, Train Loss: 10.094800449004063, Val Loss: 7.132638119953173, Val MAE: 1.4263238906860352\n",
      "Epoch 220/2000, Train Loss: 10.088723476517034, Val Loss: 7.128361238284154, Val MAE: 1.4259634017944336\n",
      "Epoch 221/2000, Train Loss: 10.082410288303393, Val Loss: 7.124154807164057, Val MAE: 1.4256196022033691\n",
      "Epoch 222/2000, Train Loss: 10.076151453016701, Val Loss: 7.119858281314373, Val MAE: 1.4252653121948242\n",
      "Epoch 223/2000, Train Loss: 10.069836049667572, Val Loss: 7.115605067602686, Val MAE: 1.4249005317687988\n",
      "Epoch 224/2000, Train Loss: 10.063466695467135, Val Loss: 7.111215050914534, Val MAE: 1.424547791481018\n",
      "Epoch 225/2000, Train Loss: 10.056617644323435, Val Loss: 7.1066415997305965, Val MAE: 1.4241321086883545\n",
      "Epoch 226/2000, Train Loss: 10.050409790134282, Val Loss: 7.102401826980414, Val MAE: 1.4237769842147827\n",
      "Epoch 227/2000, Train Loss: 10.044058782541065, Val Loss: 7.098207683230306, Val MAE: 1.4234306812286377\n",
      "Epoch 228/2000, Train Loss: 10.037968390073493, Val Loss: 7.093877243384853, Val MAE: 1.423067569732666\n",
      "Epoch 229/2000, Train Loss: 10.031511197038077, Val Loss: 7.089515092673603, Val MAE: 1.422692894935608\n",
      "Epoch 230/2000, Train Loss: 10.025261726570367, Val Loss: 7.085236933565623, Val MAE: 1.4223378896713257\n",
      "Epoch 231/2000, Train Loss: 10.019006816475606, Val Loss: 7.081160831256761, Val MAE: 1.421986699104309\n",
      "Epoch 232/2000, Train Loss: 10.01283314285361, Val Loss: 7.076959237348926, Val MAE: 1.4216196537017822\n",
      "Epoch 233/2000, Train Loss: 10.00658431700351, Val Loss: 7.072733239010647, Val MAE: 1.421266794204712\n",
      "Epoch 234/2000, Train Loss: 10.000237844291604, Val Loss: 7.068341904006026, Val MAE: 1.42087984085083\n",
      "Epoch 235/2000, Train Loss: 9.993744153360867, Val Loss: 7.064156347801825, Val MAE: 1.420519232749939\n",
      "Epoch 236/2000, Train Loss: 9.98751308660745, Val Loss: 7.0597576934609325, Val MAE: 1.4201217889785767\n",
      "Epoch 237/2000, Train Loss: 9.98109860948393, Val Loss: 7.055513077088304, Val MAE: 1.41975736618042\n",
      "Epoch 238/2000, Train Loss: 9.97430652612457, Val Loss: 7.050839878699264, Val MAE: 1.4193466901779175\n",
      "Epoch 239/2000, Train Loss: 9.967620757366305, Val Loss: 7.046462535858154, Val MAE: 1.4189351797103882\n",
      "Epoch 240/2000, Train Loss: 9.961273535551408, Val Loss: 7.042091880000389, Val MAE: 1.4185798168182373\n",
      "Epoch 241/2000, Train Loss: 9.95475683271792, Val Loss: 7.037666829151882, Val MAE: 1.4181851148605347\n",
      "Epoch 242/2000, Train Loss: 9.948261081893433, Val Loss: 7.0333028483632445, Val MAE: 1.417793869972229\n",
      "Epoch 243/2000, Train Loss: 9.941806613190126, Val Loss: 7.0291617342010815, Val MAE: 1.4174162149429321\n",
      "Epoch 244/2000, Train Loss: 9.935365606581737, Val Loss: 7.0246569658856135, Val MAE: 1.4170128107070923\n",
      "Epoch 245/2000, Train Loss: 9.928793938409148, Val Loss: 7.020266161771777, Val MAE: 1.4165980815887451\n",
      "Epoch 246/2000, Train Loss: 9.921850085444458, Val Loss: 7.0154813660761794, Val MAE: 1.416138768196106\n",
      "Epoch 247/2000, Train Loss: 9.915245002452371, Val Loss: 7.011239999948858, Val MAE: 1.415756344795227\n",
      "Epoch 248/2000, Train Loss: 9.908959957069243, Val Loss: 7.006882072575726, Val MAE: 1.415361762046814\n",
      "Epoch 249/2000, Train Loss: 9.902515147753094, Val Loss: 7.002686893792303, Val MAE: 1.4149806499481201\n",
      "Epoch 250/2000, Train Loss: 9.896152655159627, Val Loss: 6.998509523386622, Val MAE: 1.4146080017089844\n",
      "Epoch 251/2000, Train Loss: 9.88971190221968, Val Loss: 6.994013334233482, Val MAE: 1.414175271987915\n",
      "Epoch 252/2000, Train Loss: 9.882903225522332, Val Loss: 6.989371191018874, Val MAE: 1.4137413501739502\n",
      "Epoch 253/2000, Train Loss: 9.876136023987103, Val Loss: 6.984822835518164, Val MAE: 1.4133175611495972\n",
      "Epoch 254/2000, Train Loss: 9.869179625964946, Val Loss: 6.980215126390124, Val MAE: 1.4128758907318115\n",
      "Epoch 255/2000, Train Loss: 9.862324094251612, Val Loss: 6.975570116299498, Val MAE: 1.412436842918396\n",
      "Epoch 256/2000, Train Loss: 9.855741281690026, Val Loss: 6.971180729608278, Val MAE: 1.4120405912399292\n",
      "Epoch 257/2000, Train Loss: 9.849349226302783, Val Loss: 6.96697543956138, Val MAE: 1.4116530418395996\n",
      "Epoch 258/2000, Train Loss: 9.843020366618116, Val Loss: 6.962744166212039, Val MAE: 1.4112645387649536\n",
      "Epoch 259/2000, Train Loss: 9.836589253822838, Val Loss: 6.958421423219077, Val MAE: 1.4108545780181885\n",
      "Epoch 260/2000, Train Loss: 9.829752544902973, Val Loss: 6.953899711343619, Val MAE: 1.4104183912277222\n",
      "Epoch 261/2000, Train Loss: 9.823158256721198, Val Loss: 6.9494549817263005, Val MAE: 1.4099873304367065\n",
      "Epoch 262/2000, Train Loss: 9.8166383951205, Val Loss: 6.945130050967674, Val MAE: 1.409583568572998\n",
      "Epoch 263/2000, Train Loss: 9.810215433748576, Val Loss: 6.940688945688643, Val MAE: 1.4091572761535645\n",
      "Epoch 264/2000, Train Loss: 9.803616207586993, Val Loss: 6.93644126250266, Val MAE: 1.408780574798584\n",
      "Epoch 265/2000, Train Loss: 9.797104814797407, Val Loss: 6.931963187563527, Val MAE: 1.408360481262207\n",
      "Epoch 266/2000, Train Loss: 9.790507157395671, Val Loss: 6.927713608023551, Val MAE: 1.4079571962356567\n",
      "Epoch 267/2000, Train Loss: 9.783776085387897, Val Loss: 6.923084256892, Val MAE: 1.407503604888916\n",
      "Epoch 268/2000, Train Loss: 9.776993426829925, Val Loss: 6.918615143524634, Val MAE: 1.407103180885315\n",
      "Epoch 269/2000, Train Loss: 9.770422740585161, Val Loss: 6.914249695918045, Val MAE: 1.4066740274429321\n",
      "Epoch 270/2000, Train Loss: 9.763817273975349, Val Loss: 6.909822963231855, Val MAE: 1.4062445163726807\n",
      "Epoch 271/2000, Train Loss: 9.75714624021057, Val Loss: 6.905436951901999, Val MAE: 1.4058146476745605\n",
      "Epoch 272/2000, Train Loss: 9.75050220995351, Val Loss: 6.900896941501278, Val MAE: 1.4053658246994019\n",
      "Epoch 273/2000, Train Loss: 9.743693241649039, Val Loss: 6.896380765286383, Val MAE: 1.4049220085144043\n",
      "Epoch 274/2000, Train Loss: 9.736780038319946, Val Loss: 6.891656296807635, Val MAE: 1.4044374227523804\n",
      "Epoch 275/2000, Train Loss: 9.730058339754244, Val Loss: 6.88715907045984, Val MAE: 1.4039959907531738\n",
      "Epoch 276/2000, Train Loss: 9.72321015773065, Val Loss: 6.882656022161245, Val MAE: 1.4035710096359253\n",
      "Epoch 277/2000, Train Loss: 9.71608656068302, Val Loss: 6.877839829403538, Val MAE: 1.403092622756958\n",
      "Epoch 278/2000, Train Loss: 9.709297494381643, Val Loss: 6.873397552980496, Val MAE: 1.4026415348052979\n",
      "Epoch 279/2000, Train Loss: 9.702658977067497, Val Loss: 6.868902376267287, Val MAE: 1.4021857976913452\n",
      "Epoch 280/2000, Train Loss: 9.695922622442618, Val Loss: 6.864431717810598, Val MAE: 1.4017295837402344\n",
      "Epoch 281/2000, Train Loss: 9.688936506809199, Val Loss: 6.8597604247229595, Val MAE: 1.4012542963027954\n",
      "Epoch 282/2000, Train Loss: 9.682135898497846, Val Loss: 6.855261133550792, Val MAE: 1.4008036851882935\n",
      "Epoch 283/2000, Train Loss: 9.675318770773139, Val Loss: 6.850808606451159, Val MAE: 1.4003372192382812\n",
      "Epoch 284/2000, Train Loss: 9.66853179872129, Val Loss: 6.846130085958017, Val MAE: 1.3998433351516724\n",
      "Epoch 285/2000, Train Loss: 9.661375022503472, Val Loss: 6.841427035495505, Val MAE: 1.3993630409240723\n",
      "Epoch 286/2000, Train Loss: 9.654549887720583, Val Loss: 6.83683181809144, Val MAE: 1.3988896608352661\n",
      "Epoch 287/2000, Train Loss: 9.647638658883977, Val Loss: 6.832399872844821, Val MAE: 1.3984310626983643\n",
      "Epoch 288/2000, Train Loss: 9.640818121280759, Val Loss: 6.827792753681943, Val MAE: 1.3979395627975464\n",
      "Epoch 289/2000, Train Loss: 9.633571310831865, Val Loss: 6.822997087636241, Val MAE: 1.3974372148513794\n",
      "Epoch 290/2000, Train Loss: 9.626611974421603, Val Loss: 6.818308238148152, Val MAE: 1.3969453573226929\n",
      "Epoch 291/2000, Train Loss: 9.619543367168646, Val Loss: 6.813701848336705, Val MAE: 1.3964529037475586\n",
      "Epoch 292/2000, Train Loss: 9.612597063811446, Val Loss: 6.809018641133029, Val MAE: 1.3959683179855347\n",
      "Epoch 293/2000, Train Loss: 9.605523377982392, Val Loss: 6.8042149757614006, Val MAE: 1.3954572677612305\n",
      "Epoch 294/2000, Train Loss: 9.59842705559247, Val Loss: 6.799634102382907, Val MAE: 1.3949838876724243\n",
      "Epoch 295/2000, Train Loss: 9.591454876194506, Val Loss: 6.795020700742801, Val MAE: 1.3944997787475586\n",
      "Epoch 296/2000, Train Loss: 9.584425442676276, Val Loss: 6.790268937370799, Val MAE: 1.3939868211746216\n",
      "Epoch 297/2000, Train Loss: 9.577275338671322, Val Loss: 6.7854954378226315, Val MAE: 1.3934807777404785\n",
      "Epoch 298/2000, Train Loss: 9.570082701684532, Val Loss: 6.780756119457451, Val MAE: 1.3929606676101685\n",
      "Epoch 299/2000, Train Loss: 9.5626641099055, Val Loss: 6.775915503219978, Val MAE: 1.3924269676208496\n",
      "Epoch 300/2000, Train Loss: 9.555548993083132, Val Loss: 6.771068542242587, Val MAE: 1.3919000625610352\n",
      "Epoch 301/2000, Train Loss: 9.5483933744862, Val Loss: 6.766371463568093, Val MAE: 1.39138662815094\n",
      "Epoch 302/2000, Train Loss: 9.541023069908393, Val Loss: 6.761514389756564, Val MAE: 1.3908638954162598\n",
      "Epoch 303/2000, Train Loss: 9.533890334008822, Val Loss: 6.756720055122901, Val MAE: 1.3903647661209106\n",
      "Epoch 304/2000, Train Loss: 9.526647737357248, Val Loss: 6.751970806127196, Val MAE: 1.3898413181304932\n",
      "Epoch 305/2000, Train Loss: 9.519372284505371, Val Loss: 6.746982760076318, Val MAE: 1.3893020153045654\n",
      "Epoch 306/2000, Train Loss: 9.512109319059041, Val Loss: 6.742251532269759, Val MAE: 1.3887763023376465\n",
      "Epoch 307/2000, Train Loss: 9.504744663997299, Val Loss: 6.737338475325892, Val MAE: 1.3882354497909546\n",
      "Epoch 308/2000, Train Loss: 9.497427916935937, Val Loss: 6.732473123214535, Val MAE: 1.3877005577087402\n",
      "Epoch 309/2000, Train Loss: 9.49006534782754, Val Loss: 6.727591609887711, Val MAE: 1.3871428966522217\n",
      "Epoch 310/2000, Train Loss: 9.482772656052422, Val Loss: 6.72282440935572, Val MAE: 1.3866089582443237\n",
      "Epoch 311/2000, Train Loss: 9.47536533968683, Val Loss: 6.717873562583784, Val MAE: 1.3860607147216797\n",
      "Epoch 312/2000, Train Loss: 9.46783270032469, Val Loss: 6.712824764364474, Val MAE: 1.3854644298553467\n",
      "Epoch 313/2000, Train Loss: 9.460494957922402, Val Loss: 6.707906294312026, Val MAE: 1.3849183320999146\n",
      "Epoch 314/2000, Train Loss: 9.452909796825622, Val Loss: 6.702894256297532, Val MAE: 1.384344220161438\n",
      "Epoch 315/2000, Train Loss: 9.445503968170113, Val Loss: 6.698118114290205, Val MAE: 1.3838011026382446\n",
      "Epoch 316/2000, Train Loss: 9.43821701133121, Val Loss: 6.693245818056502, Val MAE: 1.383244514465332\n",
      "Epoch 317/2000, Train Loss: 9.430841182583766, Val Loss: 6.6884232984067085, Val MAE: 1.3827190399169922\n",
      "Epoch 318/2000, Train Loss: 9.423346939027402, Val Loss: 6.683498417700196, Val MAE: 1.3821642398834229\n",
      "Epoch 319/2000, Train Loss: 9.415759670660016, Val Loss: 6.678416255089614, Val MAE: 1.3815761804580688\n",
      "Epoch 320/2000, Train Loss: 9.408327938547117, Val Loss: 6.673360953446444, Val MAE: 1.3809800148010254\n",
      "Epoch 321/2000, Train Loss: 9.400809003391801, Val Loss: 6.668586286942701, Val MAE: 1.3804442882537842\n",
      "Epoch 322/2000, Train Loss: 9.39339304453869, Val Loss: 6.6636227376691926, Val MAE: 1.3798625469207764\n",
      "Epoch 323/2000, Train Loss: 9.38583584545927, Val Loss: 6.658665181146012, Val MAE: 1.379282832145691\n",
      "Epoch 324/2000, Train Loss: 9.378365027830121, Val Loss: 6.653649910720619, Val MAE: 1.3786840438842773\n",
      "Epoch 325/2000, Train Loss: 9.370771827638242, Val Loss: 6.648726749433591, Val MAE: 1.3781358003616333\n",
      "Epoch 326/2000, Train Loss: 9.363309271436028, Val Loss: 6.6437519585629845, Val MAE: 1.377614140510559\n",
      "Epoch 327/2000, Train Loss: 9.355626952071644, Val Loss: 6.638945172498892, Val MAE: 1.3770475387573242\n",
      "Epoch 328/2000, Train Loss: 9.348071251012234, Val Loss: 6.633711572286782, Val MAE: 1.3764270544052124\n",
      "Epoch 329/2000, Train Loss: 9.340445918709552, Val Loss: 6.628624276212744, Val MAE: 1.3758289813995361\n",
      "Epoch 330/2000, Train Loss: 9.332428372995391, Val Loss: 6.623511099493181, Val MAE: 1.3752079010009766\n",
      "Epoch 331/2000, Train Loss: 9.324906346555247, Val Loss: 6.618555658746947, Val MAE: 1.3746140003204346\n",
      "Epoch 332/2000, Train Loss: 9.317487801478917, Val Loss: 6.613432463117548, Val MAE: 1.3740049600601196\n",
      "Epoch 333/2000, Train Loss: 9.309676421227953, Val Loss: 6.608429601211268, Val MAE: 1.3734183311462402\n",
      "Epoch 334/2000, Train Loss: 9.302110603558663, Val Loss: 6.6034444626118685, Val MAE: 1.372824788093567\n",
      "Epoch 335/2000, Train Loss: 9.2943453650076, Val Loss: 6.5982135220825135, Val MAE: 1.372165322303772\n",
      "Epoch 336/2000, Train Loss: 9.286763189595872, Val Loss: 6.593240772966329, Val MAE: 1.371559500694275\n",
      "Epoch 337/2000, Train Loss: 9.279188983525946, Val Loss: 6.588342007054939, Val MAE: 1.3709808588027954\n",
      "Epoch 338/2000, Train Loss: 9.27161077296219, Val Loss: 6.583353506283717, Val MAE: 1.3703725337982178\n",
      "Epoch 339/2000, Train Loss: 9.264031633014055, Val Loss: 6.578377609559007, Val MAE: 1.3697744607925415\n",
      "Epoch 340/2000, Train Loss: 9.256314082748247, Val Loss: 6.573246271411578, Val MAE: 1.3691380023956299\n",
      "Epoch 341/2000, Train Loss: 9.248523896644341, Val Loss: 6.568207130091148, Val MAE: 1.3685346841812134\n",
      "Epoch 342/2000, Train Loss: 9.240482470192097, Val Loss: 6.56272428140447, Val MAE: 1.367855191230774\n",
      "Epoch 343/2000, Train Loss: 9.232662616021548, Val Loss: 6.557805653413137, Val MAE: 1.3672629594802856\n",
      "Epoch 344/2000, Train Loss: 9.224975394012404, Val Loss: 6.552766842294384, Val MAE: 1.366642713546753\n",
      "Epoch 345/2000, Train Loss: 9.217255114765138, Val Loss: 6.547496949458444, Val MAE: 1.3659944534301758\n",
      "Epoch 346/2000, Train Loss: 9.20935159024322, Val Loss: 6.542440760565234, Val MAE: 1.3653916120529175\n",
      "Epoch 347/2000, Train Loss: 9.20185918695411, Val Loss: 6.537342012666904, Val MAE: 1.3647538423538208\n",
      "Epoch 348/2000, Train Loss: 9.194208502211549, Val Loss: 6.532467164703317, Val MAE: 1.3641526699066162\n",
      "Epoch 349/2000, Train Loss: 9.186278892195727, Val Loss: 6.527152120946227, Val MAE: 1.3634814023971558\n",
      "Epoch 350/2000, Train Loss: 9.178485960660389, Val Loss: 6.522122247476836, Val MAE: 1.3628685474395752\n",
      "Epoch 351/2000, Train Loss: 9.170898374338604, Val Loss: 6.517095840823006, Val MAE: 1.3622430562973022\n",
      "Epoch 352/2000, Train Loss: 9.16329898785759, Val Loss: 6.512057087671113, Val MAE: 1.3616042137145996\n",
      "Epoch 353/2000, Train Loss: 9.155672882722804, Val Loss: 6.506933325998955, Val MAE: 1.360959768295288\n",
      "Epoch 354/2000, Train Loss: 9.147462010662567, Val Loss: 6.501507035662999, Val MAE: 1.3602585792541504\n",
      "Epoch 355/2000, Train Loss: 9.13970314610209, Val Loss: 6.496514577680343, Val MAE: 1.3596543073654175\n",
      "Epoch 356/2000, Train Loss: 9.132055048875849, Val Loss: 6.491468298408362, Val MAE: 1.3590238094329834\n",
      "Epoch 357/2000, Train Loss: 9.124435950738963, Val Loss: 6.486428094393498, Val MAE: 1.3584035634994507\n",
      "Epoch 358/2000, Train Loss: 9.116695227005552, Val Loss: 6.481392634196862, Val MAE: 1.3577821254730225\n",
      "Epoch 359/2000, Train Loss: 9.108796903756033, Val Loss: 6.476130007730948, Val MAE: 1.3571208715438843\n",
      "Epoch 360/2000, Train Loss: 9.100596350598819, Val Loss: 6.470728570397373, Val MAE: 1.3564507961273193\n",
      "Epoch 361/2000, Train Loss: 9.092815484160008, Val Loss: 6.4657047729771415, Val MAE: 1.3558248281478882\n",
      "Epoch 362/2000, Train Loss: 9.085128256571647, Val Loss: 6.4607306671572164, Val MAE: 1.3552027940750122\n",
      "Epoch 363/2000, Train Loss: 9.077498919506342, Val Loss: 6.455551967212746, Val MAE: 1.3545525074005127\n",
      "Epoch 364/2000, Train Loss: 9.069426875962481, Val Loss: 6.450289774370623, Val MAE: 1.3539117574691772\n",
      "Epoch 365/2000, Train Loss: 9.0617774912794, Val Loss: 6.445244251889688, Val MAE: 1.3532617092132568\n",
      "Epoch 366/2000, Train Loss: 9.053896911616631, Val Loss: 6.440220244900063, Val MAE: 1.3526273965835571\n",
      "Epoch 367/2000, Train Loss: 9.046260503450533, Val Loss: 6.435265009085069, Val MAE: 1.3520005941390991\n",
      "Epoch 368/2000, Train Loss: 9.03832887087523, Val Loss: 6.42997742210691, Val MAE: 1.351332426071167\n",
      "Epoch 369/2000, Train Loss: 9.030473246403306, Val Loss: 6.424898466286627, Val MAE: 1.3506951332092285\n",
      "Epoch 370/2000, Train Loss: 9.022581758648855, Val Loss: 6.419539606316133, Val MAE: 1.3500165939331055\n",
      "Epoch 371/2000, Train Loss: 9.014622226148239, Val Loss: 6.414563344244485, Val MAE: 1.3493866920471191\n",
      "Epoch 372/2000, Train Loss: 9.006791140099583, Val Loss: 6.409475580133027, Val MAE: 1.3487517833709717\n",
      "Epoch 373/2000, Train Loss: 8.99886863785193, Val Loss: 6.40414868857678, Val MAE: 1.3480768203735352\n",
      "Epoch 374/2000, Train Loss: 8.990914095024609, Val Loss: 6.399022506090167, Val MAE: 1.3474451303482056\n",
      "Epoch 375/2000, Train Loss: 8.982952184870538, Val Loss: 6.3938660515925365, Val MAE: 1.3468002080917358\n",
      "Epoch 376/2000, Train Loss: 8.974946653298952, Val Loss: 6.388727581977576, Val MAE: 1.3461753129959106\n",
      "Epoch 377/2000, Train Loss: 8.967036668298396, Val Loss: 6.383542063859132, Val MAE: 1.345542550086975\n",
      "Epoch 378/2000, Train Loss: 8.958717349166989, Val Loss: 6.37789824247226, Val MAE: 1.3448535203933716\n",
      "Epoch 379/2000, Train Loss: 8.950140050968356, Val Loss: 6.3724741861224174, Val MAE: 1.3441736698150635\n",
      "Epoch 380/2000, Train Loss: 8.94212215629736, Val Loss: 6.367368216664942, Val MAE: 1.3435428142547607\n",
      "Epoch 381/2000, Train Loss: 8.93419129465374, Val Loss: 6.362151023765674, Val MAE: 1.3428820371627808\n",
      "Epoch 382/2000, Train Loss: 8.92569195315265, Val Loss: 6.356535398429847, Val MAE: 1.3421602249145508\n",
      "Epoch 383/2000, Train Loss: 8.917623130096102, Val Loss: 6.35145512037986, Val MAE: 1.3415273427963257\n",
      "Epoch 384/2000, Train Loss: 8.909329889635679, Val Loss: 6.346110863862811, Val MAE: 1.3408678770065308\n",
      "Epoch 385/2000, Train Loss: 8.901422092955496, Val Loss: 6.34098587556331, Val MAE: 1.3402271270751953\n",
      "Epoch 386/2000, Train Loss: 8.893194636019084, Val Loss: 6.335671250884597, Val MAE: 1.3395460844039917\n",
      "Epoch 387/2000, Train Loss: 8.885111465097031, Val Loss: 6.330448984468842, Val MAE: 1.3389232158660889\n",
      "Epoch 388/2000, Train Loss: 8.87690715261629, Val Loss: 6.32496315715415, Val MAE: 1.3382165431976318\n",
      "Epoch 389/2000, Train Loss: 8.868736725190129, Val Loss: 6.319890584823516, Val MAE: 1.3375879526138306\n",
      "Epoch 390/2000, Train Loss: 8.860945415943164, Val Loss: 6.314648312417505, Val MAE: 1.3369113206863403\n",
      "Epoch 391/2000, Train Loss: 8.852693404287518, Val Loss: 6.309341759993149, Val MAE: 1.3362318277359009\n",
      "Epoch 392/2000, Train Loss: 8.844754283030207, Val Loss: 6.304318110148112, Val MAE: 1.335587739944458\n",
      "Epoch 393/2000, Train Loss: 8.836936812690936, Val Loss: 6.299122364933158, Val MAE: 1.3349609375\n",
      "Epoch 394/2000, Train Loss: 8.82897295083345, Val Loss: 6.293907860183232, Val MAE: 1.3342790603637695\n",
      "Epoch 395/2000, Train Loss: 8.820925324643829, Val Loss: 6.28877823687083, Val MAE: 1.3336278200149536\n",
      "Epoch 396/2000, Train Loss: 8.813169419114564, Val Loss: 6.283836767856065, Val MAE: 1.3329967260360718\n",
      "Epoch 397/2000, Train Loss: 8.805135487394288, Val Loss: 6.27855240224718, Val MAE: 1.332327127456665\n",
      "Epoch 398/2000, Train Loss: 8.796739634504185, Val Loss: 6.2729485342161615, Val MAE: 1.3315843343734741\n",
      "Epoch 399/2000, Train Loss: 8.78858160489063, Val Loss: 6.2677310293389334, Val MAE: 1.3309129476547241\n",
      "Epoch 400/2000, Train Loss: 8.780519448278847, Val Loss: 6.262663062198742, Val MAE: 1.330255389213562\n",
      "Epoch 401/2000, Train Loss: 8.77219944057896, Val Loss: 6.257082202030463, Val MAE: 1.3295178413391113\n",
      "Epoch 402/2000, Train Loss: 8.764028810952643, Val Loss: 6.251886334229965, Val MAE: 1.3288358449935913\n",
      "Epoch 403/2000, Train Loss: 8.755822204018532, Val Loss: 6.2467887688931585, Val MAE: 1.3281702995300293\n",
      "Epoch 404/2000, Train Loss: 8.747484631173883, Val Loss: 6.241282786818238, Val MAE: 1.3274503946304321\n",
      "Epoch 405/2000, Train Loss: 8.739444183856946, Val Loss: 6.235954220928587, Val MAE: 1.326824426651001\n",
      "Epoch 406/2000, Train Loss: 8.731303431314537, Val Loss: 6.23090788411933, Val MAE: 1.326158046722412\n",
      "Epoch 407/2000, Train Loss: 8.723292106771247, Val Loss: 6.225527036565918, Val MAE: 1.325435757637024\n",
      "Epoch 408/2000, Train Loss: 8.715335622592574, Val Loss: 6.2204836568921, Val MAE: 1.324784278869629\n",
      "Epoch 409/2000, Train Loss: 8.707309349871556, Val Loss: 6.21511366832498, Val MAE: 1.3240766525268555\n",
      "Epoch 410/2000, Train Loss: 8.69906910933124, Val Loss: 6.20998463068191, Val MAE: 1.3233981132507324\n",
      "Epoch 411/2000, Train Loss: 8.691015766488967, Val Loss: 6.204582733452857, Val MAE: 1.3226864337921143\n",
      "Epoch 412/2000, Train Loss: 8.682608060253996, Val Loss: 6.199352908899655, Val MAE: 1.3219882249832153\n",
      "Epoch 413/2000, Train Loss: 8.674446702747375, Val Loss: 6.194011008262903, Val MAE: 1.3212835788726807\n",
      "Epoch 414/2000, Train Loss: 8.666210806128015, Val Loss: 6.1885807420756365, Val MAE: 1.320560336112976\n",
      "Epoch 415/2000, Train Loss: 8.65776068558001, Val Loss: 6.183343793471923, Val MAE: 1.3198719024658203\n",
      "Epoch 416/2000, Train Loss: 8.649031310297211, Val Loss: 6.177477563749831, Val MAE: 1.319064736366272\n",
      "Epoch 417/2000, Train Loss: 8.640172148945737, Val Loss: 6.171817338419658, Val MAE: 1.3183248043060303\n",
      "Epoch 418/2000, Train Loss: 8.631860977030023, Val Loss: 6.1666096251625735, Val MAE: 1.3176277875900269\n",
      "Epoch 419/2000, Train Loss: 8.62333451250972, Val Loss: 6.161001335829496, Val MAE: 1.3168888092041016\n",
      "Epoch 420/2000, Train Loss: 8.61492360589587, Val Loss: 6.155717572992718, Val MAE: 1.3161853551864624\n",
      "Epoch 421/2000, Train Loss: 8.6067725410467, Val Loss: 6.1504573874913895, Val MAE: 1.3154910802841187\n",
      "Epoch 422/2000, Train Loss: 8.598679957635317, Val Loss: 6.145074676118187, Val MAE: 1.3147732019424438\n",
      "Epoch 423/2000, Train Loss: 8.589850521729257, Val Loss: 6.139591704543915, Val MAE: 1.3140435218811035\n",
      "Epoch 424/2000, Train Loss: 8.581702445115017, Val Loss: 6.134172199673213, Val MAE: 1.313323974609375\n",
      "Epoch 425/2000, Train Loss: 8.57324727468611, Val Loss: 6.128815063760356, Val MAE: 1.312606692314148\n",
      "Epoch 426/2000, Train Loss: 8.565016859726814, Val Loss: 6.123554548755422, Val MAE: 1.311903715133667\n",
      "Epoch 427/2000, Train Loss: 8.556854298631784, Val Loss: 6.118185725734309, Val MAE: 1.3111810684204102\n",
      "Epoch 428/2000, Train Loss: 8.548393901443333, Val Loss: 6.112765517146201, Val MAE: 1.3104661703109741\n",
      "Epoch 429/2000, Train Loss: 8.540154443702162, Val Loss: 6.107490922772401, Val MAE: 1.3097480535507202\n",
      "Epoch 430/2000, Train Loss: 8.53166466067994, Val Loss: 6.102035856038869, Val MAE: 1.3090039491653442\n",
      "Epoch 431/2000, Train Loss: 8.523259310194184, Val Loss: 6.096721384815267, Val MAE: 1.3082983493804932\n",
      "Epoch 432/2000, Train Loss: 8.514720485288528, Val Loss: 6.091191031206567, Val MAE: 1.3075463771820068\n",
      "Epoch 433/2000, Train Loss: 8.50630872223567, Val Loss: 6.085863083876199, Val MAE: 1.3068724870681763\n",
      "Epoch 434/2000, Train Loss: 8.497762272398855, Val Loss: 6.080197890129712, Val MAE: 1.3060799837112427\n",
      "Epoch 435/2000, Train Loss: 8.48898747335544, Val Loss: 6.074760626511531, Val MAE: 1.3053416013717651\n",
      "Epoch 436/2000, Train Loss: 8.480440643852846, Val Loss: 6.069238805878269, Val MAE: 1.3045997619628906\n",
      "Epoch 437/2000, Train Loss: 8.472166329668763, Val Loss: 6.063851252587529, Val MAE: 1.3038705587387085\n",
      "Epoch 438/2000, Train Loss: 8.463913214650127, Val Loss: 6.058688816126134, Val MAE: 1.3031821250915527\n",
      "Epoch 439/2000, Train Loss: 8.455648942968217, Val Loss: 6.053281077628468, Val MAE: 1.3024672269821167\n",
      "Epoch 440/2000, Train Loss: 8.446773898106377, Val Loss: 6.047550228728099, Val MAE: 1.3017536401748657\n",
      "Epoch 441/2000, Train Loss: 8.438308062633375, Val Loss: 6.04234816506505, Val MAE: 1.3010571002960205\n",
      "Epoch 442/2000, Train Loss: 8.430098197389505, Val Loss: 6.03691819855893, Val MAE: 1.3003093004226685\n",
      "Epoch 443/2000, Train Loss: 8.421574900563161, Val Loss: 6.031338398366629, Val MAE: 1.299544334411621\n",
      "Epoch 444/2000, Train Loss: 8.412653188065693, Val Loss: 6.0257176494141955, Val MAE: 1.2987511157989502\n",
      "Epoch 445/2000, Train Loss: 8.40417698555729, Val Loss: 6.02015258359748, Val MAE: 1.2979984283447266\n",
      "Epoch 446/2000, Train Loss: 8.395722809894222, Val Loss: 6.014897650896429, Val MAE: 1.297269582748413\n",
      "Epoch 447/2000, Train Loss: 8.387322940990073, Val Loss: 6.009491030193932, Val MAE: 1.2965272665023804\n",
      "Epoch 448/2000, Train Loss: 8.378845190178957, Val Loss: 6.004258014771852, Val MAE: 1.2958253622055054\n",
      "Epoch 449/2000, Train Loss: 8.37058566819859, Val Loss: 5.998651085290555, Val MAE: 1.2950377464294434\n",
      "Epoch 450/2000, Train Loss: 8.362162204688872, Val Loss: 5.993302056837726, Val MAE: 1.2942901849746704\n",
      "Epoch 451/2000, Train Loss: 8.353563952929516, Val Loss: 5.987877480363523, Val MAE: 1.2935426235198975\n",
      "Epoch 452/2000, Train Loss: 8.344910566975448, Val Loss: 5.98259800663134, Val MAE: 1.292816162109375\n",
      "Epoch 453/2000, Train Loss: 8.33657454737635, Val Loss: 5.976884138080719, Val MAE: 1.2920182943344116\n",
      "Epoch 454/2000, Train Loss: 8.3278868810249, Val Loss: 5.97134739741146, Val MAE: 1.2912615537643433\n",
      "Epoch 455/2000, Train Loss: 8.319182460991064, Val Loss: 5.965945158361851, Val MAE: 1.2905193567276\n",
      "Epoch 456/2000, Train Loss: 8.310680490574114, Val Loss: 5.960436100063023, Val MAE: 1.2897534370422363\n",
      "Epoch 457/2000, Train Loss: 8.301649609333639, Val Loss: 5.954490671118906, Val MAE: 1.2889273166656494\n",
      "Epoch 458/2000, Train Loss: 8.29296580566073, Val Loss: 5.949057797510345, Val MAE: 1.2881882190704346\n",
      "Epoch 459/2000, Train Loss: 8.284457586112893, Val Loss: 5.943676060317336, Val MAE: 1.2874583005905151\n",
      "Epoch 460/2000, Train Loss: 8.276192140177056, Val Loss: 5.93847473220111, Val MAE: 1.2867498397827148\n",
      "Epoch 461/2000, Train Loss: 8.267738111429033, Val Loss: 5.933110324541728, Val MAE: 1.2860198020935059\n",
      "Epoch 462/2000, Train Loss: 8.259202043649372, Val Loss: 5.927609946632439, Val MAE: 1.2852565050125122\n",
      "Epoch 463/2000, Train Loss: 8.250241141237446, Val Loss: 5.921703887240843, Val MAE: 1.2844431400299072\n",
      "Epoch 464/2000, Train Loss: 8.24083302545473, Val Loss: 5.91580511432913, Val MAE: 1.2835928201675415\n",
      "Epoch 465/2000, Train Loss: 8.232069436268947, Val Loss: 5.910565672935666, Val MAE: 1.2828725576400757\n",
      "Epoch 466/2000, Train Loss: 8.223675550416392, Val Loss: 5.904949374577484, Val MAE: 1.2820948362350464\n",
      "Epoch 467/2000, Train Loss: 8.215086874928378, Val Loss: 5.899597229996512, Val MAE: 1.2813676595687866\n",
      "Epoch 468/2000, Train Loss: 8.206587802191583, Val Loss: 5.8940920039944285, Val MAE: 1.2806085348129272\n",
      "Epoch 469/2000, Train Loss: 8.197999191730517, Val Loss: 5.8887258020137345, Val MAE: 1.2798908948898315\n",
      "Epoch 470/2000, Train Loss: 8.18937904740046, Val Loss: 5.883179886847198, Val MAE: 1.2791264057159424\n",
      "Epoch 471/2000, Train Loss: 8.180752386726938, Val Loss: 5.877766906933204, Val MAE: 1.2783888578414917\n",
      "Epoch 472/2000, Train Loss: 8.1719164439185, Val Loss: 5.872000517676005, Val MAE: 1.2776070833206177\n",
      "Epoch 473/2000, Train Loss: 8.162540956239061, Val Loss: 5.86609699066292, Val MAE: 1.27681303024292\n",
      "Epoch 474/2000, Train Loss: 8.153931190740671, Val Loss: 5.860597907221532, Val MAE: 1.2760528326034546\n",
      "Epoch 475/2000, Train Loss: 8.145102419274425, Val Loss: 5.855022371600609, Val MAE: 1.2753098011016846\n",
      "Epoch 476/2000, Train Loss: 8.136524106708592, Val Loss: 5.849689320978281, Val MAE: 1.2745866775512695\n",
      "Epoch 477/2000, Train Loss: 8.127903272059285, Val Loss: 5.844086587570003, Val MAE: 1.2738358974456787\n",
      "Epoch 478/2000, Train Loss: 8.11927356660459, Val Loss: 5.838762772110131, Val MAE: 1.2731093168258667\n",
      "Epoch 479/2000, Train Loss: 8.110557338934793, Val Loss: 5.832992609268104, Val MAE: 1.2723369598388672\n",
      "Epoch 480/2000, Train Loss: 8.101727527070809, Val Loss: 5.827523720814838, Val MAE: 1.271613597869873\n",
      "Epoch 481/2000, Train Loss: 8.093190702000944, Val Loss: 5.822043092186386, Val MAE: 1.2708944082260132\n",
      "Epoch 482/2000, Train Loss: 8.084557201672641, Val Loss: 5.816576523085435, Val MAE: 1.270193099975586\n",
      "Epoch 483/2000, Train Loss: 8.07592742640217, Val Loss: 5.811268707277539, Val MAE: 1.2694916725158691\n",
      "Epoch 484/2000, Train Loss: 8.06725870391322, Val Loss: 5.805543665097909, Val MAE: 1.2687259912490845\n",
      "Epoch 485/2000, Train Loss: 8.057992584061884, Val Loss: 5.799715718660537, Val MAE: 1.2679593563079834\n",
      "Epoch 486/2000, Train Loss: 8.049116661321726, Val Loss: 5.794206693372479, Val MAE: 1.267242670059204\n",
      "Epoch 487/2000, Train Loss: 8.040143805295926, Val Loss: 5.788462572846864, Val MAE: 1.2664844989776611\n",
      "Epoch 488/2000, Train Loss: 8.031414589904214, Val Loss: 5.783152622830223, Val MAE: 1.2657790184020996\n",
      "Epoch 489/2000, Train Loss: 8.022773821886146, Val Loss: 5.77768378875277, Val MAE: 1.2650724649429321\n",
      "Epoch 490/2000, Train Loss: 8.014209317900647, Val Loss: 5.7721596609499, Val MAE: 1.2643251419067383\n",
      "Epoch 491/2000, Train Loss: 8.005472800289041, Val Loss: 5.766621696767775, Val MAE: 1.2635828256607056\n",
      "Epoch 492/2000, Train Loss: 7.996726692187805, Val Loss: 5.761101176066173, Val MAE: 1.2628326416015625\n",
      "Epoch 493/2000, Train Loss: 7.987963615319519, Val Loss: 5.755603749633909, Val MAE: 1.2621053457260132\n",
      "Epoch 494/2000, Train Loss: 7.979385314186724, Val Loss: 5.750079971968054, Val MAE: 1.2613554000854492\n",
      "Epoch 495/2000, Train Loss: 7.970718953985134, Val Loss: 5.744546597511381, Val MAE: 1.260608434677124\n",
      "Epoch 496/2000, Train Loss: 7.961911841115497, Val Loss: 5.739363637009451, Val MAE: 1.2599111795425415\n",
      "Epoch 497/2000, Train Loss: 7.953409011204045, Val Loss: 5.733814666212142, Val MAE: 1.2591516971588135\n",
      "Epoch 498/2000, Train Loss: 7.9447861459402604, Val Loss: 5.728429475668315, Val MAE: 1.2584151029586792\n",
      "Epoch 499/2000, Train Loss: 7.936108904770124, Val Loss: 5.7230936802118215, Val MAE: 1.2576886415481567\n",
      "Epoch 500/2000, Train Loss: 7.92763613613087, Val Loss: 5.717367216327169, Val MAE: 1.25688898563385\n",
      "Epoch 501/2000, Train Loss: 7.9190063967533675, Val Loss: 5.712320023405928, Val MAE: 1.2562110424041748\n",
      "Epoch 502/2000, Train Loss: 7.910514480424188, Val Loss: 5.706730499152128, Val MAE: 1.2554442882537842\n",
      "Epoch 503/2000, Train Loss: 7.901338008934176, Val Loss: 5.700879507156106, Val MAE: 1.2546511888504028\n",
      "Epoch 504/2000, Train Loss: 7.892310835827903, Val Loss: 5.695163199600873, Val MAE: 1.2538716793060303\n",
      "Epoch 505/2000, Train Loss: 7.883158733989816, Val Loss: 5.689478481896558, Val MAE: 1.253077507019043\n",
      "Epoch 506/2000, Train Loss: 7.874331234769777, Val Loss: 5.684015378020368, Val MAE: 1.2523274421691895\n",
      "Epoch 507/2000, Train Loss: 7.865337368943576, Val Loss: 5.678335604588459, Val MAE: 1.2515281438827515\n",
      "Epoch 508/2000, Train Loss: 7.856690543676129, Val Loss: 5.672856301284051, Val MAE: 1.2507588863372803\n",
      "Epoch 509/2000, Train Loss: 7.848341232156977, Val Loss: 5.667937614070671, Val MAE: 1.2501187324523926\n",
      "Epoch 510/2000, Train Loss: 7.83948930032993, Val Loss: 5.661913647368416, Val MAE: 1.249280571937561\n",
      "Epoch 511/2000, Train Loss: 7.830687814076866, Val Loss: 5.656721645982953, Val MAE: 1.2485758066177368\n",
      "Epoch 512/2000, Train Loss: 7.822255205996509, Val Loss: 5.6513111125174404, Val MAE: 1.2478289604187012\n",
      "Epoch 513/2000, Train Loss: 7.813458001929773, Val Loss: 5.645867328849193, Val MAE: 1.247098684310913\n",
      "Epoch 514/2000, Train Loss: 7.804700736508541, Val Loss: 5.640266150659, Val MAE: 1.246309518814087\n",
      "Epoch 515/2000, Train Loss: 7.795924022305588, Val Loss: 5.634890711367936, Val MAE: 1.2455649375915527\n",
      "Epoch 516/2000, Train Loss: 7.787274215711126, Val Loss: 5.62943717823238, Val MAE: 1.2448194026947021\n",
      "Epoch 517/2000, Train Loss: 7.778647760519186, Val Loss: 5.624158123732956, Val MAE: 1.244081974029541\n",
      "Epoch 518/2000, Train Loss: 7.769788533774627, Val Loss: 5.61856744541107, Val MAE: 1.2433022260665894\n",
      "Epoch 519/2000, Train Loss: 7.760842074842051, Val Loss: 5.612710575954066, Val MAE: 1.2424870729446411\n",
      "Epoch 520/2000, Train Loss: 7.7519532350965665, Val Loss: 5.6073934761857664, Val MAE: 1.2417879104614258\n",
      "Epoch 521/2000, Train Loss: 7.742636770806196, Val Loss: 5.601623064138599, Val MAE: 1.2409621477127075\n",
      "Epoch 522/2000, Train Loss: 7.734056868530844, Val Loss: 5.596324394871523, Val MAE: 1.2402323484420776\n",
      "Epoch 523/2000, Train Loss: 7.725384597659297, Val Loss: 5.590826979259381, Val MAE: 1.239488959312439\n",
      "Epoch 524/2000, Train Loss: 7.716359758897802, Val Loss: 5.584976874533537, Val MAE: 1.23866868019104\n",
      "Epoch 525/2000, Train Loss: 7.707487059300896, Val Loss: 5.57982387823296, Val MAE: 1.2379642724990845\n",
      "Epoch 526/2000, Train Loss: 7.699202869779048, Val Loss: 5.574468439387846, Val MAE: 1.2372255325317383\n",
      "Epoch 527/2000, Train Loss: 7.690883990568973, Val Loss: 5.569353227901297, Val MAE: 1.236504077911377\n",
      "Epoch 528/2000, Train Loss: 7.682053862235476, Val Loss: 5.563973132843102, Val MAE: 1.235779047012329\n",
      "Epoch 529/2000, Train Loss: 7.673595059247522, Val Loss: 5.558486068255461, Val MAE: 1.2350053787231445\n",
      "Epoch 530/2000, Train Loss: 7.665082931518555, Val Loss: 5.553270493246414, Val MAE: 1.234265923500061\n",
      "Epoch 531/2000, Train Loss: 7.656342925484962, Val Loss: 5.54768847501627, Val MAE: 1.2334747314453125\n",
      "Epoch 532/2000, Train Loss: 7.647755857283909, Val Loss: 5.5426049959686425, Val MAE: 1.2327747344970703\n",
      "Epoch 533/2000, Train Loss: 7.639333777048287, Val Loss: 5.53735336478766, Val MAE: 1.2320367097854614\n",
      "Epoch 534/2000, Train Loss: 7.630650139597568, Val Loss: 5.531731093627912, Val MAE: 1.2312737703323364\n",
      "Epoch 535/2000, Train Loss: 7.622065761345969, Val Loss: 5.526569392250196, Val MAE: 1.2305549383163452\n",
      "Epoch 536/2000, Train Loss: 7.61307845184472, Val Loss: 5.520898887783558, Val MAE: 1.2297301292419434\n",
      "Epoch 537/2000, Train Loss: 7.604469636626623, Val Loss: 5.515434107842209, Val MAE: 1.228947639465332\n",
      "Epoch 538/2000, Train Loss: 7.595926876931034, Val Loss: 5.510367295884334, Val MAE: 1.2282209396362305\n",
      "Epoch 539/2000, Train Loss: 7.58728997086214, Val Loss: 5.504770934125324, Val MAE: 1.227454423904419\n",
      "Epoch 540/2000, Train Loss: 7.57818763341621, Val Loss: 5.499133814971994, Val MAE: 1.2266716957092285\n",
      "Epoch 541/2000, Train Loss: 7.56959969911858, Val Loss: 5.494016366921835, Val MAE: 1.2259459495544434\n",
      "Epoch 542/2000, Train Loss: 7.561181452456577, Val Loss: 5.488711648917682, Val MAE: 1.2252171039581299\n",
      "Epoch 543/2000, Train Loss: 7.552793867243149, Val Loss: 5.483442439427515, Val MAE: 1.224448323249817\n",
      "Epoch 544/2000, Train Loss: 7.544428970395949, Val Loss: 5.478266824231492, Val MAE: 1.2237125635147095\n",
      "Epoch 545/2000, Train Loss: 7.5360907243678055, Val Loss: 5.473043448249768, Val MAE: 1.2229617834091187\n",
      "Epoch 546/2000, Train Loss: 7.527668725059901, Val Loss: 5.467844213974906, Val MAE: 1.2221907377243042\n",
      "Epoch 547/2000, Train Loss: 7.519258755417584, Val Loss: 5.46263591718969, Val MAE: 1.2214351892471313\n",
      "Epoch 548/2000, Train Loss: 7.510772392641922, Val Loss: 5.457268219810349, Val MAE: 1.220652461051941\n",
      "Epoch 549/2000, Train Loss: 7.501553514632346, Val Loss: 5.451658497039262, Val MAE: 1.219862937927246\n",
      "Epoch 550/2000, Train Loss: 7.4926276299027315, Val Loss: 5.446094315982348, Val MAE: 1.2190407514572144\n",
      "Epoch 551/2000, Train Loss: 7.484368468270101, Val Loss: 5.4411360438208325, Val MAE: 1.2183433771133423\n",
      "Epoch 552/2000, Train Loss: 7.4761229071783575, Val Loss: 5.43600960032897, Val MAE: 1.217613935470581\n",
      "Epoch 553/2000, Train Loss: 7.467920370295341, Val Loss: 5.430731460230576, Val MAE: 1.216850996017456\n",
      "Epoch 554/2000, Train Loss: 7.459281930766779, Val Loss: 5.425305915623904, Val MAE: 1.2160553932189941\n",
      "Epoch 555/2000, Train Loss: 7.450837056461995, Val Loss: 5.42025139462035, Val MAE: 1.215335726737976\n",
      "Epoch 556/2000, Train Loss: 7.442068208584361, Val Loss: 5.414935605422602, Val MAE: 1.2145699262619019\n",
      "Epoch 557/2000, Train Loss: 7.433782577514648, Val Loss: 5.4096936854626145, Val MAE: 1.2138391733169556\n",
      "Epoch 558/2000, Train Loss: 7.42511732263168, Val Loss: 5.404228468199034, Val MAE: 1.213011622428894\n",
      "Epoch 559/2000, Train Loss: 7.416688237658901, Val Loss: 5.39920470987757, Val MAE: 1.21229887008667\n",
      "Epoch 560/2000, Train Loss: 7.4084604831642, Val Loss: 5.394086290258277, Val MAE: 1.2115530967712402\n",
      "Epoch 561/2000, Train Loss: 7.400017209897361, Val Loss: 5.388507103215198, Val MAE: 1.2107313871383667\n",
      "Epoch 562/2000, Train Loss: 7.391453739633426, Val Loss: 5.383583984094429, Val MAE: 1.2100080251693726\n",
      "Epoch 563/2000, Train Loss: 7.383195549761077, Val Loss: 5.3786198492724075, Val MAE: 1.209304928779602\n",
      "Epoch 564/2000, Train Loss: 7.374662263159075, Val Loss: 5.3731536238647255, Val MAE: 1.2084910869598389\n",
      "Epoch 565/2000, Train Loss: 7.366124154154252, Val Loss: 5.367971326316799, Val MAE: 1.2077102661132812\n",
      "Epoch 566/2000, Train Loss: 7.357837589222258, Val Loss: 5.362846669447315, Val MAE: 1.206952452659607\n",
      "Epoch 567/2000, Train Loss: 7.34929916053406, Val Loss: 5.357436419822074, Val MAE: 1.2061375379562378\n",
      "Epoch 568/2000, Train Loss: 7.340841896634392, Val Loss: 5.352348116214748, Val MAE: 1.2053791284561157\n",
      "Epoch 569/2000, Train Loss: 7.332381970052972, Val Loss: 5.347207991490224, Val MAE: 1.2046109437942505\n",
      "Epoch 570/2000, Train Loss: 7.323273605377923, Val Loss: 5.341488853385588, Val MAE: 1.2037272453308105\n",
      "Epoch 571/2000, Train Loss: 7.314764244136126, Val Loss: 5.336356980205925, Val MAE: 1.2029509544372559\n",
      "Epoch 572/2000, Train Loss: 7.306536508610765, Val Loss: 5.331249123751312, Val MAE: 1.2021803855895996\n",
      "Epoch 573/2000, Train Loss: 7.298431948733962, Val Loss: 5.326165681808918, Val MAE: 1.2014086246490479\n",
      "Epoch 574/2000, Train Loss: 7.290311366784591, Val Loss: 5.321359323804175, Val MAE: 1.2007054090499878\n",
      "Epoch 575/2000, Train Loss: 7.282276312758138, Val Loss: 5.316422659098296, Val MAE: 1.2000035047531128\n",
      "Epoch 576/2000, Train Loss: 7.273899258368659, Val Loss: 5.311083711482383, Val MAE: 1.1992260217666626\n",
      "Epoch 577/2000, Train Loss: 7.265571069968327, Val Loss: 5.306062943481647, Val MAE: 1.1984978914260864\n",
      "Epoch 578/2000, Train Loss: 7.257366849972565, Val Loss: 5.300990454261904, Val MAE: 1.197752594947815\n",
      "Epoch 579/2000, Train Loss: 7.249160533390254, Val Loss: 5.29576369196445, Val MAE: 1.1969863176345825\n",
      "Epoch 580/2000, Train Loss: 7.240761667025071, Val Loss: 5.29066947469706, Val MAE: 1.1962486505508423\n",
      "Epoch 581/2000, Train Loss: 7.232352848543949, Val Loss: 5.285829393545518, Val MAE: 1.195528507232666\n",
      "Epoch 582/2000, Train Loss: 7.224100741675463, Val Loss: 5.280524935016224, Val MAE: 1.1947572231292725\n",
      "Epoch 583/2000, Train Loss: 7.215884514838782, Val Loss: 5.27534080588737, Val MAE: 1.1940261125564575\n",
      "Epoch 584/2000, Train Loss: 7.207503375322696, Val Loss: 5.2704019087727545, Val MAE: 1.1933448314666748\n",
      "Epoch 585/2000, Train Loss: 7.198920405413171, Val Loss: 5.265086630678123, Val MAE: 1.1925889253616333\n",
      "Epoch 586/2000, Train Loss: 7.190628831918452, Val Loss: 5.26008335210987, Val MAE: 1.1918874979019165\n",
      "Epoch 587/2000, Train Loss: 7.182562587227725, Val Loss: 5.255329001748615, Val MAE: 1.1912128925323486\n",
      "Epoch 588/2000, Train Loss: 7.174635367302367, Val Loss: 5.250052981857244, Val MAE: 1.1905031204223633\n",
      "Epoch 589/2000, Train Loss: 7.1662588878280475, Val Loss: 5.245225928649978, Val MAE: 1.1898510456085205\n",
      "Epoch 590/2000, Train Loss: 7.157987244322594, Val Loss: 5.239839195903088, Val MAE: 1.1890780925750732\n",
      "Epoch 591/2000, Train Loss: 7.149686535137492, Val Loss: 5.235042241591591, Val MAE: 1.1884225606918335\n",
      "Epoch 592/2000, Train Loss: 7.141298289529909, Val Loss: 5.229894502997936, Val MAE: 1.1876838207244873\n",
      "Epoch 593/2000, Train Loss: 7.133084801541476, Val Loss: 5.2248795977651, Val MAE: 1.1869726181030273\n",
      "Epoch 594/2000, Train Loss: 7.124547319756981, Val Loss: 5.219573974750332, Val MAE: 1.186226487159729\n",
      "Epoch 595/2000, Train Loss: 7.116363944947812, Val Loss: 5.214523784366546, Val MAE: 1.1855502128601074\n",
      "Epoch 596/2000, Train Loss: 7.1082408344896155, Val Loss: 5.209698596321516, Val MAE: 1.1849919557571411\n",
      "Epoch 597/2000, Train Loss: 7.100196059706058, Val Loss: 5.204787898090508, Val MAE: 1.1843316555023193\n",
      "Epoch 598/2000, Train Loss: 7.091975522673632, Val Loss: 5.19980250954091, Val MAE: 1.1836615800857544\n",
      "Epoch 599/2000, Train Loss: 7.083770924536933, Val Loss: 5.194696063447643, Val MAE: 1.1829702854156494\n",
      "Epoch 600/2000, Train Loss: 7.07548985475683, Val Loss: 5.1896866931034635, Val MAE: 1.1822803020477295\n",
      "Epoch 601/2000, Train Loss: 7.06735567053469, Val Loss: 5.184839134871423, Val MAE: 1.1816214323043823\n",
      "Epoch 602/2000, Train Loss: 7.0593602175645636, Val Loss: 5.179794069124503, Val MAE: 1.1809483766555786\n",
      "Epoch 603/2000, Train Loss: 7.051143086830651, Val Loss: 5.174866403941367, Val MAE: 1.180268406867981\n",
      "Epoch 604/2000, Train Loss: 7.042344404271166, Val Loss: 5.169336319385885, Val MAE: 1.1795159578323364\n",
      "Epoch 605/2000, Train Loss: 7.033696404845685, Val Loss: 5.164376182666233, Val MAE: 1.178859829902649\n",
      "Epoch 606/2000, Train Loss: 7.025749489223343, Val Loss: 5.159347296700822, Val MAE: 1.1781635284423828\n",
      "Epoch 607/2000, Train Loss: 7.017207525077737, Val Loss: 5.154373239705691, Val MAE: 1.1775102615356445\n",
      "Epoch 608/2000, Train Loss: 7.00911143491867, Val Loss: 5.149019563090694, Val MAE: 1.1767628192901611\n",
      "Epoch 609/2000, Train Loss: 7.000942027629259, Val Loss: 5.144225359983272, Val MAE: 1.1761399507522583\n",
      "Epoch 610/2000, Train Loss: 6.993014381009964, Val Loss: 5.139420264516328, Val MAE: 1.1754963397979736\n",
      "Epoch 611/2000, Train Loss: 6.984970268332828, Val Loss: 5.134675314445216, Val MAE: 1.174883246421814\n",
      "Epoch 612/2000, Train Loss: 6.976923906181862, Val Loss: 5.129910557602977, Val MAE: 1.1742621660232544\n",
      "Epoch 613/2000, Train Loss: 6.969237871735404, Val Loss: 5.124766921392969, Val MAE: 1.1735520362854004\n",
      "Epoch 614/2000, Train Loss: 6.961235835474106, Val Loss: 5.120245897783352, Val MAE: 1.1729549169540405\n",
      "Epoch 615/2000, Train Loss: 6.953355500111342, Val Loss: 5.115536017407168, Val MAE: 1.1723390817642212\n",
      "Epoch 616/2000, Train Loss: 6.945581043659246, Val Loss: 5.110739967018903, Val MAE: 1.1716907024383545\n",
      "Epoch 617/2000, Train Loss: 6.937770260291613, Val Loss: 5.10607334037756, Val MAE: 1.1710840463638306\n",
      "Epoch 618/2000, Train Loss: 6.92963343886615, Val Loss: 5.100853199142594, Val MAE: 1.1703822612762451\n",
      "Epoch 619/2000, Train Loss: 6.921526212588115, Val Loss: 5.096192095911986, Val MAE: 1.1698075532913208\n",
      "Epoch 620/2000, Train Loss: 6.9134954856636, Val Loss: 5.091064356049305, Val MAE: 1.1691536903381348\n",
      "Epoch 621/2000, Train Loss: 6.905514280063314, Val Loss: 5.086393257441956, Val MAE: 1.1685490608215332\n",
      "Epoch 622/2000, Train Loss: 6.8973230164434165, Val Loss: 5.081529980488457, Val MAE: 1.1679238080978394\n",
      "Epoch 623/2000, Train Loss: 6.889484106535426, Val Loss: 5.0767759045065795, Val MAE: 1.1673181056976318\n",
      "Epoch 624/2000, Train Loss: 6.881733595534532, Val Loss: 5.0721179481327265, Val MAE: 1.166745662689209\n",
      "Epoch 625/2000, Train Loss: 6.874159127613312, Val Loss: 5.067237153414402, Val MAE: 1.1661341190338135\n",
      "Epoch 626/2000, Train Loss: 6.866180473947302, Val Loss: 5.062666763244448, Val MAE: 1.165584683418274\n",
      "Epoch 627/2000, Train Loss: 6.858478331528662, Val Loss: 5.057822177163115, Val MAE: 1.1649892330169678\n",
      "Epoch 628/2000, Train Loss: 6.850509482873211, Val Loss: 5.053334421258386, Val MAE: 1.1645047664642334\n",
      "Epoch 629/2000, Train Loss: 6.842746739082515, Val Loss: 5.0482425837030815, Val MAE: 1.163912057876587\n",
      "Epoch 630/2000, Train Loss: 6.834682556350865, Val Loss: 5.0434872583259605, Val MAE: 1.1633599996566772\n",
      "Epoch 631/2000, Train Loss: 6.8269006835875015, Val Loss: 5.0390676714043625, Val MAE: 1.1628613471984863\n",
      "Epoch 632/2000, Train Loss: 6.8192993021234525, Val Loss: 5.034323578280908, Val MAE: 1.162293791770935\n",
      "Epoch 633/2000, Train Loss: 6.810976355756501, Val Loss: 5.029330734586393, Val MAE: 1.161699891090393\n",
      "Epoch 634/2000, Train Loss: 6.803455693487444, Val Loss: 5.0245566321620805, Val MAE: 1.1612569093704224\n",
      "Epoch 635/2000, Train Loss: 6.7955310287974, Val Loss: 5.019958922515313, Val MAE: 1.160754919052124\n",
      "Epoch 636/2000, Train Loss: 6.7878650443602275, Val Loss: 5.015593154594168, Val MAE: 1.160274624824524\n",
      "Epoch 637/2000, Train Loss: 6.780820661250577, Val Loss: 5.011357846097635, Val MAE: 1.1598098278045654\n",
      "Epoch 638/2000, Train Loss: 6.773580614401844, Val Loss: 5.006653352128762, Val MAE: 1.1592804193496704\n",
      "Epoch 639/2000, Train Loss: 6.76602384518954, Val Loss: 5.002374167783975, Val MAE: 1.1588134765625\n",
      "Epoch 640/2000, Train Loss: 6.758717303343013, Val Loss: 4.9978289569619, Val MAE: 1.1583054065704346\n",
      "Epoch 641/2000, Train Loss: 6.75095072402597, Val Loss: 4.993010459269758, Val MAE: 1.1578551530838013\n",
      "Epoch 642/2000, Train Loss: 6.743153049867722, Val Loss: 4.98844936035305, Val MAE: 1.1573487520217896\n",
      "Epoch 643/2000, Train Loss: 6.735363946666769, Val Loss: 4.983784563121226, Val MAE: 1.1568171977996826\n",
      "Epoch 644/2000, Train Loss: 6.727941718190769, Val Loss: 4.979503112353451, Val MAE: 1.156347632408142\n",
      "Epoch 645/2000, Train Loss: 6.720755364333225, Val Loss: 4.975090984580619, Val MAE: 1.155860424041748\n",
      "Epoch 646/2000, Train Loss: 6.713042132661049, Val Loss: 4.970553809387592, Val MAE: 1.1553411483764648\n",
      "Epoch 647/2000, Train Loss: 6.705816047565428, Val Loss: 4.9662312863612765, Val MAE: 1.154870867729187\n",
      "Epoch 648/2000, Train Loss: 6.698650238108524, Val Loss: 4.961882379385936, Val MAE: 1.1543910503387451\n",
      "Epoch 649/2000, Train Loss: 6.691740019645036, Val Loss: 4.958027621868763, Val MAE: 1.1539984941482544\n",
      "Epoch 650/2000, Train Loss: 6.685017587881472, Val Loss: 4.953767214463772, Val MAE: 1.1535743474960327\n",
      "Epoch 651/2000, Train Loss: 6.677733244362771, Val Loss: 4.949624702542483, Val MAE: 1.1531662940979004\n",
      "Epoch 652/2000, Train Loss: 6.670687317569245, Val Loss: 4.945163751913754, Val MAE: 1.1527011394500732\n",
      "Epoch 653/2000, Train Loss: 6.663402974971697, Val Loss: 4.940955144677076, Val MAE: 1.1522836685180664\n",
      "Epoch 654/2000, Train Loss: 6.656235364595553, Val Loss: 4.936762256376647, Val MAE: 1.1518645286560059\n",
      "Epoch 655/2000, Train Loss: 6.648839115724549, Val Loss: 4.931915951547054, Val MAE: 1.1513584852218628\n",
      "Epoch 656/2000, Train Loss: 6.6413975419380735, Val Loss: 4.927827548618252, Val MAE: 1.1509549617767334\n",
      "Epoch 657/2000, Train Loss: 6.63436042010354, Val Loss: 4.923712621803756, Val MAE: 1.1505645513534546\n",
      "Epoch 658/2000, Train Loss: 6.62734583350314, Val Loss: 4.919435901691516, Val MAE: 1.150134563446045\n",
      "Epoch 659/2000, Train Loss: 6.620133375600794, Val Loss: 4.9150809499493855, Val MAE: 1.1497178077697754\n",
      "Epoch 660/2000, Train Loss: 6.612783966774874, Val Loss: 4.910899659853664, Val MAE: 1.1493052244186401\n",
      "Epoch 661/2000, Train Loss: 6.605767712578201, Val Loss: 4.906654804485204, Val MAE: 1.1488982439041138\n",
      "Epoch 662/2000, Train Loss: 6.598848805968416, Val Loss: 4.9025076213150145, Val MAE: 1.1484949588775635\n",
      "Epoch 663/2000, Train Loss: 6.59172488672313, Val Loss: 4.898534929399958, Val MAE: 1.148129940032959\n",
      "Epoch 664/2000, Train Loss: 6.584552361701096, Val Loss: 4.893869761891059, Val MAE: 1.1476887464523315\n",
      "Epoch 665/2000, Train Loss: 6.57729607634537, Val Loss: 4.889645970367768, Val MAE: 1.1473149061203003\n",
      "Epoch 666/2000, Train Loss: 6.5702292919158936, Val Loss: 4.885737263810661, Val MAE: 1.1469911336898804\n",
      "Epoch 667/2000, Train Loss: 6.5633983522793065, Val Loss: 4.881606800760235, Val MAE: 1.1466714143753052\n",
      "Epoch 668/2000, Train Loss: 6.556488018326008, Val Loss: 4.877508591058421, Val MAE: 1.146400809288025\n",
      "Epoch 669/2000, Train Loss: 6.549646267838858, Val Loss: 4.8734611232303555, Val MAE: 1.1460697650909424\n",
      "Epoch 670/2000, Train Loss: 6.542624860502164, Val Loss: 4.869242208175831, Val MAE: 1.1457278728485107\n",
      "Epoch 671/2000, Train Loss: 6.535636050838762, Val Loss: 4.865209137994024, Val MAE: 1.145422339439392\n",
      "Epoch 672/2000, Train Loss: 6.528362932099195, Val Loss: 4.860757622475157, Val MAE: 1.145056962966919\n",
      "Epoch 673/2000, Train Loss: 6.521269454389764, Val Loss: 4.856847777207558, Val MAE: 1.1447539329528809\n",
      "Epoch 674/2000, Train Loss: 6.514452451383826, Val Loss: 4.852788580262715, Val MAE: 1.1444129943847656\n",
      "Epoch 675/2000, Train Loss: 6.507829489904149, Val Loss: 4.848599267885223, Val MAE: 1.144071102142334\n",
      "Epoch 676/2000, Train Loss: 6.500856260427633, Val Loss: 4.844820324499328, Val MAE: 1.1437894105911255\n",
      "Epoch 677/2000, Train Loss: 6.494184749851849, Val Loss: 4.840733123486777, Val MAE: 1.143495798110962\n",
      "Epoch 678/2000, Train Loss: 6.487379708267783, Val Loss: 4.836785479490687, Val MAE: 1.1432418823242188\n",
      "Epoch 679/2000, Train Loss: 6.48061555261359, Val Loss: 4.832856813892051, Val MAE: 1.1429671049118042\n",
      "Epoch 680/2000, Train Loss: 6.4738536909478315, Val Loss: 4.828846911555743, Val MAE: 1.142699956893921\n",
      "Epoch 681/2000, Train Loss: 6.467209739729692, Val Loss: 4.824706579892485, Val MAE: 1.1424113512039185\n",
      "Epoch 682/2000, Train Loss: 6.460656360582145, Val Loss: 4.821286240941635, Val MAE: 1.142238974571228\n",
      "Epoch 683/2000, Train Loss: 6.454173578607578, Val Loss: 4.817526518405827, Val MAE: 1.1420352458953857\n",
      "Epoch 684/2000, Train Loss: 6.447655501678097, Val Loss: 4.813554535739057, Val MAE: 1.1417967081069946\n",
      "Epoch 685/2000, Train Loss: 6.441143280118936, Val Loss: 4.8097183598054425, Val MAE: 1.1415953636169434\n",
      "Epoch 686/2000, Train Loss: 6.434569916103105, Val Loss: 4.805943253881357, Val MAE: 1.1414130926132202\n",
      "Epoch 687/2000, Train Loss: 6.428049081573248, Val Loss: 4.802155383753481, Val MAE: 1.1412218809127808\n",
      "Epoch 688/2000, Train Loss: 6.421599682472797, Val Loss: 4.798161078314926, Val MAE: 1.1410531997680664\n",
      "Epoch 689/2000, Train Loss: 6.414937066585523, Val Loss: 4.794458591421177, Val MAE: 1.140871524810791\n",
      "Epoch 690/2000, Train Loss: 6.40836148496351, Val Loss: 4.790584734787006, Val MAE: 1.1406879425048828\n",
      "Epoch 691/2000, Train Loss: 6.402023878342991, Val Loss: 4.786979301179851, Val MAE: 1.140546202659607\n",
      "Epoch 692/2000, Train Loss: 6.395648920631789, Val Loss: 4.783246113017604, Val MAE: 1.140366792678833\n",
      "Epoch 693/2000, Train Loss: 6.389251544555088, Val Loss: 4.779573681490781, Val MAE: 1.1401914358139038\n",
      "Epoch 694/2000, Train Loss: 6.382938821971498, Val Loss: 4.775800237945608, Val MAE: 1.1400338411331177\n",
      "Epoch 695/2000, Train Loss: 6.376589393780774, Val Loss: 4.7720633072023455, Val MAE: 1.1398625373840332\n",
      "Epoch 696/2000, Train Loss: 6.370201855293488, Val Loss: 4.768455122907956, Val MAE: 1.1397022008895874\n",
      "Epoch 697/2000, Train Loss: 6.363683723250343, Val Loss: 4.7645697976252785, Val MAE: 1.1395180225372314\n",
      "Epoch 698/2000, Train Loss: 6.357400468656686, Val Loss: 4.7607977117537645, Val MAE: 1.1393967866897583\n",
      "Epoch 699/2000, Train Loss: 6.3503880374331185, Val Loss: 4.756983204340344, Val MAE: 1.1392799615859985\n",
      "Epoch 700/2000, Train Loss: 6.343769518335971, Val Loss: 4.753175527659488, Val MAE: 1.1391210556030273\n",
      "Epoch 701/2000, Train Loss: 6.33860566761118, Val Loss: 4.750312489240959, Val MAE: 1.139068603515625\n",
      "Epoch 702/2000, Train Loss: 6.33302675692042, Val Loss: 4.746756652591599, Val MAE: 1.138973593711853\n",
      "Epoch 703/2000, Train Loss: 6.326678298378884, Val Loss: 4.743228261441261, Val MAE: 1.1388721466064453\n",
      "Epoch 704/2000, Train Loss: 6.320419781572547, Val Loss: 4.739244381731024, Val MAE: 1.1387664079666138\n",
      "Epoch 705/2000, Train Loss: 6.3143669223636625, Val Loss: 4.736094158042121, Val MAE: 1.1386998891830444\n",
      "Epoch 706/2000, Train Loss: 6.3095847676681025, Val Loss: 4.73361383559736, Val MAE: 1.1386901140213013\n",
      "Epoch 707/2000, Train Loss: 6.304370568062698, Val Loss: 4.7303752896954885, Val MAE: 1.1386150121688843\n",
      "Epoch 708/2000, Train Loss: 6.299068929624632, Val Loss: 4.727524935001054, Val MAE: 1.1385616064071655\n",
      "Epoch 709/2000, Train Loss: 6.293772077062944, Val Loss: 4.724144038004247, Val MAE: 1.1384567022323608\n",
      "Epoch 710/2000, Train Loss: 6.288127411900788, Val Loss: 4.7209761583892345, Val MAE: 1.1383975744247437\n",
      "Epoch 711/2000, Train Loss: 6.28259503699595, Val Loss: 4.717750659991089, Val MAE: 1.138329029083252\n",
      "Epoch 712/2000, Train Loss: 6.277041102999272, Val Loss: 4.714545161857664, Val MAE: 1.1382567882537842\n",
      "Epoch 713/2000, Train Loss: 6.271486980375629, Val Loss: 4.7114031163623205, Val MAE: 1.1382094621658325\n",
      "Epoch 714/2000, Train Loss: 6.266063731843707, Val Loss: 4.708039786716974, Val MAE: 1.1381347179412842\n",
      "Epoch 715/2000, Train Loss: 6.260110760558042, Val Loss: 4.704644146920727, Val MAE: 1.138075828552246\n",
      "Epoch 716/2000, Train Loss: 6.254484828288395, Val Loss: 4.701583202498722, Val MAE: 1.1380311250686646\n",
      "Epoch 717/2000, Train Loss: 6.248524955951852, Val Loss: 4.6980372497560206, Val MAE: 1.1379451751708984\n",
      "Epoch 718/2000, Train Loss: 6.242868284725362, Val Loss: 4.694626482317711, Val MAE: 1.1378825902938843\n",
      "Epoch 719/2000, Train Loss: 6.237322485484125, Val Loss: 4.6915583676683745, Val MAE: 1.1378710269927979\n",
      "Epoch 720/2000, Train Loss: 6.231741032111292, Val Loss: 4.688527778713955, Val MAE: 1.1378238201141357\n",
      "Epoch 721/2000, Train Loss: 6.226376006458182, Val Loss: 4.685207217290729, Val MAE: 1.137755274772644\n",
      "Epoch 722/2000, Train Loss: 6.220783014828267, Val Loss: 4.682240615573686, Val MAE: 1.137734055519104\n",
      "Epoch 723/2000, Train Loss: 6.215596189687549, Val Loss: 4.678940950364277, Val MAE: 1.1376906633377075\n",
      "Epoch 724/2000, Train Loss: 6.210007342624706, Val Loss: 4.676009719003294, Val MAE: 1.137673258781433\n",
      "Epoch 725/2000, Train Loss: 6.204718690580585, Val Loss: 4.672910653866895, Val MAE: 1.1376396417617798\n",
      "Epoch 726/2000, Train Loss: 6.199076933998399, Val Loss: 4.669587816925602, Val MAE: 1.1375868320465088\n",
      "Epoch 727/2000, Train Loss: 6.193802001881711, Val Loss: 4.666643589543733, Val MAE: 1.1375634670257568\n",
      "Epoch 728/2000, Train Loss: 6.188225168147809, Val Loss: 4.663352796301112, Val MAE: 1.1375285387039185\n",
      "Epoch 729/2000, Train Loss: 6.182543360870826, Val Loss: 4.660265743651905, Val MAE: 1.137513518333435\n",
      "Epoch 730/2000, Train Loss: 6.1772703604653545, Val Loss: 4.657293466696734, Val MAE: 1.1375160217285156\n",
      "Epoch 731/2000, Train Loss: 6.171840147183578, Val Loss: 4.654250700248254, Val MAE: 1.137498378753662\n",
      "Epoch 732/2000, Train Loss: 6.166704731314863, Val Loss: 4.651367524531376, Val MAE: 1.137494683265686\n",
      "Epoch 733/2000, Train Loss: 6.161818876281357, Val Loss: 4.648512859152513, Val MAE: 1.137516975402832\n",
      "Epoch 734/2000, Train Loss: 6.157262709334655, Val Loss: 4.646068789400496, Val MAE: 1.1375914812088013\n",
      "Epoch 735/2000, Train Loss: 6.152446265339665, Val Loss: 4.643161458176401, Val MAE: 1.1376398801803589\n",
      "Epoch 736/2000, Train Loss: 6.147230753846548, Val Loss: 4.640412470795684, Val MAE: 1.1376938819885254\n",
      "Epoch 737/2000, Train Loss: 6.1418990000919695, Val Loss: 4.637175213689874, Val MAE: 1.1377570629119873\n",
      "Epoch 738/2000, Train Loss: 6.13683609622596, Val Loss: 4.634465313884052, Val MAE: 1.1378288269042969\n",
      "Epoch 739/2000, Train Loss: 6.131926423115961, Val Loss: 4.6316940819592896, Val MAE: 1.13792884349823\n",
      "Epoch 740/2000, Train Loss: 6.126914305917558, Val Loss: 4.62884340085559, Val MAE: 1.138044834136963\n",
      "Epoch 741/2000, Train Loss: 6.122194027565943, Val Loss: 4.626065834229057, Val MAE: 1.1381311416625977\n",
      "Epoch 742/2000, Train Loss: 6.117140616715977, Val Loss: 4.6234710556933205, Val MAE: 1.1382720470428467\n",
      "Epoch 743/2000, Train Loss: 6.112189419927909, Val Loss: 4.620662136120839, Val MAE: 1.1383835077285767\n",
      "Epoch 744/2000, Train Loss: 6.107395371576181, Val Loss: 4.617866126072031, Val MAE: 1.1384776830673218\n",
      "Epoch 745/2000, Train Loss: 6.102409782350156, Val Loss: 4.61520053197269, Val MAE: 1.138596534729004\n",
      "Epoch 746/2000, Train Loss: 6.096895516941588, Val Loss: 4.611938765829613, Val MAE: 1.1387012004852295\n",
      "Epoch 747/2000, Train Loss: 6.0919911501792585, Val Loss: 4.609306333422124, Val MAE: 1.1388492584228516\n",
      "Epoch 748/2000, Train Loss: 6.087250460096157, Val Loss: 4.60664621031432, Val MAE: 1.138945460319519\n",
      "Epoch 749/2000, Train Loss: 6.082586217782055, Val Loss: 4.604037809774682, Val MAE: 1.139086127281189\n",
      "Epoch 750/2000, Train Loss: 6.077824819943462, Val Loss: 4.601337488713834, Val MAE: 1.1392252445220947\n",
      "Epoch 751/2000, Train Loss: 6.073151104535774, Val Loss: 4.598782214448527, Val MAE: 1.1393613815307617\n",
      "Epoch 752/2000, Train Loss: 6.068506926716583, Val Loss: 4.596183748442579, Val MAE: 1.1394898891448975\n",
      "Epoch 753/2000, Train Loss: 6.063771581129053, Val Loss: 4.593516220061763, Val MAE: 1.1396386623382568\n",
      "Epoch 754/2000, Train Loss: 6.058996658499081, Val Loss: 4.591101000800326, Val MAE: 1.1397578716278076\n",
      "Epoch 755/2000, Train Loss: 6.054531594613413, Val Loss: 4.588422270055424, Val MAE: 1.139873743057251\n",
      "Epoch 756/2000, Train Loss: 6.0498894880788745, Val Loss: 4.585902739349787, Val MAE: 1.1399983167648315\n",
      "Epoch 757/2000, Train Loss: 6.045300466501025, Val Loss: 4.583359402602723, Val MAE: 1.1401339769363403\n",
      "Epoch 758/2000, Train Loss: 6.04075860605225, Val Loss: 4.580883718771977, Val MAE: 1.14026939868927\n",
      "Epoch 759/2000, Train Loss: 6.035790614516427, Val Loss: 4.578084303076203, Val MAE: 1.1404087543487549\n",
      "Epoch 760/2000, Train Loss: 6.030561195632702, Val Loss: 4.575331650610577, Val MAE: 1.1405922174453735\n",
      "Epoch 761/2000, Train Loss: 6.026055365382416, Val Loss: 4.572828355904769, Val MAE: 1.140761375427246\n",
      "Epoch 762/2000, Train Loss: 6.021689075017682, Val Loss: 4.570435939121756, Val MAE: 1.1409294605255127\n",
      "Epoch 763/2000, Train Loss: 6.017388330048965, Val Loss: 4.568081657769712, Val MAE: 1.141098141670227\n",
      "Epoch 764/2000, Train Loss: 6.0132377453601675, Val Loss: 4.565862819411465, Val MAE: 1.1412655115127563\n",
      "Epoch 765/2000, Train Loss: 6.009050443652267, Val Loss: 4.563380060589931, Val MAE: 1.1414330005645752\n",
      "Epoch 766/2000, Train Loss: 6.004244512690931, Val Loss: 4.560744532694419, Val MAE: 1.1416295766830444\n",
      "Epoch 767/2000, Train Loss: 5.999843446401278, Val Loss: 4.558493122606127, Val MAE: 1.141794204711914\n",
      "Epoch 768/2000, Train Loss: 5.995622552129296, Val Loss: 4.556183916565265, Val MAE: 1.1420273780822754\n",
      "Epoch 769/2000, Train Loss: 5.991594329453109, Val Loss: 4.553936414175608, Val MAE: 1.1422287225723267\n",
      "Epoch 770/2000, Train Loss: 5.98741887251784, Val Loss: 4.5516873508121245, Val MAE: 1.1424366235733032\n",
      "Epoch 771/2000, Train Loss: 5.98286936993532, Val Loss: 4.549397838397606, Val MAE: 1.1426392793655396\n",
      "Epoch 772/2000, Train Loss: 5.979654886002845, Val Loss: 4.547701902187488, Val MAE: 1.1428196430206299\n",
      "Epoch 773/2000, Train Loss: 5.975907145861345, Val Loss: 4.545555684168463, Val MAE: 1.1430206298828125\n",
      "Epoch 774/2000, Train Loss: 5.971948578884002, Val Loss: 4.5433880646687905, Val MAE: 1.1432228088378906\n",
      "Epoch 775/2000, Train Loss: 5.967979557544877, Val Loss: 4.541298005246633, Val MAE: 1.1434276103973389\n",
      "Epoch 776/2000, Train Loss: 5.9639764479281565, Val Loss: 4.539113996181268, Val MAE: 1.1436197757720947\n",
      "Epoch 777/2000, Train Loss: 5.959314309862586, Val Loss: 4.536600663479384, Val MAE: 1.143851399421692\n",
      "Epoch 778/2000, Train Loss: 5.955531591279272, Val Loss: 4.534701123437634, Val MAE: 1.1440629959106445\n",
      "Epoch 779/2000, Train Loss: 5.951868348300178, Val Loss: 4.532718053762172, Val MAE: 1.1442639827728271\n",
      "Epoch 780/2000, Train Loss: 5.9481996351768744, Val Loss: 4.530789525726357, Val MAE: 1.144465684890747\n",
      "Epoch 781/2000, Train Loss: 5.944195571816098, Val Loss: 4.528538995132119, Val MAE: 1.1446696519851685\n",
      "Epoch 782/2000, Train Loss: 5.940227255573883, Val Loss: 4.526419079589012, Val MAE: 1.1449027061462402\n",
      "Epoch 783/2000, Train Loss: 5.936598321019022, Val Loss: 4.524425720074424, Val MAE: 1.1451151371002197\n",
      "Epoch 784/2000, Train Loss: 5.932812221987571, Val Loss: 4.522520041358364, Val MAE: 1.1453166007995605\n",
      "Epoch 785/2000, Train Loss: 5.929279710240255, Val Loss: 4.520549455320311, Val MAE: 1.1455267667770386\n",
      "Epoch 786/2000, Train Loss: 5.925686273113614, Val Loss: 4.518565699458122, Val MAE: 1.1457412242889404\n",
      "Epoch 787/2000, Train Loss: 5.922224043916429, Val Loss: 4.516886477603636, Val MAE: 1.145939588546753\n",
      "Epoch 788/2000, Train Loss: 5.918778995269919, Val Loss: 4.514960422635347, Val MAE: 1.1461482048034668\n",
      "Epoch 789/2000, Train Loss: 5.914937712838394, Val Loss: 4.512970210105046, Val MAE: 1.1463590860366821\n",
      "Epoch 790/2000, Train Loss: 5.911300938698505, Val Loss: 4.511094377770483, Val MAE: 1.1465739011764526\n",
      "Epoch 791/2000, Train Loss: 5.907523659573703, Val Loss: 4.509057161248885, Val MAE: 1.1467849016189575\n",
      "Epoch 792/2000, Train Loss: 5.904054655113756, Val Loss: 4.507177980328063, Val MAE: 1.1469990015029907\n",
      "Epoch 793/2000, Train Loss: 5.900738943757579, Val Loss: 4.5054220983859254, Val MAE: 1.147230863571167\n",
      "Epoch 794/2000, Train Loss: 5.896737278716612, Val Loss: 4.50343780183141, Val MAE: 1.1474616527557373\n",
      "Epoch 795/2000, Train Loss: 5.89335329870166, Val Loss: 4.501721544795342, Val MAE: 1.1476720571517944\n",
      "Epoch 796/2000, Train Loss: 5.889980429815428, Val Loss: 4.4998570759117875, Val MAE: 1.1479010581970215\n",
      "Epoch 797/2000, Train Loss: 5.88650677654189, Val Loss: 4.498064926192836, Val MAE: 1.1481409072875977\n",
      "Epoch 798/2000, Train Loss: 5.883125045858382, Val Loss: 4.496279876549972, Val MAE: 1.1483533382415771\n",
      "Epoch 799/2000, Train Loss: 5.879759180082035, Val Loss: 4.494537554922942, Val MAE: 1.1485700607299805\n",
      "Epoch 800/2000, Train Loss: 5.876361170721164, Val Loss: 4.492796007584076, Val MAE: 1.1488159894943237\n",
      "Epoch 801/2000, Train Loss: 5.8729871243656895, Val Loss: 4.4911187455477615, Val MAE: 1.1490267515182495\n",
      "Epoch 802/2000, Train Loss: 5.869790177402928, Val Loss: 4.4893824372776185, Val MAE: 1.1492537260055542\n",
      "Epoch 803/2000, Train Loss: 5.866383454775476, Val Loss: 4.487779843191373, Val MAE: 1.1494739055633545\n",
      "Epoch 804/2000, Train Loss: 5.863221226541933, Val Loss: 4.485915817338806, Val MAE: 1.1497019529342651\n",
      "Epoch 805/2000, Train Loss: 5.859279066091767, Val Loss: 4.483900098739242, Val MAE: 1.149954080581665\n",
      "Epoch 806/2000, Train Loss: 5.856073014709849, Val Loss: 4.4825501114549535, Val MAE: 1.1501703262329102\n",
      "Epoch 807/2000, Train Loss: 5.853023502272489, Val Loss: 4.480794209027075, Val MAE: 1.1504029035568237\n",
      "Epoch 808/2000, Train Loss: 5.849663497505247, Val Loss: 4.479112484651777, Val MAE: 1.1506553888320923\n",
      "Epoch 809/2000, Train Loss: 5.846559848316746, Val Loss: 4.47758718576148, Val MAE: 1.150898814201355\n",
      "Epoch 810/2000, Train Loss: 5.843491557235836, Val Loss: 4.475978088201032, Val MAE: 1.1511379480361938\n",
      "Epoch 811/2000, Train Loss: 5.840488722469431, Val Loss: 4.47436165794361, Val MAE: 1.1513769626617432\n",
      "Epoch 812/2000, Train Loss: 5.836907181534306, Val Loss: 4.472599555674504, Val MAE: 1.151633858680725\n",
      "Epoch 813/2000, Train Loss: 5.833858980961411, Val Loss: 4.471032614562962, Val MAE: 1.151878833770752\n",
      "Epoch 814/2000, Train Loss: 5.830771884568582, Val Loss: 4.46955639140865, Val MAE: 1.1521306037902832\n",
      "Epoch 815/2000, Train Loss: 5.827890984167733, Val Loss: 4.467976820426951, Val MAE: 1.152381181716919\n",
      "Epoch 816/2000, Train Loss: 5.825053174670512, Val Loss: 4.466712148356679, Val MAE: 1.1526132822036743\n",
      "Epoch 817/2000, Train Loss: 5.822389341181786, Val Loss: 4.465152165738379, Val MAE: 1.1528671979904175\n",
      "Epoch 818/2000, Train Loss: 5.819387260353696, Val Loss: 4.46388444788754, Val MAE: 1.1530827283859253\n",
      "Epoch 819/2000, Train Loss: 5.816636136095163, Val Loss: 4.4625020376213635, Val MAE: 1.15340256690979\n",
      "Epoch 820/2000, Train Loss: 5.813503730815491, Val Loss: 4.460756011216624, Val MAE: 1.1536706686019897\n",
      "Epoch 821/2000, Train Loss: 5.810475274827477, Val Loss: 4.459410401549492, Val MAE: 1.153899908065796\n",
      "Epoch 822/2000, Train Loss: 5.807675434000407, Val Loss: 4.4579536643533695, Val MAE: 1.1541416645050049\n",
      "Epoch 823/2000, Train Loss: 5.804881249500697, Val Loss: 4.456643383315689, Val MAE: 1.1543980836868286\n",
      "Epoch 824/2000, Train Loss: 5.802225083700787, Val Loss: 4.455154094403727, Val MAE: 1.1546472311019897\n",
      "Epoch 825/2000, Train Loss: 5.79934664883661, Val Loss: 4.453852128161973, Val MAE: 1.1548839807510376\n",
      "Epoch 826/2000, Train Loss: 5.796631639442652, Val Loss: 4.452510471180552, Val MAE: 1.1551367044448853\n",
      "Epoch 827/2000, Train Loss: 5.793943072629979, Val Loss: 4.451161342918001, Val MAE: 1.1553692817687988\n",
      "Epoch 828/2000, Train Loss: 5.7914216986359, Val Loss: 4.449972760679016, Val MAE: 1.1556167602539062\n",
      "Epoch 829/2000, Train Loss: 5.788838399739793, Val Loss: 4.4487202899463405, Val MAE: 1.155856966972351\n",
      "Epoch 830/2000, Train Loss: 5.786312945361442, Val Loss: 4.447311895023528, Val MAE: 1.1561102867126465\n",
      "Epoch 831/2000, Train Loss: 5.783339845222914, Val Loss: 4.446004365491975, Val MAE: 1.156383752822876\n",
      "Epoch 832/2000, Train Loss: 5.780901001403094, Val Loss: 4.4447289595749595, Val MAE: 1.1566588878631592\n",
      "Epoch 833/2000, Train Loss: 5.778245690832272, Val Loss: 4.443498566671199, Val MAE: 1.15695321559906\n",
      "Epoch 834/2000, Train Loss: 5.774954937131096, Val Loss: 4.441939289305728, Val MAE: 1.157285213470459\n",
      "Epoch 835/2000, Train Loss: 5.772352210445077, Val Loss: 4.440768157029608, Val MAE: 1.1575623750686646\n",
      "Epoch 836/2000, Train Loss: 5.770163159660542, Val Loss: 4.439728507060591, Val MAE: 1.1578333377838135\n",
      "Epoch 837/2000, Train Loss: 5.767369224947068, Val Loss: 4.438367592284942, Val MAE: 1.1582081317901611\n",
      "Epoch 838/2000, Train Loss: 5.7646861678910515, Val Loss: 4.437006653088506, Val MAE: 1.1585588455200195\n",
      "Epoch 839/2000, Train Loss: 5.76156698560194, Val Loss: 4.435592262760677, Val MAE: 1.1589035987854004\n",
      "Epoch 840/2000, Train Loss: 5.759277534152435, Val Loss: 4.434545233213083, Val MAE: 1.1592028141021729\n",
      "Epoch 841/2000, Train Loss: 5.757057015311886, Val Loss: 4.433511532158465, Val MAE: 1.159500241279602\n",
      "Epoch 842/2000, Train Loss: 5.754892562630815, Val Loss: 4.432379457235403, Val MAE: 1.1598360538482666\n",
      "Epoch 843/2000, Train Loss: 5.75258738336672, Val Loss: 4.431453073615426, Val MAE: 1.1601072549819946\n",
      "Epoch 844/2000, Train Loss: 5.750537533934886, Val Loss: 4.4302582149184095, Val MAE: 1.1604417562484741\n",
      "Epoch 845/2000, Train Loss: 5.748144958394924, Val Loss: 4.429261036341389, Val MAE: 1.1607505083084106\n",
      "Epoch 846/2000, Train Loss: 5.745773633677576, Val Loss: 4.428189380374644, Val MAE: 1.161048412322998\n",
      "Epoch 847/2000, Train Loss: 5.74362598008559, Val Loss: 4.427135555172624, Val MAE: 1.161379337310791\n",
      "Epoch 848/2000, Train Loss: 5.741236482879115, Val Loss: 4.426145493396902, Val MAE: 1.1617063283920288\n",
      "Epoch 849/2000, Train Loss: 5.738954064998538, Val Loss: 4.42482396992645, Val MAE: 1.1620763540267944\n",
      "Epoch 850/2000, Train Loss: 5.735799349042443, Val Loss: 4.423418833543589, Val MAE: 1.162480354309082\n",
      "Epoch 851/2000, Train Loss: 5.733274087080276, Val Loss: 4.422470823314612, Val MAE: 1.162788987159729\n",
      "Epoch 852/2000, Train Loss: 5.731337462498134, Val Loss: 4.421582949168242, Val MAE: 1.1630760431289673\n",
      "Epoch 853/2000, Train Loss: 5.729234620115128, Val Loss: 4.420530568108567, Val MAE: 1.163371205329895\n",
      "Epoch 854/2000, Train Loss: 5.727361249477369, Val Loss: 4.419711102427797, Val MAE: 1.1636483669281006\n",
      "Epoch 855/2000, Train Loss: 5.7254440414366226, Val Loss: 4.41886342697114, Val MAE: 1.1639384031295776\n",
      "Epoch 856/2000, Train Loss: 5.723679837868106, Val Loss: 4.41798724469368, Val MAE: 1.1642338037490845\n",
      "Epoch 857/2000, Train Loss: 5.721865591411472, Val Loss: 4.417262806550407, Val MAE: 1.1645095348358154\n",
      "Epoch 858/2000, Train Loss: 5.720137524716382, Val Loss: 4.416514696355339, Val MAE: 1.1647826433181763\n",
      "Epoch 859/2000, Train Loss: 5.718457230868024, Val Loss: 4.41568477965455, Val MAE: 1.1650806665420532\n",
      "Epoch 860/2000, Train Loss: 5.7166478162483925, Val Loss: 4.414802537334932, Val MAE: 1.1654081344604492\n",
      "Epoch 861/2000, Train Loss: 5.7147776977280405, Val Loss: 4.41399970550325, Val MAE: 1.1657110452651978\n",
      "Epoch 862/2000, Train Loss: 5.7129967773221315, Val Loss: 4.413163741840771, Val MAE: 1.1660418510437012\n",
      "Epoch 863/2000, Train Loss: 5.7111947778234615, Val Loss: 4.412355396297534, Val MAE: 1.166358470916748\n",
      "Epoch 864/2000, Train Loss: 5.709586119921457, Val Loss: 4.411728157589699, Val MAE: 1.1666384935379028\n",
      "Epoch 865/2000, Train Loss: 5.707941834900569, Val Loss: 4.410978679212968, Val MAE: 1.1669563055038452\n",
      "Epoch 866/2000, Train Loss: 5.706033819179267, Val Loss: 4.41004040601185, Val MAE: 1.167341947555542\n",
      "Epoch 867/2000, Train Loss: 5.7040575123660835, Val Loss: 4.409203621036969, Val MAE: 1.1676807403564453\n",
      "Epoch 868/2000, Train Loss: 5.7023530470878585, Val Loss: 4.40841361543516, Val MAE: 1.168000340461731\n",
      "Epoch 869/2000, Train Loss: 5.700531635386942, Val Loss: 4.407630870905747, Val MAE: 1.1683329343795776\n",
      "Epoch 870/2000, Train Loss: 5.698850507111334, Val Loss: 4.40685427645473, Val MAE: 1.1686736345291138\n",
      "Epoch 871/2000, Train Loss: 5.697143590592092, Val Loss: 4.406160564240706, Val MAE: 1.1689682006835938\n",
      "Epoch 872/2000, Train Loss: 5.6954982663837495, Val Loss: 4.405409631992246, Val MAE: 1.1692956686019897\n",
      "Epoch 873/2000, Train Loss: 5.693621730606265, Val Loss: 4.404595322964025, Val MAE: 1.1696434020996094\n",
      "Epoch 874/2000, Train Loss: 5.691991241041472, Val Loss: 4.403788569655169, Val MAE: 1.1699881553649902\n",
      "Epoch 875/2000, Train Loss: 5.690156858908405, Val Loss: 4.403164596226368, Val MAE: 1.1702967882156372\n",
      "Epoch 876/2000, Train Loss: 5.6887438248918505, Val Loss: 4.402457580295903, Val MAE: 1.1706087589263916\n",
      "Epoch 877/2000, Train Loss: 5.687337511601203, Val Loss: 4.401806664574254, Val MAE: 1.1709263324737549\n",
      "Epoch 878/2000, Train Loss: 5.685822615106467, Val Loss: 4.401250552691452, Val MAE: 1.171198844909668\n",
      "Epoch 879/2000, Train Loss: 5.684379043682317, Val Loss: 4.400598147377238, Val MAE: 1.1714932918548584\n",
      "Epoch 880/2000, Train Loss: 5.682796463904999, Val Loss: 4.399848261517515, Val MAE: 1.1718336343765259\n",
      "Epoch 881/2000, Train Loss: 5.68117306570925, Val Loss: 4.399178708517538, Val MAE: 1.172173261642456\n",
      "Epoch 882/2000, Train Loss: 5.679634751656126, Val Loss: 4.398544567482764, Val MAE: 1.1725273132324219\n",
      "Epoch 883/2000, Train Loss: 5.677869411042997, Val Loss: 4.397745381233593, Val MAE: 1.1729031801223755\n",
      "Epoch 884/2000, Train Loss: 5.676127601897289, Val Loss: 4.397018705357101, Val MAE: 1.1732417345046997\n",
      "Epoch 885/2000, Train Loss: 5.674024238787277, Val Loss: 4.39616772106404, Val MAE: 1.1736747026443481\n",
      "Epoch 886/2000, Train Loss: 5.672463350102608, Val Loss: 4.3955483295140905, Val MAE: 1.173992395401001\n",
      "Epoch 887/2000, Train Loss: 5.6708832620272585, Val Loss: 4.394889871633469, Val MAE: 1.174330234527588\n",
      "Epoch 888/2000, Train Loss: 5.669066986502054, Val Loss: 4.3942346502296825, Val MAE: 1.1747159957885742\n",
      "Epoch 889/2000, Train Loss: 5.667594641726028, Val Loss: 4.393666543569919, Val MAE: 1.1750127077102661\n",
      "Epoch 890/2000, Train Loss: 5.666278018239833, Val Loss: 4.393084905148895, Val MAE: 1.1753265857696533\n",
      "Epoch 891/2000, Train Loss: 5.664916267633066, Val Loss: 4.392473223630909, Val MAE: 1.1756373643875122\n",
      "Epoch 892/2000, Train Loss: 5.663514335888224, Val Loss: 4.391984972559117, Val MAE: 1.175910472869873\n",
      "Epoch 893/2000, Train Loss: 5.662073452368541, Val Loss: 4.391383934160275, Val MAE: 1.176242709159851\n",
      "Epoch 894/2000, Train Loss: 5.660525942369482, Val Loss: 4.3907631514677865, Val MAE: 1.176573395729065\n",
      "Epoch 895/2000, Train Loss: 5.65882147813736, Val Loss: 4.390117302087245, Val MAE: 1.176938772201538\n",
      "Epoch 896/2000, Train Loss: 5.657487997212759, Val Loss: 4.38954470572775, Val MAE: 1.1772645711898804\n",
      "Epoch 897/2000, Train Loss: 5.656198580811808, Val Loss: 4.389066368185387, Val MAE: 1.1775460243225098\n",
      "Epoch 898/2000, Train Loss: 5.6549596191382445, Val Loss: 4.3885589801127445, Val MAE: 1.17784583568573\n",
      "Epoch 899/2000, Train Loss: 5.653734164007369, Val Loss: 4.388046107045165, Val MAE: 1.1781401634216309\n",
      "Epoch 900/2000, Train Loss: 5.652279111412869, Val Loss: 4.387464959191175, Val MAE: 1.1784824132919312\n",
      "Epoch 901/2000, Train Loss: 5.650797329157266, Val Loss: 4.386814920676385, Val MAE: 1.1788493394851685\n",
      "Epoch 902/2000, Train Loss: 5.649317975628208, Val Loss: 4.386358559729011, Val MAE: 1.1791445016860962\n",
      "Epoch 903/2000, Train Loss: 5.648086848385434, Val Loss: 4.385834284140183, Val MAE: 1.1794662475585938\n",
      "Epoch 904/2000, Train Loss: 5.646627807022071, Val Loss: 4.3852396671165215, Val MAE: 1.179835557937622\n",
      "Epoch 905/2000, Train Loss: 5.6453027338393955, Val Loss: 4.3847435628206615, Val MAE: 1.1801639795303345\n",
      "Epoch 906/2000, Train Loss: 5.644240627794668, Val Loss: 4.3842812473038295, Val MAE: 1.180457592010498\n",
      "Epoch 907/2000, Train Loss: 5.642996637014071, Val Loss: 4.38384949015873, Val MAE: 1.180798888206482\n",
      "Epoch 908/2000, Train Loss: 5.641763416355746, Val Loss: 4.383326237353387, Val MAE: 1.1811281442642212\n",
      "Epoch 909/2000, Train Loss: 5.640390805260812, Val Loss: 4.382832309761429, Val MAE: 1.1814558506011963\n",
      "Epoch 910/2000, Train Loss: 5.639139728129561, Val Loss: 4.382319169115644, Val MAE: 1.1818339824676514\n",
      "Epoch 911/2000, Train Loss: 5.637921980502267, Val Loss: 4.381882120084923, Val MAE: 1.182101845741272\n",
      "Epoch 912/2000, Train Loss: 5.6366848982812465, Val Loss: 4.381376144269834, Val MAE: 1.1824350357055664\n",
      "Epoch 913/2000, Train Loss: 5.635499651086126, Val Loss: 4.380921672751104, Val MAE: 1.1827365159988403\n",
      "Epoch 914/2000, Train Loss: 5.6343202605820295, Val Loss: 4.3805237357211, Val MAE: 1.1830055713653564\n",
      "Epoch 915/2000, Train Loss: 5.63328123021095, Val Loss: 4.380072603670058, Val MAE: 1.1833012104034424\n",
      "Epoch 916/2000, Train Loss: 5.632150466212431, Val Loss: 4.37966975655306, Val MAE: 1.1835860013961792\n",
      "Epoch 917/2000, Train Loss: 5.631037845812424, Val Loss: 4.379239308585723, Val MAE: 1.1838840246200562\n",
      "Epoch 918/2000, Train Loss: 5.629841755112333, Val Loss: 4.37881017887109, Val MAE: 1.1841812133789062\n",
      "Epoch 919/2000, Train Loss: 5.628758207684188, Val Loss: 4.378406318451638, Val MAE: 1.1844961643218994\n",
      "Epoch 920/2000, Train Loss: 5.627545678485388, Val Loss: 4.37797908862499, Val MAE: 1.1848152875900269\n",
      "Epoch 921/2000, Train Loss: 5.626386294312857, Val Loss: 4.377578870341316, Val MAE: 1.185123324394226\n",
      "Epoch 922/2000, Train Loss: 5.625219954622331, Val Loss: 4.377111117434394, Val MAE: 1.1854604482650757\n",
      "Epoch 923/2000, Train Loss: 5.624031351062698, Val Loss: 4.376743407688431, Val MAE: 1.1857454776763916\n",
      "Epoch 924/2000, Train Loss: 5.622968145540091, Val Loss: 4.376348861276701, Val MAE: 1.186057209968567\n",
      "Epoch 925/2000, Train Loss: 5.621800145939249, Val Loss: 4.375899336164868, Val MAE: 1.1864120960235596\n",
      "Epoch 926/2000, Train Loss: 5.620646683920564, Val Loss: 4.3754810688161365, Val MAE: 1.1867573261260986\n",
      "Epoch 927/2000, Train Loss: 5.619453267438422, Val Loss: 4.3751426337706345, Val MAE: 1.187048316001892\n",
      "Epoch 928/2000, Train Loss: 5.618473488901409, Val Loss: 4.374786072513005, Val MAE: 1.187343955039978\n",
      "Epoch 929/2000, Train Loss: 5.617313518910995, Val Loss: 4.374378626189522, Val MAE: 1.1876870393753052\n",
      "Epoch 930/2000, Train Loss: 5.616624263538028, Val Loss: 4.374101867721424, Val MAE: 1.1878890991210938\n",
      "Epoch 931/2000, Train Loss: 5.615859718525465, Val Loss: 4.373778001279445, Val MAE: 1.1881630420684814\n",
      "Epoch 932/2000, Train Loss: 5.614908266620778, Val Loss: 4.373420308063831, Val MAE: 1.1884688138961792\n",
      "Epoch 933/2000, Train Loss: 5.613957539735458, Val Loss: 4.373076218205529, Val MAE: 1.1887532472610474\n",
      "Epoch 934/2000, Train Loss: 5.6129162847158875, Val Loss: 4.372750791132048, Val MAE: 1.1890501976013184\n",
      "Epoch 935/2000, Train Loss: 5.612095843239247, Val Loss: 4.372467559089397, Val MAE: 1.1892927885055542\n",
      "Epoch 936/2000, Train Loss: 5.6108964778912975, Val Loss: 4.372113104587471, Val MAE: 1.1897284984588623\n",
      "Epoch 937/2000, Train Loss: 5.609890320090586, Val Loss: 4.37179979843398, Val MAE: 1.1900012493133545\n",
      "Epoch 938/2000, Train Loss: 5.608981961207159, Val Loss: 4.371471538085927, Val MAE: 1.1903043985366821\n",
      "Epoch 939/2000, Train Loss: 5.60772038785546, Val Loss: 4.371044255215843, Val MAE: 1.190752625465393\n",
      "Epoch 940/2000, Train Loss: 5.606763340939597, Val Loss: 4.370790374141421, Val MAE: 1.1909797191619873\n",
      "Epoch 941/2000, Train Loss: 5.605992893997853, Val Loss: 4.370480422289656, Val MAE: 1.1912837028503418\n",
      "Epoch 942/2000, Train Loss: 5.605179010054995, Val Loss: 4.3702369893255, Val MAE: 1.1915256977081299\n",
      "Epoch 943/2000, Train Loss: 5.604196834861767, Val Loss: 4.369897429987385, Val MAE: 1.1918386220932007\n",
      "Epoch 944/2000, Train Loss: 5.603428710176495, Val Loss: 4.369647251760906, Val MAE: 1.1920883655548096\n",
      "Epoch 945/2000, Train Loss: 5.6027268336827225, Val Loss: 4.369384350000066, Val MAE: 1.1923383474349976\n",
      "Epoch 946/2000, Train Loss: 5.601707787298003, Val Loss: 4.369083847944532, Val MAE: 1.1926887035369873\n",
      "Epoch 947/2000, Train Loss: 5.600928450755879, Val Loss: 4.368827045614924, Val MAE: 1.1929327249526978\n",
      "Epoch 948/2000, Train Loss: 5.600110904846846, Val Loss: 4.368557216287464, Val MAE: 1.1932028532028198\n",
      "Epoch 949/2000, Train Loss: 5.599355730363247, Val Loss: 4.368324492750941, Val MAE: 1.193439245223999\n",
      "Epoch 950/2000, Train Loss: 5.598868911400112, Val Loss: 4.368079906769164, Val MAE: 1.1937180757522583\n",
      "Epoch 951/2000, Train Loss: 5.598003391914546, Val Loss: 4.367833543948091, Val MAE: 1.1939551830291748\n",
      "Epoch 952/2000, Train Loss: 5.597225424280032, Val Loss: 4.36757101969676, Val MAE: 1.194254994392395\n",
      "Epoch 953/2000, Train Loss: 5.596358666658494, Val Loss: 4.367276191536908, Val MAE: 1.1945356130599976\n",
      "Epoch 954/2000, Train Loss: 5.595587100217616, Val Loss: 4.367041340735447, Val MAE: 1.194804072380066\n",
      "Epoch 955/2000, Train Loss: 5.594851494579345, Val Loss: 4.3667867894712336, Val MAE: 1.1950892210006714\n",
      "Epoch 956/2000, Train Loss: 5.5939623070953415, Val Loss: 4.366517290869006, Val MAE: 1.1953973770141602\n",
      "Epoch 957/2000, Train Loss: 5.593233361445053, Val Loss: 4.366288992799483, Val MAE: 1.1956268548965454\n",
      "Epoch 958/2000, Train Loss: 5.592296648058639, Val Loss: 4.3660103624132836, Val MAE: 1.1959871053695679\n",
      "Epoch 959/2000, Train Loss: 5.591578888105354, Val Loss: 4.3657904114909805, Val MAE: 1.196236252784729\n",
      "Epoch 960/2000, Train Loss: 5.590806791265372, Val Loss: 4.365574598231831, Val MAE: 1.1964844465255737\n",
      "Epoch 961/2000, Train Loss: 5.590201444438602, Val Loss: 4.365351778822573, Val MAE: 1.1967272758483887\n",
      "Epoch 962/2000, Train Loss: 5.5895761565745525, Val Loss: 4.365141897917062, Val MAE: 1.1969656944274902\n",
      "Epoch 963/2000, Train Loss: 5.588769415983358, Val Loss: 4.3648924493440635, Val MAE: 1.1972614526748657\n",
      "Epoch 964/2000, Train Loss: 5.588134967964637, Val Loss: 4.364681821967568, Val MAE: 1.1975274085998535\n",
      "Epoch 965/2000, Train Loss: 5.587578238480958, Val Loss: 4.364449634496961, Val MAE: 1.1976940631866455\n",
      "Epoch 966/2000, Train Loss: 5.586966779321292, Val Loss: 4.364244897858248, Val MAE: 1.1979676485061646\n",
      "Epoch 967/2000, Train Loss: 5.586223858102064, Val Loss: 4.364035137514541, Val MAE: 1.1982065439224243\n",
      "Epoch 968/2000, Train Loss: 5.585604962432254, Val Loss: 4.363815947135424, Val MAE: 1.1984566450119019\n",
      "Epoch 969/2000, Train Loss: 5.584894756259115, Val Loss: 4.363591751778448, Val MAE: 1.1987342834472656\n",
      "Epoch 970/2000, Train Loss: 5.5842110141298145, Val Loss: 4.363408142989417, Val MAE: 1.198966145515442\n",
      "Epoch 971/2000, Train Loss: 5.583592960085028, Val Loss: 4.3631963754727225, Val MAE: 1.1992205381393433\n",
      "Epoch 972/2000, Train Loss: 5.582913173342272, Val Loss: 4.362979216685703, Val MAE: 1.1994565725326538\n",
      "Epoch 973/2000, Train Loss: 5.582040654329726, Val Loss: 4.362735096418911, Val MAE: 1.1998103857040405\n",
      "Epoch 974/2000, Train Loss: 5.58125371828838, Val Loss: 4.362534758317712, Val MAE: 1.2001515626907349\n",
      "Epoch 975/2000, Train Loss: 5.5803392746757465, Val Loss: 4.362292744987854, Val MAE: 1.200499176979065\n",
      "Epoch 976/2000, Train Loss: 5.579599564979303, Val Loss: 4.362109389736712, Val MAE: 1.2007434368133545\n",
      "Epoch 977/2000, Train Loss: 5.579064829394151, Val Loss: 4.361906191196527, Val MAE: 1.2008967399597168\n",
      "Epoch 978/2000, Train Loss: 5.57859621330654, Val Loss: 4.361706471506943, Val MAE: 1.201143503189087\n",
      "Epoch 979/2000, Train Loss: 5.577984541940812, Val Loss: 4.361492340145884, Val MAE: 1.2013882398605347\n",
      "Epoch 980/2000, Train Loss: 5.577439587499348, Val Loss: 4.36132009113627, Val MAE: 1.20168936252594\n",
      "Epoch 981/2000, Train Loss: 5.576747037319237, Val Loss: 4.3611290837226955, Val MAE: 1.2018816471099854\n",
      "Epoch 982/2000, Train Loss: 5.576218305251528, Val Loss: 4.360942509767037, Val MAE: 1.2020940780639648\n",
      "Epoch 983/2000, Train Loss: 5.575684122659859, Val Loss: 4.360759708157799, Val MAE: 1.2023365497589111\n",
      "Epoch 984/2000, Train Loss: 5.5751742513243014, Val Loss: 4.360583376693162, Val MAE: 1.202527403831482\n",
      "Epoch 985/2000, Train Loss: 5.574392435517215, Val Loss: 4.360392379109656, Val MAE: 1.202933430671692\n",
      "Epoch 986/2000, Train Loss: 5.573573822164312, Val Loss: 4.360203387923875, Val MAE: 1.2031692266464233\n",
      "Epoch 987/2000, Train Loss: 5.573016153296889, Val Loss: 4.360008599394345, Val MAE: 1.2034399509429932\n",
      "Epoch 988/2000, Train Loss: 5.5723586863549, Val Loss: 4.359836388627688, Val MAE: 1.203691840171814\n",
      "Epoch 989/2000, Train Loss: 5.57170554665433, Val Loss: 4.359689943495769, Val MAE: 1.2039662599563599\n",
      "Epoch 990/2000, Train Loss: 5.5710467131759005, Val Loss: 4.359518929859539, Val MAE: 1.204235553741455\n",
      "Epoch 991/2000, Train Loss: 5.570377777756283, Val Loss: 4.359374755191373, Val MAE: 1.2045849561691284\n",
      "Epoch 992/2000, Train Loss: 5.569729483955178, Val Loss: 4.35921874502422, Val MAE: 1.204798936843872\n",
      "Epoch 993/2000, Train Loss: 5.569229790147586, Val Loss: 4.3590734031017835, Val MAE: 1.2050395011901855\n",
      "Epoch 994/2000, Train Loss: 5.568781128623951, Val Loss: 4.358914512150863, Val MAE: 1.2053033113479614\n",
      "Epoch 995/2000, Train Loss: 5.56816236686409, Val Loss: 4.358754628369803, Val MAE: 1.2055245637893677\n",
      "Epoch 996/2000, Train Loss: 5.567618364848882, Val Loss: 4.358595803054469, Val MAE: 1.2057757377624512\n",
      "Epoch 997/2000, Train Loss: 5.567085998850568, Val Loss: 4.358428148720582, Val MAE: 1.2059929370880127\n",
      "Epoch 998/2000, Train Loss: 5.56658061267009, Val Loss: 4.358286725608884, Val MAE: 1.2062269449234009\n",
      "Epoch 999/2000, Train Loss: 5.566043860245607, Val Loss: 4.3581312744366425, Val MAE: 1.2064523696899414\n",
      "Epoch 1000/2000, Train Loss: 5.5655250515841095, Val Loss: 4.357965577712601, Val MAE: 1.2066985368728638\n",
      "Epoch 1001/2000, Train Loss: 5.565046513601324, Val Loss: 4.357801201842255, Val MAE: 1.2069003582000732\n",
      "Epoch 1002/2000, Train Loss: 5.564553374735502, Val Loss: 4.357674934319011, Val MAE: 1.2071722745895386\n",
      "Epoch 1003/2000, Train Loss: 5.564001482102131, Val Loss: 4.357507350913307, Val MAE: 1.2073851823806763\n",
      "Epoch 1004/2000, Train Loss: 5.563383887383895, Val Loss: 4.357343078723496, Val MAE: 1.2076520919799805\n",
      "Epoch 1005/2000, Train Loss: 5.562863298396866, Val Loss: 4.357223120667376, Val MAE: 1.2079333066940308\n",
      "Epoch 1006/2000, Train Loss: 5.562305801455077, Val Loss: 4.357074398510494, Val MAE: 1.2081639766693115\n",
      "Epoch 1007/2000, Train Loss: 5.561766129016132, Val Loss: 4.356910933007125, Val MAE: 1.2084044218063354\n",
      "Epoch 1008/2000, Train Loss: 5.561319964600061, Val Loss: 4.356751849874854, Val MAE: 1.208574891090393\n",
      "Epoch 1009/2000, Train Loss: 5.560966995688571, Val Loss: 4.35660828257332, Val MAE: 1.208883285522461\n",
      "Epoch 1010/2000, Train Loss: 5.560305135670393, Val Loss: 4.356442448043743, Val MAE: 1.209023118019104\n",
      "Epoch 1011/2000, Train Loss: 5.560131950794999, Val Loss: 4.3562651844477065, Val MAE: 1.209159016609192\n",
      "Epoch 1012/2000, Train Loss: 5.559502000927739, Val Loss: 4.35609173635776, Val MAE: 1.2094775438308716\n",
      "Epoch 1013/2000, Train Loss: 5.558911150965009, Val Loss: 4.35595782047792, Val MAE: 1.2097214460372925\n",
      "Epoch 1014/2000, Train Loss: 5.5584050520161945, Val Loss: 4.355816842538414, Val MAE: 1.2099425792694092\n",
      "Epoch 1015/2000, Train Loss: 5.557916537165502, Val Loss: 4.355667189740249, Val MAE: 1.2101571559906006\n",
      "Epoch 1016/2000, Train Loss: 5.557522511147486, Val Loss: 4.355503397666522, Val MAE: 1.21035635471344\n",
      "Epoch 1017/2000, Train Loss: 5.5570120810765, Val Loss: 4.355378780638178, Val MAE: 1.2105696201324463\n",
      "Epoch 1018/2000, Train Loss: 5.556536614243958, Val Loss: 4.355233276387056, Val MAE: 1.210797667503357\n",
      "Epoch 1019/2000, Train Loss: 5.555968229930598, Val Loss: 4.3551111635827535, Val MAE: 1.211073637008667\n",
      "Epoch 1020/2000, Train Loss: 5.555551604064132, Val Loss: 4.354984984432792, Val MAE: 1.2113442420959473\n",
      "Epoch 1021/2000, Train Loss: 5.555000016590735, Val Loss: 4.354850205315931, Val MAE: 1.2115812301635742\n",
      "Epoch 1022/2000, Train Loss: 5.554519620588901, Val Loss: 4.354685765971337, Val MAE: 1.2117843627929688\n",
      "Epoch 1023/2000, Train Loss: 5.554005074612622, Val Loss: 4.354581223656465, Val MAE: 1.2120411396026611\n",
      "Epoch 1024/2000, Train Loss: 5.553567580797371, Val Loss: 4.35438516057088, Val MAE: 1.212220549583435\n",
      "Epoch 1025/2000, Train Loss: 5.553131531730771, Val Loss: 4.354229520727788, Val MAE: 1.2124234437942505\n",
      "Epoch 1026/2000, Train Loss: 5.552654837669337, Val Loss: 4.354098118600008, Val MAE: 1.2126530408859253\n",
      "Epoch 1027/2000, Train Loss: 5.552250397256496, Val Loss: 4.353942833146131, Val MAE: 1.212850570678711\n",
      "Epoch 1028/2000, Train Loss: 5.551779741058112, Val Loss: 4.353821994985143, Val MAE: 1.2131048440933228\n",
      "Epoch 1029/2000, Train Loss: 5.551201391145703, Val Loss: 4.353695834773752, Val MAE: 1.2133700847625732\n",
      "Epoch 1030/2000, Train Loss: 5.550769584134291, Val Loss: 4.353537302356851, Val MAE: 1.213579773902893\n",
      "Epoch 1031/2000, Train Loss: 5.550301155322427, Val Loss: 4.353401768757953, Val MAE: 1.2137877941131592\n",
      "Epoch 1032/2000, Train Loss: 5.549841639776126, Val Loss: 4.3532604707992295, Val MAE: 1.2140244245529175\n",
      "Epoch 1033/2000, Train Loss: 5.549335339883933, Val Loss: 4.353122058292633, Val MAE: 1.2143288850784302\n",
      "Epoch 1034/2000, Train Loss: 5.548716913333363, Val Loss: 4.352991239206346, Val MAE: 1.214556336402893\n",
      "Epoch 1035/2000, Train Loss: 5.548367573206956, Val Loss: 4.352803464646677, Val MAE: 1.2147116661071777\n",
      "Epoch 1036/2000, Train Loss: 5.5479683686257015, Val Loss: 4.352771082621168, Val MAE: 1.2150402069091797\n",
      "Epoch 1037/2000, Train Loss: 5.547396484384671, Val Loss: 4.352657978226607, Val MAE: 1.215247631072998\n",
      "Epoch 1038/2000, Train Loss: 5.546964802167578, Val Loss: 4.352484585576364, Val MAE: 1.2154422998428345\n",
      "Epoch 1039/2000, Train Loss: 5.546491593168976, Val Loss: 4.352348502238725, Val MAE: 1.2156633138656616\n",
      "Epoch 1040/2000, Train Loss: 5.546104350811606, Val Loss: 4.352199849298408, Val MAE: 1.2158564329147339\n",
      "Epoch 1041/2000, Train Loss: 5.5457154733714376, Val Loss: 4.352119303900782, Val MAE: 1.2161191701889038\n",
      "Epoch 1042/2000, Train Loss: 5.545263378370943, Val Loss: 4.351908525686946, Val MAE: 1.2162529230117798\n",
      "Epoch 1043/2000, Train Loss: 5.544894459281064, Val Loss: 4.351790837564312, Val MAE: 1.216492772102356\n",
      "Epoch 1044/2000, Train Loss: 5.5444629329973, Val Loss: 4.351625102295263, Val MAE: 1.2166402339935303\n",
      "Epoch 1045/2000, Train Loss: 5.544006834919571, Val Loss: 4.3515337381444805, Val MAE: 1.2169716358184814\n",
      "Epoch 1046/2000, Train Loss: 5.543455480971314, Val Loss: 4.3514275242381535, Val MAE: 1.2172155380249023\n",
      "Epoch 1047/2000, Train Loss: 5.542963839271185, Val Loss: 4.351285522048538, Val MAE: 1.2174490690231323\n",
      "Epoch 1048/2000, Train Loss: 5.542564313643715, Val Loss: 4.351125078608056, Val MAE: 1.2176270484924316\n",
      "Epoch 1049/2000, Train Loss: 5.54213722484904, Val Loss: 4.351037117288456, Val MAE: 1.2179019451141357\n",
      "Epoch 1050/2000, Train Loss: 5.5416420959517385, Val Loss: 4.3509361026019935, Val MAE: 1.218153476715088\n",
      "Epoch 1051/2000, Train Loss: 5.541176038564658, Val Loss: 4.350808616694029, Val MAE: 1.218366026878357\n",
      "Epoch 1052/2000, Train Loss: 5.540719375967422, Val Loss: 4.3506973646883225, Val MAE: 1.2185953855514526\n",
      "Epoch 1053/2000, Train Loss: 5.540283557219364, Val Loss: 4.350483995323648, Val MAE: 1.2188220024108887\n",
      "Epoch 1054/2000, Train Loss: 5.539782559071986, Val Loss: 4.350572372885706, Val MAE: 1.219232439994812\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1055/2000, Train Loss: 5.53915153539125, Val Loss: 4.350462869054697, Val MAE: 1.2194751501083374\n",
      "Epoch 1056/2000, Train Loss: 5.538662337289772, Val Loss: 4.350367202958814, Val MAE: 1.2197452783584595\n",
      "Epoch 1057/2000, Train Loss: 5.538284335227308, Val Loss: 4.350245251644168, Val MAE: 1.2199243307113647\n",
      "Epoch 1058/2000, Train Loss: 5.537882669481584, Val Loss: 4.350136882939317, Val MAE: 1.2201076745986938\n",
      "Epoch 1059/2000, Train Loss: 5.537448607048081, Val Loss: 4.349988041704034, Val MAE: 1.2203121185302734\n",
      "Epoch 1060/2000, Train Loss: 5.537041045202294, Val Loss: 4.349871419635308, Val MAE: 1.220537781715393\n",
      "Epoch 1061/2000, Train Loss: 5.536563456709411, Val Loss: 4.349783883508932, Val MAE: 1.220798134803772\n",
      "Epoch 1062/2000, Train Loss: 5.536109806036986, Val Loss: 4.3496776532482455, Val MAE: 1.2210050821304321\n",
      "Epoch 1063/2000, Train Loss: 5.535704293600668, Val Loss: 4.349559257713121, Val MAE: 1.221204161643982\n",
      "Epoch 1064/2000, Train Loss: 5.535395796325017, Val Loss: 4.349441330450344, Val MAE: 1.2214809656143188\n",
      "Epoch 1065/2000, Train Loss: 5.534737567633809, Val Loss: 4.349382491614501, Val MAE: 1.2217503786087036\n",
      "Epoch 1066/2000, Train Loss: 5.534383645853647, Val Loss: 4.349305112092748, Val MAE: 1.2220149040222168\n",
      "Epoch 1067/2000, Train Loss: 5.53391007030624, Val Loss: 4.3490990002111, Val MAE: 1.222136378288269\n",
      "Epoch 1068/2000, Train Loss: 5.533542389105337, Val Loss: 4.348987395615056, Val MAE: 1.2223410606384277\n",
      "Epoch 1069/2000, Train Loss: 5.5331667023776285, Val Loss: 4.34889455896777, Val MAE: 1.22255539894104\n",
      "Epoch 1070/2000, Train Loss: 5.5327586668329936, Val Loss: 4.34876710477176, Val MAE: 1.2227767705917358\n",
      "Epoch 1071/2000, Train Loss: 5.532379939663986, Val Loss: 4.348631894430733, Val MAE: 1.22295081615448\n",
      "Epoch 1072/2000, Train Loss: 5.531942494062477, Val Loss: 4.348498678106714, Val MAE: 1.2231416702270508\n",
      "Epoch 1073/2000, Train Loss: 5.531599137042874, Val Loss: 4.34835599748468, Val MAE: 1.2233185768127441\n",
      "Epoch 1074/2000, Train Loss: 5.531228480491549, Val Loss: 4.3481820443460535, Val MAE: 1.2235029935836792\n",
      "Epoch 1075/2000, Train Loss: 5.530851836904721, Val Loss: 4.348069450675367, Val MAE: 1.2237110137939453\n",
      "Epoch 1076/2000, Train Loss: 5.5304584529210175, Val Loss: 4.347959466728272, Val MAE: 1.2239060401916504\n",
      "Epoch 1077/2000, Train Loss: 5.530040146594114, Val Loss: 4.347859457175474, Val MAE: 1.224119782447815\n",
      "Epoch 1078/2000, Train Loss: 5.5296466785734415, Val Loss: 4.34776621328348, Val MAE: 1.224379539489746\n",
      "Epoch 1079/2000, Train Loss: 5.529184173496205, Val Loss: 4.347582973768045, Val MAE: 1.2245407104492188\n",
      "Epoch 1080/2000, Train Loss: 5.528800819481777, Val Loss: 4.347461643905656, Val MAE: 1.2247284650802612\n",
      "Epoch 1081/2000, Train Loss: 5.528452621421279, Val Loss: 4.347362999178402, Val MAE: 1.2249082326889038\n",
      "Epoch 1082/2000, Train Loss: 5.5280656740185625, Val Loss: 4.3471958783899876, Val MAE: 1.225063443183899\n",
      "Epoch 1083/2000, Train Loss: 5.5277929858372845, Val Loss: 4.346985331335449, Val MAE: 1.2252005338668823\n",
      "Epoch 1084/2000, Train Loss: 5.527432856107837, Val Loss: 4.346892689131536, Val MAE: 1.2254276275634766\n",
      "Epoch 1085/2000, Train Loss: 5.527044638722566, Val Loss: 4.346713711415325, Val MAE: 1.2255648374557495\n",
      "Epoch 1086/2000, Train Loss: 5.526657341515962, Val Loss: 4.3466652892966255, Val MAE: 1.225792407989502\n",
      "Epoch 1087/2000, Train Loss: 5.526274599617292, Val Loss: 4.346504108020448, Val MAE: 1.225977897644043\n",
      "Epoch 1088/2000, Train Loss: 5.525902685516059, Val Loss: 4.3464097348184465, Val MAE: 1.226193904876709\n",
      "Epoch 1089/2000, Train Loss: 5.525487685538305, Val Loss: 4.346278713965738, Val MAE: 1.2263911962509155\n",
      "Epoch 1090/2000, Train Loss: 5.525155661072635, Val Loss: 4.346044478874218, Val MAE: 1.2264987230300903\n",
      "Epoch 1091/2000, Train Loss: 5.524721468394904, Val Loss: 4.345923182740807, Val MAE: 1.226758360862732\n",
      "Epoch 1092/2000, Train Loss: 5.524314136475371, Val Loss: 4.345795217203396, Val MAE: 1.2269469499588013\n",
      "Epoch 1093/2000, Train Loss: 5.523904137232003, Val Loss: 4.345761695265904, Val MAE: 1.2272037267684937\n",
      "Epoch 1094/2000, Train Loss: 5.5235121960573, Val Loss: 4.345591824041965, Val MAE: 1.2273802757263184\n",
      "Epoch 1095/2000, Train Loss: 5.523203278480565, Val Loss: 4.34543178931617, Val MAE: 1.2275416851043701\n",
      "Epoch 1096/2000, Train Loss: 5.522952488915597, Val Loss: 4.345246440232605, Val MAE: 1.2276500463485718\n",
      "Epoch 1097/2000, Train Loss: 5.522442328204603, Val Loss: 4.345271041717481, Val MAE: 1.227948546409607\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1098/2000, Train Loss: 5.5220146887759896, Val Loss: 4.345169853523105, Val MAE: 1.2281432151794434\n",
      "Epoch 1099/2000, Train Loss: 5.521709198637774, Val Loss: 4.345036562988619, Val MAE: 1.2283036708831787\n",
      "Epoch 1100/2000, Train Loss: 5.521339863957183, Val Loss: 4.344926366361009, Val MAE: 1.2285090684890747\n",
      "Epoch 1101/2000, Train Loss: 5.521088193991627, Val Loss: 4.344856784600127, Val MAE: 1.2287054061889648\n",
      "Epoch 1102/2000, Train Loss: 5.520588135942468, Val Loss: 4.3447398570240345, Val MAE: 1.2289127111434937\n",
      "Epoch 1103/2000, Train Loss: 5.52023831693319, Val Loss: 4.344652476520823, Val MAE: 1.229105830192566\n",
      "Epoch 1104/2000, Train Loss: 5.519845378330898, Val Loss: 4.3446645158368185, Val MAE: 1.2294062376022339\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1105/2000, Train Loss: 5.519435444948454, Val Loss: 4.34457944107351, Val MAE: 1.2295900583267212\n",
      "Epoch 1106/2000, Train Loss: 5.519182508663157, Val Loss: 4.344518386807527, Val MAE: 1.2298095226287842\n",
      "Epoch 1107/2000, Train Loss: 5.51874760789695, Val Loss: 4.344308447955294, Val MAE: 1.2299327850341797\n",
      "Epoch 1108/2000, Train Loss: 5.518496118172097, Val Loss: 4.344249254857769, Val MAE: 1.2301548719406128\n",
      "Epoch 1109/2000, Train Loss: 5.518064357933127, Val Loss: 4.344139507896191, Val MAE: 1.2303380966186523\n",
      "Epoch 1110/2000, Train Loss: 5.517704654409435, Val Loss: 4.343996181889429, Val MAE: 1.2305169105529785\n",
      "Epoch 1111/2000, Train Loss: 5.517326595630735, Val Loss: 4.343840350539566, Val MAE: 1.2306429147720337\n",
      "Epoch 1112/2000, Train Loss: 5.516989896896291, Val Loss: 4.343738486309041, Val MAE: 1.230819821357727\n",
      "Epoch 1113/2000, Train Loss: 5.5165856896250185, Val Loss: 4.343813211692346, Val MAE: 1.231173038482666\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1114/2000, Train Loss: 5.516213715355407, Val Loss: 4.343637516369691, Val MAE: 1.2313051223754883\n",
      "Epoch 1115/2000, Train Loss: 5.5158385129503085, Val Loss: 4.343580850220478, Val MAE: 1.2315315008163452\n",
      "Epoch 1116/2000, Train Loss: 5.515479126698142, Val Loss: 4.343348680288942, Val MAE: 1.2317113876342773\n",
      "Epoch 1117/2000, Train Loss: 5.515094573430821, Val Loss: 4.343222670646401, Val MAE: 1.2318719625473022\n",
      "Epoch 1118/2000, Train Loss: 5.514889371216692, Val Loss: 4.343179662968661, Val MAE: 1.2321016788482666\n",
      "Epoch 1119/2000, Train Loss: 5.514463208952858, Val Loss: 4.342999556741199, Val MAE: 1.2322131395339966\n",
      "Epoch 1120/2000, Train Loss: 5.514158447521525, Val Loss: 4.342832605903213, Val MAE: 1.232328176498413\n",
      "Epoch 1121/2000, Train Loss: 5.513753271465554, Val Loss: 4.3427787283116634, Val MAE: 1.23255455493927\n",
      "Epoch 1122/2000, Train Loss: 5.513423366219317, Val Loss: 4.3426981572363825, Val MAE: 1.232782006263733\n",
      "Epoch 1123/2000, Train Loss: 5.513063298372694, Val Loss: 4.342546306530366, Val MAE: 1.2329286336898804\n",
      "Epoch 1124/2000, Train Loss: 5.512706683968977, Val Loss: 4.342447046875148, Val MAE: 1.2331665754318237\n",
      "Epoch 1125/2000, Train Loss: 5.512303320414935, Val Loss: 4.342244025178858, Val MAE: 1.2332595586776733\n",
      "Epoch 1126/2000, Train Loss: 5.511920005036591, Val Loss: 4.342181716731808, Val MAE: 1.2334825992584229\n",
      "Epoch 1127/2000, Train Loss: 5.511575964516299, Val Loss: 4.3421046475502285, Val MAE: 1.233696699142456\n",
      "Epoch 1128/2000, Train Loss: 5.51123163220291, Val Loss: 4.34207222421427, Val MAE: 1.2339189052581787\n",
      "Epoch 1129/2000, Train Loss: 5.510845690175263, Val Loss: 4.3419729355085, Val MAE: 1.234128713607788\n",
      "Epoch 1130/2000, Train Loss: 5.510526144932287, Val Loss: 4.342105153961493, Val MAE: 1.2344743013381958\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1131/2000, Train Loss: 5.510138416439062, Val Loss: 4.341961565103617, Val MAE: 1.2346168756484985\n",
      "Epoch 1132/2000, Train Loss: 5.509816858586209, Val Loss: 4.341963917003558, Val MAE: 1.2348856925964355\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1133/2000, Train Loss: 5.509412789144865, Val Loss: 4.341820022181885, Val MAE: 1.235041618347168\n",
      "Epoch 1134/2000, Train Loss: 5.509127670512556, Val Loss: 4.3417503939421325, Val MAE: 1.2352628707885742\n",
      "Epoch 1135/2000, Train Loss: 5.5087526640542395, Val Loss: 4.341607330927441, Val MAE: 1.235398292541504\n",
      "Epoch 1136/2000, Train Loss: 5.508471546976503, Val Loss: 4.341508672956948, Val MAE: 1.2355763912200928\n",
      "Epoch 1137/2000, Train Loss: 5.508044375272325, Val Loss: 4.341519770469215, Val MAE: 1.2358275651931763\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1138/2000, Train Loss: 5.507626787786969, Val Loss: 4.34142544035976, Val MAE: 1.2360609769821167\n",
      "Epoch 1139/2000, Train Loss: 5.50745577154777, Val Loss: 4.341276996012207, Val MAE: 1.2362060546875\n",
      "Epoch 1140/2000, Train Loss: 5.507106612192382, Val Loss: 4.341252280677761, Val MAE: 1.2364376783370972\n",
      "Epoch 1141/2000, Train Loss: 5.506683436366214, Val Loss: 4.34105992417212, Val MAE: 1.2365525960922241\n",
      "Epoch 1142/2000, Train Loss: 5.506422150386105, Val Loss: 4.340927025783169, Val MAE: 1.2367074489593506\n",
      "Epoch 1143/2000, Train Loss: 5.506121044647234, Val Loss: 4.340773969210751, Val MAE: 1.2368420362472534\n",
      "Epoch 1144/2000, Train Loss: 5.505800551102844, Val Loss: 4.340667346730694, Val MAE: 1.237021803855896\n",
      "Epoch 1145/2000, Train Loss: 5.505451446390375, Val Loss: 4.340515780771101, Val MAE: 1.237173080444336\n",
      "Epoch 1146/2000, Train Loss: 5.505135954263244, Val Loss: 4.340375049794848, Val MAE: 1.2373627424240112\n",
      "Epoch 1147/2000, Train Loss: 5.504810446696051, Val Loss: 4.340481633471476, Val MAE: 1.2376883029937744\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1148/2000, Train Loss: 5.504485539452706, Val Loss: 4.340319316518737, Val MAE: 1.2378473281860352\n",
      "Epoch 1149/2000, Train Loss: 5.50406051016821, Val Loss: 4.340216713371846, Val MAE: 1.238038182258606\n",
      "Epoch 1150/2000, Train Loss: 5.503754700587804, Val Loss: 4.340027378721012, Val MAE: 1.238185167312622\n",
      "Epoch 1151/2000, Train Loss: 5.503360486904656, Val Loss: 4.340009448858532, Val MAE: 1.2384346723556519\n",
      "Epoch 1152/2000, Train Loss: 5.5030746965810025, Val Loss: 4.339975862325849, Val MAE: 1.2386393547058105\n",
      "Epoch 1153/2000, Train Loss: 5.502695985027994, Val Loss: 4.339868272377832, Val MAE: 1.2388206720352173\n",
      "Epoch 1154/2000, Train Loss: 5.502388818308641, Val Loss: 4.33972244853372, Val MAE: 1.23898446559906\n",
      "Epoch 1155/2000, Train Loss: 5.502063990662139, Val Loss: 4.339588354930684, Val MAE: 1.239137053489685\n",
      "Epoch 1156/2000, Train Loss: 5.501743472868492, Val Loss: 4.339439766719803, Val MAE: 1.2393035888671875\n",
      "Epoch 1157/2000, Train Loss: 5.5014750622735935, Val Loss: 4.33930540821171, Val MAE: 1.2394386529922485\n",
      "Epoch 1158/2000, Train Loss: 5.501152009867399, Val Loss: 4.339138669252127, Val MAE: 1.239585280418396\n",
      "Epoch 1159/2000, Train Loss: 5.5008332123384225, Val Loss: 4.339034550026193, Val MAE: 1.2397565841674805\n",
      "Epoch 1160/2000, Train Loss: 5.500491253857308, Val Loss: 4.338915381285253, Val MAE: 1.239935040473938\n",
      "Epoch 1161/2000, Train Loss: 5.500094581877757, Val Loss: 4.338824030723389, Val MAE: 1.2402032613754272\n",
      "Epoch 1162/2000, Train Loss: 5.499785782580442, Val Loss: 4.338573479974592, Val MAE: 1.240289568901062\n",
      "Epoch 1163/2000, Train Loss: 5.499421531808172, Val Loss: 4.338516512056729, Val MAE: 1.240531086921692\n",
      "Epoch 1164/2000, Train Loss: 5.499166655094129, Val Loss: 4.338294632122055, Val MAE: 1.2406164407730103\n",
      "Epoch 1165/2000, Train Loss: 5.49878450563285, Val Loss: 4.33817404653575, Val MAE: 1.2407735586166382\n",
      "Epoch 1166/2000, Train Loss: 5.498437009577446, Val Loss: 4.338115558545063, Val MAE: 1.2410074472427368\n",
      "Epoch 1167/2000, Train Loss: 5.498135288494425, Val Loss: 4.337999250645842, Val MAE: 1.2411705255508423\n",
      "Epoch 1168/2000, Train Loss: 5.497837691522798, Val Loss: 4.337828012831039, Val MAE: 1.2413060665130615\n",
      "Epoch 1169/2000, Train Loss: 5.497511725343892, Val Loss: 4.3376200345655285, Val MAE: 1.2414414882659912\n",
      "Epoch 1170/2000, Train Loss: 5.497225175967641, Val Loss: 4.3372983504388785, Val MAE: 1.241481065750122\n",
      "Epoch 1171/2000, Train Loss: 5.496950144805421, Val Loss: 4.337239005691833, Val MAE: 1.241684913635254\n",
      "Epoch 1172/2000, Train Loss: 5.496545161453312, Val Loss: 4.337076803491459, Val MAE: 1.2418239116668701\n",
      "Epoch 1173/2000, Train Loss: 5.496253163877776, Val Loss: 4.3369551712798105, Val MAE: 1.2419878244400024\n",
      "Epoch 1174/2000, Train Loss: 5.4959744799434675, Val Loss: 4.336623085108963, Val MAE: 1.242035984992981\n",
      "Epoch 1175/2000, Train Loss: 5.495610836701535, Val Loss: 4.336562027345907, Val MAE: 1.2422446012496948\n",
      "Epoch 1176/2000, Train Loss: 5.495319318659778, Val Loss: 4.3363658953693, Val MAE: 1.2423502206802368\n",
      "Epoch 1177/2000, Train Loss: 5.494976685311233, Val Loss: 4.33628442149173, Val MAE: 1.242536187171936\n",
      "Epoch 1178/2000, Train Loss: 5.49471965111362, Val Loss: 4.3361535077562205, Val MAE: 1.2427006959915161\n",
      "Epoch 1179/2000, Train Loss: 5.494364751854478, Val Loss: 4.335930169836895, Val MAE: 1.2428163290023804\n",
      "Epoch 1180/2000, Train Loss: 5.49420003920747, Val Loss: 4.335937170744748, Val MAE: 1.2431156635284424\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1181/2000, Train Loss: 5.4937396856626375, Val Loss: 4.335794852828389, Val MAE: 1.243262767791748\n",
      "Epoch 1182/2000, Train Loss: 5.493471642924173, Val Loss: 4.3357305090908, Val MAE: 1.243486762046814\n",
      "Epoch 1183/2000, Train Loss: 5.493121719955468, Val Loss: 4.33562110165755, Val MAE: 1.2436552047729492\n",
      "Epoch 1184/2000, Train Loss: 5.492765317655019, Val Loss: 4.335341522701689, Val MAE: 1.2437317371368408\n",
      "Epoch 1185/2000, Train Loss: 5.492455455984531, Val Loss: 4.335252475436475, Val MAE: 1.243914246559143\n",
      "Epoch 1186/2000, Train Loss: 5.492119665841707, Val Loss: 4.3351582973121525, Val MAE: 1.2440823316574097\n",
      "Epoch 1187/2000, Train Loss: 5.491955652623764, Val Loss: 4.334980348155305, Val MAE: 1.2442196607589722\n",
      "Epoch 1188/2000, Train Loss: 5.49150473562864, Val Loss: 4.334853157354099, Val MAE: 1.2443732023239136\n",
      "Epoch 1189/2000, Train Loss: 5.491198308010369, Val Loss: 4.334700573732456, Val MAE: 1.2445157766342163\n",
      "Epoch 1190/2000, Train Loss: 5.490894380486142, Val Loss: 4.334633910078723, Val MAE: 1.2447015047073364\n",
      "Epoch 1191/2000, Train Loss: 5.490633811819107, Val Loss: 4.334571112262773, Val MAE: 1.2449166774749756\n",
      "Epoch 1192/2000, Train Loss: 5.490262822502303, Val Loss: 4.334381849231484, Val MAE: 1.2450201511383057\n",
      "Epoch 1193/2000, Train Loss: 5.489961403562991, Val Loss: 4.334272054253935, Val MAE: 1.245186686515808\n",
      "Epoch 1194/2000, Train Loss: 5.489676679887006, Val Loss: 4.334129709582608, Val MAE: 1.2453259229660034\n",
      "Epoch 1195/2000, Train Loss: 5.4893365921147526, Val Loss: 4.334052977751235, Val MAE: 1.245513677597046\n",
      "Epoch 1196/2000, Train Loss: 5.489031864588792, Val Loss: 4.3338974885575405, Val MAE: 1.2456731796264648\n",
      "Epoch 1197/2000, Train Loss: 5.488825109265687, Val Loss: 4.333977892142427, Val MAE: 1.2459651231765747\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1198/2000, Train Loss: 5.48833453897009, Val Loss: 4.333815857788196, Val MAE: 1.2460917234420776\n",
      "Epoch 1199/2000, Train Loss: 5.488058666543172, Val Loss: 4.333593046316156, Val MAE: 1.246176838874817\n",
      "Epoch 1200/2000, Train Loss: 5.487749379436237, Val Loss: 4.333501256478799, Val MAE: 1.2463527917861938\n",
      "Epoch 1201/2000, Train Loss: 5.487410510386022, Val Loss: 4.333378676727817, Val MAE: 1.2465158700942993\n",
      "Epoch 1202/2000, Train Loss: 5.48706863055735, Val Loss: 4.3333552966045366, Val MAE: 1.246760368347168\n",
      "Epoch 1203/2000, Train Loss: 5.486799761583206, Val Loss: 4.333261614178752, Val MAE: 1.2469500303268433\n",
      "Epoch 1204/2000, Train Loss: 5.486407954943208, Val Loss: 4.333099409964707, Val MAE: 1.2471033334732056\n",
      "Epoch 1205/2000, Train Loss: 5.48609502787895, Val Loss: 4.3330107412561105, Val MAE: 1.2472811937332153\n",
      "Epoch 1206/2000, Train Loss: 5.485775554793859, Val Loss: 4.332922029126066, Val MAE: 1.2474675178527832\n",
      "Epoch 1207/2000, Train Loss: 5.485425587935306, Val Loss: 4.332705660452982, Val MAE: 1.2475736141204834\n",
      "Epoch 1208/2000, Train Loss: 5.485133547800798, Val Loss: 4.332520303915481, Val MAE: 1.2476887702941895\n",
      "Epoch 1209/2000, Train Loss: 5.484891468993215, Val Loss: 4.332386941221115, Val MAE: 1.2478580474853516\n",
      "Epoch 1210/2000, Train Loss: 5.484521267752566, Val Loss: 4.332271623618163, Val MAE: 1.2480303049087524\n",
      "Epoch 1211/2000, Train Loss: 5.484169592537485, Val Loss: 4.332255397643055, Val MAE: 1.248280644416809\n",
      "Epoch 1212/2000, Train Loss: 5.483726314003122, Val Loss: 4.332202619253784, Val MAE: 1.2485301494598389\n",
      "Epoch 1213/2000, Train Loss: 5.483352751423156, Val Loss: 4.332248115646946, Val MAE: 1.2487908601760864\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1214/2000, Train Loss: 5.48306584767358, Val Loss: 4.332110650092363, Val MAE: 1.2489477396011353\n",
      "Epoch 1215/2000, Train Loss: 5.482758762407903, Val Loss: 4.332049205181029, Val MAE: 1.2491769790649414\n",
      "Epoch 1216/2000, Train Loss: 5.482438486749408, Val Loss: 4.331916126514877, Val MAE: 1.2493008375167847\n",
      "Epoch 1217/2000, Train Loss: 5.482143146199481, Val Loss: 4.331757646973606, Val MAE: 1.2494497299194336\n",
      "Epoch 1218/2000, Train Loss: 5.481727055937935, Val Loss: 4.3316975104177855, Val MAE: 1.2497496604919434\n",
      "Epoch 1219/2000, Train Loss: 5.481471393019101, Val Loss: 4.331490574803976, Val MAE: 1.2498599290847778\n",
      "Epoch 1220/2000, Train Loss: 5.481101907173668, Val Loss: 4.331400512843518, Val MAE: 1.2500386238098145\n",
      "Epoch 1221/2000, Train Loss: 5.480793655383605, Val Loss: 4.331252857586285, Val MAE: 1.250172734260559\n",
      "Epoch 1222/2000, Train Loss: 5.480558740785825, Val Loss: 4.331237551751169, Val MAE: 1.2503712177276611\n",
      "Epoch 1223/2000, Train Loss: 5.480167944234172, Val Loss: 4.331000969122659, Val MAE: 1.250464916229248\n",
      "Epoch 1224/2000, Train Loss: 5.47996850690678, Val Loss: 4.330766175298003, Val MAE: 1.2505598068237305\n",
      "Epoch 1225/2000, Train Loss: 5.4795512980120415, Val Loss: 4.330695235024433, Val MAE: 1.2508361339569092\n",
      "Epoch 1226/2000, Train Loss: 5.479217980934566, Val Loss: 4.330516614261511, Val MAE: 1.2509881258010864\n",
      "Epoch 1227/2000, Train Loss: 5.4789158447112385, Val Loss: 4.3303734679949715, Val MAE: 1.2511231899261475\n",
      "Epoch 1228/2000, Train Loss: 5.478611263209683, Val Loss: 4.330220726206227, Val MAE: 1.2512381076812744\n",
      "Epoch 1229/2000, Train Loss: 5.478221452924279, Val Loss: 4.329660603645686, Val MAE: 1.2511788606643677\n",
      "Epoch 1230/2000, Train Loss: 5.478014262692419, Val Loss: 4.329537053587469, Val MAE: 1.251381754875183\n",
      "Epoch 1231/2000, Train Loss: 5.477689238680692, Val Loss: 4.329397133033018, Val MAE: 1.2515010833740234\n",
      "Epoch 1232/2000, Train Loss: 5.477377101895218, Val Loss: 4.329340960266622, Val MAE: 1.251711130142212\n",
      "Epoch 1233/2000, Train Loss: 5.477082899970681, Val Loss: 4.329204093329273, Val MAE: 1.251844048500061\n",
      "Epoch 1234/2000, Train Loss: 5.476783661872102, Val Loss: 4.329065864378805, Val MAE: 1.2519700527191162\n",
      "Epoch 1235/2000, Train Loss: 5.476515395778947, Val Loss: 4.32894935403187, Val MAE: 1.2521296739578247\n",
      "Epoch 1236/2000, Train Loss: 5.476218157365058, Val Loss: 4.328900877117842, Val MAE: 1.2523112297058105\n",
      "Epoch 1237/2000, Train Loss: 5.475929744855476, Val Loss: 4.328872878057463, Val MAE: 1.2525262832641602\n",
      "Epoch 1238/2000, Train Loss: 5.475661765778214, Val Loss: 4.328790346277995, Val MAE: 1.2526639699935913\n",
      "Epoch 1239/2000, Train Loss: 5.475356210877482, Val Loss: 4.328667128670054, Val MAE: 1.252805233001709\n",
      "Epoch 1240/2000, Train Loss: 5.475075485181883, Val Loss: 4.328520751059861, Val MAE: 1.2529200315475464\n",
      "Epoch 1241/2000, Train Loss: 5.474789977631591, Val Loss: 4.3285059894997255, Val MAE: 1.2531516551971436\n",
      "Epoch 1242/2000, Train Loss: 5.4744356574952695, Val Loss: 4.32834794864327, Val MAE: 1.2532559633255005\n",
      "Epoch 1243/2000, Train Loss: 5.474145724695298, Val Loss: 4.328228459406543, Val MAE: 1.2534388303756714\n",
      "Epoch 1244/2000, Train Loss: 5.47379908044699, Val Loss: 4.328174488125621, Val MAE: 1.2536308765411377\n",
      "Epoch 1245/2000, Train Loss: 5.4735116623865085, Val Loss: 4.328064734065855, Val MAE: 1.253800392150879\n",
      "Epoch 1246/2000, Train Loss: 5.473233280985292, Val Loss: 4.3279987952059455, Val MAE: 1.253974199295044\n",
      "Epoch 1247/2000, Train Loss: 5.472915092980062, Val Loss: 4.32784216812065, Val MAE: 1.2541061639785767\n",
      "Epoch 1248/2000, Train Loss: 5.472838309923312, Val Loss: 4.327955436706543, Val MAE: 1.254399299621582\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1249/2000, Train Loss: 5.472294768646894, Val Loss: 4.3276729666434965, Val MAE: 1.2544325590133667\n",
      "Epoch 1250/2000, Train Loss: 5.472017707253767, Val Loss: 4.327626221479328, Val MAE: 1.2546252012252808\n",
      "Epoch 1251/2000, Train Loss: 5.471728974357596, Val Loss: 4.327477114103936, Val MAE: 1.2547357082366943\n",
      "Epoch 1252/2000, Train Loss: 5.47143140925725, Val Loss: 4.3274252173465655, Val MAE: 1.2549114227294922\n",
      "Epoch 1253/2000, Train Loss: 5.471137551175265, Val Loss: 4.327311229336638, Val MAE: 1.2550616264343262\n",
      "Epoch 1254/2000, Train Loss: 5.470830944417606, Val Loss: 4.327267544991798, Val MAE: 1.255271077156067\n",
      "Epoch 1255/2000, Train Loss: 5.470538723969748, Val Loss: 4.3271143751101455, Val MAE: 1.2553789615631104\n",
      "Epoch 1256/2000, Train Loss: 5.4702690064256165, Val Loss: 4.327009065986217, Val MAE: 1.2555259466171265\n",
      "Epoch 1257/2000, Train Loss: 5.470028811051582, Val Loss: 4.326941566424327, Val MAE: 1.255702257156372\n",
      "Epoch 1258/2000, Train Loss: 5.4696699858827635, Val Loss: 4.326791236918789, Val MAE: 1.2558143138885498\n",
      "Epoch 1259/2000, Train Loss: 5.4694211525775716, Val Loss: 4.326601884493956, Val MAE: 1.2559101581573486\n",
      "Epoch 1260/2000, Train Loss: 5.469110034066202, Val Loss: 4.326572133949748, Val MAE: 1.2561315298080444\n",
      "Epoch 1261/2000, Train Loss: 5.468790803517269, Val Loss: 4.3264390618146, Val MAE: 1.256244421005249\n",
      "Epoch 1262/2000, Train Loss: 5.46853784912276, Val Loss: 4.3263696607504345, Val MAE: 1.2564457654953003\n",
      "Epoch 1263/2000, Train Loss: 5.468233153154251, Val Loss: 4.3262507346702055, Val MAE: 1.2565909624099731\n",
      "Epoch 1264/2000, Train Loss: 5.467896896293866, Val Loss: 4.326235859727, Val MAE: 1.2568596601486206\n",
      "Epoch 1265/2000, Train Loss: 5.467555308118811, Val Loss: 4.32612792468957, Val MAE: 1.2570027112960815\n",
      "Epoch 1266/2000, Train Loss: 5.467301004762397, Val Loss: 4.325930371706013, Val MAE: 1.2571110725402832\n",
      "Epoch 1267/2000, Train Loss: 5.466990833907343, Val Loss: 4.3258376639437035, Val MAE: 1.2572604417800903\n",
      "Epoch 1268/2000, Train Loss: 5.466703798953159, Val Loss: 4.32572661154979, Val MAE: 1.2574106454849243\n",
      "Epoch 1269/2000, Train Loss: 5.466470862327611, Val Loss: 4.325552472660133, Val MAE: 1.25751793384552\n",
      "Epoch 1270/2000, Train Loss: 5.46613232226528, Val Loss: 4.325417249327576, Val MAE: 1.2576539516448975\n",
      "Epoch 1271/2000, Train Loss: 5.465885197316615, Val Loss: 4.325242831041147, Val MAE: 1.2577581405639648\n",
      "Epoch 1272/2000, Train Loss: 5.465601610133131, Val Loss: 4.325093963688558, Val MAE: 1.2578812837600708\n",
      "Epoch 1273/2000, Train Loss: 5.465331535815449, Val Loss: 4.325238507428954, Val MAE: 1.2581886053085327\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1274/2000, Train Loss: 5.464947592318872, Val Loss: 4.325110448413604, Val MAE: 1.2583603858947754\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1275/2000, Train Loss: 5.4646532210796, Val Loss: 4.325030635975234, Val MAE: 1.2585338354110718\n",
      "Epoch 1276/2000, Train Loss: 5.464397790465451, Val Loss: 4.324889459373715, Val MAE: 1.2586592435836792\n",
      "Epoch 1277/2000, Train Loss: 5.464136413776559, Val Loss: 4.324695789344139, Val MAE: 1.2587672472000122\n",
      "Epoch 1278/2000, Train Loss: 5.463794076367748, Val Loss: 4.3244566451456095, Val MAE: 1.2588335275650024\n",
      "Epoch 1279/2000, Train Loss: 5.463515064831643, Val Loss: 4.324382784835121, Val MAE: 1.2589874267578125\n",
      "Epoch 1280/2000, Train Loss: 5.463219330667892, Val Loss: 4.324270013927876, Val MAE: 1.2591485977172852\n",
      "Epoch 1281/2000, Train Loss: 5.462970168049361, Val Loss: 4.324108899310902, Val MAE: 1.2592766284942627\n",
      "Epoch 1282/2000, Train Loss: 5.462639667965506, Val Loss: 4.3240537139276665, Val MAE: 1.2594527006149292\n",
      "Epoch 1283/2000, Train Loss: 5.462390077560443, Val Loss: 4.323946913318323, Val MAE: 1.2596186399459839\n",
      "Epoch 1284/2000, Train Loss: 5.462080542829219, Val Loss: 4.32378542683683, Val MAE: 1.2597371339797974\n",
      "Epoch 1285/2000, Train Loss: 5.461796492756622, Val Loss: 4.323653018904162, Val MAE: 1.2598826885223389\n",
      "Epoch 1286/2000, Train Loss: 5.461517435340167, Val Loss: 4.323489002655219, Val MAE: 1.260008692741394\n",
      "Epoch 1287/2000, Train Loss: 5.461348594583327, Val Loss: 4.3234737852269465, Val MAE: 1.2602365016937256\n",
      "Epoch 1288/2000, Train Loss: 5.460958445127966, Val Loss: 4.323232465734084, Val MAE: 1.2603020668029785\n",
      "Epoch 1289/2000, Train Loss: 5.4607393743467405, Val Loss: 4.323090975201345, Val MAE: 1.2604353427886963\n",
      "Epoch 1290/2000, Train Loss: 5.460376023130372, Val Loss: 4.322968929737538, Val MAE: 1.260597825050354\n",
      "Epoch 1291/2000, Train Loss: 5.460127390677025, Val Loss: 4.322849412544354, Val MAE: 1.2607489824295044\n",
      "Epoch 1292/2000, Train Loss: 5.459786641988293, Val Loss: 4.3228186678443405, Val MAE: 1.2609803676605225\n",
      "Epoch 1293/2000, Train Loss: 5.459543962002544, Val Loss: 4.322681578872977, Val MAE: 1.2611039876937866\n",
      "Epoch 1294/2000, Train Loss: 5.459252463444463, Val Loss: 4.322502488085815, Val MAE: 1.2612049579620361\n",
      "Epoch 1295/2000, Train Loss: 5.458951027642546, Val Loss: 4.322321573920078, Val MAE: 1.26134192943573\n",
      "Epoch 1296/2000, Train Loss: 5.4585337259468165, Val Loss: 4.322224107149753, Val MAE: 1.2615727186203003\n",
      "Epoch 1297/2000, Train Loss: 5.458215200767944, Val Loss: 4.322221546680541, Val MAE: 1.2618495225906372\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1298/2000, Train Loss: 5.457937858405984, Val Loss: 4.322165907288457, Val MAE: 1.2620271444320679\n",
      "Epoch 1299/2000, Train Loss: 5.457654331478054, Val Loss: 4.32205832412651, Val MAE: 1.262178897857666\n",
      "Epoch 1300/2000, Train Loss: 5.4573792399469525, Val Loss: 4.321867835051841, Val MAE: 1.2622947692871094\n",
      "Epoch 1301/2000, Train Loss: 5.457104074415662, Val Loss: 4.321783210155932, Val MAE: 1.2624603509902954\n",
      "Epoch 1302/2000, Train Loss: 5.456715067863092, Val Loss: 4.321687305577703, Val MAE: 1.2626664638519287\n",
      "Epoch 1303/2000, Train Loss: 5.456448205361686, Val Loss: 4.321508784994886, Val MAE: 1.262782096862793\n",
      "Epoch 1304/2000, Train Loss: 5.456099588476272, Val Loss: 4.321328524238354, Val MAE: 1.2629106044769287\n",
      "Epoch 1305/2000, Train Loss: 5.455794574689939, Val Loss: 4.321257619385246, Val MAE: 1.2630891799926758\n",
      "Epoch 1306/2000, Train Loss: 5.455584420056871, Val Loss: 4.321045220448627, Val MAE: 1.263157844543457\n",
      "Epoch 1307/2000, Train Loss: 5.45531395779757, Val Loss: 4.321043713619043, Val MAE: 1.2633936405181885\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1308/2000, Train Loss: 5.454955205530533, Val Loss: 4.320789207450979, Val MAE: 1.2634594440460205\n",
      "Epoch 1309/2000, Train Loss: 5.454679953779984, Val Loss: 4.320709127952924, Val MAE: 1.2636250257492065\n",
      "Epoch 1310/2000, Train Loss: 5.454433818456721, Val Loss: 4.320551342369469, Val MAE: 1.263768196105957\n",
      "Epoch 1311/2000, Train Loss: 5.454159380028289, Val Loss: 4.320411840584632, Val MAE: 1.2638968229293823\n",
      "Epoch 1312/2000, Train Loss: 5.453897906028536, Val Loss: 4.320394509372948, Val MAE: 1.2640973329544067\n",
      "Epoch 1313/2000, Train Loss: 5.453538226263014, Val Loss: 4.320245207275625, Val MAE: 1.2642418146133423\n",
      "Epoch 1314/2000, Train Loss: 5.453269316700059, Val Loss: 4.32009905916345, Val MAE: 1.2643611431121826\n",
      "Epoch 1315/2000, Train Loss: 5.452984855997023, Val Loss: 4.319910379378377, Val MAE: 1.264474630355835\n",
      "Epoch 1316/2000, Train Loss: 5.452709971463625, Val Loss: 4.319888371873546, Val MAE: 1.2647022008895874\n",
      "Epoch 1317/2000, Train Loss: 5.452385424079836, Val Loss: 4.3197009134131505, Val MAE: 1.2648324966430664\n",
      "Epoch 1318/2000, Train Loss: 5.452120291461439, Val Loss: 4.319546002478481, Val MAE: 1.2649562358856201\n",
      "Epoch 1319/2000, Train Loss: 5.4517857325430406, Val Loss: 4.319501104044753, Val MAE: 1.265161395072937\n",
      "Epoch 1320/2000, Train Loss: 5.451499291403617, Val Loss: 4.319512119164338, Val MAE: 1.2653944492340088\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1321/2000, Train Loss: 5.451217020543615, Val Loss: 4.3193471967234265, Val MAE: 1.265520691871643\n",
      "Epoch 1322/2000, Train Loss: 5.4509543926221165, Val Loss: 4.3192545790930055, Val MAE: 1.2656866312026978\n",
      "Epoch 1323/2000, Train Loss: 5.450662787730534, Val Loss: 4.318615581028096, Val MAE: 1.2656172513961792\n",
      "Epoch 1324/2000, Train Loss: 5.450381564647657, Val Loss: 4.318605148980209, Val MAE: 1.2658116817474365\n",
      "Epoch 1325/2000, Train Loss: 5.4501089132252885, Val Loss: 4.3185170914273, Val MAE: 1.265993356704712\n",
      "Epoch 1326/2000, Train Loss: 5.449787559449765, Val Loss: 4.318479321722512, Val MAE: 1.266169548034668\n",
      "Epoch 1327/2000, Train Loss: 5.449532514429316, Val Loss: 4.318446454123871, Val MAE: 1.266370177268982\n",
      "Epoch 1328/2000, Train Loss: 5.4492648075597705, Val Loss: 4.318322128755552, Val MAE: 1.2665003538131714\n",
      "Epoch 1329/2000, Train Loss: 5.448967676638813, Val Loss: 4.318085659839012, Val MAE: 1.2665624618530273\n",
      "Epoch 1330/2000, Train Loss: 5.448690936271561, Val Loss: 4.317968144502726, Val MAE: 1.266711950302124\n",
      "Epoch 1331/2000, Train Loss: 5.448424877131041, Val Loss: 4.317942313970746, Val MAE: 1.2669110298156738\n",
      "Epoch 1332/2000, Train Loss: 5.448203316809793, Val Loss: 4.31784304705289, Val MAE: 1.2670665979385376\n",
      "Epoch 1333/2000, Train Loss: 5.447983277011401, Val Loss: 4.3177973015217095, Val MAE: 1.2672218084335327\n",
      "Epoch 1334/2000, Train Loss: 5.447711064923386, Val Loss: 4.317575751003381, Val MAE: 1.2673019170761108\n",
      "Epoch 1335/2000, Train Loss: 5.447412573231178, Val Loss: 4.317524695906553, Val MAE: 1.2674710750579834\n",
      "Epoch 1336/2000, Train Loss: 5.447184158189806, Val Loss: 4.3173373572461236, Val MAE: 1.2675774097442627\n",
      "Epoch 1337/2000, Train Loss: 5.446933082782906, Val Loss: 4.317280257486545, Val MAE: 1.2677114009857178\n",
      "Epoch 1338/2000, Train Loss: 5.446678587640875, Val Loss: 4.317043626576931, Val MAE: 1.2677780389785767\n",
      "Epoch 1339/2000, Train Loss: 5.446496469554217, Val Loss: 4.31693749588889, Val MAE: 1.2679166793823242\n",
      "Epoch 1340/2000, Train Loss: 5.446137410520275, Val Loss: 4.317071987299232, Val MAE: 1.268231749534607\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1341/2000, Train Loss: 5.4458576991758925, Val Loss: 4.316978790819108, Val MAE: 1.268351674079895\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1342/2000, Train Loss: 5.445581063465469, Val Loss: 4.316871418501879, Val MAE: 1.2684977054595947\n",
      "Epoch 1343/2000, Train Loss: 5.44534961332955, Val Loss: 4.316747738058503, Val MAE: 1.2686116695404053\n",
      "Epoch 1344/2000, Train Loss: 5.445113534302496, Val Loss: 4.316676441291431, Val MAE: 1.2687675952911377\n",
      "Epoch 1345/2000, Train Loss: 5.444832481199791, Val Loss: 4.3166409293005055, Val MAE: 1.2689751386642456\n",
      "Epoch 1346/2000, Train Loss: 5.4445879863874405, Val Loss: 4.316542129339399, Val MAE: 1.269110083580017\n",
      "Epoch 1347/2000, Train Loss: 5.444298639312363, Val Loss: 4.316428252960647, Val MAE: 1.269248604774475\n",
      "Epoch 1348/2000, Train Loss: 5.444011313901769, Val Loss: 4.3163199257072025, Val MAE: 1.269407868385315\n",
      "Epoch 1349/2000, Train Loss: 5.443805247870697, Val Loss: 4.316206729949058, Val MAE: 1.2695579528808594\n",
      "Epoch 1350/2000, Train Loss: 5.443483314920048, Val Loss: 4.316121859929046, Val MAE: 1.269709825515747\n",
      "Epoch 1351/2000, Train Loss: 5.4432444304646275, Val Loss: 4.316011027642736, Val MAE: 1.2698416709899902\n",
      "Epoch 1352/2000, Train Loss: 5.443005799503297, Val Loss: 4.315949347722638, Val MAE: 1.2700237035751343\n",
      "Epoch 1353/2000, Train Loss: 5.442735252997805, Val Loss: 4.315860522518287, Val MAE: 1.270172119140625\n",
      "Epoch 1354/2000, Train Loss: 5.44246038537641, Val Loss: 4.315724201943423, Val MAE: 1.2702959775924683\n",
      "Epoch 1355/2000, Train Loss: 5.442214979210435, Val Loss: 4.315631284907058, Val MAE: 1.2704603672027588\n",
      "Epoch 1356/2000, Train Loss: 5.4419587487921515, Val Loss: 4.3155559524491025, Val MAE: 1.270627737045288\n",
      "Epoch 1357/2000, Train Loss: 5.441665393513934, Val Loss: 4.315295838517649, Val MAE: 1.2706631422042847\n",
      "Epoch 1358/2000, Train Loss: 5.441450313779754, Val Loss: 4.315258869743562, Val MAE: 1.2708740234375\n",
      "Epoch 1359/2000, Train Loss: 5.441157122298075, Val Loss: 4.315136046157227, Val MAE: 1.2710071802139282\n",
      "Epoch 1360/2000, Train Loss: 5.4410670488933315, Val Loss: 4.315245932823903, Val MAE: 1.27131986618042\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1361/2000, Train Loss: 5.4405814273495015, Val Loss: 4.315047277120857, Val MAE: 1.2713897228240967\n",
      "Epoch 1362/2000, Train Loss: 5.440234538638461, Val Loss: 4.314916681505001, Val MAE: 1.2715833187103271\n",
      "Epoch 1363/2000, Train Loss: 5.439985642753042, Val Loss: 4.314839504578629, Val MAE: 1.2717515230178833\n",
      "Epoch 1364/2000, Train Loss: 5.439666133681995, Val Loss: 4.314680194747341, Val MAE: 1.2718396186828613\n",
      "Epoch 1365/2000, Train Loss: 5.439431175054886, Val Loss: 4.314560879095717, Val MAE: 1.2719827890396118\n",
      "Epoch 1366/2000, Train Loss: 5.439243575526102, Val Loss: 4.314428668301384, Val MAE: 1.2720988988876343\n",
      "Epoch 1367/2000, Train Loss: 5.438927633714006, Val Loss: 4.314370505968193, Val MAE: 1.2723612785339355\n",
      "Epoch 1368/2000, Train Loss: 5.438586945600471, Val Loss: 4.3141793859031825, Val MAE: 1.2725096940994263\n",
      "Epoch 1369/2000, Train Loss: 5.43830121670052, Val Loss: 4.3140936561398675, Val MAE: 1.2726590633392334\n",
      "Epoch 1370/2000, Train Loss: 5.43806453056157, Val Loss: 4.313948594785488, Val MAE: 1.2727771997451782\n",
      "Epoch 1371/2000, Train Loss: 5.437820728804318, Val Loss: 4.313806010259165, Val MAE: 1.2728885412216187\n",
      "Epoch 1372/2000, Train Loss: 5.437523403273265, Val Loss: 4.313819695297663, Val MAE: 1.273105263710022\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1373/2000, Train Loss: 5.437235206226105, Val Loss: 4.3136605714624, Val MAE: 1.2732348442077637\n",
      "Epoch 1374/2000, Train Loss: 5.43694318438097, Val Loss: 4.31357419988862, Val MAE: 1.273419737815857\n",
      "Epoch 1375/2000, Train Loss: 5.43670672973307, Val Loss: 4.313420178372044, Val MAE: 1.2735495567321777\n",
      "Epoch 1376/2000, Train Loss: 5.436414435761582, Val Loss: 4.313277635783763, Val MAE: 1.2736668586730957\n",
      "Epoch 1377/2000, Train Loss: 5.436144137066352, Val Loss: 4.313165156878867, Val MAE: 1.2738206386566162\n",
      "Epoch 1378/2000, Train Loss: 5.435912500956501, Val Loss: 4.313037053851394, Val MAE: 1.273947834968567\n",
      "Epoch 1379/2000, Train Loss: 5.435577928183045, Val Loss: 4.31292011219639, Val MAE: 1.2741119861602783\n",
      "Epoch 1380/2000, Train Loss: 5.4352964390644605, Val Loss: 4.3129178379838535, Val MAE: 1.274381399154663\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1381/2000, Train Loss: 5.435002556085214, Val Loss: 4.312786310857481, Val MAE: 1.2745155096054077\n",
      "Epoch 1382/2000, Train Loss: 5.434674021623762, Val Loss: 4.312814112155287, Val MAE: 1.2747386693954468\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1383/2000, Train Loss: 5.434402352748163, Val Loss: 4.312661418385871, Val MAE: 1.2748981714248657\n",
      "Epoch 1384/2000, Train Loss: 5.43411589933446, Val Loss: 4.312395936344658, Val MAE: 1.2749650478363037\n",
      "Epoch 1385/2000, Train Loss: 5.433933647857999, Val Loss: 4.312147980525687, Val MAE: 1.2750099897384644\n",
      "Epoch 1386/2000, Train Loss: 5.433546428189449, Val Loss: 4.312137649550631, Val MAE: 1.2752331495285034\n",
      "Epoch 1387/2000, Train Loss: 5.433340069098331, Val Loss: 4.312098566489714, Val MAE: 1.275413990020752\n",
      "Epoch 1388/2000, Train Loss: 5.433095955811499, Val Loss: 4.311908049089414, Val MAE: 1.2755249738693237\n",
      "Epoch 1389/2000, Train Loss: 5.432807188510151, Val Loss: 4.3117474292044164, Val MAE: 1.2756447792053223\n",
      "Epoch 1390/2000, Train Loss: 5.432770715674819, Val Loss: 4.311486445219667, Val MAE: 1.2757102251052856\n",
      "Epoch 1391/2000, Train Loss: 5.432201560313541, Val Loss: 4.311623338012545, Val MAE: 1.276016354560852\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1392/2000, Train Loss: 5.432033703591262, Val Loss: 4.311578606485247, Val MAE: 1.2762117385864258\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1393/2000, Train Loss: 5.431699452645693, Val Loss: 4.3113896473705235, Val MAE: 1.276324987411499\n",
      "Epoch 1394/2000, Train Loss: 5.431470357788242, Val Loss: 4.311186914709774, Val MAE: 1.2764090299606323\n",
      "Epoch 1395/2000, Train Loss: 5.431194457546598, Val Loss: 4.31110908603883, Val MAE: 1.2765666246414185\n",
      "Epoch 1396/2000, Train Loss: 5.430904639585528, Val Loss: 4.311012000664397, Val MAE: 1.2767322063446045\n",
      "Epoch 1397/2000, Train Loss: 5.430675338099416, Val Loss: 4.310914675408118, Val MAE: 1.27688467502594\n",
      "Epoch 1398/2000, Train Loss: 5.430445418156998, Val Loss: 4.310820436585057, Val MAE: 1.2770321369171143\n",
      "Epoch 1399/2000, Train Loss: 5.430197324470127, Val Loss: 4.31063314190319, Val MAE: 1.2771309614181519\n",
      "Epoch 1400/2000, Train Loss: 5.429849330051083, Val Loss: 4.310619450300126, Val MAE: 1.2773716449737549\n",
      "Epoch 1401/2000, Train Loss: 5.429612583749566, Val Loss: 4.310404352387329, Val MAE: 1.2775503396987915\n",
      "Epoch 1402/2000, Train Loss: 5.429260620832816, Val Loss: 4.310401408535403, Val MAE: 1.2777950763702393\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1403/2000, Train Loss: 5.4289972439942975, Val Loss: 4.310220867220883, Val MAE: 1.2779102325439453\n",
      "Epoch 1404/2000, Train Loss: 5.428813625237499, Val Loss: 4.310215725649048, Val MAE: 1.278110146522522\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1405/2000, Train Loss: 5.4285008315921015, Val Loss: 4.310001384124562, Val MAE: 1.2781883478164673\n",
      "Epoch 1406/2000, Train Loss: 5.428161258072637, Val Loss: 4.310023340341207, Val MAE: 1.2784074544906616\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1407/2000, Train Loss: 5.427935206016029, Val Loss: 4.310189369347718, Val MAE: 1.278771162033081\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1408/2000, Train Loss: 5.427559074880366, Val Loss: 4.310340851558759, Val MAE: 1.279060959815979\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1409/2000, Train Loss: 5.427346408460144, Val Loss: 4.3102677961459035, Val MAE: 1.2792150974273682\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1410/2000, Train Loss: 5.427069231054154, Val Loss: 4.3100564630584675, Val MAE: 1.2792961597442627\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1411/2000, Train Loss: 5.426841374864444, Val Loss: 4.310142271985879, Val MAE: 1.2795467376708984\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1412/2000, Train Loss: 5.426600163886774, Val Loss: 4.309997349104902, Val MAE: 1.279649019241333\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1413/2000, Train Loss: 5.426359247538303, Val Loss: 4.3098167152152405, Val MAE: 1.279752254486084\n",
      "Epoch 1414/2000, Train Loss: 5.426177469504977, Val Loss: 4.309765518691625, Val MAE: 1.2799086570739746\n",
      "Epoch 1415/2000, Train Loss: 5.425940177369974, Val Loss: 4.309547637295616, Val MAE: 1.2799797058105469\n",
      "Epoch 1416/2000, Train Loss: 5.425620485393566, Val Loss: 4.309563537747473, Val MAE: 1.280184268951416\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1417/2000, Train Loss: 5.425430249498341, Val Loss: 4.309455345275703, Val MAE: 1.2803168296813965\n",
      "Epoch 1418/2000, Train Loss: 5.425168527792936, Val Loss: 4.309248126707636, Val MAE: 1.2804028987884521\n",
      "Epoch 1419/2000, Train Loss: 5.424923014622211, Val Loss: 4.309181268848814, Val MAE: 1.2805558443069458\n",
      "Epoch 1420/2000, Train Loss: 5.424659187250223, Val Loss: 4.309049062269765, Val MAE: 1.2806758880615234\n",
      "Epoch 1421/2000, Train Loss: 5.4244314542798655, Val Loss: 4.3089539040182085, Val MAE: 1.2808501720428467\n",
      "Epoch 1422/2000, Train Loss: 5.424174587365804, Val Loss: 4.308866481617227, Val MAE: 1.2809937000274658\n",
      "Epoch 1423/2000, Train Loss: 5.42393290643201, Val Loss: 4.308786442889287, Val MAE: 1.2811367511749268\n",
      "Epoch 1424/2000, Train Loss: 5.42370426859387, Val Loss: 4.308553999461032, Val MAE: 1.281209111213684\n",
      "Epoch 1425/2000, Train Loss: 5.423406897022646, Val Loss: 4.3083586481374665, Val MAE: 1.281300663948059\n",
      "Epoch 1426/2000, Train Loss: 5.42321987419902, Val Loss: 4.308184333830266, Val MAE: 1.2813827991485596\n",
      "Epoch 1427/2000, Train Loss: 5.422911349771176, Val Loss: 4.308247772422996, Val MAE: 1.28165864944458\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1428/2000, Train Loss: 5.422690139848059, Val Loss: 4.3080917783685635, Val MAE: 1.2817658185958862\n",
      "Epoch 1429/2000, Train Loss: 5.422378313708789, Val Loss: 4.308045687398932, Val MAE: 1.2819489240646362\n",
      "Epoch 1430/2000, Train Loss: 5.422195986540939, Val Loss: 4.308017860540936, Val MAE: 1.2821297645568848\n",
      "Epoch 1431/2000, Train Loss: 5.421895159946031, Val Loss: 4.307878823027955, Val MAE: 1.2822742462158203\n",
      "Epoch 1432/2000, Train Loss: 5.42165562171088, Val Loss: 4.307793728174927, Val MAE: 1.2824032306671143\n",
      "Epoch 1433/2000, Train Loss: 5.421395261648478, Val Loss: 4.30763373182969, Val MAE: 1.2825145721435547\n",
      "Epoch 1434/2000, Train Loss: 5.421120713355947, Val Loss: 4.307579090479795, Val MAE: 1.282698392868042\n",
      "Epoch 1435/2000, Train Loss: 5.420865225531568, Val Loss: 4.307546074661586, Val MAE: 1.2828993797302246\n",
      "Epoch 1436/2000, Train Loss: 5.4205969990508605, Val Loss: 4.307398316009087, Val MAE: 1.2830357551574707\n",
      "Epoch 1437/2000, Train Loss: 5.420325401978634, Val Loss: 4.30737549257171, Val MAE: 1.2832401990890503\n",
      "Epoch 1438/2000, Train Loss: 5.420122750463798, Val Loss: 4.307217131890692, Val MAE: 1.2833269834518433\n",
      "Epoch 1439/2000, Train Loss: 5.419866400464277, Val Loss: 4.307113790861121, Val MAE: 1.283469319343567\n",
      "Epoch 1440/2000, Train Loss: 5.419632895083955, Val Loss: 4.306915296077191, Val MAE: 1.2835683822631836\n",
      "Epoch 1441/2000, Train Loss: 5.419377569289364, Val Loss: 4.306860449467156, Val MAE: 1.2837402820587158\n",
      "Epoch 1442/2000, Train Loss: 5.419130563364014, Val Loss: 4.306695394634127, Val MAE: 1.2838610410690308\n",
      "Epoch 1443/2000, Train Loss: 5.418880788077616, Val Loss: 4.3065698565797765, Val MAE: 1.2839511632919312\n",
      "Epoch 1444/2000, Train Loss: 5.4186701318961035, Val Loss: 4.306544914049608, Val MAE: 1.2841559648513794\n",
      "Epoch 1445/2000, Train Loss: 5.418394984395567, Val Loss: 4.306389419989543, Val MAE: 1.284284234046936\n",
      "Epoch 1446/2000, Train Loss: 5.418155967724305, Val Loss: 4.306374407364978, Val MAE: 1.2845392227172852\n",
      "Epoch 1447/2000, Train Loss: 5.417707036489071, Val Loss: 4.306418551987893, Val MAE: 1.284807801246643\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1448/2000, Train Loss: 5.417487956432396, Val Loss: 4.306290047367414, Val MAE: 1.2849268913269043\n",
      "Epoch 1449/2000, Train Loss: 5.417242002439527, Val Loss: 4.306260649940452, Val MAE: 1.285114049911499\n",
      "Epoch 1450/2000, Train Loss: 5.416972831286804, Val Loss: 4.306195057284188, Val MAE: 1.2852803468704224\n",
      "Epoch 1451/2000, Train Loss: 5.416735402135507, Val Loss: 4.306043770788489, Val MAE: 1.2854129076004028\n",
      "Epoch 1452/2000, Train Loss: 5.416457882909433, Val Loss: 4.30603246518352, Val MAE: 1.285623550415039\n",
      "Epoch 1453/2000, Train Loss: 5.416107341437927, Val Loss: 4.306116455108733, Val MAE: 1.2859069108963013\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1454/2000, Train Loss: 5.415918493047705, Val Loss: 4.306036115310214, Val MAE: 1.2860419750213623\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1455/2000, Train Loss: 5.415788276891068, Val Loss: 4.306090827863495, Val MAE: 1.2862967252731323\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1456/2000, Train Loss: 5.415396061404818, Val Loss: 4.3059859415969335, Val MAE: 1.2864389419555664\n",
      "Epoch 1457/2000, Train Loss: 5.415188287981959, Val Loss: 4.3058831834041325, Val MAE: 1.2865692377090454\n",
      "Epoch 1458/2000, Train Loss: 5.414943579020627, Val Loss: 4.305942424903582, Val MAE: 1.2867679595947266\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1459/2000, Train Loss: 5.414707163427252, Val Loss: 4.305887712511393, Val MAE: 1.286953330039978\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1460/2000, Train Loss: 5.4144610733770175, Val Loss: 4.305883832772573, Val MAE: 1.2870984077453613\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1461/2000, Train Loss: 5.41429090406891, Val Loss: 4.305935079772193, Val MAE: 1.2873526811599731\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1462/2000, Train Loss: 5.413939718895137, Val Loss: 4.305728941256398, Val MAE: 1.2874133586883545\n",
      "Epoch 1463/2000, Train Loss: 5.413735688755553, Val Loss: 4.305643468504553, Val MAE: 1.2875807285308838\n",
      "Epoch 1464/2000, Train Loss: 5.413397549467042, Val Loss: 4.305472380298752, Val MAE: 1.2877895832061768\n",
      "Epoch 1465/2000, Train Loss: 5.413087180549753, Val Loss: 4.305514495745973, Val MAE: 1.2880195379257202\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1466/2000, Train Loss: 5.412908519578984, Val Loss: 4.305411071914273, Val MAE: 1.2881377935409546\n",
      "Epoch 1467/2000, Train Loss: 5.412704877659981, Val Loss: 4.3052918026307685, Val MAE: 1.2882623672485352\n",
      "Epoch 1468/2000, Train Loss: 5.41258249267959, Val Loss: 4.305216164309699, Val MAE: 1.2884029150009155\n",
      "Epoch 1469/2000, Train Loss: 5.4122175534318275, Val Loss: 4.3051020641987385, Val MAE: 1.2885444164276123\n",
      "Epoch 1470/2000, Train Loss: 5.411963414848316, Val Loss: 4.304960656434566, Val MAE: 1.2886457443237305\n",
      "Epoch 1471/2000, Train Loss: 5.411833214499463, Val Loss: 4.304757388875828, Val MAE: 1.2887345552444458\n",
      "Epoch 1472/2000, Train Loss: 5.411515864980946, Val Loss: 4.3046324331078445, Val MAE: 1.288856029510498\n",
      "Epoch 1473/2000, Train Loss: 5.411331898569131, Val Loss: 4.304424604088873, Val MAE: 1.288928508758545\n",
      "Epoch 1474/2000, Train Loss: 5.41102624497436, Val Loss: 4.304324437328824, Val MAE: 1.289080262184143\n",
      "Epoch 1475/2000, Train Loss: 5.410754814534775, Val Loss: 4.304327440275265, Val MAE: 1.2893292903900146\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1476/2000, Train Loss: 5.410539092818586, Val Loss: 4.3043343043944855, Val MAE: 1.2895281314849854\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1477/2000, Train Loss: 5.410297173605694, Val Loss: 4.304234074727372, Val MAE: 1.2896549701690674\n",
      "Epoch 1478/2000, Train Loss: 5.409961518185001, Val Loss: 4.3042190585990205, Val MAE: 1.2899367809295654\n",
      "Epoch 1479/2000, Train Loss: 5.4097393128131745, Val Loss: 4.304050264943827, Val MAE: 1.2900153398513794\n",
      "Epoch 1480/2000, Train Loss: 5.409485770090136, Val Loss: 4.303865154651371, Val MAE: 1.2901486158370972\n",
      "Epoch 1481/2000, Train Loss: 5.40922657822298, Val Loss: 4.303768326892509, Val MAE: 1.2902841567993164\n",
      "Epoch 1482/2000, Train Loss: 5.408946104243095, Val Loss: 4.3036619034167884, Val MAE: 1.2904659509658813\n",
      "Epoch 1483/2000, Train Loss: 5.408725797130425, Val Loss: 4.303472748613572, Val MAE: 1.290542483329773\n",
      "Epoch 1484/2000, Train Loss: 5.408508625119785, Val Loss: 4.303387810759716, Val MAE: 1.290704369544983\n",
      "Epoch 1485/2000, Train Loss: 5.408252179157715, Val Loss: 4.3034002231450765, Val MAE: 1.2909183502197266\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1486/2000, Train Loss: 5.407985752718683, Val Loss: 4.303282721716541, Val MAE: 1.291040301322937\n",
      "Epoch 1487/2000, Train Loss: 5.407700461270099, Val Loss: 4.3032958357989255, Val MAE: 1.2912895679473877\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1488/2000, Train Loss: 5.407436603317023, Val Loss: 4.303304733765555, Val MAE: 1.2915091514587402\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1489/2000, Train Loss: 5.407238152395358, Val Loss: 4.303079121069866, Val MAE: 1.2915617227554321\n",
      "Epoch 1490/2000, Train Loss: 5.406989643614676, Val Loss: 4.303026905159156, Val MAE: 1.291710376739502\n",
      "Epoch 1491/2000, Train Loss: 5.406745760781507, Val Loss: 4.3030314046117635, Val MAE: 1.2919172048568726\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1492/2000, Train Loss: 5.406521844789502, Val Loss: 4.302927232849169, Val MAE: 1.2920472621917725\n",
      "Epoch 1493/2000, Train Loss: 5.406277318873011, Val Loss: 4.302807525162761, Val MAE: 1.2921842336654663\n",
      "Epoch 1494/2000, Train Loss: 5.406040541281381, Val Loss: 4.302745816930457, Val MAE: 1.2923071384429932\n",
      "Epoch 1495/2000, Train Loss: 5.40586270891746, Val Loss: 4.30274144474182, Val MAE: 1.2925093173980713\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1496/2000, Train Loss: 5.405583037606872, Val Loss: 4.302525966030521, Val MAE: 1.292579174041748\n",
      "Epoch 1497/2000, Train Loss: 5.405376991550561, Val Loss: 4.302461383860927, Val MAE: 1.2927296161651611\n",
      "Epoch 1498/2000, Train Loss: 5.405169020576894, Val Loss: 4.302375137376356, Val MAE: 1.2928712368011475\n",
      "Epoch 1499/2000, Train Loss: 5.404876331085348, Val Loss: 4.302208935046518, Val MAE: 1.2929834127426147\n",
      "Epoch 1500/2000, Train Loss: 5.404644588935486, Val Loss: 4.302288520967101, Val MAE: 1.2932274341583252\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1501/2000, Train Loss: 5.404403892908007, Val Loss: 4.302175963555907, Val MAE: 1.293358564376831\n",
      "Epoch 1502/2000, Train Loss: 5.404161280663263, Val Loss: 4.302060213604489, Val MAE: 1.293481469154358\n",
      "Epoch 1503/2000, Train Loss: 5.403958270032767, Val Loss: 4.3019216713470385, Val MAE: 1.293601155281067\n",
      "Epoch 1504/2000, Train Loss: 5.403756555826541, Val Loss: 4.301705573284411, Val MAE: 1.2936985492706299\n",
      "Epoch 1505/2000, Train Loss: 5.403543727295168, Val Loss: 4.301526135246496, Val MAE: 1.2938239574432373\n",
      "Epoch 1506/2000, Train Loss: 5.403268209485666, Val Loss: 4.3015267357095945, Val MAE: 1.294007420539856\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1507/2000, Train Loss: 5.403049001641653, Val Loss: 4.301335839739254, Val MAE: 1.2940964698791504\n",
      "Epoch 1508/2000, Train Loss: 5.402748893835987, Val Loss: 4.301230258850364, Val MAE: 1.2942235469818115\n",
      "Epoch 1509/2000, Train Loss: 5.402551164493174, Val Loss: 4.301043043292321, Val MAE: 1.2943168878555298\n",
      "Epoch 1510/2000, Train Loss: 5.402248655206142, Val Loss: 4.301016448303923, Val MAE: 1.2945446968078613\n",
      "Epoch 1511/2000, Train Loss: 5.401987624056812, Val Loss: 4.300827264047421, Val MAE: 1.2946504354476929\n",
      "Epoch 1512/2000, Train Loss: 5.401733599800774, Val Loss: 4.300826293068963, Val MAE: 1.2948495149612427\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1513/2000, Train Loss: 5.4015997169356265, Val Loss: 4.300802332726684, Val MAE: 1.295019507408142\n",
      "Epoch 1514/2000, Train Loss: 5.401266179850991, Val Loss: 4.300637960876967, Val MAE: 1.2951176166534424\n",
      "Epoch 1515/2000, Train Loss: 5.401000944237906, Val Loss: 4.3005697441664905, Val MAE: 1.2953124046325684\n",
      "Epoch 1516/2000, Train Loss: 5.400763780947966, Val Loss: 4.300501524905363, Val MAE: 1.2954918146133423\n",
      "Epoch 1517/2000, Train Loss: 5.400517365861795, Val Loss: 4.300555717690034, Val MAE: 1.2957208156585693\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1518/2000, Train Loss: 5.400266447021697, Val Loss: 4.300471149103061, Val MAE: 1.2958898544311523\n",
      "Epoch 1519/2000, Train Loss: 5.400040946587014, Val Loss: 4.300394007026612, Val MAE: 1.296045184135437\n",
      "Epoch 1520/2000, Train Loss: 5.3998093886094605, Val Loss: 4.30027467413797, Val MAE: 1.2961753606796265\n",
      "Epoch 1521/2000, Train Loss: 5.399550105952157, Val Loss: 4.300165626299274, Val MAE: 1.2962960004806519\n",
      "Epoch 1522/2000, Train Loss: 5.399320154777741, Val Loss: 4.300085750638365, Val MAE: 1.296462059020996\n",
      "Epoch 1523/2000, Train Loss: 5.399075638671375, Val Loss: 4.2999800564738, Val MAE: 1.2965933084487915\n",
      "Epoch 1524/2000, Train Loss: 5.3988888981561765, Val Loss: 4.2997796306336244, Val MAE: 1.2966973781585693\n",
      "Epoch 1525/2000, Train Loss: 5.398568070065027, Val Loss: 4.299784933648131, Val MAE: 1.296889305114746\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1526/2000, Train Loss: 5.398284486217451, Val Loss: 4.299766428876031, Val MAE: 1.2971209287643433\n",
      "Epoch 1527/2000, Train Loss: 5.398026888158504, Val Loss: 4.299714560148952, Val MAE: 1.2972784042358398\n",
      "Epoch 1528/2000, Train Loss: 5.397754945918662, Val Loss: 4.299465691600297, Val MAE: 1.2973419427871704\n",
      "Epoch 1529/2000, Train Loss: 5.397526476200955, Val Loss: 4.299260247210125, Val MAE: 1.297538161277771\n",
      "Epoch 1530/2000, Train Loss: 5.39725988436717, Val Loss: 4.29932754215625, Val MAE: 1.2977733612060547\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1531/2000, Train Loss: 5.396959636215115, Val Loss: 4.299239276644883, Val MAE: 1.2979234457015991\n",
      "Epoch 1532/2000, Train Loss: 5.39676738708235, Val Loss: 4.299080144634118, Val MAE: 1.2980598211288452\n",
      "Epoch 1533/2000, Train Loss: 5.396463060899755, Val Loss: 4.299040221375925, Val MAE: 1.2982380390167236\n",
      "Epoch 1534/2000, Train Loss: 5.396276083895457, Val Loss: 4.299287913632286, Val MAE: 1.2985897064208984\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1535/2000, Train Loss: 5.396045642040449, Val Loss: 4.299328806848677, Val MAE: 1.2988109588623047\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1536/2000, Train Loss: 5.3957505923305025, Val Loss: 4.299189312447298, Val MAE: 1.298933982849121\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1537/2000, Train Loss: 5.39552947176786, Val Loss: 4.299074535769922, Val MAE: 1.2990460395812988\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1538/2000, Train Loss: 5.395347522963972, Val Loss: 4.299379852696045, Val MAE: 1.2993724346160889\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1539/2000, Train Loss: 5.395102846083143, Val Loss: 4.2992207609989626, Val MAE: 1.2994709014892578\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1540/2000, Train Loss: 5.394866384879661, Val Loss: 4.299068893008941, Val MAE: 1.2995561361312866\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1541/2000, Train Loss: 5.39471508721805, Val Loss: 4.298906856439672, Val MAE: 1.299646019935608\n",
      "Epoch 1542/2000, Train Loss: 5.39446403753367, Val Loss: 4.298862733376455, Val MAE: 1.2998145818710327\n",
      "Epoch 1543/2000, Train Loss: 5.394252797184794, Val Loss: 4.298901338848445, Val MAE: 1.300022006034851\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1544/2000, Train Loss: 5.393958274734188, Val Loss: 4.29888616982881, Val MAE: 1.3002091646194458\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1545/2000, Train Loss: 5.393736964094565, Val Loss: 4.298864565037929, Val MAE: 1.3003895282745361\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1546/2000, Train Loss: 5.393532873315856, Val Loss: 4.2986796601264325, Val MAE: 1.3004707098007202\n",
      "Epoch 1547/2000, Train Loss: 5.393267234174212, Val Loss: 4.298568086661734, Val MAE: 1.300595998764038\n",
      "Epoch 1548/2000, Train Loss: 5.3930709942268695, Val Loss: 4.298477791504817, Val MAE: 1.300722360610962\n",
      "Epoch 1549/2000, Train Loss: 5.392874998294759, Val Loss: 4.298329613552437, Val MAE: 1.3008111715316772\n",
      "Epoch 1550/2000, Train Loss: 5.3926375244806595, Val Loss: 4.298258868665308, Val MAE: 1.3009618520736694\n",
      "Epoch 1551/2000, Train Loss: 5.392373129283768, Val Loss: 4.2981909589053275, Val MAE: 1.301110029220581\n",
      "Epoch 1552/2000, Train Loss: 5.39222331947172, Val Loss: 4.298015207052231, Val MAE: 1.3012218475341797\n",
      "Epoch 1553/2000, Train Loss: 5.391943567070686, Val Loss: 4.298024078612929, Val MAE: 1.301439642906189\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1554/2000, Train Loss: 5.391708247189217, Val Loss: 4.2979167549459785, Val MAE: 1.3015400171279907\n",
      "Epoch 1555/2000, Train Loss: 5.391488349753869, Val Loss: 4.297842105726401, Val MAE: 1.3017044067382812\n",
      "Epoch 1556/2000, Train Loss: 5.3912707955319314, Val Loss: 4.2976684563063285, Val MAE: 1.3017784357070923\n",
      "Epoch 1557/2000, Train Loss: 5.391071835471762, Val Loss: 4.297651996054091, Val MAE: 1.3019683361053467\n",
      "Epoch 1558/2000, Train Loss: 5.390808358393295, Val Loss: 4.297585582853975, Val MAE: 1.3021429777145386\n",
      "Epoch 1559/2000, Train Loss: 5.39064004194346, Val Loss: 4.2974369495972855, Val MAE: 1.3022993803024292\n",
      "Epoch 1560/2000, Train Loss: 5.390286554621646, Val Loss: 4.297407501117066, Val MAE: 1.3024693727493286\n",
      "Epoch 1561/2000, Train Loss: 5.390046362199947, Val Loss: 4.297320134669274, Val MAE: 1.3026257753372192\n",
      "Epoch 1562/2000, Train Loss: 5.389868264265254, Val Loss: 4.297324214204474, Val MAE: 1.302794337272644\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1563/2000, Train Loss: 5.389652776213685, Val Loss: 4.297292289755366, Val MAE: 1.3029608726501465\n",
      "Epoch 1564/2000, Train Loss: 5.389403798185905, Val Loss: 4.29724547182386, Val MAE: 1.303108811378479\n",
      "Epoch 1565/2000, Train Loss: 5.389182367116538, Val Loss: 4.297091233663194, Val MAE: 1.3032126426696777\n",
      "Epoch 1566/2000, Train Loss: 5.389014558933455, Val Loss: 4.296981782003029, Val MAE: 1.3033560514450073\n",
      "Epoch 1567/2000, Train Loss: 5.388782609829478, Val Loss: 4.2969405738217334, Val MAE: 1.3034987449645996\n",
      "Epoch 1568/2000, Train Loss: 5.38848614692688, Val Loss: 4.296918944262706, Val MAE: 1.3036892414093018\n",
      "Epoch 1569/2000, Train Loss: 5.388294602075938, Val Loss: 4.29688560683448, Val MAE: 1.3038530349731445\n",
      "Epoch 1570/2000, Train Loss: 5.38811766822327, Val Loss: 4.296679988328938, Val MAE: 1.303925633430481\n",
      "Epoch 1571/2000, Train Loss: 5.387831085371711, Val Loss: 4.296752330043295, Val MAE: 1.3041331768035889\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1572/2000, Train Loss: 5.3876040715509195, Val Loss: 4.2967443409266775, Val MAE: 1.3043811321258545\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1573/2000, Train Loss: 5.387408301908401, Val Loss: 4.296698204112483, Val MAE: 1.3045482635498047\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1574/2000, Train Loss: 5.387095891369301, Val Loss: 4.296477498342325, Val MAE: 1.3046064376831055\n",
      "Epoch 1575/2000, Train Loss: 5.386926254202349, Val Loss: 4.296415129102565, Val MAE: 1.3047815561294556\n",
      "Epoch 1576/2000, Train Loss: 5.386669610980055, Val Loss: 4.296355527236655, Val MAE: 1.3049176931381226\n",
      "Epoch 1577/2000, Train Loss: 5.386507724776469, Val Loss: 4.296314782723114, Val MAE: 1.3050816059112549\n",
      "Epoch 1578/2000, Train Loss: 5.386225003348126, Val Loss: 4.296239227472662, Val MAE: 1.3052247762680054\n",
      "Epoch 1579/2000, Train Loss: 5.3860049539023365, Val Loss: 4.296137651492346, Val MAE: 1.3053569793701172\n",
      "Epoch 1580/2000, Train Loss: 5.385856254982316, Val Loss: 4.295993797011204, Val MAE: 1.3054717779159546\n",
      "Epoch 1581/2000, Train Loss: 5.385543465614319, Val Loss: 4.296089328315344, Val MAE: 1.3057478666305542\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1582/2000, Train Loss: 5.385329633346772, Val Loss: 4.296073279831861, Val MAE: 1.3059030771255493\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1583/2000, Train Loss: 5.385078200869925, Val Loss: 4.296038112648436, Val MAE: 1.3061034679412842\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1584/2000, Train Loss: 5.384930144978016, Val Loss: 4.296018621097277, Val MAE: 1.3062560558319092\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1585/2000, Train Loss: 5.384561618293131, Val Loss: 4.295660869965145, Val MAE: 1.3062843084335327\n",
      "Epoch 1586/2000, Train Loss: 5.384371487473177, Val Loss: 4.295457483089722, Val MAE: 1.3063654899597168\n",
      "Epoch 1587/2000, Train Loss: 5.384166081870402, Val Loss: 4.295433224012723, Val MAE: 1.3065216541290283\n",
      "Epoch 1588/2000, Train Loss: 5.38397574201575, Val Loss: 4.295243948510101, Val MAE: 1.3066147565841675\n",
      "Epoch 1589/2000, Train Loss: 5.383753800122489, Val Loss: 4.295190446481511, Val MAE: 1.3067567348480225\n",
      "Epoch 1590/2000, Train Loss: 5.383545576503236, Val Loss: 4.295177564186019, Val MAE: 1.3069301843643188\n",
      "Epoch 1591/2000, Train Loss: 5.383268790758344, Val Loss: 4.295139600349976, Val MAE: 1.307127594947815\n",
      "Epoch 1592/2000, Train Loss: 5.383031677472145, Val Loss: 4.295148823081373, Val MAE: 1.3073341846466064\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1593/2000, Train Loss: 5.382813346599826, Val Loss: 4.295032592507096, Val MAE: 1.3074394464492798\n",
      "Epoch 1594/2000, Train Loss: 5.38258364122483, Val Loss: 4.294978507373247, Val MAE: 1.3075965642929077\n",
      "Epoch 1595/2000, Train Loss: 5.382413336760169, Val Loss: 4.2949761214557, Val MAE: 1.3077718019485474\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1596/2000, Train Loss: 5.382248717797528, Val Loss: 4.294897476281669, Val MAE: 1.3079049587249756\n",
      "Epoch 1597/2000, Train Loss: 5.382136833649158, Val Loss: 4.295158388203866, Val MAE: 1.3082177639007568\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1598/2000, Train Loss: 5.381713534433644, Val Loss: 4.295002804360948, Val MAE: 1.3083436489105225\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1599/2000, Train Loss: 5.381487474426651, Val Loss: 4.294892187403129, Val MAE: 1.3084633350372314\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1600/2000, Train Loss: 5.381257956187922, Val Loss: 4.294825703885641, Val MAE: 1.3086127042770386\n",
      "Epoch 1601/2000, Train Loss: 5.381062969425911, Val Loss: 4.294692268940779, Val MAE: 1.308709740638733\n",
      "Epoch 1602/2000, Train Loss: 5.3808557819836595, Val Loss: 4.294590916477882, Val MAE: 1.3088574409484863\n",
      "Epoch 1603/2000, Train Loss: 5.3806033513847265, Val Loss: 4.294612965793223, Val MAE: 1.30906081199646\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1604/2000, Train Loss: 5.380416853007586, Val Loss: 4.294531272982692, Val MAE: 1.3091964721679688\n",
      "Epoch 1605/2000, Train Loss: 5.380203245954469, Val Loss: 4.294432470121899, Val MAE: 1.3093339204788208\n",
      "Epoch 1606/2000, Train Loss: 5.380009727526381, Val Loss: 4.294358779879303, Val MAE: 1.309476375579834\n",
      "Epoch 1607/2000, Train Loss: 5.379756795076796, Val Loss: 4.294328689602044, Val MAE: 1.3096492290496826\n",
      "Epoch 1608/2000, Train Loss: 5.379525762144331, Val Loss: 4.294144062013239, Val MAE: 1.3097156286239624\n",
      "Epoch 1609/2000, Train Loss: 5.379340898972034, Val Loss: 4.294096245529415, Val MAE: 1.3098809719085693\n",
      "Epoch 1610/2000, Train Loss: 5.37911002163582, Val Loss: 4.293964191116729, Val MAE: 1.3099746704101562\n",
      "Epoch 1611/2000, Train Loss: 5.378882175302729, Val Loss: 4.29403197521025, Val MAE: 1.3102011680603027\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1612/2000, Train Loss: 5.378648706036221, Val Loss: 4.293992983516272, Val MAE: 1.3104041814804077\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1613/2000, Train Loss: 5.378413625514823, Val Loss: 4.293922022871069, Val MAE: 1.3105559349060059\n",
      "Epoch 1614/2000, Train Loss: 5.378313380951721, Val Loss: 4.293767274392618, Val MAE: 1.3106608390808105\n",
      "Epoch 1615/2000, Train Loss: 5.377958956077206, Val Loss: 4.293762599186854, Val MAE: 1.3108493089675903\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1616/2000, Train Loss: 5.377809298763781, Val Loss: 4.2934688592547765, Val MAE: 1.3109022378921509\n",
      "Epoch 1617/2000, Train Loss: 5.377539888373032, Val Loss: 4.293214890978358, Val MAE: 1.3109546899795532\n",
      "Epoch 1618/2000, Train Loss: 5.377279677368735, Val Loss: 4.293319873632611, Val MAE: 1.3112207651138306\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1619/2000, Train Loss: 5.377003562269084, Val Loss: 4.2932494599271465, Val MAE: 1.311466097831726\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1620/2000, Train Loss: 5.376787437291673, Val Loss: 4.293152263814265, Val MAE: 1.3115849494934082\n",
      "Epoch 1621/2000, Train Loss: 5.376526261640599, Val Loss: 4.293126638354482, Val MAE: 1.3117660284042358\n",
      "Epoch 1622/2000, Train Loss: 5.376366673476192, Val Loss: 4.293006570811745, Val MAE: 1.3118664026260376\n",
      "Epoch 1623/2000, Train Loss: 5.3761359064515775, Val Loss: 4.292946517923931, Val MAE: 1.3120269775390625\n",
      "Epoch 1624/2000, Train Loss: 5.375970574883328, Val Loss: 4.292729341742155, Val MAE: 1.3121064901351929\n",
      "Epoch 1625/2000, Train Loss: 5.3757624482959745, Val Loss: 4.292514802528931, Val MAE: 1.3121705055236816\n",
      "Epoch 1626/2000, Train Loss: 5.375482158616255, Val Loss: 4.2924743430034535, Val MAE: 1.312333106994629\n",
      "Epoch 1627/2000, Train Loss: 5.375292798007334, Val Loss: 4.29238275200934, Val MAE: 1.312447190284729\n",
      "Epoch 1628/2000, Train Loss: 5.375113889877213, Val Loss: 4.2922836931170645, Val MAE: 1.3125368356704712\n",
      "Epoch 1629/2000, Train Loss: 5.37488892018888, Val Loss: 4.292233555408211, Val MAE: 1.3126881122589111\n",
      "Epoch 1630/2000, Train Loss: 5.374691551077571, Val Loss: 4.2922042623028025, Val MAE: 1.3128552436828613\n",
      "Epoch 1631/2000, Train Loss: 5.374466303916134, Val Loss: 4.292175189492939, Val MAE: 1.313043236732483\n",
      "Epoch 1632/2000, Train Loss: 5.374206416692451, Val Loss: 4.292208667703577, Val MAE: 1.3132578134536743\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1633/2000, Train Loss: 5.37410570939134, Val Loss: 4.292218693258526, Val MAE: 1.3134088516235352\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1634/2000, Train Loss: 5.373747733379489, Val Loss: 4.292227107044813, Val MAE: 1.3136407136917114\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1635/2000, Train Loss: 5.373525811431932, Val Loss: 4.291975886972101, Val MAE: 1.3137058019638062\n",
      "Epoch 1636/2000, Train Loss: 5.373308213750211, Val Loss: 4.291861758548934, Val MAE: 1.3137823343276978\n",
      "Epoch 1637/2000, Train Loss: 5.37313672086564, Val Loss: 4.291690581124108, Val MAE: 1.3138494491577148\n",
      "Epoch 1638/2000, Train Loss: 5.372958021305281, Val Loss: 4.2916415413489215, Val MAE: 1.3139688968658447\n",
      "Epoch 1639/2000, Train Loss: 5.372803947994378, Val Loss: 4.291494242028073, Val MAE: 1.3140928745269775\n",
      "Epoch 1640/2000, Train Loss: 5.3725256890105015, Val Loss: 4.291467000127913, Val MAE: 1.3142377138137817\n",
      "Epoch 1641/2000, Train Loss: 5.372347971206522, Val Loss: 4.291414250258927, Val MAE: 1.3144053220748901\n",
      "Epoch 1642/2000, Train Loss: 5.372109966977338, Val Loss: 4.291393652128744, Val MAE: 1.3145681619644165\n",
      "Epoch 1643/2000, Train Loss: 5.371923822322613, Val Loss: 4.291351863178047, Val MAE: 1.314720869064331\n",
      "Epoch 1644/2000, Train Loss: 5.3716847005877035, Val Loss: 4.291351577585882, Val MAE: 1.3149052858352661\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1645/2000, Train Loss: 5.371488389749423, Val Loss: 4.291258553422249, Val MAE: 1.3150502443313599\n",
      "Epoch 1646/2000, Train Loss: 5.371276773826194, Val Loss: 4.291247537544182, Val MAE: 1.3151934146881104\n",
      "Epoch 1647/2000, Train Loss: 5.371060905144107, Val Loss: 4.291159574378718, Val MAE: 1.3153092861175537\n",
      "Epoch 1648/2000, Train Loss: 5.370931584358959, Val Loss: 4.290979155334266, Val MAE: 1.3154067993164062\n",
      "Epoch 1649/2000, Train Loss: 5.3706564481098455, Val Loss: 4.290956824158763, Val MAE: 1.3155837059020996\n",
      "Epoch 1650/2000, Train Loss: 5.370437939336027, Val Loss: 4.290923762052983, Val MAE: 1.315726637840271\n",
      "Epoch 1651/2000, Train Loss: 5.370243209386579, Val Loss: 4.290867412466187, Val MAE: 1.3158838748931885\n",
      "Epoch 1652/2000, Train Loss: 5.370037038931144, Val Loss: 4.29091509180563, Val MAE: 1.3161317110061646\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1653/2000, Train Loss: 5.369805543417585, Val Loss: 4.290874375308956, Val MAE: 1.3162997961044312\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1654/2000, Train Loss: 5.369599504515459, Val Loss: 4.290775783239185, Val MAE: 1.3164019584655762\n",
      "Epoch 1655/2000, Train Loss: 5.369536576888491, Val Loss: 4.2903758403417225, Val MAE: 1.3164176940917969\n",
      "Epoch 1656/2000, Train Loss: 5.369232350155455, Val Loss: 4.290337932109833, Val MAE: 1.3165783882141113\n",
      "Epoch 1657/2000, Train Loss: 5.369020204648213, Val Loss: 4.290289588098053, Val MAE: 1.316692590713501\n",
      "Epoch 1658/2000, Train Loss: 5.3688986816104975, Val Loss: 4.29015071778684, Val MAE: 1.316801905632019\n",
      "Epoch 1659/2000, Train Loss: 5.36866523962404, Val Loss: 4.290158623215315, Val MAE: 1.3169512748718262\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1660/2000, Train Loss: 5.368440996280141, Val Loss: 4.290110411890992, Val MAE: 1.3170949220657349\n",
      "Epoch 1661/2000, Train Loss: 5.368303450704922, Val Loss: 4.289874359025611, Val MAE: 1.3171663284301758\n",
      "Epoch 1662/2000, Train Loss: 5.36800252926331, Val Loss: 4.28985266502913, Val MAE: 1.3173123598098755\n",
      "Epoch 1663/2000, Train Loss: 5.3678027351216855, Val Loss: 4.289877768438141, Val MAE: 1.3174842596054077\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1664/2000, Train Loss: 5.367605960685266, Val Loss: 4.289812189045253, Val MAE: 1.3176127672195435\n",
      "Epoch 1665/2000, Train Loss: 5.367414746846126, Val Loss: 4.289668229893521, Val MAE: 1.3177073001861572\n",
      "Epoch 1666/2000, Train Loss: 5.367218288356168, Val Loss: 4.289601596035399, Val MAE: 1.3178330659866333\n",
      "Epoch 1667/2000, Train Loss: 5.366996391701066, Val Loss: 4.289632823875358, Val MAE: 1.3180184364318848\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1668/2000, Train Loss: 5.366820986296941, Val Loss: 4.289558540217511, Val MAE: 1.3181613683700562\n",
      "Epoch 1669/2000, Train Loss: 5.366628977372754, Val Loss: 4.2895660196338685, Val MAE: 1.3183099031448364\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1670/2000, Train Loss: 5.366469044581218, Val Loss: 4.2894802695459076, Val MAE: 1.3184819221496582\n",
      "Epoch 1671/2000, Train Loss: 5.366221506770427, Val Loss: 4.2894655130736465, Val MAE: 1.3186534643173218\n",
      "Epoch 1672/2000, Train Loss: 5.365970207822862, Val Loss: 4.289547584695859, Val MAE: 1.3189561367034912\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1673/2000, Train Loss: 5.365749086865025, Val Loss: 4.289483659922539, Val MAE: 1.3190860748291016\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1674/2000, Train Loss: 5.365591165114211, Val Loss: 4.289504921275216, Val MAE: 1.3192561864852905\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1675/2000, Train Loss: 5.365375634381626, Val Loss: 4.289438138163842, Val MAE: 1.3194236755371094\n",
      "Epoch 1676/2000, Train Loss: 5.365203690510272, Val Loss: 4.289332276206832, Val MAE: 1.3195239305496216\n",
      "Epoch 1677/2000, Train Loss: 5.364989286651849, Val Loss: 4.289338661528922, Val MAE: 1.3197171688079834\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1678/2000, Train Loss: 5.3647244171168245, Val Loss: 4.28934579929253, Val MAE: 1.3199421167373657\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1679/2000, Train Loss: 5.3644986939783585, Val Loss: 4.2892827216032385, Val MAE: 1.3200905323028564\n",
      "Epoch 1680/2000, Train Loss: 5.364308892099794, Val Loss: 4.289204700465675, Val MAE: 1.3202139139175415\n",
      "Epoch 1681/2000, Train Loss: 5.364154353342636, Val Loss: 4.2892638876899944, Val MAE: 1.32047700881958\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1682/2000, Train Loss: 5.363914584964746, Val Loss: 4.289102565114563, Val MAE: 1.3205718994140625\n",
      "Epoch 1683/2000, Train Loss: 5.363726355983947, Val Loss: 4.289337302086589, Val MAE: 1.3208866119384766\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1684/2000, Train Loss: 5.3634698111999795, Val Loss: 4.289285280817264, Val MAE: 1.3210387229919434\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1685/2000, Train Loss: 5.363314659844694, Val Loss: 4.289250188260465, Val MAE: 1.3212029933929443\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1686/2000, Train Loss: 5.3630809066076175, Val Loss: 4.289163900173462, Val MAE: 1.321354866027832\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1687/2000, Train Loss: 5.362866636743411, Val Loss: 4.289128003786276, Val MAE: 1.3214757442474365\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1688/2000, Train Loss: 5.362696808976428, Val Loss: 4.289072571976765, Val MAE: 1.3216251134872437\n",
      "Epoch 1689/2000, Train Loss: 5.362571260300515, Val Loss: 4.289065035211073, Val MAE: 1.3217600584030151\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1690/2000, Train Loss: 5.362290981965207, Val Loss: 4.2887587622479275, Val MAE: 1.3217930793762207\n",
      "Epoch 1691/2000, Train Loss: 5.362066480187283, Val Loss: 4.288678802455868, Val MAE: 1.3219184875488281\n",
      "Epoch 1692/2000, Train Loss: 5.361932797023268, Val Loss: 4.288641594524856, Val MAE: 1.3221336603164673\n",
      "Epoch 1693/2000, Train Loss: 5.361700387901151, Val Loss: 4.2884536524076715, Val MAE: 1.322206974029541\n",
      "Epoch 1694/2000, Train Loss: 5.361467932390163, Val Loss: 4.288433571334358, Val MAE: 1.3223716020584106\n",
      "Epoch 1695/2000, Train Loss: 5.3612518994930936, Val Loss: 4.288422063019898, Val MAE: 1.3225550651550293\n",
      "Epoch 1696/2000, Train Loss: 5.36112656719785, Val Loss: 4.288494536871308, Val MAE: 1.3227382898330688\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1697/2000, Train Loss: 5.360890641413315, Val Loss: 4.2884312651715835, Val MAE: 1.3228975534439087\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1698/2000, Train Loss: 5.360679222622453, Val Loss: 4.288343142711365, Val MAE: 1.3230212926864624\n",
      "Epoch 1699/2000, Train Loss: 5.3605118639151135, Val Loss: 4.288336277840374, Val MAE: 1.3232253789901733\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1700/2000, Train Loss: 5.360298502859013, Val Loss: 4.28826334702002, Val MAE: 1.3233221769332886\n",
      "Epoch 1701/2000, Train Loss: 5.360084041604385, Val Loss: 4.288228371095013, Val MAE: 1.323476791381836\n",
      "Epoch 1702/2000, Train Loss: 5.359893049911106, Val Loss: 4.288165156825169, Val MAE: 1.3236325979232788\n",
      "Epoch 1703/2000, Train Loss: 5.359681036468601, Val Loss: 4.288124748124733, Val MAE: 1.3237751722335815\n",
      "Epoch 1704/2000, Train Loss: 5.359498187643875, Val Loss: 4.288052239826134, Val MAE: 1.323914885520935\n",
      "Epoch 1705/2000, Train Loss: 5.3592842783250045, Val Loss: 4.287990429648408, Val MAE: 1.3240702152252197\n",
      "Epoch 1706/2000, Train Loss: 5.35908412338233, Val Loss: 4.2879657189319795, Val MAE: 1.3242449760437012\n",
      "Epoch 1707/2000, Train Loss: 5.358912974380991, Val Loss: 4.287929957192223, Val MAE: 1.3243831396102905\n",
      "Epoch 1708/2000, Train Loss: 5.35869164530089, Val Loss: 4.287884045452685, Val MAE: 1.3245279788970947\n",
      "Epoch 1709/2000, Train Loss: 5.358490313375703, Val Loss: 4.2877991847626795, Val MAE: 1.3246450424194336\n",
      "Epoch 1710/2000, Train Loss: 5.358329770438571, Val Loss: 4.287819015845522, Val MAE: 1.3248330354690552\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1711/2000, Train Loss: 5.3581615632484185, Val Loss: 4.2877214120315, Val MAE: 1.3249499797821045\n",
      "Epoch 1712/2000, Train Loss: 5.357884988026016, Val Loss: 4.287617694955689, Val MAE: 1.3250895738601685\n",
      "Epoch 1713/2000, Train Loss: 5.357763341065734, Val Loss: 4.2874865800679265, Val MAE: 1.325181484222412\n",
      "Epoch 1714/2000, Train Loss: 5.357565183721355, Val Loss: 4.287498253691304, Val MAE: 1.3253403902053833\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1715/2000, Train Loss: 5.357373614021099, Val Loss: 4.287365471564971, Val MAE: 1.3254592418670654\n",
      "Epoch 1716/2000, Train Loss: 5.35721021386186, Val Loss: 4.287635024066444, Val MAE: 1.325753092765808\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1717/2000, Train Loss: 5.35697962006615, Val Loss: 4.287576345548973, Val MAE: 1.3258765935897827\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1718/2000, Train Loss: 5.35678873930632, Val Loss: 4.287466320905599, Val MAE: 1.3259754180908203\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1719/2000, Train Loss: 5.356591022191293, Val Loss: 4.287367294930123, Val MAE: 1.3261003494262695\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1720/2000, Train Loss: 5.356392924759578, Val Loss: 4.287353177677404, Val MAE: 1.32628333568573\n",
      "Epoch 1721/2000, Train Loss: 5.356175159329371, Val Loss: 4.287347775939349, Val MAE: 1.326472520828247\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1722/2000, Train Loss: 5.356016257251852, Val Loss: 4.287591022145641, Val MAE: 1.3267704248428345\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1723/2000, Train Loss: 5.355773328432985, Val Loss: 4.287638403220219, Val MAE: 1.3269646167755127\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1724/2000, Train Loss: 5.35561632877579, Val Loss: 4.287698195565929, Val MAE: 1.3271989822387695\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1725/2000, Train Loss: 5.355402243909151, Val Loss: 4.287658678626155, Val MAE: 1.3273452520370483\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1726/2000, Train Loss: 5.3552066130496785, Val Loss: 4.287594744670498, Val MAE: 1.3274770975112915\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1727/2000, Train Loss: 5.355052948369995, Val Loss: 4.28748799618837, Val MAE: 1.3275808095932007\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1728/2000, Train Loss: 5.354904198190909, Val Loss: 4.287555835698102, Val MAE: 1.3277904987335205\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1729/2000, Train Loss: 5.354646895493435, Val Loss: 4.287483304685301, Val MAE: 1.3279457092285156\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1730/2000, Train Loss: 5.354473831129148, Val Loss: 4.2874098624463555, Val MAE: 1.3280612230300903\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1731/2000, Train Loss: 5.354276424637079, Val Loss: 4.287471969600197, Val MAE: 1.328270435333252\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1732/2000, Train Loss: 5.35409446941709, Val Loss: 4.287386623239732, Val MAE: 1.3283957242965698\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1733/2000, Train Loss: 5.353884847227385, Val Loss: 4.287318709385287, Val MAE: 1.328493356704712\n",
      "Epoch 1734/2000, Train Loss: 5.353692014578166, Val Loss: 4.287396659292616, Val MAE: 1.3287684917449951\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1735/2000, Train Loss: 5.3534694856117, Val Loss: 4.287385465513479, Val MAE: 1.3289583921432495\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1736/2000, Train Loss: 5.353197682247891, Val Loss: 4.287403702655354, Val MAE: 1.3292171955108643\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1737/2000, Train Loss: 5.353026961434092, Val Loss: 4.287403340919598, Val MAE: 1.3293769359588623\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1738/2000, Train Loss: 5.352822238309149, Val Loss: 4.287375595521283, Val MAE: 1.3295143842697144\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1739/2000, Train Loss: 5.352651741872897, Val Loss: 4.287359054883321, Val MAE: 1.3296912908554077\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1740/2000, Train Loss: 5.352466122036605, Val Loss: 4.287257786860337, Val MAE: 1.329795002937317\n",
      "Epoch 1741/2000, Train Loss: 5.3522563241015355, Val Loss: 4.2872384387093625, Val MAE: 1.3299319744110107\n",
      "Epoch 1742/2000, Train Loss: 5.352038150353941, Val Loss: 4.287345514990188, Val MAE: 1.3301936388015747\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1743/2000, Train Loss: 5.3519024549984895, Val Loss: 4.287363406747311, Val MAE: 1.330376148223877\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1744/2000, Train Loss: 5.351670617618352, Val Loss: 4.287230712175369, Val MAE: 1.3304643630981445\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1745/2000, Train Loss: 5.351506464566902, Val Loss: 4.2871411603313305, Val MAE: 1.3305647373199463\n",
      "Epoch 1746/2000, Train Loss: 5.3513072790482115, Val Loss: 4.287088413684217, Val MAE: 1.3306933641433716\n",
      "Epoch 1747/2000, Train Loss: 5.351137547522737, Val Loss: 4.286916930611069, Val MAE: 1.3307832479476929\n",
      "Epoch 1748/2000, Train Loss: 5.350929473008083, Val Loss: 4.28684575450313, Val MAE: 1.3309121131896973\n",
      "Epoch 1749/2000, Train Loss: 5.350748977776437, Val Loss: 4.2867775429476485, Val MAE: 1.331036925315857\n",
      "Epoch 1750/2000, Train Loss: 5.350552657093161, Val Loss: 4.286728483814377, Val MAE: 1.3311889171600342\n",
      "Epoch 1751/2000, Train Loss: 5.350365232565846, Val Loss: 4.2869206423963515, Val MAE: 1.331440806388855\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1752/2000, Train Loss: 5.350312435310828, Val Loss: 4.287007781460479, Val MAE: 1.3316993713378906\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1753/2000, Train Loss: 5.349928782808242, Val Loss: 4.286998511622618, Val MAE: 1.3318732976913452\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1754/2000, Train Loss: 5.349771544267532, Val Loss: 4.286923427920084, Val MAE: 1.331979751586914\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1755/2000, Train Loss: 5.349546463738737, Val Loss: 4.286896689518078, Val MAE: 1.332161784172058\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1756/2000, Train Loss: 5.349339484703522, Val Loss: 4.286948154772724, Val MAE: 1.3323545455932617\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1757/2000, Train Loss: 5.349169674790036, Val Loss: 4.286891977878304, Val MAE: 1.3324775695800781\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1758/2000, Train Loss: 5.34897145800955, Val Loss: 4.286816903572899, Val MAE: 1.332604169845581\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1759/2000, Train Loss: 5.348936854770143, Val Loss: 4.286652313051997, Val MAE: 1.332686424255371\n",
      "Epoch 1760/2000, Train Loss: 5.34865283519727, Val Loss: 4.286642741083025, Val MAE: 1.332830548286438\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1761/2000, Train Loss: 5.348474178195883, Val Loss: 4.286521411854942, Val MAE: 1.3329622745513916\n",
      "Epoch 1762/2000, Train Loss: 5.348236665245896, Val Loss: 4.286531844708296, Val MAE: 1.333127737045288\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1763/2000, Train Loss: 5.348056521226017, Val Loss: 4.286535267786936, Val MAE: 1.3332864046096802\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1764/2000, Train Loss: 5.347893450077908, Val Loss: 4.286423731226105, Val MAE: 1.3334194421768188\n",
      "Epoch 1765/2000, Train Loss: 5.347703459156842, Val Loss: 4.286521584923203, Val MAE: 1.3336461782455444\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1766/2000, Train Loss: 5.347476097053373, Val Loss: 4.286384431628494, Val MAE: 1.3337199687957764\n",
      "Epoch 1767/2000, Train Loss: 5.347323936716815, Val Loss: 4.286285019914309, Val MAE: 1.3338277339935303\n",
      "Epoch 1768/2000, Train Loss: 5.347138875732184, Val Loss: 4.286269432386836, Val MAE: 1.3339778184890747\n",
      "Epoch 1769/2000, Train Loss: 5.347006782912613, Val Loss: 4.286281982386434, Val MAE: 1.334165334701538\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1770/2000, Train Loss: 5.346827630505733, Val Loss: 4.286053329733041, Val MAE: 1.3342093229293823\n",
      "Epoch 1771/2000, Train Loss: 5.346637899157781, Val Loss: 4.286293258323326, Val MAE: 1.3345072269439697\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1772/2000, Train Loss: 5.346439100743828, Val Loss: 4.286180838754585, Val MAE: 1.334619402885437\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1773/2000, Train Loss: 5.346286447111418, Val Loss: 4.2861797071791985, Val MAE: 1.3347686529159546\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1774/2000, Train Loss: 5.346066440695347, Val Loss: 4.286117388321473, Val MAE: 1.334918737411499\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1775/2000, Train Loss: 5.345966818459134, Val Loss: 4.286384962566264, Val MAE: 1.335237979888916\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1776/2000, Train Loss: 5.345632611868348, Val Loss: 4.286164691587826, Val MAE: 1.3353196382522583\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1777/2000, Train Loss: 5.345441609760528, Val Loss: 4.286230849199467, Val MAE: 1.3355127573013306\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1778/2000, Train Loss: 5.345235781241412, Val Loss: 4.28613934130282, Val MAE: 1.3356270790100098\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1779/2000, Train Loss: 5.3450558330637055, Val Loss: 4.286081854180173, Val MAE: 1.3357737064361572\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1780/2000, Train Loss: 5.344888292125161, Val Loss: 4.2862610758424875, Val MAE: 1.336019515991211\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1781/2000, Train Loss: 5.344707331214792, Val Loss: 4.286259556246233, Val MAE: 1.3362369537353516\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1782/2000, Train Loss: 5.344484397662039, Val Loss: 4.286075587047113, Val MAE: 1.3362928628921509\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1783/2000, Train Loss: 5.344278487697965, Val Loss: 4.286283297232679, Val MAE: 1.3366162776947021\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1784/2000, Train Loss: 5.344131056120541, Val Loss: 4.286194549406971, Val MAE: 1.336708664894104\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1785/2000, Train Loss: 5.3439595325502705, Val Loss: 4.286136256655057, Val MAE: 1.3368914127349854\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1786/2000, Train Loss: 5.343774108579397, Val Loss: 4.28609266026063, Val MAE: 1.3370130062103271\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 1787/2000, Train Loss: 5.343570146099454, Val Loss: 4.286085623341638, Val MAE: 1.3371771574020386\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 1788/2000, Train Loss: 5.343412283569714, Val Loss: 4.286029242798015, Val MAE: 1.3373196125030518\n",
      "Epoch 1789/2000, Train Loss: 5.343244745932205, Val Loss: 4.285999287329279, Val MAE: 1.3374520540237427\n",
      "Epoch 1790/2000, Train Loss: 5.343037058522288, Val Loss: 4.285932046416644, Val MAE: 1.3375710248947144\n",
      "Epoch 1791/2000, Train Loss: 5.342925537395031, Val Loss: 4.2859262956692294, Val MAE: 1.3377280235290527\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1792/2000, Train Loss: 5.342690662735152, Val Loss: 4.2861163351986855, Val MAE: 1.3379924297332764\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1793/2000, Train Loss: 5.342436827521614, Val Loss: 4.286005258184296, Val MAE: 1.338186502456665\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1794/2000, Train Loss: 5.342204473505898, Val Loss: 4.285908900698026, Val MAE: 1.3383164405822754\n",
      "Epoch 1795/2000, Train Loss: 5.342071408205397, Val Loss: 4.2858732072888195, Val MAE: 1.338436484336853\n",
      "Epoch 1796/2000, Train Loss: 5.341905535682571, Val Loss: 4.285841329274951, Val MAE: 1.3385913372039795\n",
      "Epoch 1797/2000, Train Loss: 5.341739110753243, Val Loss: 4.28573318488963, Val MAE: 1.33867347240448\n",
      "Epoch 1798/2000, Train Loss: 5.341576704182974, Val Loss: 4.285715035409541, Val MAE: 1.338808298110962\n",
      "Epoch 1799/2000, Train Loss: 5.341411257310888, Val Loss: 4.285700115480939, Val MAE: 1.338930368423462\n",
      "Epoch 1800/2000, Train Loss: 5.341199329974313, Val Loss: 4.285919744017962, Val MAE: 1.3391923904418945\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1801/2000, Train Loss: 5.341024805900645, Val Loss: 4.285838006477098, Val MAE: 1.3392857313156128\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1802/2000, Train Loss: 5.3408729862683275, Val Loss: 4.285725738363223, Val MAE: 1.3393893241882324\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1803/2000, Train Loss: 5.340719299178786, Val Loss: 4.285750249592033, Val MAE: 1.339582085609436\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1804/2000, Train Loss: 5.340572215465599, Val Loss: 4.285659005298271, Val MAE: 1.3396719694137573\n",
      "Epoch 1805/2000, Train Loss: 5.340350688898619, Val Loss: 4.285609182285833, Val MAE: 1.339816927909851\n",
      "Epoch 1806/2000, Train Loss: 5.340193569939148, Val Loss: 4.285569576127035, Val MAE: 1.3399912118911743\n",
      "Epoch 1807/2000, Train Loss: 5.3399680632162205, Val Loss: 4.285385303373809, Val MAE: 1.3400218486785889\n",
      "Epoch 1808/2000, Train Loss: 5.339860039212495, Val Loss: 4.285335077036608, Val MAE: 1.3401672840118408\n",
      "Epoch 1809/2000, Train Loss: 5.339648874613126, Val Loss: 4.285322621035147, Val MAE: 1.3403164148330688\n",
      "Epoch 1810/2000, Train Loss: 5.339463863283536, Val Loss: 4.285361351059364, Val MAE: 1.340524673461914\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1811/2000, Train Loss: 5.33933887934071, Val Loss: 4.285378960580439, Val MAE: 1.3406578302383423\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1812/2000, Train Loss: 5.339195651122821, Val Loss: 4.2852827529917965, Val MAE: 1.340753197669983\n",
      "Epoch 1813/2000, Train Loss: 5.339005074151407, Val Loss: 4.285241840336774, Val MAE: 1.340889573097229\n",
      "Epoch 1814/2000, Train Loss: 5.338832628336414, Val Loss: 4.285136640957884, Val MAE: 1.3409777879714966\n",
      "Epoch 1815/2000, Train Loss: 5.33871776562957, Val Loss: 4.284951677569398, Val MAE: 1.3410558700561523\n",
      "Epoch 1816/2000, Train Loss: 5.338560575848901, Val Loss: 4.2848131017105, Val MAE: 1.3411504030227661\n",
      "Epoch 1817/2000, Train Loss: 5.338329736230526, Val Loss: 4.284870127514676, Val MAE: 1.3413279056549072\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1818/2000, Train Loss: 5.338146376721387, Val Loss: 4.284751598389299, Val MAE: 1.3414157629013062\n",
      "Epoch 1819/2000, Train Loss: 5.337986836195364, Val Loss: 4.284766663329021, Val MAE: 1.341599941253662\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1820/2000, Train Loss: 5.337810809452336, Val Loss: 4.284611480864319, Val MAE: 1.341687798500061\n",
      "Epoch 1821/2000, Train Loss: 5.337628738975004, Val Loss: 4.284639657483445, Val MAE: 1.341861367225647\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1822/2000, Train Loss: 5.337557139151182, Val Loss: 4.284703983433611, Val MAE: 1.3420374393463135\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1823/2000, Train Loss: 5.337252626181021, Val Loss: 4.28452489504943, Val MAE: 1.342079997062683\n",
      "Epoch 1824/2000, Train Loss: 5.337134880506304, Val Loss: 4.284399140444962, Val MAE: 1.342195987701416\n",
      "Epoch 1825/2000, Train Loss: 5.336908301771524, Val Loss: 4.284287218118573, Val MAE: 1.342325210571289\n",
      "Epoch 1826/2000, Train Loss: 5.336732676918161, Val Loss: 4.2842530522529065, Val MAE: 1.3425155878067017\n",
      "Epoch 1827/2000, Train Loss: 5.3365417337640775, Val Loss: 4.284305841965718, Val MAE: 1.3426963090896606\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1828/2000, Train Loss: 5.336363318557858, Val Loss: 4.284490417118545, Val MAE: 1.3429757356643677\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1829/2000, Train Loss: 5.336168572980789, Val Loss: 4.284503380242769, Val MAE: 1.3431345224380493\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1830/2000, Train Loss: 5.3360238171962795, Val Loss: 4.284429547190666, Val MAE: 1.3432334661483765\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1831/2000, Train Loss: 5.3358876441551635, Val Loss: 4.284580532521815, Val MAE: 1.3434330224990845\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1832/2000, Train Loss: 5.335762663524999, Val Loss: 4.284534055021432, Val MAE: 1.3435381650924683\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1833/2000, Train Loss: 5.335596448592202, Val Loss: 4.2845262248236855, Val MAE: 1.3436825275421143\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1834/2000, Train Loss: 5.335409536376572, Val Loss: 4.284448497359817, Val MAE: 1.3437740802764893\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1835/2000, Train Loss: 5.3352057967282684, Val Loss: 4.284259384628888, Val MAE: 1.3438304662704468\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1836/2000, Train Loss: 5.3350836654163185, Val Loss: 4.28419058360495, Val MAE: 1.3440266847610474\n",
      "Epoch 1837/2000, Train Loss: 5.334866754908272, Val Loss: 4.284138000790064, Val MAE: 1.3442177772521973\n",
      "Epoch 1838/2000, Train Loss: 5.334704175196274, Val Loss: 4.284078135817975, Val MAE: 1.3443166017532349\n",
      "Epoch 1839/2000, Train Loss: 5.334549884342366, Val Loss: 4.283993752341012, Val MAE: 1.3444019556045532\n",
      "Epoch 1840/2000, Train Loss: 5.3344301076696, Val Loss: 4.283941422899564, Val MAE: 1.3445228338241577\n",
      "Epoch 1841/2000, Train Loss: 5.33425669662666, Val Loss: 4.283909239854898, Val MAE: 1.3446518182754517\n",
      "Epoch 1842/2000, Train Loss: 5.334084186836636, Val Loss: 4.283847897439389, Val MAE: 1.3447232246398926\n",
      "Epoch 1843/2000, Train Loss: 5.333930222738924, Val Loss: 4.283834355720528, Val MAE: 1.3448771238327026\n",
      "Epoch 1844/2000, Train Loss: 5.333794080522987, Val Loss: 4.283715849712088, Val MAE: 1.3449335098266602\n",
      "Epoch 1845/2000, Train Loss: 5.333591360011822, Val Loss: 4.2838111350665224, Val MAE: 1.3451472520828247\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1846/2000, Train Loss: 5.333556160307525, Val Loss: 4.283583831089037, Val MAE: 1.3451827764511108\n",
      "Epoch 1847/2000, Train Loss: 5.333298982212584, Val Loss: 4.283521579648998, Val MAE: 1.3453153371810913\n",
      "Epoch 1848/2000, Train Loss: 5.3331511834761285, Val Loss: 4.283427236396987, Val MAE: 1.3454079627990723\n",
      "Epoch 1849/2000, Train Loss: 5.332994606944496, Val Loss: 4.283378208166845, Val MAE: 1.3455458879470825\n",
      "Epoch 1850/2000, Train Loss: 5.332819202202902, Val Loss: 4.283326988875329, Val MAE: 1.3456445932388306\n",
      "Epoch 1851/2000, Train Loss: 5.332725910275198, Val Loss: 4.2833642315757166, Val MAE: 1.3457832336425781\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1852/2000, Train Loss: 5.332505171086971, Val Loss: 4.283342579893164, Val MAE: 1.345920443534851\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1853/2000, Train Loss: 5.332326993183487, Val Loss: 4.283341460748836, Val MAE: 1.3460873365402222\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1854/2000, Train Loss: 5.332106163274851, Val Loss: 4.283433667335425, Val MAE: 1.3463126420974731\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1855/2000, Train Loss: 5.331999148667881, Val Loss: 4.283131063521445, Val MAE: 1.3463144302368164\n",
      "Epoch 1856/2000, Train Loss: 5.331796575064499, Val Loss: 4.283169725510451, Val MAE: 1.3464716672897339\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1857/2000, Train Loss: 5.331631990751126, Val Loss: 4.28315788926305, Val MAE: 1.3466026782989502\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1858/2000, Train Loss: 5.331493146333977, Val Loss: 4.282889165722572, Val MAE: 1.3465955257415771\n",
      "Epoch 1859/2000, Train Loss: 5.331362144274756, Val Loss: 4.282763117014825, Val MAE: 1.3466426134109497\n",
      "Epoch 1860/2000, Train Loss: 5.331242777070091, Val Loss: 4.2828016703193255, Val MAE: 1.3468139171600342\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1861/2000, Train Loss: 5.3310618983602005, Val Loss: 4.282902992657713, Val MAE: 1.3470449447631836\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1862/2000, Train Loss: 5.330900871437537, Val Loss: 4.282871045293034, Val MAE: 1.3471713066101074\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1863/2000, Train Loss: 5.330880958092938, Val Loss: 4.283069880024807, Val MAE: 1.3473891019821167\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1864/2000, Train Loss: 5.330568816472141, Val Loss: 4.282997906825564, Val MAE: 1.3475223779678345\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1865/2000, Train Loss: 5.33038312856939, Val Loss: 4.282925245998142, Val MAE: 1.3477320671081543\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1866/2000, Train Loss: 5.330192927358303, Val Loss: 4.282862806857169, Val MAE: 1.3478264808654785\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1867/2000, Train Loss: 5.330100253108139, Val Loss: 4.282839967941379, Val MAE: 1.3479347229003906\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1868/2000, Train Loss: 5.329938272162458, Val Loss: 4.28272748557297, Val MAE: 1.3480141162872314\n",
      "Epoch 1869/2000, Train Loss: 5.329805650502768, Val Loss: 4.282729305018176, Val MAE: 1.348145842552185\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1870/2000, Train Loss: 5.329658525387136, Val Loss: 4.282791537314922, Val MAE: 1.3483272790908813\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1871/2000, Train Loss: 5.329474351800362, Val Loss: 4.282688449014414, Val MAE: 1.3484172821044922\n",
      "Epoch 1872/2000, Train Loss: 5.329329968614623, Val Loss: 4.28271395773501, Val MAE: 1.348535418510437\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1873/2000, Train Loss: 5.3292007922382325, Val Loss: 4.2825677451518205, Val MAE: 1.348597526550293\n",
      "Epoch 1874/2000, Train Loss: 5.328999902454441, Val Loss: 4.282510293738262, Val MAE: 1.348698377609253\n",
      "Epoch 1875/2000, Train Loss: 5.328867463351412, Val Loss: 4.282321917446884, Val MAE: 1.3487560749053955\n",
      "Epoch 1876/2000, Train Loss: 5.328716772022932, Val Loss: 4.282342344280836, Val MAE: 1.3489071130752563\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1877/2000, Train Loss: 5.328704435209774, Val Loss: 4.282654582756059, Val MAE: 1.3491930961608887\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1878/2000, Train Loss: 5.328390281345236, Val Loss: 4.282545470036902, Val MAE: 1.349281907081604\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1879/2000, Train Loss: 5.328277170727852, Val Loss: 4.282458339510737, Val MAE: 1.3493664264678955\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1880/2000, Train Loss: 5.328111734851474, Val Loss: 4.282440946955939, Val MAE: 1.3494997024536133\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1881/2000, Train Loss: 5.327922674497464, Val Loss: 4.282307527543188, Val MAE: 1.3495514392852783\n",
      "Epoch 1882/2000, Train Loss: 5.327864339961649, Val Loss: 4.282205547808527, Val MAE: 1.3495935201644897\n",
      "Epoch 1883/2000, Train Loss: 5.327755841212972, Val Loss: 4.282268543098424, Val MAE: 1.3497556447982788\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1884/2000, Train Loss: 5.327591394671412, Val Loss: 4.282090007641294, Val MAE: 1.3498075008392334\n",
      "Epoch 1885/2000, Train Loss: 5.327427027749941, Val Loss: 4.282178057099248, Val MAE: 1.3499947786331177\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1886/2000, Train Loss: 5.327284427037152, Val Loss: 4.282062277063593, Val MAE: 1.3500804901123047\n",
      "Epoch 1887/2000, Train Loss: 5.327092016728917, Val Loss: 4.282083953071285, Val MAE: 1.3502254486083984\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1888/2000, Train Loss: 5.326975234771109, Val Loss: 4.28211929929686, Val MAE: 1.3503719568252563\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1889/2000, Train Loss: 5.3267424523179505, Val Loss: 4.282107732666505, Val MAE: 1.3506383895874023\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1890/2000, Train Loss: 5.326645325665913, Val Loss: 4.282144277321326, Val MAE: 1.3507903814315796\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1891/2000, Train Loss: 5.326444427792256, Val Loss: 4.282124789367925, Val MAE: 1.3509312868118286\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1892/2000, Train Loss: 5.32628572166152, Val Loss: 4.28219604250547, Val MAE: 1.3510922193527222\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1893/2000, Train Loss: 5.326166785823387, Val Loss: 4.282199345918389, Val MAE: 1.3512307405471802\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1894/2000, Train Loss: 5.325996933252131, Val Loss: 4.282115166922948, Val MAE: 1.351324200630188\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1895/2000, Train Loss: 5.325880844395916, Val Loss: 4.282079259581394, Val MAE: 1.3514348268508911\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1896/2000, Train Loss: 5.325668876517024, Val Loss: 4.2821411473525535, Val MAE: 1.3516203165054321\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1897/2000, Train Loss: 5.325478773407185, Val Loss: 4.28185467360256, Val MAE: 1.3516173362731934\n",
      "Epoch 1898/2000, Train Loss: 5.325414182987302, Val Loss: 4.281787629304706, Val MAE: 1.3517107963562012\n",
      "Epoch 1899/2000, Train Loss: 5.325237792641993, Val Loss: 4.281792038867065, Val MAE: 1.351874828338623\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1900/2000, Train Loss: 5.3251214878235515, Val Loss: 4.281731804534122, Val MAE: 1.35196852684021\n",
      "Epoch 1901/2000, Train Loss: 5.324942316540318, Val Loss: 4.281718922252054, Val MAE: 1.3520902395248413\n",
      "Epoch 1902/2000, Train Loss: 5.324773763229434, Val Loss: 4.281984697940113, Val MAE: 1.3524365425109863\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 1903/2000, Train Loss: 5.324676135772848, Val Loss: 4.28202793042402, Val MAE: 1.3526060581207275\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 1904/2000, Train Loss: 5.324427654934747, Val Loss: 4.282208379539283, Val MAE: 1.3528090715408325\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 1905/2000, Train Loss: 5.32434835490682, Val Loss: 4.282260004464571, Val MAE: 1.352946162223816\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 1906/2000, Train Loss: 5.324243974016163, Val Loss: 4.282240505691047, Val MAE: 1.3530290126800537\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 1907/2000, Train Loss: 5.324208528872771, Val Loss: 4.282599905589679, Val MAE: 1.3533440828323364\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 1908/2000, Train Loss: 5.323978023112471, Val Loss: 4.2825185012709985, Val MAE: 1.353489637374878\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 1909/2000, Train Loss: 5.323801029490978, Val Loss: 4.2824391485603, Val MAE: 1.3535616397857666\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 1910/2000, Train Loss: 5.323684170168015, Val Loss: 4.282356519983695, Val MAE: 1.3536314964294434\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 1911/2000, Train Loss: 5.323575979421226, Val Loss: 4.28224503008632, Val MAE: 1.3536930084228516\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 1912/2000, Train Loss: 5.323440460258639, Val Loss: 4.282204695220466, Val MAE: 1.3537715673446655\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 1913/2000, Train Loss: 5.323313971205546, Val Loss: 4.282171283541499, Val MAE: 1.353868842124939\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 1914/2000, Train Loss: 5.323180059003012, Val Loss: 4.282207769101804, Val MAE: 1.3540047407150269\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 1915/2000, Train Loss: 5.323065229585501, Val Loss: 4.2820951927621085, Val MAE: 1.354082465171814\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 1916/2000, Train Loss: 5.322907550751512, Val Loss: 4.28222698032319, Val MAE: 1.354245901107788\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 1917/2000, Train Loss: 5.32278812917272, Val Loss: 4.282217261222032, Val MAE: 1.3543537855148315\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 1918/2000, Train Loss: 5.322698468351141, Val Loss: 4.282068169922442, Val MAE: 1.3543742895126343\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 1919/2000, Train Loss: 5.322568935090778, Val Loss: 4.282034090456662, Val MAE: 1.3544965982437134\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 1920/2000, Train Loss: 5.3224215745553956, Val Loss: 4.282176918757928, Val MAE: 1.35474693775177\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 1921/2000, Train Loss: 5.32230065653737, Val Loss: 4.282062612809576, Val MAE: 1.3548691272735596\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.537674903869629\n",
      "Test Mean Absolute Error (MAE): 1.6349790034668188\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(CNNModel(\n",
       "   (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (conv1d): Conv1d(10, 64, kernel_size=(3,), stride=(1,))\n",
       "   (convolutional_stack): Sequential(\n",
       "     (0): Conv1d(10, 10, kernel_size=(3,), stride=(1,))\n",
       "     (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "     (2): Conv1d(10, 64, kernel_size=(2,), stride=(1,))\n",
       "     (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "   )\n",
       "   (linear_relu_stack): Sequential(\n",
       "     (0): Linear(in_features=65, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'test_mae': 1.6349790034668188})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 229\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'clean_data')\n",
    "\n",
    "full_cnn_pipeline(DATA_DIR,\n",
    "                season = ['2020-21', '2021-22'], \n",
    "                position = 'GK', \n",
    "                window_size=10,\n",
    "                kernel_size=3,\n",
    "                num_filters=64,\n",
    "                num_dense=64,\n",
    "                batch_size = 32,\n",
    "                epochs = 2000,  \n",
    "                drop_low_playtime = True,\n",
    "                low_playtime_cutoff = 1e-6,\n",
    "                num_features = NUM_FEATURES_DICT['GK']['large'],\n",
    "                cat_features = STANDARD_CAT_FEATURES, \n",
    "                stratify_by = 'stdev', \n",
    "                conv_activation = 'relu',\n",
    "                dense_activation = 'relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate= 0.000001,  \n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'],\n",
    "                verbose = True,\n",
    "                regularization = 0.01, \n",
    "                early_stopping = True, \n",
    "                tolerance = 1e-5, # only used if early stopping is turned on, threshold to define low val loss decrease\n",
    "                patience = 20,   # num of iterations before early stopping bc of low val loss decrease\n",
    "                plot = True, \n",
    "                draw_model = False,\n",
    "                standardize= True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Total Number of Iterations:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 400, 'position': 'DEF', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'medium', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 400\n",
      "position DEF\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features medium\n",
      "stratify_by stdev\n",
      "Running Iteration:  0\n",
      "Epoch 1/750, Train Loss: 8.91152523793992, Val Loss: 11.432309972973178, Val MAE: 1.8613654375076294\n",
      "Epoch 2/750, Train Loss: 8.541535758288802, Val Loss: 10.969701104685711, Val MAE: 1.8621766567230225\n",
      "Epoch 3/750, Train Loss: 8.19554848435578, Val Loss: 10.53698668604389, Val MAE: 1.8609570264816284\n",
      "Epoch 4/750, Train Loss: 7.882908401367771, Val Loss: 10.133005077902615, Val MAE: 1.8563870191574097\n",
      "Epoch 5/750, Train Loss: 7.608280039820701, Val Loss: 9.765890065448636, Val MAE: 1.848335862159729\n",
      "Epoch 6/750, Train Loss: 7.376295425633716, Val Loss: 9.4469399834509, Val MAE: 1.8375818729400635\n",
      "Epoch 7/750, Train Loss: 7.175224409589342, Val Loss: 9.158494024709414, Val MAE: 1.83229660987854\n",
      "Epoch 8/750, Train Loss: 7.002733511226192, Val Loss: 8.901430940835501, Val MAE: 1.8416390419006348\n",
      "Epoch 9/750, Train Loss: 6.853969977312027, Val Loss: 8.677755211213125, Val MAE: 1.8526579141616821\n",
      "Epoch 10/750, Train Loss: 6.7232168191557475, Val Loss: 8.47883256138149, Val MAE: 1.8614037036895752\n",
      "Epoch 11/750, Train Loss: 6.605537015465415, Val Loss: 8.301502891621137, Val MAE: 1.8660770654678345\n",
      "Epoch 12/750, Train Loss: 6.50204676427659, Val Loss: 8.149989825735336, Val MAE: 1.8682645559310913\n",
      "Epoch 13/750, Train Loss: 6.410067939454583, Val Loss: 8.015713207465362, Val MAE: 1.8690320253372192\n",
      "Epoch 14/750, Train Loss: 6.3283618125186605, Val Loss: 7.899224341767704, Val MAE: 1.868958592414856\n",
      "Epoch 15/750, Train Loss: 6.25604417794829, Val Loss: 7.798286353823115, Val MAE: 1.8695087432861328\n",
      "Epoch 16/750, Train Loss: 6.192609813410765, Val Loss: 7.712162835765433, Val MAE: 1.8698142766952515\n",
      "Epoch 17/750, Train Loss: 6.137405284954484, Val Loss: 7.64061405319394, Val MAE: 1.8682215213775635\n",
      "Epoch 18/750, Train Loss: 6.0895825911479395, Val Loss: 7.579365376436318, Val MAE: 1.8688653707504272\n",
      "Epoch 19/750, Train Loss: 6.049467907134135, Val Loss: 7.529797848890552, Val MAE: 1.868148684501648\n",
      "Epoch 20/750, Train Loss: 6.014704209224433, Val Loss: 7.4887939492244024, Val MAE: 1.867133617401123\n",
      "Epoch 21/750, Train Loss: 5.9844507144514925, Val Loss: 7.4549498018676115, Val MAE: 1.865912914276123\n",
      "Epoch 22/750, Train Loss: 5.958151539723585, Val Loss: 7.425070016705671, Val MAE: 1.8658082485198975\n",
      "Epoch 23/750, Train Loss: 5.934305523914896, Val Loss: 7.3995833565834666, Val MAE: 1.8647173643112183\n",
      "Epoch 24/750, Train Loss: 5.913769672175122, Val Loss: 7.3789779649453555, Val MAE: 1.863680124282837\n",
      "Epoch 25/750, Train Loss: 5.895650468814146, Val Loss: 7.361620826555232, Val MAE: 1.8635563850402832\n",
      "Epoch 26/750, Train Loss: 5.879000875145007, Val Loss: 7.3440029806346905, Val MAE: 1.863736867904663\n",
      "Epoch 27/750, Train Loss: 5.864325269771989, Val Loss: 7.329965403244464, Val MAE: 1.8638802766799927\n",
      "Epoch 28/750, Train Loss: 5.850645390893244, Val Loss: 7.318999147178076, Val MAE: 1.8600728511810303\n",
      "Epoch 29/750, Train Loss: 5.838008383428974, Val Loss: 7.308311467440399, Val MAE: 1.8592311143875122\n",
      "Epoch 30/750, Train Loss: 5.826777196993493, Val Loss: 7.298830335521046, Val MAE: 1.8584742546081543\n",
      "Epoch 31/750, Train Loss: 5.816768853375867, Val Loss: 7.28932349825587, Val MAE: 1.859816551208496\n",
      "Epoch 32/750, Train Loss: 5.806675934487847, Val Loss: 7.2818424196403315, Val MAE: 1.8576054573059082\n",
      "Epoch 33/750, Train Loss: 5.79778663679293, Val Loss: 7.274777812791804, Val MAE: 1.8585591316223145\n",
      "Epoch 34/750, Train Loss: 5.789517712137502, Val Loss: 7.268286074963766, Val MAE: 1.8567676544189453\n",
      "Epoch 35/750, Train Loss: 5.781995439833137, Val Loss: 7.2613543337957696, Val MAE: 1.8581552505493164\n",
      "Epoch 36/750, Train Loss: 5.774678834988054, Val Loss: 7.256819724740265, Val MAE: 1.8563628196716309\n",
      "Epoch 37/750, Train Loss: 5.767974045018482, Val Loss: 7.25101495144751, Val MAE: 1.858508586883545\n",
      "Epoch 38/750, Train Loss: 5.761603904529742, Val Loss: 7.2470285410907715, Val MAE: 1.8561503887176514\n",
      "Epoch 39/750, Train Loss: 5.755960734786501, Val Loss: 7.2435171570075525, Val MAE: 1.8530972003936768\n",
      "Epoch 40/750, Train Loss: 5.75050017788152, Val Loss: 7.238315595611421, Val MAE: 1.8550759553909302\n",
      "Epoch 41/750, Train Loss: 5.745819965289656, Val Loss: 7.234063150276673, Val MAE: 1.8563264608383179\n",
      "Epoch 42/750, Train Loss: 5.740618322457477, Val Loss: 7.230979176172194, Val MAE: 1.8559223413467407\n",
      "Epoch 43/750, Train Loss: 5.736167102862316, Val Loss: 7.2294209010857395, Val MAE: 1.8525490760803223\n",
      "Epoch 44/750, Train Loss: 5.732471255891642, Val Loss: 7.2254363917059745, Val MAE: 1.8527274131774902\n",
      "Epoch 45/750, Train Loss: 5.728718798327598, Val Loss: 7.222946804750773, Val MAE: 1.853798747062683\n",
      "Epoch 46/750, Train Loss: 5.724913159145671, Val Loss: 7.2183698392788616, Val MAE: 1.8560489416122437\n",
      "Epoch 47/750, Train Loss: 5.721233860550413, Val Loss: 7.215911588141309, Val MAE: 1.8556662797927856\n",
      "Epoch 48/750, Train Loss: 5.7180778582384635, Val Loss: 7.2137093577346425, Val MAE: 1.8522241115570068\n",
      "Epoch 49/750, Train Loss: 5.7151832893395875, Val Loss: 7.211506130163384, Val MAE: 1.8526932001113892\n",
      "Epoch 50/750, Train Loss: 5.712114448911825, Val Loss: 7.209352403638376, Val MAE: 1.851928949356079\n",
      "Epoch 51/750, Train Loss: 5.709413420926234, Val Loss: 7.20649533911919, Val MAE: 1.8508656024932861\n",
      "Epoch 52/750, Train Loss: 5.706729486489752, Val Loss: 7.204403155112281, Val MAE: 1.8497532606124878\n",
      "Epoch 53/750, Train Loss: 5.704018307643332, Val Loss: 7.202192116998752, Val MAE: 1.8519524335861206\n",
      "Epoch 54/750, Train Loss: 5.701337800663747, Val Loss: 7.200631751860055, Val MAE: 1.850724220275879\n",
      "Epoch 55/750, Train Loss: 5.6993447473854015, Val Loss: 7.199070559813415, Val MAE: 1.8494617938995361\n",
      "Epoch 56/750, Train Loss: 5.6974029182628465, Val Loss: 7.196151062447955, Val MAE: 1.8533686399459839\n",
      "Epoch 57/750, Train Loss: 5.694721709694832, Val Loss: 7.194057802890234, Val MAE: 1.8518089056015015\n",
      "Epoch 58/750, Train Loss: 5.692672335266308, Val Loss: 7.192701004024172, Val MAE: 1.8503241539001465\n",
      "Epoch 59/750, Train Loss: 5.690462218728035, Val Loss: 7.191615827414025, Val MAE: 1.8500455617904663\n",
      "Epoch 60/750, Train Loss: 5.688497774768027, Val Loss: 7.190624594021614, Val MAE: 1.8488445281982422\n",
      "Epoch 61/750, Train Loss: 5.686557247987978, Val Loss: 7.188628780167617, Val MAE: 1.8494048118591309\n",
      "Epoch 62/750, Train Loss: 5.6851563508343546, Val Loss: 7.1867135534974045, Val MAE: 1.8499923944473267\n",
      "Epoch 63/750, Train Loss: 5.683226935574963, Val Loss: 7.184985860006049, Val MAE: 1.853323221206665\n",
      "Epoch 64/750, Train Loss: 5.681245337929695, Val Loss: 7.183178486625477, Val MAE: 1.8522902727127075\n",
      "Epoch 65/750, Train Loss: 5.679490771263269, Val Loss: 7.18194782044889, Val MAE: 1.8498263359069824\n",
      "Epoch 66/750, Train Loss: 5.67776257521028, Val Loss: 7.179889174334524, Val MAE: 1.8508210182189941\n",
      "Epoch 67/750, Train Loss: 5.675780970655429, Val Loss: 7.178921342933304, Val MAE: 1.849170446395874\n",
      "Epoch 68/750, Train Loss: 5.674032874016246, Val Loss: 7.176898214667387, Val MAE: 1.8482664823532104\n",
      "Epoch 69/750, Train Loss: 5.672218149330965, Val Loss: 7.175332458068155, Val MAE: 1.8481544256210327\n",
      "Epoch 70/750, Train Loss: 5.670778370511, Val Loss: 7.1736898480771565, Val MAE: 1.8493125438690186\n",
      "Epoch 71/750, Train Loss: 5.66934579253956, Val Loss: 7.171422057080669, Val MAE: 1.8491365909576416\n",
      "Epoch 72/750, Train Loss: 5.667693860971244, Val Loss: 7.170267305883878, Val MAE: 1.848962664604187\n",
      "Epoch 73/750, Train Loss: 5.6663834905928105, Val Loss: 7.16832755884819, Val MAE: 1.85115385055542\n",
      "Epoch 74/750, Train Loss: 5.665185455152184, Val Loss: 7.1673443315190655, Val MAE: 1.8489952087402344\n",
      "Epoch 75/750, Train Loss: 5.66342711813131, Val Loss: 7.16582958486378, Val MAE: 1.8478015661239624\n",
      "Epoch 76/750, Train Loss: 5.662233405811771, Val Loss: 7.164958777155026, Val MAE: 1.845747709274292\n",
      "Epoch 77/750, Train Loss: 5.660801095871409, Val Loss: 7.162340436684115, Val MAE: 1.851045846939087\n",
      "Epoch 78/750, Train Loss: 5.6596142768859865, Val Loss: 7.162304220398144, Val MAE: 1.8466368913650513\n",
      "Epoch 79/750, Train Loss: 5.658131155390648, Val Loss: 7.160808198417915, Val MAE: 1.848030686378479\n",
      "Epoch 80/750, Train Loss: 5.656993702566548, Val Loss: 7.159490029362879, Val MAE: 1.8467899560928345\n",
      "Epoch 81/750, Train Loss: 5.655671442238389, Val Loss: 7.158270186930432, Val MAE: 1.844589352607727\n",
      "Epoch 82/750, Train Loss: 5.654231284074722, Val Loss: 7.156411540737946, Val MAE: 1.8482623100280762\n",
      "Epoch 83/750, Train Loss: 5.653418763276118, Val Loss: 7.15583915285178, Val MAE: 1.8470520973205566\n",
      "Epoch 84/750, Train Loss: 5.652156660359377, Val Loss: 7.153978555746297, Val MAE: 1.8479033708572388\n",
      "Epoch 85/750, Train Loss: 5.651450313883982, Val Loss: 7.153104082777902, Val MAE: 1.8473800420761108\n",
      "Epoch 86/750, Train Loss: 5.650063456395629, Val Loss: 7.151649975421045, Val MAE: 1.8474829196929932\n",
      "Epoch 87/750, Train Loss: 5.648667626472036, Val Loss: 7.150038387628725, Val MAE: 1.8468788862228394\n",
      "Epoch 88/750, Train Loss: 5.64759822438477, Val Loss: 7.148930116719233, Val MAE: 1.847558856010437\n",
      "Epoch 89/750, Train Loss: 5.646565748779637, Val Loss: 7.147853793825253, Val MAE: 1.8457560539245605\n",
      "Epoch 90/750, Train Loss: 5.645229529727037, Val Loss: 7.1469028012362825, Val MAE: 1.8448889255523682\n",
      "Epoch 91/750, Train Loss: 5.644392199425181, Val Loss: 7.145522114799362, Val MAE: 1.8459367752075195\n",
      "Epoch 92/750, Train Loss: 5.643400017319212, Val Loss: 7.14434272291647, Val MAE: 1.8455392122268677\n",
      "Epoch 93/750, Train Loss: 5.642255841394898, Val Loss: 7.143548469946501, Val MAE: 1.8434484004974365\n",
      "Epoch 94/750, Train Loss: 5.641263692697902, Val Loss: 7.141842370747483, Val MAE: 1.8468319177627563\n",
      "Epoch 95/750, Train Loss: 5.640402292142248, Val Loss: 7.140670130458806, Val MAE: 1.844578504562378\n",
      "Epoch 96/750, Train Loss: 5.639231884099876, Val Loss: 7.138940844497301, Val MAE: 1.8468866348266602\n",
      "Epoch 97/750, Train Loss: 5.638313219197996, Val Loss: 7.138923095130565, Val MAE: 1.8441166877746582\n",
      "Epoch 98/750, Train Loss: 5.637417750753415, Val Loss: 7.137611413757723, Val MAE: 1.8457813262939453\n",
      "Epoch 99/750, Train Loss: 5.636048178460188, Val Loss: 7.136137238935775, Val MAE: 1.8496804237365723\n",
      "Epoch 100/750, Train Loss: 5.635442388133638, Val Loss: 7.135074699495504, Val MAE: 1.8468159437179565\n",
      "Epoch 101/750, Train Loss: 5.634313539031205, Val Loss: 7.134029911200106, Val MAE: 1.8441791534423828\n",
      "Epoch 102/750, Train Loss: 5.633430408064727, Val Loss: 7.133266116092485, Val MAE: 1.843257188796997\n",
      "Epoch 103/750, Train Loss: 5.63277317399432, Val Loss: 7.132112539207217, Val MAE: 1.8455638885498047\n",
      "Epoch 104/750, Train Loss: 5.63136839593292, Val Loss: 7.130847401764172, Val MAE: 1.845475196838379\n",
      "Epoch 105/750, Train Loss: 5.630728587982761, Val Loss: 7.12946484080007, Val MAE: 1.8469972610473633\n",
      "Epoch 106/750, Train Loss: 5.629782255743719, Val Loss: 7.128784149310987, Val MAE: 1.8446910381317139\n",
      "Epoch 107/750, Train Loss: 5.628508366445067, Val Loss: 7.127936130580582, Val MAE: 1.84480881690979\n",
      "Epoch 108/750, Train Loss: 5.628093775973958, Val Loss: 7.126743517209462, Val MAE: 1.8472496271133423\n",
      "Epoch 109/750, Train Loss: 5.626836276206241, Val Loss: 7.125465889018541, Val MAE: 1.8458470106124878\n",
      "Epoch 110/750, Train Loss: 5.625939884610996, Val Loss: 7.124181514130089, Val MAE: 1.8456560373306274\n",
      "Epoch 111/750, Train Loss: 5.624787894631647, Val Loss: 7.123840338402909, Val MAE: 1.843479037284851\n",
      "Epoch 112/750, Train Loss: 5.623967755979793, Val Loss: 7.1235926432814045, Val MAE: 1.8414783477783203\n",
      "Epoch 113/750, Train Loss: 5.623379429738233, Val Loss: 7.121979189333966, Val MAE: 1.8414820432662964\n",
      "Epoch 114/750, Train Loss: 5.622331644167566, Val Loss: 7.12169012762583, Val MAE: 1.8433300256729126\n",
      "Epoch 115/750, Train Loss: 5.621764874306454, Val Loss: 7.1204130522133475, Val MAE: 1.8443135023117065\n",
      "Epoch 116/750, Train Loss: 5.620599049367723, Val Loss: 7.119276058903535, Val MAE: 1.8436577320098877\n",
      "Epoch 117/750, Train Loss: 5.619542467366358, Val Loss: 7.118556290772333, Val MAE: 1.8444961309432983\n",
      "Epoch 118/750, Train Loss: 5.619002142985155, Val Loss: 7.118268486059105, Val MAE: 1.8443057537078857\n",
      "Epoch 119/750, Train Loss: 5.617740320703786, Val Loss: 7.116449689628027, Val MAE: 1.8415632247924805\n",
      "Epoch 120/750, Train Loss: 5.617076676058921, Val Loss: 7.116071433588316, Val MAE: 1.841346025466919\n",
      "Epoch 121/750, Train Loss: 5.615997295926331, Val Loss: 7.115669527210902, Val MAE: 1.8393090963363647\n",
      "Epoch 122/750, Train Loss: 5.615500453171457, Val Loss: 7.114859212919227, Val MAE: 1.8401581048965454\n",
      "Epoch 123/750, Train Loss: 5.614997324366478, Val Loss: 7.113195058497825, Val MAE: 1.8422892093658447\n",
      "Epoch 124/750, Train Loss: 5.61412111695405, Val Loss: 7.112413603597135, Val MAE: 1.841304898262024\n",
      "Epoch 125/750, Train Loss: 5.612972807124922, Val Loss: 7.111839986573546, Val MAE: 1.8400343656539917\n",
      "Epoch 126/750, Train Loss: 5.612184879278681, Val Loss: 7.110723176604106, Val MAE: 1.8414381742477417\n",
      "Epoch 127/750, Train Loss: 5.611458277246754, Val Loss: 7.110128562890498, Val MAE: 1.8411749601364136\n",
      "Epoch 128/750, Train Loss: 5.6105811204120615, Val Loss: 7.108812834328935, Val MAE: 1.8417327404022217\n",
      "Epoch 129/750, Train Loss: 5.609831726778844, Val Loss: 7.108245384922822, Val MAE: 1.840705394744873\n",
      "Epoch 130/750, Train Loss: 5.60852134182195, Val Loss: 7.107356897194687, Val MAE: 1.8392632007598877\n",
      "Epoch 131/750, Train Loss: 5.607847922015342, Val Loss: 7.106350314698655, Val MAE: 1.840501308441162\n",
      "Epoch 132/750, Train Loss: 5.6073402076769785, Val Loss: 7.105170144584329, Val MAE: 1.8394076824188232\n",
      "Epoch 133/750, Train Loss: 5.606076341373905, Val Loss: 7.104565878993165, Val MAE: 1.8404786586761475\n",
      "Epoch 134/750, Train Loss: 5.605512180753574, Val Loss: 7.103666919147546, Val MAE: 1.8403798341751099\n",
      "Epoch 135/750, Train Loss: 5.6047584855632415, Val Loss: 7.102862651337742, Val MAE: 1.8405193090438843\n",
      "Epoch 136/750, Train Loss: 5.6035268932391125, Val Loss: 7.102171738300359, Val MAE: 1.8396395444869995\n",
      "Epoch 137/750, Train Loss: 5.602736464883112, Val Loss: 7.101520058455269, Val MAE: 1.8400640487670898\n",
      "Epoch 138/750, Train Loss: 5.602089488582247, Val Loss: 7.10103908512739, Val MAE: 1.8399460315704346\n",
      "Epoch 139/750, Train Loss: 5.601592442640074, Val Loss: 7.099211211972092, Val MAE: 1.8403888940811157\n",
      "Epoch 140/750, Train Loss: 5.6003350124237645, Val Loss: 7.098698769540354, Val MAE: 1.8406561613082886\n",
      "Epoch 141/750, Train Loss: 5.5996548414230345, Val Loss: 7.0975463265880725, Val MAE: 1.8420556783676147\n",
      "Epoch 142/750, Train Loss: 5.599070415982775, Val Loss: 7.096989946383138, Val MAE: 1.839532732963562\n",
      "Epoch 143/750, Train Loss: 5.598089308343875, Val Loss: 7.095978983075371, Val MAE: 1.8424419164657593\n",
      "Epoch 144/750, Train Loss: 5.59770193920014, Val Loss: 7.095660073628257, Val MAE: 1.839295744895935\n",
      "Epoch 145/750, Train Loss: 5.596298535462398, Val Loss: 7.09448340740761, Val MAE: 1.8406325578689575\n",
      "Epoch 146/750, Train Loss: 5.595613058813059, Val Loss: 7.093830264600039, Val MAE: 1.8373318910598755\n",
      "Epoch 147/750, Train Loss: 5.595086929296992, Val Loss: 7.093202895448399, Val MAE: 1.8372979164123535\n",
      "Epoch 148/750, Train Loss: 5.594017474666523, Val Loss: 7.092310253769215, Val MAE: 1.8383979797363281\n",
      "Epoch 149/750, Train Loss: 5.59343597235953, Val Loss: 7.091293438981971, Val MAE: 1.8378655910491943\n",
      "Epoch 150/750, Train Loss: 5.592471428889378, Val Loss: 7.090590946566177, Val MAE: 1.8381191492080688\n",
      "Epoch 151/750, Train Loss: 5.592314477786896, Val Loss: 7.09009050456095, Val MAE: 1.8401468992233276\n",
      "Epoch 152/750, Train Loss: 5.591505147241483, Val Loss: 7.089635852258942, Val MAE: 1.8365063667297363\n",
      "Epoch 153/750, Train Loss: 5.590616612525503, Val Loss: 7.088095546287361, Val MAE: 1.8374439477920532\n",
      "Epoch 154/750, Train Loss: 5.5894448073806275, Val Loss: 7.087909262694507, Val MAE: 1.8362315893173218\n",
      "Epoch 155/750, Train Loss: 5.588808941385548, Val Loss: 7.086732633444298, Val MAE: 1.8386459350585938\n",
      "Epoch 156/750, Train Loss: 5.588010302622607, Val Loss: 7.085926095842204, Val MAE: 1.8394346237182617\n",
      "Epoch 157/750, Train Loss: 5.587007421141218, Val Loss: 7.085258407945437, Val MAE: 1.8378132581710815\n",
      "Epoch 158/750, Train Loss: 5.586545008155191, Val Loss: 7.08468439328589, Val MAE: 1.8388564586639404\n",
      "Epoch 159/750, Train Loss: 5.585759699876141, Val Loss: 7.08355255012856, Val MAE: 1.839287519454956\n",
      "Epoch 160/750, Train Loss: 5.584807677481584, Val Loss: 7.082481650442274, Val MAE: 1.837009072303772\n",
      "Epoch 161/750, Train Loss: 5.584129186192895, Val Loss: 7.0823129229178114, Val MAE: 1.8352007865905762\n",
      "Epoch 162/750, Train Loss: 5.583357699813357, Val Loss: 7.0814809981453415, Val MAE: 1.8393672704696655\n",
      "Epoch 163/750, Train Loss: 5.5824730791104065, Val Loss: 7.080593781637212, Val MAE: 1.8389251232147217\n",
      "Epoch 164/750, Train Loss: 5.582019095815671, Val Loss: 7.080263579832685, Val MAE: 1.837736964225769\n",
      "Epoch 165/750, Train Loss: 5.581272236708623, Val Loss: 7.079003320338935, Val MAE: 1.8370237350463867\n",
      "Epoch 166/750, Train Loss: 5.580599828282739, Val Loss: 7.078852762969997, Val MAE: 1.8347073793411255\n",
      "Epoch 167/750, Train Loss: 5.579804004985056, Val Loss: 7.077960548154903, Val MAE: 1.834446907043457\n",
      "Epoch 168/750, Train Loss: 5.579082955220702, Val Loss: 7.077166546309787, Val MAE: 1.8364228010177612\n",
      "Epoch 169/750, Train Loss: 5.578184150282745, Val Loss: 7.076548774950693, Val MAE: 1.834527611732483\n",
      "Epoch 170/750, Train Loss: 5.577805003998386, Val Loss: 7.07589744536161, Val MAE: 1.8360252380371094\n",
      "Epoch 171/750, Train Loss: 5.576856993414035, Val Loss: 7.075134939144234, Val MAE: 1.8372899293899536\n",
      "Epoch 172/750, Train Loss: 5.576107071919046, Val Loss: 7.074229883510478, Val MAE: 1.8360692262649536\n",
      "Epoch 173/750, Train Loss: 5.575458734961832, Val Loss: 7.07410456165162, Val MAE: 1.8338353633880615\n",
      "Epoch 174/750, Train Loss: 5.575109814078944, Val Loss: 7.072800187460008, Val MAE: 1.8351354598999023\n",
      "Epoch 175/750, Train Loss: 5.574462515229632, Val Loss: 7.072534773940992, Val MAE: 1.8349369764328003\n",
      "Epoch 176/750, Train Loss: 5.573391631606278, Val Loss: 7.071779995175013, Val MAE: 1.8354007005691528\n",
      "Epoch 177/750, Train Loss: 5.572658417331185, Val Loss: 7.071024413357765, Val MAE: 1.8333148956298828\n",
      "Epoch 178/750, Train Loss: 5.572579293342153, Val Loss: 7.0712176353906235, Val MAE: 1.8345073461532593\n",
      "Epoch 179/750, Train Loss: 5.571044078754012, Val Loss: 7.070411641639238, Val MAE: 1.8343008756637573\n",
      "Epoch 180/750, Train Loss: 5.57076397914036, Val Loss: 7.069639123286276, Val MAE: 1.8332030773162842\n",
      "Epoch 181/750, Train Loss: 5.56999083689064, Val Loss: 7.06853965674964, Val MAE: 1.835945725440979\n",
      "Epoch 182/750, Train Loss: 5.569365805121744, Val Loss: 7.067760666199098, Val MAE: 1.8338836431503296\n",
      "Epoch 183/750, Train Loss: 5.568627882914938, Val Loss: 7.067088318323037, Val MAE: 1.8349530696868896\n",
      "Epoch 184/750, Train Loss: 5.567746581241583, Val Loss: 7.066285596196143, Val MAE: 1.8345650434494019\n",
      "Epoch 185/750, Train Loss: 5.567295089648788, Val Loss: 7.066527297513095, Val MAE: 1.8340823650360107\n",
      "Epoch 186/750, Train Loss: 5.5664369479865785, Val Loss: 7.064910959463967, Val MAE: 1.833056926727295\n",
      "Epoch 187/750, Train Loss: 5.565534510885834, Val Loss: 7.064265010999996, Val MAE: 1.8352034091949463\n",
      "Epoch 188/750, Train Loss: 5.564995530608353, Val Loss: 7.064047447637862, Val MAE: 1.8336586952209473\n",
      "Epoch 189/750, Train Loss: 5.564307009414502, Val Loss: 7.063057510729826, Val MAE: 1.831765055656433\n",
      "Epoch 190/750, Train Loss: 5.563801540690623, Val Loss: 7.0626815977935244, Val MAE: 1.8329442739486694\n",
      "Epoch 191/750, Train Loss: 5.563198370538699, Val Loss: 7.061734123175155, Val MAE: 1.8308073282241821\n",
      "Epoch 192/750, Train Loss: 5.562587921179024, Val Loss: 7.061295849221919, Val MAE: 1.8306349515914917\n",
      "Epoch 193/750, Train Loss: 5.562015929495453, Val Loss: 7.060742081137753, Val MAE: 1.8287110328674316\n",
      "Epoch 194/750, Train Loss: 5.5613143055302325, Val Loss: 7.059679993270271, Val MAE: 1.830283522605896\n",
      "Epoch 195/750, Train Loss: 5.560555442275515, Val Loss: 7.058984247033385, Val MAE: 1.8332754373550415\n",
      "Epoch 196/750, Train Loss: 5.559794285950387, Val Loss: 7.0584635217416505, Val MAE: 1.8296715021133423\n",
      "Epoch 197/750, Train Loss: 5.55895747956197, Val Loss: 7.057791444401774, Val MAE: 1.8322385549545288\n",
      "Epoch 198/750, Train Loss: 5.558747700976718, Val Loss: 7.056991273605579, Val MAE: 1.830574870109558\n",
      "Epoch 199/750, Train Loss: 5.558296981131195, Val Loss: 7.056646506818946, Val MAE: 1.8313636779785156\n",
      "Epoch 200/750, Train Loss: 5.557386408338122, Val Loss: 7.056135298784954, Val MAE: 1.828365683555603\n",
      "Epoch 201/750, Train Loss: 5.55678967457668, Val Loss: 7.055276821273095, Val MAE: 1.8294556140899658\n",
      "Epoch 202/750, Train Loss: 5.555906662060197, Val Loss: 7.054677762401408, Val MAE: 1.829668402671814\n",
      "Epoch 203/750, Train Loss: 5.5553078080438505, Val Loss: 7.054137108042938, Val MAE: 1.831855297088623\n",
      "Epoch 204/750, Train Loss: 5.554706221173523, Val Loss: 7.053239279944679, Val MAE: 1.830614686012268\n",
      "Epoch 205/750, Train Loss: 5.554414003821695, Val Loss: 7.052473386486841, Val MAE: 1.8313826322555542\n",
      "Epoch 206/750, Train Loss: 5.553785677624356, Val Loss: 7.052691095841457, Val MAE: 1.8289085626602173\n",
      "Epoch 207/750, Train Loss: 5.5528675455955945, Val Loss: 7.051923007124918, Val MAE: 1.8318345546722412\n",
      "Epoch 208/750, Train Loss: 5.552558870983731, Val Loss: 7.051073189586345, Val MAE: 1.832760214805603\n",
      "Epoch 209/750, Train Loss: 5.551517297052274, Val Loss: 7.050685257627181, Val MAE: 1.8304344415664673\n",
      "Epoch 210/750, Train Loss: 5.551437008608678, Val Loss: 7.049929095988988, Val MAE: 1.8316398859024048\n",
      "Epoch 211/750, Train Loss: 5.550750256192153, Val Loss: 7.049437260612929, Val MAE: 1.827304720878601\n",
      "Epoch 212/750, Train Loss: 5.549990332050688, Val Loss: 7.04857707223661, Val MAE: 1.8286882638931274\n",
      "Epoch 213/750, Train Loss: 5.549414232582044, Val Loss: 7.047782449488168, Val MAE: 1.8308666944503784\n",
      "Epoch 214/750, Train Loss: 5.5489698920280315, Val Loss: 7.047005566808287, Val MAE: 1.830025315284729\n",
      "Epoch 215/750, Train Loss: 5.548586478810401, Val Loss: 7.0468422797913135, Val MAE: 1.8289819955825806\n",
      "Epoch 216/750, Train Loss: 5.5475747515441505, Val Loss: 7.045696778242599, Val MAE: 1.8282794952392578\n",
      "Epoch 217/750, Train Loss: 5.547215362111475, Val Loss: 7.0450399512456325, Val MAE: 1.8303755521774292\n",
      "Epoch 218/750, Train Loss: 5.546408263285449, Val Loss: 7.04476489255412, Val MAE: 1.825448751449585\n",
      "Epoch 219/750, Train Loss: 5.545910774218808, Val Loss: 7.043967170178927, Val MAE: 1.8289425373077393\n",
      "Epoch 220/750, Train Loss: 5.545092011029554, Val Loss: 7.0433872694032695, Val MAE: 1.8281259536743164\n",
      "Epoch 221/750, Train Loss: 5.5451568621738705, Val Loss: 7.043045861110545, Val MAE: 1.8270869255065918\n",
      "Epoch 222/750, Train Loss: 5.544061756437751, Val Loss: 7.042120236317662, Val MAE: 1.828657627105713\n",
      "Epoch 223/750, Train Loss: 5.543654588832977, Val Loss: 7.041622967694842, Val MAE: 1.8284002542495728\n",
      "Epoch 224/750, Train Loss: 5.543391849129064, Val Loss: 7.041058903047207, Val MAE: 1.8295907974243164\n",
      "Epoch 225/750, Train Loss: 5.542322913248827, Val Loss: 7.040304632124655, Val MAE: 1.827157735824585\n",
      "Epoch 226/750, Train Loss: 5.541958189314338, Val Loss: 7.039508416831975, Val MAE: 1.8271156549453735\n",
      "Epoch 227/750, Train Loss: 5.541450216208294, Val Loss: 7.0392613459478035, Val MAE: 1.8239413499832153\n",
      "Epoch 228/750, Train Loss: 5.540741465198007, Val Loss: 7.0379311697611975, Val MAE: 1.8269603252410889\n",
      "Epoch 229/750, Train Loss: 5.540309474726391, Val Loss: 7.037627346549736, Val MAE: 1.8271307945251465\n",
      "Epoch 230/750, Train Loss: 5.5396947654189574, Val Loss: 7.037078610260788, Val MAE: 1.827143907546997\n",
      "Epoch 231/750, Train Loss: 5.539023598591993, Val Loss: 7.0367601820220855, Val MAE: 1.8256629705429077\n",
      "Epoch 232/750, Train Loss: 5.5390431896136825, Val Loss: 7.036389219827575, Val MAE: 1.8228245973587036\n",
      "Epoch 233/750, Train Loss: 5.538258418307942, Val Loss: 7.03572903859682, Val MAE: 1.824126958847046\n",
      "Epoch 234/750, Train Loss: 5.537633352522637, Val Loss: 7.034872578566974, Val MAE: 1.826330542564392\n",
      "Epoch 235/750, Train Loss: 5.537076549165568, Val Loss: 7.034735746417748, Val MAE: 1.8247580528259277\n",
      "Epoch 236/750, Train Loss: 5.536650685444, Val Loss: 7.033669684894934, Val MAE: 1.8279380798339844\n",
      "Epoch 237/750, Train Loss: 5.536275229788131, Val Loss: 7.033237196111472, Val MAE: 1.8279616832733154\n",
      "Epoch 238/750, Train Loss: 5.535651341820978, Val Loss: 7.033044693186981, Val MAE: 1.8280514478683472\n",
      "Epoch 239/750, Train Loss: 5.535125385879711, Val Loss: 7.03241675217897, Val MAE: 1.8299448490142822\n",
      "Epoch 240/750, Train Loss: 5.534673286243609, Val Loss: 7.031561531362658, Val MAE: 1.8269705772399902\n",
      "Epoch 241/750, Train Loss: 5.533866930919088, Val Loss: 7.031249276520378, Val MAE: 1.82463538646698\n",
      "Epoch 242/750, Train Loss: 5.533582648502034, Val Loss: 7.030827489863871, Val MAE: 1.8266650438308716\n",
      "Epoch 243/750, Train Loss: 5.532921914082424, Val Loss: 7.030393514076029, Val MAE: 1.825942039489746\n",
      "Epoch 244/750, Train Loss: 5.532168830276295, Val Loss: 7.029812558012916, Val MAE: 1.823595404624939\n",
      "Epoch 245/750, Train Loss: 5.532023652192134, Val Loss: 7.02948801379681, Val MAE: 1.8248931169509888\n",
      "Epoch 246/750, Train Loss: 5.5317169091504095, Val Loss: 7.0288013265768585, Val MAE: 1.828734040260315\n",
      "Epoch 247/750, Train Loss: 5.531040219592441, Val Loss: 7.028509710574461, Val MAE: 1.8260829448699951\n",
      "Epoch 248/750, Train Loss: 5.530420505620872, Val Loss: 7.027898483612732, Val MAE: 1.8255382776260376\n",
      "Epoch 249/750, Train Loss: 5.529892972472367, Val Loss: 7.027763770188212, Val MAE: 1.8244850635528564\n",
      "Epoch 250/750, Train Loss: 5.529530993844293, Val Loss: 7.026842287936664, Val MAE: 1.8270833492279053\n",
      "Epoch 251/750, Train Loss: 5.52885221189754, Val Loss: 7.026436900262732, Val MAE: 1.824877142906189\n",
      "Epoch 252/750, Train Loss: 5.5284033173968075, Val Loss: 7.02629998435583, Val MAE: 1.823168396949768\n",
      "Epoch 253/750, Train Loss: 5.528077131623675, Val Loss: 7.025784807889783, Val MAE: 1.8261330127716064\n",
      "Epoch 254/750, Train Loss: 5.527625284984613, Val Loss: 7.025298665924202, Val MAE: 1.8233222961425781\n",
      "Epoch 255/750, Train Loss: 5.527212554785856, Val Loss: 7.024654958543013, Val MAE: 1.8259072303771973\n",
      "Epoch 256/750, Train Loss: 5.526281446863891, Val Loss: 7.024309183515474, Val MAE: 1.8247826099395752\n",
      "Epoch 257/750, Train Loss: 5.525857896258117, Val Loss: 7.023437375900833, Val MAE: 1.8237777948379517\n",
      "Epoch 258/750, Train Loss: 5.525538120148288, Val Loss: 7.02330192729062, Val MAE: 1.825516700744629\n",
      "Epoch 259/750, Train Loss: 5.525173421422387, Val Loss: 7.022880798100388, Val MAE: 1.8234995603561401\n",
      "Epoch 260/750, Train Loss: 5.524591980466417, Val Loss: 7.022125777063198, Val MAE: 1.8219090700149536\n",
      "Epoch 261/750, Train Loss: 5.524555528695417, Val Loss: 7.021667633759005, Val MAE: 1.824918508529663\n",
      "Epoch 262/750, Train Loss: 5.523402771676422, Val Loss: 7.021085090560154, Val MAE: 1.8227676153182983\n",
      "Epoch 263/750, Train Loss: 5.523139428181254, Val Loss: 7.020769112224235, Val MAE: 1.825465440750122\n",
      "Epoch 264/750, Train Loss: 5.522910748317742, Val Loss: 7.020861243520041, Val MAE: 1.8232444524765015\n",
      "Epoch 265/750, Train Loss: 5.522198211159676, Val Loss: 7.020204016553311, Val MAE: 1.8248578310012817\n",
      "Epoch 266/750, Train Loss: 5.522076424367868, Val Loss: 7.019658884178266, Val MAE: 1.822153925895691\n",
      "Epoch 267/750, Train Loss: 5.521386214578228, Val Loss: 7.018977768714565, Val MAE: 1.823167324066162\n",
      "Epoch 268/750, Train Loss: 5.520789787799689, Val Loss: 7.018345754438487, Val MAE: 1.8228938579559326\n",
      "Epoch 269/750, Train Loss: 5.5205862819768825, Val Loss: 7.018064847675594, Val MAE: 1.8212052583694458\n",
      "Epoch 270/750, Train Loss: 5.519984320014905, Val Loss: 7.017027985427304, Val MAE: 1.8227105140686035\n",
      "Epoch 271/750, Train Loss: 5.5194057877656, Val Loss: 7.016335791427204, Val MAE: 1.8258143663406372\n",
      "Epoch 272/750, Train Loss: 5.519018834715436, Val Loss: 7.016498436462516, Val MAE: 1.8226186037063599\n",
      "Epoch 273/750, Train Loss: 5.519011044957836, Val Loss: 7.015596039069965, Val MAE: 1.8237601518630981\n",
      "Epoch 274/750, Train Loss: 5.5184209884351985, Val Loss: 7.015126922621055, Val MAE: 1.8219693899154663\n",
      "Epoch 275/750, Train Loss: 5.517761878299106, Val Loss: 7.014783442650173, Val MAE: 1.8202123641967773\n",
      "Epoch 276/750, Train Loss: 5.516973796164154, Val Loss: 7.0142460591382605, Val MAE: 1.821241021156311\n",
      "Epoch 277/750, Train Loss: 5.516896906324253, Val Loss: 7.0137671800486565, Val MAE: 1.8186118602752686\n",
      "Epoch 278/750, Train Loss: 5.516275953183508, Val Loss: 7.012578575488127, Val MAE: 1.8236180543899536\n",
      "Epoch 279/750, Train Loss: 5.516177217519966, Val Loss: 7.013331222008147, Val MAE: 1.8224908113479614\n",
      "Epoch 280/750, Train Loss: 5.515643843268133, Val Loss: 7.012927069583985, Val MAE: 1.8234995603561401\n",
      "Epoch 281/750, Train Loss: 5.514838800916246, Val Loss: 7.012476971388011, Val MAE: 1.8204960823059082\n",
      "Epoch 282/750, Train Loss: 5.515168867293437, Val Loss: 7.011713578250558, Val MAE: 1.8235828876495361\n",
      "Epoch 283/750, Train Loss: 5.514543238111362, Val Loss: 7.0113102502153115, Val MAE: 1.822510838508606\n",
      "Epoch 284/750, Train Loss: 5.514081901349839, Val Loss: 7.010891886139746, Val MAE: 1.824053406715393\n",
      "Epoch 285/750, Train Loss: 5.513337493094669, Val Loss: 7.010342396513285, Val MAE: 1.822446584701538\n",
      "Epoch 286/750, Train Loss: 5.513031088166936, Val Loss: 7.009755961675685, Val MAE: 1.8254923820495605\n",
      "Epoch 287/750, Train Loss: 5.512875233789917, Val Loss: 7.009372099465358, Val MAE: 1.8226217031478882\n",
      "Epoch 288/750, Train Loss: 5.512145252592244, Val Loss: 7.008781521790068, Val MAE: 1.8228986263275146\n",
      "Epoch 289/750, Train Loss: 5.512270192431796, Val Loss: 7.008264623790739, Val MAE: 1.8222301006317139\n",
      "Epoch 290/750, Train Loss: 5.511592926341257, Val Loss: 7.009034221293839, Val MAE: 1.8163938522338867\n",
      "Epoch 291/750, Train Loss: 5.511185529429442, Val Loss: 7.008128434215887, Val MAE: 1.8198760747909546\n",
      "Epoch 292/750, Train Loss: 5.510546562777963, Val Loss: 7.007762969021771, Val MAE: 1.817372441291809\n",
      "Epoch 293/750, Train Loss: 5.510114493643402, Val Loss: 7.006937523003763, Val MAE: 1.822152018547058\n",
      "Epoch 294/750, Train Loss: 5.5098310652811815, Val Loss: 7.006313436515884, Val MAE: 1.8203376531600952\n",
      "Epoch 295/750, Train Loss: 5.509512228267208, Val Loss: 7.006088852697163, Val MAE: 1.820866584777832\n",
      "Epoch 296/750, Train Loss: 5.50900702689104, Val Loss: 7.005795974180223, Val MAE: 1.8185410499572754\n",
      "Epoch 297/750, Train Loss: 5.508682096080416, Val Loss: 7.005343319541405, Val MAE: 1.8206055164337158\n",
      "Epoch 298/750, Train Loss: 5.508194998115491, Val Loss: 7.004820463152091, Val MAE: 1.8198782205581665\n",
      "Epoch 299/750, Train Loss: 5.507951833032498, Val Loss: 7.004576747761223, Val MAE: 1.8222583532333374\n",
      "Epoch 300/750, Train Loss: 5.5081659552398, Val Loss: 7.003410949441792, Val MAE: 1.8226898908615112\n",
      "Epoch 301/750, Train Loss: 5.507405482128168, Val Loss: 7.002962742405993, Val MAE: 1.820050597190857\n",
      "Epoch 302/750, Train Loss: 5.507112520211821, Val Loss: 7.00335741028273, Val MAE: 1.8179404735565186\n",
      "Epoch 303/750, Train Loss: 5.506351845127762, Val Loss: 7.002689021910993, Val MAE: 1.8196223974227905\n",
      "Epoch 304/750, Train Loss: 5.506063568212424, Val Loss: 7.002315810483905, Val MAE: 1.820250153541565\n",
      "Epoch 305/750, Train Loss: 5.505950379675361, Val Loss: 7.002126185845114, Val MAE: 1.8218023777008057\n",
      "Epoch 306/750, Train Loss: 5.505139777235165, Val Loss: 7.0020858108293496, Val MAE: 1.8192203044891357\n",
      "Epoch 307/750, Train Loss: 5.505106129615929, Val Loss: 7.001535423576943, Val MAE: 1.820723533630371\n",
      "Epoch 308/750, Train Loss: 5.505021920173791, Val Loss: 7.000866549736672, Val MAE: 1.822131633758545\n",
      "Epoch 309/750, Train Loss: 5.503919370614799, Val Loss: 7.000458712159958, Val MAE: 1.8236658573150635\n",
      "Epoch 310/750, Train Loss: 5.504315034902779, Val Loss: 7.000224776746621, Val MAE: 1.8226934671401978\n",
      "Epoch 311/750, Train Loss: 5.503463173520034, Val Loss: 7.000033800464154, Val MAE: 1.8231556415557861\n",
      "Epoch 312/750, Train Loss: 5.5033094946745855, Val Loss: 6.9995353862393195, Val MAE: 1.820751667022705\n",
      "Epoch 313/750, Train Loss: 5.502445695050962, Val Loss: 6.998741432515044, Val MAE: 1.8231465816497803\n",
      "Epoch 314/750, Train Loss: 5.501968823572633, Val Loss: 6.9984816885868755, Val MAE: 1.8235515356063843\n",
      "Epoch 315/750, Train Loss: 5.501672098287352, Val Loss: 6.998271917763373, Val MAE: 1.8244742155075073\n",
      "Epoch 316/750, Train Loss: 5.501735285740749, Val Loss: 6.997881619881369, Val MAE: 1.8245434761047363\n",
      "Epoch 317/750, Train Loss: 5.501182617503367, Val Loss: 6.997355955146425, Val MAE: 1.8218344449996948\n",
      "Epoch 318/750, Train Loss: 5.50069948548724, Val Loss: 6.997142561331086, Val MAE: 1.819611668586731\n",
      "Epoch 319/750, Train Loss: 5.5004783906754415, Val Loss: 6.997225504583865, Val MAE: 1.817169427871704\n",
      "Epoch 320/750, Train Loss: 5.49999537559072, Val Loss: 6.996346084541086, Val MAE: 1.8220778703689575\n",
      "Epoch 321/750, Train Loss: 5.499885716407921, Val Loss: 6.995994954764139, Val MAE: 1.8224067687988281\n",
      "Epoch 322/750, Train Loss: 5.499503528084724, Val Loss: 6.995596714350213, Val MAE: 1.8219876289367676\n",
      "Epoch 323/750, Train Loss: 5.498893506359902, Val Loss: 6.995351814201265, Val MAE: 1.8187390565872192\n",
      "Epoch 324/750, Train Loss: 5.498842112729504, Val Loss: 6.9953154320299, Val MAE: 1.8146848678588867\n",
      "Epoch 325/750, Train Loss: 5.498400014828724, Val Loss: 6.994192039470186, Val MAE: 1.8205525875091553\n",
      "Epoch 326/750, Train Loss: 5.498353810522966, Val Loss: 6.994062957295429, Val MAE: 1.8182427883148193\n",
      "Epoch 327/750, Train Loss: 5.497500735179634, Val Loss: 6.993694452264429, Val MAE: 1.8153108358383179\n",
      "Epoch 328/750, Train Loss: 5.497576225183572, Val Loss: 6.993477259940283, Val MAE: 1.8174827098846436\n",
      "Epoch 329/750, Train Loss: 5.497201161931275, Val Loss: 6.993428011500665, Val MAE: 1.815980076789856\n",
      "Epoch 330/750, Train Loss: 5.496778858087625, Val Loss: 6.992798716922663, Val MAE: 1.8199458122253418\n",
      "Epoch 331/750, Train Loss: 5.496316863321195, Val Loss: 6.992351623186049, Val MAE: 1.818223476409912\n",
      "Epoch 332/750, Train Loss: 5.496196679704508, Val Loss: 6.991925282905085, Val MAE: 1.8202362060546875\n",
      "Epoch 333/750, Train Loss: 5.49593530278297, Val Loss: 6.991901629196331, Val MAE: 1.8188873529434204\n",
      "Epoch 334/750, Train Loss: 5.495900327840428, Val Loss: 6.991436233277495, Val MAE: 1.8181647062301636\n",
      "Epoch 335/750, Train Loss: 5.495221673758926, Val Loss: 6.991093438852641, Val MAE: 1.8167476654052734\n",
      "Epoch 336/750, Train Loss: 5.494855944518071, Val Loss: 6.990698579603874, Val MAE: 1.819002389907837\n",
      "Epoch 337/750, Train Loss: 5.494168330453763, Val Loss: 6.990507235802649, Val MAE: 1.8181618452072144\n",
      "Epoch 338/750, Train Loss: 5.494234685229648, Val Loss: 6.990268973181009, Val MAE: 1.816184163093567\n",
      "Epoch 339/750, Train Loss: 5.493800010195204, Val Loss: 6.989834046719458, Val MAE: 1.8187720775604248\n",
      "Epoch 340/750, Train Loss: 5.493448188806036, Val Loss: 6.989164352342903, Val MAE: 1.819549322128296\n",
      "Epoch 341/750, Train Loss: 5.493428765740364, Val Loss: 6.9895592101098885, Val MAE: 1.819675326347351\n",
      "Epoch 342/750, Train Loss: 5.492784395946819, Val Loss: 6.989300036852667, Val MAE: 1.818381428718567\n",
      "Epoch 343/750, Train Loss: 5.492789024456291, Val Loss: 6.989264344413359, Val MAE: 1.8207780122756958\n",
      "Epoch 344/750, Train Loss: 5.492014613728615, Val Loss: 6.988790419251375, Val MAE: 1.8185253143310547\n",
      "Epoch 345/750, Train Loss: 5.491546057439914, Val Loss: 6.988446849262292, Val MAE: 1.817961573600769\n",
      "Epoch 346/750, Train Loss: 5.491456039088547, Val Loss: 6.9880730066335, Val MAE: 1.816569209098816\n",
      "Epoch 347/750, Train Loss: 5.491007763564967, Val Loss: 6.987926355424766, Val MAE: 1.8175548315048218\n",
      "Epoch 348/750, Train Loss: 5.49112382755158, Val Loss: 6.987572240933193, Val MAE: 1.8195390701293945\n",
      "Epoch 349/750, Train Loss: 5.49039479030925, Val Loss: 6.987036776031449, Val MAE: 1.818368911743164\n",
      "Epoch 350/750, Train Loss: 5.490454181136599, Val Loss: 6.987959525354016, Val MAE: 1.815933346748352\n",
      "Epoch 351/750, Train Loss: 5.490048934244046, Val Loss: 6.986996989282782, Val MAE: 1.8179869651794434\n",
      "Epoch 352/750, Train Loss: 5.489867420352189, Val Loss: 6.986892248029809, Val MAE: 1.8158429861068726\n",
      "Epoch 353/750, Train Loss: 5.489470036014629, Val Loss: 6.987288334971263, Val MAE: 1.8105651140213013\n",
      "Epoch 354/750, Train Loss: 5.489517918969415, Val Loss: 6.985865014469201, Val MAE: 1.8176850080490112\n",
      "Epoch 355/750, Train Loss: 5.488926690702985, Val Loss: 6.985568663879831, Val MAE: 1.8222122192382812\n",
      "Epoch 356/750, Train Loss: 5.488478751698876, Val Loss: 6.985296346779712, Val MAE: 1.8172098398208618\n",
      "Epoch 357/750, Train Loss: 5.488611342800651, Val Loss: 6.985646095166227, Val MAE: 1.816768765449524\n",
      "Epoch 358/750, Train Loss: 5.488006566892004, Val Loss: 6.985226863507827, Val MAE: 1.815588355064392\n",
      "Epoch 359/750, Train Loss: 5.488351900258642, Val Loss: 6.984637361790839, Val MAE: 1.8195887804031372\n",
      "Epoch 360/750, Train Loss: 5.487727201364602, Val Loss: 6.983837771815791, Val MAE: 1.8200629949569702\n",
      "Epoch 361/750, Train Loss: 5.487731270577497, Val Loss: 6.983297241704667, Val MAE: 1.8200511932373047\n",
      "Epoch 362/750, Train Loss: 5.4871609630098765, Val Loss: 6.983302111948699, Val MAE: 1.8155030012130737\n",
      "Epoch 363/750, Train Loss: 5.486689537194125, Val Loss: 6.983024954499482, Val MAE: 1.8147635459899902\n",
      "Epoch 364/750, Train Loss: 5.4869526932953265, Val Loss: 6.982579293571102, Val MAE: 1.8154627084732056\n",
      "Epoch 365/750, Train Loss: 5.486215163188375, Val Loss: 6.9826514651273035, Val MAE: 1.8156142234802246\n",
      "Epoch 366/750, Train Loss: 5.48584371372393, Val Loss: 6.982138730895823, Val MAE: 1.8149000406265259\n",
      "Epoch 367/750, Train Loss: 5.485696717887927, Val Loss: 6.981786614215618, Val MAE: 1.817099690437317\n",
      "Epoch 368/750, Train Loss: 5.485292182606496, Val Loss: 6.9813972838996285, Val MAE: 1.8163055181503296\n",
      "Epoch 369/750, Train Loss: 5.485009292128739, Val Loss: 6.981083302263742, Val MAE: 1.8165862560272217\n",
      "Epoch 370/750, Train Loss: 5.484908928233347, Val Loss: 6.980589125303099, Val MAE: 1.8179105520248413\n",
      "Epoch 371/750, Train Loss: 5.4848309869219545, Val Loss: 6.980694091564532, Val MAE: 1.8181126117706299\n",
      "Epoch 372/750, Train Loss: 5.484305070160301, Val Loss: 6.980940842865564, Val MAE: 1.8157479763031006\n",
      "Epoch 373/750, Train Loss: 5.484301328051622, Val Loss: 6.979926981786677, Val MAE: 1.8154186010360718\n",
      "Epoch 374/750, Train Loss: 5.483944938744709, Val Loss: 6.979712920762353, Val MAE: 1.8173798322677612\n",
      "Epoch 375/750, Train Loss: 5.483383557750921, Val Loss: 6.979746017343439, Val MAE: 1.8147754669189453\n",
      "Epoch 376/750, Train Loss: 5.483677126040124, Val Loss: 6.979299910213727, Val MAE: 1.8159185647964478\n",
      "Epoch 377/750, Train Loss: 5.483352082853864, Val Loss: 6.979254956494362, Val MAE: 1.8189295530319214\n",
      "Epoch 378/750, Train Loss: 5.483090141320684, Val Loss: 6.979028934866394, Val MAE: 1.8165792226791382\n",
      "Epoch 379/750, Train Loss: 5.482371510365966, Val Loss: 6.978429904444904, Val MAE: 1.8170219659805298\n",
      "Epoch 380/750, Train Loss: 5.482090445840434, Val Loss: 6.978745156485816, Val MAE: 1.8108882904052734\n",
      "Epoch 381/750, Train Loss: 5.482030974527833, Val Loss: 6.978114935646448, Val MAE: 1.8150502443313599\n",
      "Epoch 382/750, Train Loss: 5.482067111495194, Val Loss: 6.977593712216953, Val MAE: 1.8142681121826172\n",
      "Epoch 383/750, Train Loss: 5.481563360372166, Val Loss: 6.977530221601241, Val MAE: 1.8167611360549927\n",
      "Epoch 384/750, Train Loss: 5.48142972387326, Val Loss: 6.977846431539546, Val MAE: 1.8126935958862305\n",
      "Epoch 385/750, Train Loss: 5.481157589793964, Val Loss: 6.977574508482066, Val MAE: 1.8148962259292603\n",
      "Epoch 386/750, Train Loss: 5.480914745816759, Val Loss: 6.977177852499626, Val MAE: 1.8134453296661377\n",
      "Epoch 387/750, Train Loss: 5.480506128414421, Val Loss: 6.977545076382686, Val MAE: 1.8101004362106323\n",
      "Epoch 388/750, Train Loss: 5.480643091991449, Val Loss: 6.976864456574765, Val MAE: 1.809393286705017\n",
      "Epoch 389/750, Train Loss: 5.480594170928761, Val Loss: 6.977066597262845, Val MAE: 1.8104997873306274\n",
      "Epoch 390/750, Train Loss: 5.479897392479478, Val Loss: 6.97654220352564, Val MAE: 1.8161557912826538\n",
      "Epoch 391/750, Train Loss: 5.479715813193351, Val Loss: 6.975802422088448, Val MAE: 1.812949299812317\n",
      "Epoch 392/750, Train Loss: 5.479350349401972, Val Loss: 6.977104870335073, Val MAE: 1.8140987157821655\n",
      "Epoch 393/750, Train Loss: 5.479563098956065, Val Loss: 6.976357328365721, Val MAE: 1.8138840198516846\n",
      "Epoch 394/750, Train Loss: 5.478992818723059, Val Loss: 6.975627166725523, Val MAE: 1.818192481994629\n",
      "Epoch 395/750, Train Loss: 5.479069614410401, Val Loss: 6.975627807299493, Val MAE: 1.8184007406234741\n",
      "Epoch 396/750, Train Loss: 5.478367387868796, Val Loss: 6.975050186429281, Val MAE: 1.8127361536026\n",
      "Epoch 397/750, Train Loss: 5.478589058529799, Val Loss: 6.9750336193015965, Val MAE: 1.8114044666290283\n",
      "Epoch 398/750, Train Loss: 5.478146903530048, Val Loss: 6.974528731101044, Val MAE: 1.815796136856079\n",
      "Epoch 399/750, Train Loss: 5.478575767225521, Val Loss: 6.974372789087764, Val MAE: 1.8119913339614868\n",
      "Epoch 400/750, Train Loss: 5.477647885243604, Val Loss: 6.9741661918318885, Val MAE: 1.813489317893982\n",
      "Epoch 401/750, Train Loss: 5.477876358882637, Val Loss: 6.973206334228172, Val MAE: 1.8165178298950195\n",
      "Epoch 402/750, Train Loss: 5.4771167351182095, Val Loss: 6.973321580160768, Val MAE: 1.816712498664856\n",
      "Epoch 403/750, Train Loss: 5.477044656170402, Val Loss: 6.9730000724104935, Val MAE: 1.8183677196502686\n",
      "Epoch 404/750, Train Loss: 5.476760023718427, Val Loss: 6.97341502457394, Val MAE: 1.8129462003707886\n",
      "Epoch 405/750, Train Loss: 5.476668011003239, Val Loss: 6.972708366760339, Val MAE: 1.815589189529419\n",
      "Epoch 406/750, Train Loss: 5.476335670689869, Val Loss: 6.972791585179869, Val MAE: 1.8130106925964355\n",
      "Epoch 407/750, Train Loss: 5.47639912526319, Val Loss: 6.972611186591789, Val MAE: 1.8156704902648926\n",
      "Epoch 408/750, Train Loss: 5.47603688270423, Val Loss: 6.972123022550674, Val MAE: 1.8124033212661743\n",
      "Epoch 409/750, Train Loss: 5.475792876322558, Val Loss: 6.972574765449581, Val MAE: 1.8128741979599\n",
      "Epoch 410/750, Train Loss: 5.475418048299802, Val Loss: 6.972136076412414, Val MAE: 1.8145889043807983\n",
      "Epoch 411/750, Train Loss: 5.4758277473935655, Val Loss: 6.971613324822069, Val MAE: 1.8129242658615112\n",
      "Epoch 412/750, Train Loss: 5.475179606638137, Val Loss: 6.971831682085472, Val MAE: 1.8127379417419434\n",
      "Epoch 413/750, Train Loss: 5.474782886019178, Val Loss: 6.971427285167429, Val MAE: 1.815438151359558\n",
      "Epoch 414/750, Train Loss: 5.474625361497235, Val Loss: 6.971332149004773, Val MAE: 1.8138926029205322\n",
      "Epoch 415/750, Train Loss: 5.474806599252543, Val Loss: 6.971048261453974, Val MAE: 1.813916563987732\n",
      "Epoch 416/750, Train Loss: 5.474303981756709, Val Loss: 6.970376937822054, Val MAE: 1.8155204057693481\n",
      "Epoch 417/750, Train Loss: 5.474435589419809, Val Loss: 6.970652857280207, Val MAE: 1.8120936155319214\n",
      "Epoch 418/750, Train Loss: 5.474379938423254, Val Loss: 6.970002984541258, Val MAE: 1.8151570558547974\n",
      "Epoch 419/750, Train Loss: 5.473745578717274, Val Loss: 6.9697423387270225, Val MAE: 1.814466118812561\n",
      "Epoch 420/750, Train Loss: 5.473688634034175, Val Loss: 6.969395872189289, Val MAE: 1.8148646354675293\n",
      "Epoch 421/750, Train Loss: 5.473280480257265, Val Loss: 6.969135748739936, Val MAE: 1.8147252798080444\n",
      "Epoch 422/750, Train Loss: 5.473365989612167, Val Loss: 6.969423106696163, Val MAE: 1.8117129802703857\n",
      "Epoch 423/750, Train Loss: 5.472773341768107, Val Loss: 6.968928601706228, Val MAE: 1.812935471534729\n",
      "Epoch 424/750, Train Loss: 5.47287214485703, Val Loss: 6.9689957912550575, Val MAE: 1.8106517791748047\n",
      "Epoch 425/750, Train Loss: 5.47266458849998, Val Loss: 6.968740086439902, Val MAE: 1.8132201433181763\n",
      "Epoch 426/750, Train Loss: 5.47269632922616, Val Loss: 6.96881486674582, Val MAE: 1.810887098312378\n",
      "Epoch 427/750, Train Loss: 5.472587389854868, Val Loss: 6.9686479948558295, Val MAE: 1.81193208694458\n",
      "Epoch 428/750, Train Loss: 5.472084997262165, Val Loss: 6.967994563767745, Val MAE: 1.8114089965820312\n",
      "Epoch 429/750, Train Loss: 5.471801612635327, Val Loss: 6.967507575371756, Val MAE: 1.812696933746338\n",
      "Epoch 430/750, Train Loss: 5.4715039711848945, Val Loss: 6.967631729587107, Val MAE: 1.8121968507766724\n",
      "Epoch 431/750, Train Loss: 5.47166142919261, Val Loss: 6.96772937370132, Val MAE: 1.8094531297683716\n",
      "Epoch 432/750, Train Loss: 5.471261118779516, Val Loss: 6.966954892099533, Val MAE: 1.8153586387634277\n",
      "Epoch 433/750, Train Loss: 5.471208727891278, Val Loss: 6.967348267307483, Val MAE: 1.8119194507598877\n",
      "Epoch 434/750, Train Loss: 5.470965177693944, Val Loss: 6.966719707248372, Val MAE: 1.813908576965332\n",
      "Epoch 435/750, Train Loss: 5.471138476110568, Val Loss: 6.966896180116738, Val MAE: 1.8112146854400635\n",
      "Epoch 436/750, Train Loss: 5.470864288062806, Val Loss: 6.967065603337427, Val MAE: 1.8107714653015137\n",
      "Epoch 437/750, Train Loss: 5.470792333639351, Val Loss: 6.966385542864826, Val MAE: 1.810981273651123\n",
      "Epoch 438/750, Train Loss: 5.470222661449651, Val Loss: 6.966624803280816, Val MAE: 1.811930537223816\n",
      "Epoch 439/750, Train Loss: 5.470364583373829, Val Loss: 6.966244219696396, Val MAE: 1.8103777170181274\n",
      "Epoch 440/750, Train Loss: 5.470154880110625, Val Loss: 6.96623895415021, Val MAE: 1.8124393224716187\n",
      "Epoch 441/750, Train Loss: 5.469967516820142, Val Loss: 6.965842512914896, Val MAE: 1.814210295677185\n",
      "Epoch 442/750, Train Loss: 5.469467367791826, Val Loss: 6.965970090859569, Val MAE: 1.8120421171188354\n",
      "Epoch 443/750, Train Loss: 5.469478754784651, Val Loss: 6.965261840316507, Val MAE: 1.8137757778167725\n",
      "Epoch 444/750, Train Loss: 5.469223035217091, Val Loss: 6.965494321583961, Val MAE: 1.8123892545700073\n",
      "Epoch 445/750, Train Loss: 5.4696531526602, Val Loss: 6.965418072425482, Val MAE: 1.8129245042800903\n",
      "Epoch 446/750, Train Loss: 5.469145214329859, Val Loss: 6.965095861866431, Val MAE: 1.8119022846221924\n",
      "Epoch 447/750, Train Loss: 5.468988717741268, Val Loss: 6.964977516307084, Val MAE: 1.811235785484314\n",
      "Epoch 448/750, Train Loss: 5.468520412931017, Val Loss: 6.965141134576222, Val MAE: 1.809546947479248\n",
      "Epoch 449/750, Train Loss: 5.468709930796532, Val Loss: 6.9650759100914, Val MAE: 1.8082520961761475\n",
      "Epoch 450/750, Train Loss: 5.468559519652349, Val Loss: 6.964971199029717, Val MAE: 1.8096392154693604\n",
      "Epoch 451/750, Train Loss: 5.468370071945676, Val Loss: 6.964594940097529, Val MAE: 1.8119173049926758\n",
      "Epoch 452/750, Train Loss: 5.46818439914922, Val Loss: 6.964500736567898, Val MAE: 1.8090859651565552\n",
      "Epoch 453/750, Train Loss: 5.4683251156169135, Val Loss: 6.964180111662803, Val MAE: 1.8126335144042969\n",
      "Epoch 454/750, Train Loss: 5.467932097737197, Val Loss: 6.96398843142318, Val MAE: 1.8134007453918457\n",
      "Epoch 455/750, Train Loss: 5.467716910125343, Val Loss: 6.963994289348405, Val MAE: 1.8114078044891357\n",
      "Epoch 456/750, Train Loss: 5.46761601988677, Val Loss: 6.9643596206265554, Val MAE: 1.807707667350769\n",
      "Epoch 457/750, Train Loss: 5.467378791274538, Val Loss: 6.963444119806687, Val MAE: 1.8130654096603394\n",
      "Epoch 458/750, Train Loss: 5.467364257156469, Val Loss: 6.963237309322511, Val MAE: 1.8139758110046387\n",
      "Epoch 459/750, Train Loss: 5.467128573785162, Val Loss: 6.9636277310073265, Val MAE: 1.815427303314209\n",
      "Epoch 460/750, Train Loss: 5.466981834970462, Val Loss: 6.96351779737111, Val MAE: 1.8109872341156006\n",
      "Epoch 461/750, Train Loss: 5.466889552705607, Val Loss: 6.963310674008855, Val MAE: 1.809772253036499\n",
      "Epoch 462/750, Train Loss: 5.466590664493051, Val Loss: 6.963330937032006, Val MAE: 1.8105329275131226\n",
      "Epoch 463/750, Train Loss: 5.466614484179551, Val Loss: 6.963055369348686, Val MAE: 1.8121964931488037\n",
      "Epoch 464/750, Train Loss: 5.466688184981134, Val Loss: 6.963140496709763, Val MAE: 1.8094617128372192\n",
      "Epoch 465/750, Train Loss: 5.466976777459406, Val Loss: 6.962717942120201, Val MAE: 1.8100553750991821\n",
      "Epoch 466/750, Train Loss: 5.466262626647949, Val Loss: 6.962409333644853, Val MAE: 1.8105520009994507\n",
      "Epoch 467/750, Train Loss: 5.466089526255419, Val Loss: 6.962261342128069, Val MAE: 1.810928463935852\n",
      "Epoch 468/750, Train Loss: 5.46591789525026, Val Loss: 6.9620010990989805, Val MAE: 1.8136155605316162\n",
      "Epoch 469/750, Train Loss: 5.465853172350841, Val Loss: 6.961983021851132, Val MAE: 1.8118860721588135\n",
      "Epoch 470/750, Train Loss: 5.465756805079757, Val Loss: 6.962074401587563, Val MAE: 1.8113826513290405\n",
      "Epoch 471/750, Train Loss: 5.465813480061331, Val Loss: 6.962278136338708, Val MAE: 1.8120076656341553\n",
      "Epoch 472/750, Train Loss: 5.465518884142493, Val Loss: 6.961983463125543, Val MAE: 1.8113317489624023\n",
      "Epoch 473/750, Train Loss: 5.465705280850647, Val Loss: 6.961749269956384, Val MAE: 1.8102922439575195\n",
      "Epoch 474/750, Train Loss: 5.465023708343506, Val Loss: 6.961420338964373, Val MAE: 1.8118072748184204\n",
      "Epoch 475/750, Train Loss: 5.464923669122586, Val Loss: 6.960961438766394, Val MAE: 1.8129318952560425\n",
      "Epoch 476/750, Train Loss: 5.464672684973213, Val Loss: 6.9613963580783516, Val MAE: 1.8102854490280151\n",
      "Epoch 477/750, Train Loss: 5.4650010631342605, Val Loss: 6.961841392213325, Val MAE: 1.8090168237686157\n",
      "Epoch 478/750, Train Loss: 5.464210026734953, Val Loss: 6.961080290094675, Val MAE: 1.809439778327942\n",
      "Epoch 479/750, Train Loss: 5.464442890313021, Val Loss: 6.960987550972262, Val MAE: 1.8130944967269897\n",
      "Epoch 480/750, Train Loss: 5.46483849446485, Val Loss: 6.961228736814614, Val MAE: 1.8111392259597778\n",
      "Epoch 481/750, Train Loss: 5.464223893281001, Val Loss: 6.960799890767721, Val MAE: 1.8120073080062866\n",
      "Epoch 482/750, Train Loss: 5.464099018740806, Val Loss: 6.961041538666464, Val MAE: 1.8140811920166016\n",
      "Epoch 483/750, Train Loss: 5.464269138749238, Val Loss: 6.961156131244728, Val MAE: 1.8082207441329956\n",
      "Epoch 484/750, Train Loss: 5.463774900983093, Val Loss: 6.960658283393675, Val MAE: 1.8113559484481812\n",
      "Epoch 485/750, Train Loss: 5.463793484268675, Val Loss: 6.960375053160972, Val MAE: 1.8096832036972046\n",
      "Epoch 486/750, Train Loss: 5.463477957476476, Val Loss: 6.961019003443128, Val MAE: 1.815345287322998\n",
      "Epoch 487/750, Train Loss: 5.463606366078565, Val Loss: 6.961123095750068, Val MAE: 1.8107450008392334\n",
      "Epoch 488/750, Train Loss: 5.46358065392561, Val Loss: 6.960844783771105, Val MAE: 1.8129866123199463\n",
      "Epoch 489/750, Train Loss: 5.463261846979712, Val Loss: 6.960438485839043, Val MAE: 1.8122475147247314\n",
      "Epoch 490/750, Train Loss: 5.46322538655275, Val Loss: 6.960170291794835, Val MAE: 1.8102421760559082\n",
      "Epoch 491/750, Train Loss: 5.463059845699626, Val Loss: 6.960579603743746, Val MAE: 1.8092507123947144\n",
      "Epoch 492/750, Train Loss: 5.462922973769485, Val Loss: 6.959916275728556, Val MAE: 1.8102105855941772\n",
      "Epoch 493/750, Train Loss: 5.462859311680885, Val Loss: 6.960055854210874, Val MAE: 1.8093894720077515\n",
      "Epoch 494/750, Train Loss: 5.4627787650770445, Val Loss: 6.960523049604678, Val MAE: 1.8086804151535034\n",
      "Epoch 495/750, Train Loss: 5.4627135969271325, Val Loss: 6.960285443041619, Val MAE: 1.8065837621688843\n",
      "Epoch 496/750, Train Loss: 5.46242340719624, Val Loss: 6.9599887482715435, Val MAE: 1.8058459758758545\n",
      "Epoch 497/750, Train Loss: 5.462190835339249, Val Loss: 6.959559957791441, Val MAE: 1.8077101707458496\n",
      "Epoch 498/750, Train Loss: 5.46225319842624, Val Loss: 6.959859820164977, Val MAE: 1.807402491569519\n",
      "Epoch 499/750, Train Loss: 5.461995995879932, Val Loss: 6.959339901925318, Val MAE: 1.8081631660461426\n",
      "Epoch 500/750, Train Loss: 5.462054103195288, Val Loss: 6.959134786672811, Val MAE: 1.8074114322662354\n",
      "Epoch 501/750, Train Loss: 5.461842947552918, Val Loss: 6.959042676751698, Val MAE: 1.8095725774765015\n",
      "Epoch 502/750, Train Loss: 5.461921227206091, Val Loss: 6.959008332733192, Val MAE: 1.8079358339309692\n",
      "Epoch 503/750, Train Loss: 5.461614906408225, Val Loss: 6.959362820040891, Val MAE: 1.8089367151260376\n",
      "Epoch 504/750, Train Loss: 5.46164150845473, Val Loss: 6.958667502409186, Val MAE: 1.8140112161636353\n",
      "Epoch 505/750, Train Loss: 5.461226623680941, Val Loss: 6.958395297813297, Val MAE: 1.8086800575256348\n",
      "Epoch 506/750, Train Loss: 5.461169063665305, Val Loss: 6.958157609752278, Val MAE: 1.8133457899093628\n",
      "Epoch 507/750, Train Loss: 5.46134501839899, Val Loss: 6.957479317377931, Val MAE: 1.8143818378448486\n",
      "Epoch 508/750, Train Loss: 5.460894442515768, Val Loss: 6.957386309942005, Val MAE: 1.8125941753387451\n",
      "Epoch 509/750, Train Loss: 5.460994661537705, Val Loss: 6.957759285580675, Val MAE: 1.8133618831634521\n",
      "Epoch 510/750, Train Loss: 5.460776807092557, Val Loss: 6.958018547226139, Val MAE: 1.81101393699646\n",
      "Epoch 511/750, Train Loss: 5.4608677335605496, Val Loss: 6.957300103733153, Val MAE: 1.8104658126831055\n",
      "Epoch 512/750, Train Loss: 5.460784117583256, Val Loss: 6.957429944765057, Val MAE: 1.8138641119003296\n",
      "Epoch 513/750, Train Loss: 5.460583082126204, Val Loss: 6.957389943861976, Val MAE: 1.8094879388809204\n",
      "Epoch 514/750, Train Loss: 5.4604729877156055, Val Loss: 6.9569262588446446, Val MAE: 1.811185359954834\n",
      "Epoch 515/750, Train Loss: 5.460911406984755, Val Loss: 6.957406060083673, Val MAE: 1.8091192245483398\n",
      "Epoch 516/750, Train Loss: 5.460143091724177, Val Loss: 6.956857379361515, Val MAE: 1.810196042060852\n",
      "Epoch 517/750, Train Loss: 5.4596601565172715, Val Loss: 6.9571227002692115, Val MAE: 1.8100028038024902\n",
      "Epoch 518/750, Train Loss: 5.459879145652625, Val Loss: 6.956687471524316, Val MAE: 1.8095015287399292\n",
      "Epoch 519/750, Train Loss: 5.459863615947165, Val Loss: 6.956516125840826, Val MAE: 1.8101551532745361\n",
      "Epoch 520/750, Train Loss: 5.459884737099811, Val Loss: 6.956812347737508, Val MAE: 1.8161566257476807\n",
      "Epoch 521/750, Train Loss: 5.459668928498675, Val Loss: 6.95688063402662, Val MAE: 1.8114333152770996\n",
      "Epoch 522/750, Train Loss: 5.45958434047213, Val Loss: 6.9566099791885385, Val MAE: 1.8128001689910889\n",
      "Epoch 523/750, Train Loss: 5.459307963984787, Val Loss: 6.956101923347482, Val MAE: 1.8118749856948853\n",
      "Epoch 524/750, Train Loss: 5.459193314412597, Val Loss: 6.956383206779131, Val MAE: 1.8130372762680054\n",
      "Epoch 525/750, Train Loss: 5.45917845197544, Val Loss: 6.95617836747131, Val MAE: 1.8121161460876465\n",
      "Epoch 526/750, Train Loss: 5.4587943581259175, Val Loss: 6.956055270432685, Val MAE: 1.811893343925476\n",
      "Epoch 527/750, Train Loss: 5.45916212349181, Val Loss: 6.955405129861802, Val MAE: 1.8125461339950562\n",
      "Epoch 528/750, Train Loss: 5.458661328151727, Val Loss: 6.955668022389586, Val MAE: 1.8139171600341797\n",
      "Epoch 529/750, Train Loss: 5.45868493280593, Val Loss: 6.956037182219656, Val MAE: 1.8095349073410034\n",
      "Epoch 530/750, Train Loss: 5.4586878514593575, Val Loss: 6.955805716972458, Val MAE: 1.8123036623001099\n",
      "Epoch 531/750, Train Loss: 5.458400372183247, Val Loss: 6.9554848652807655, Val MAE: 1.811919927597046\n",
      "Epoch 532/750, Train Loss: 5.458219117875312, Val Loss: 6.9559418609454955, Val MAE: 1.807997465133667\n",
      "Epoch 533/750, Train Loss: 5.4584162110735654, Val Loss: 6.955751704347289, Val MAE: 1.8126238584518433\n",
      "Epoch 534/750, Train Loss: 5.458569014907643, Val Loss: 6.955730795563935, Val MAE: 1.8118760585784912\n",
      "Epoch 535/750, Train Loss: 5.457995402281451, Val Loss: 6.955854123649277, Val MAE: 1.8108782768249512\n",
      "Epoch 536/750, Train Loss: 5.458329680011531, Val Loss: 6.9565035533727215, Val MAE: 1.8084899187088013\n",
      "Epoch 537/750, Train Loss: 5.457907499932939, Val Loss: 6.956235684394244, Val MAE: 1.8071222305297852\n",
      "Epoch 538/750, Train Loss: 5.458063139885095, Val Loss: 6.9557144734857985, Val MAE: 1.8087724447250366\n",
      "Epoch 539/750, Train Loss: 5.457741603729831, Val Loss: 6.956352529798404, Val MAE: 1.8042782545089722\n",
      "Epoch 540/750, Train Loss: 5.457963156851993, Val Loss: 6.955275446343229, Val MAE: 1.8088518381118774\n",
      "Epoch 541/750, Train Loss: 5.457376532190165, Val Loss: 6.955499503351724, Val MAE: 1.8100889921188354\n",
      "Epoch 542/750, Train Loss: 5.457583591436884, Val Loss: 6.955749732355956, Val MAE: 1.8078001737594604\n",
      "Epoch 543/750, Train Loss: 5.457161862075709, Val Loss: 6.955132711630123, Val MAE: 1.8143317699432373\n",
      "Epoch 544/750, Train Loss: 5.457622091633499, Val Loss: 6.955408944373845, Val MAE: 1.8080347776412964\n",
      "Epoch 545/750, Train Loss: 5.457135637246879, Val Loss: 6.954926219432528, Val MAE: 1.8090204000473022\n",
      "Epoch 546/750, Train Loss: 5.457256561935328, Val Loss: 6.954430333496696, Val MAE: 1.8102606534957886\n",
      "Epoch 547/750, Train Loss: 5.457173236919816, Val Loss: 6.954875355147664, Val MAE: 1.8099260330200195\n",
      "Epoch 548/750, Train Loss: 5.456868567740082, Val Loss: 6.954350853573839, Val MAE: 1.8084880113601685\n",
      "Epoch 549/750, Train Loss: 5.4565222858623335, Val Loss: 6.9541584476689655, Val MAE: 1.8054254055023193\n",
      "Epoch 550/750, Train Loss: 5.456972927652346, Val Loss: 6.954855487130309, Val MAE: 1.8062502145767212\n",
      "Epoch 551/750, Train Loss: 5.456412200563273, Val Loss: 6.954368268354403, Val MAE: 1.807512640953064\n",
      "Epoch 552/750, Train Loss: 5.4566208638962665, Val Loss: 6.953713417164356, Val MAE: 1.8106921911239624\n",
      "Epoch 553/750, Train Loss: 5.456309288778122, Val Loss: 6.953593590120264, Val MAE: 1.8134914636611938\n",
      "Epoch 554/750, Train Loss: 5.456736283089705, Val Loss: 6.953441406496569, Val MAE: 1.812635064125061\n",
      "Epoch 555/750, Train Loss: 5.455966415526761, Val Loss: 6.952961323867309, Val MAE: 1.8103693723678589\n",
      "Epoch 556/750, Train Loss: 5.456012410267143, Val Loss: 6.953086724260566, Val MAE: 1.8104783296585083\n",
      "Epoch 557/750, Train Loss: 5.455769452016065, Val Loss: 6.953908791891753, Val MAE: 1.8065640926361084\n",
      "Epoch 558/750, Train Loss: 5.45606715937329, Val Loss: 6.953340054002884, Val MAE: 1.806626796722412\n",
      "Epoch 559/750, Train Loss: 5.455627427435225, Val Loss: 6.9527350105507315, Val MAE: 1.8085880279541016\n",
      "Epoch 560/750, Train Loss: 5.455678811042931, Val Loss: 6.952848457823339, Val MAE: 1.810487985610962\n",
      "Epoch 561/750, Train Loss: 5.455358817471061, Val Loss: 6.952704987665228, Val MAE: 1.810967206954956\n",
      "Epoch 562/750, Train Loss: 5.455248738550077, Val Loss: 6.9529139073536665, Val MAE: 1.8079209327697754\n",
      "Epoch 563/750, Train Loss: 5.455294823494686, Val Loss: 6.951986670716347, Val MAE: 1.8081883192062378\n",
      "Epoch 564/750, Train Loss: 5.455198846197432, Val Loss: 6.952765002874501, Val MAE: 1.8076387643814087\n",
      "Epoch 565/750, Train Loss: 5.455563492987566, Val Loss: 6.952548090903339, Val MAE: 1.8101457357406616\n",
      "Epoch 566/750, Train Loss: 5.455289156725452, Val Loss: 6.952775192305196, Val MAE: 1.8080577850341797\n",
      "Epoch 567/750, Train Loss: 5.455202121187927, Val Loss: 6.952492708334499, Val MAE: 1.811031460762024\n",
      "Epoch 568/750, Train Loss: 5.454796325780784, Val Loss: 6.953218378214602, Val MAE: 1.8080527782440186\n",
      "Epoch 569/750, Train Loss: 5.454858399652371, Val Loss: 6.952732739017349, Val MAE: 1.8111295700073242\n",
      "Epoch 570/750, Train Loss: 5.454765837967016, Val Loss: 6.953245391047023, Val MAE: 1.8084304332733154\n",
      "Epoch 571/750, Train Loss: 5.454602462015334, Val Loss: 6.952728659933291, Val MAE: 1.8077552318572998\n",
      "Epoch 572/750, Train Loss: 5.4544415492161065, Val Loss: 6.952527866665808, Val MAE: 1.8079712390899658\n",
      "Epoch 573/750, Train Loss: 5.454840097154022, Val Loss: 6.9528220572628685, Val MAE: 1.8051544427871704\n",
      "Epoch 574/750, Train Loss: 5.454597705185034, Val Loss: 6.952084553471406, Val MAE: 1.8065245151519775\n",
      "Epoch 575/750, Train Loss: 5.454023076318632, Val Loss: 6.951864747100176, Val MAE: 1.8110157251358032\n",
      "Epoch 576/750, Train Loss: 5.454165110618446, Val Loss: 6.951851518869844, Val MAE: 1.8097202777862549\n",
      "Epoch 577/750, Train Loss: 5.454138597105719, Val Loss: 6.95211285812059, Val MAE: 1.8083575963974\n",
      "Epoch 578/750, Train Loss: 5.45436078727625, Val Loss: 6.951669815832817, Val MAE: 1.8102045059204102\n",
      "Epoch 579/750, Train Loss: 5.45391643457352, Val Loss: 6.95145026958826, Val MAE: 1.8091977834701538\n",
      "Epoch 580/750, Train Loss: 5.454038019544759, Val Loss: 6.951871768927411, Val MAE: 1.8091788291931152\n",
      "Epoch 581/750, Train Loss: 5.453717902967125, Val Loss: 6.95165642816284, Val MAE: 1.8099629878997803\n",
      "Epoch 582/750, Train Loss: 5.453584904883318, Val Loss: 6.951903438627387, Val MAE: 1.807987928390503\n",
      "Epoch 583/750, Train Loss: 5.453756907183653, Val Loss: 6.951263011539108, Val MAE: 1.8080389499664307\n",
      "Epoch 584/750, Train Loss: 5.453500696656051, Val Loss: 6.951478925790011, Val MAE: 1.807712435722351\n",
      "Epoch 585/750, Train Loss: 5.453523885520401, Val Loss: 6.951552981171569, Val MAE: 1.8040740489959717\n",
      "Epoch 586/750, Train Loss: 5.453432419649355, Val Loss: 6.951818710458138, Val MAE: 1.8059078454971313\n",
      "Epoch 587/750, Train Loss: 5.453692620878766, Val Loss: 6.951559111225894, Val MAE: 1.8068681955337524\n",
      "Epoch 588/750, Train Loss: 5.4535997360375275, Val Loss: 6.951896183530761, Val MAE: 1.805854320526123\n",
      "Epoch 589/750, Train Loss: 5.453165262671793, Val Loss: 6.951030213689123, Val MAE: 1.8074204921722412\n",
      "Epoch 590/750, Train Loss: 5.453169930816456, Val Loss: 6.951345968498363, Val MAE: 1.805092215538025\n",
      "Epoch 591/750, Train Loss: 5.4528098829232965, Val Loss: 6.951438096311983, Val MAE: 1.8062609434127808\n",
      "Epoch 592/750, Train Loss: 5.452941862033431, Val Loss: 6.951130421136462, Val MAE: 1.8108203411102295\n",
      "Epoch 593/750, Train Loss: 5.452808358866697, Val Loss: 6.951226901978421, Val MAE: 1.8098516464233398\n",
      "Epoch 594/750, Train Loss: 5.45249580759911, Val Loss: 6.951358735968861, Val MAE: 1.8065201044082642\n",
      "Epoch 595/750, Train Loss: 5.453012959668591, Val Loss: 6.951117847260664, Val MAE: 1.8049150705337524\n",
      "Epoch 596/750, Train Loss: 5.4525122156568395, Val Loss: 6.951026060701231, Val MAE: 1.8102831840515137\n",
      "Epoch 597/750, Train Loss: 5.452205743000007, Val Loss: 6.951034089102369, Val MAE: 1.806276798248291\n",
      "Epoch 598/750, Train Loss: 5.452608015886538, Val Loss: 6.9503180589862605, Val MAE: 1.8061590194702148\n",
      "Epoch 599/750, Train Loss: 5.452562075207947, Val Loss: 6.9511340849143215, Val MAE: 1.802951693534851\n",
      "Epoch 600/750, Train Loss: 5.45248542591265, Val Loss: 6.950609237900278, Val MAE: 1.806877613067627\n",
      "Epoch 601/750, Train Loss: 5.452304726497383, Val Loss: 6.950456841455772, Val MAE: 1.8105186223983765\n",
      "Epoch 602/750, Train Loss: 5.451996470408835, Val Loss: 6.950401126412296, Val MAE: 1.811061143875122\n",
      "Epoch 603/750, Train Loss: 5.451812696760627, Val Loss: 6.949708159552219, Val MAE: 1.8111590147018433\n",
      "Epoch 604/750, Train Loss: 5.451972710251049, Val Loss: 6.949928832320858, Val MAE: 1.809073805809021\n",
      "Epoch 605/750, Train Loss: 5.451595481945451, Val Loss: 6.950255495780295, Val MAE: 1.808288812637329\n",
      "Epoch 606/750, Train Loss: 5.4515870243121105, Val Loss: 6.949780427498281, Val MAE: 1.8095663785934448\n",
      "Epoch 607/750, Train Loss: 5.452081080151212, Val Loss: 6.950042613327614, Val MAE: 1.8062618970870972\n",
      "Epoch 608/750, Train Loss: 5.4519782473327245, Val Loss: 6.949449034638105, Val MAE: 1.8094301223754883\n",
      "Epoch 609/750, Train Loss: 5.451899170268113, Val Loss: 6.949798920348685, Val MAE: 1.8090382814407349\n",
      "Epoch 610/750, Train Loss: 5.4511502662282085, Val Loss: 6.95017933141674, Val MAE: 1.8066394329071045\n",
      "Epoch 611/750, Train Loss: 5.451251726089769, Val Loss: 6.949532744305115, Val MAE: 1.806249737739563\n",
      "Epoch 612/750, Train Loss: 5.451160374720385, Val Loss: 6.950093443901959, Val MAE: 1.8055083751678467\n",
      "Epoch 613/750, Train Loss: 5.451025321377311, Val Loss: 6.949419443580658, Val MAE: 1.8087596893310547\n",
      "Epoch 614/750, Train Loss: 5.451461851520903, Val Loss: 6.949220075230928, Val MAE: 1.810451865196228\n",
      "Epoch 615/750, Train Loss: 5.451202393792997, Val Loss: 6.949696969067258, Val MAE: 1.8108937740325928\n",
      "Epoch 616/750, Train Loss: 5.450763017812352, Val Loss: 6.949754088839216, Val MAE: 1.8078399896621704\n",
      "Epoch 617/750, Train Loss: 5.4508833967196715, Val Loss: 6.949257276679063, Val MAE: 1.8089497089385986\n",
      "Epoch 618/750, Train Loss: 5.451056896501286, Val Loss: 6.949779012915953, Val MAE: 1.8075015544891357\n",
      "Epoch 619/750, Train Loss: 5.450564751351715, Val Loss: 6.948971553868281, Val MAE: 1.811525821685791\n",
      "Epoch 620/750, Train Loss: 5.451146656844267, Val Loss: 6.949699619936602, Val MAE: 1.8061376810073853\n",
      "Epoch 621/750, Train Loss: 5.450698788606437, Val Loss: 6.94917467130942, Val MAE: 1.8111697435379028\n",
      "Epoch 622/750, Train Loss: 5.450579747121045, Val Loss: 6.949289063254523, Val MAE: 1.8089569807052612\n",
      "Epoch 623/750, Train Loss: 5.450406570799032, Val Loss: 6.949599588413725, Val MAE: 1.806868076324463\n",
      "Epoch 624/750, Train Loss: 5.4508362454213914, Val Loss: 6.948864881706356, Val MAE: 1.805609941482544\n",
      "Epoch 625/750, Train Loss: 5.450343253809935, Val Loss: 6.9484688418114535, Val MAE: 1.8055499792099\n",
      "Epoch 626/750, Train Loss: 5.450391819522639, Val Loss: 6.948834525271926, Val MAE: 1.8069946765899658\n",
      "Epoch 627/750, Train Loss: 5.450197385071189, Val Loss: 6.9482777789188805, Val MAE: 1.8071070909500122\n",
      "Epoch 628/750, Train Loss: 5.4502971217890455, Val Loss: 6.9483633200673305, Val MAE: 1.8068292140960693\n",
      "Epoch 629/750, Train Loss: 5.450293921209445, Val Loss: 6.948203686715061, Val MAE: 1.8100868463516235\n",
      "Epoch 630/750, Train Loss: 5.4500385633699455, Val Loss: 6.948039037718989, Val MAE: 1.8072456121444702\n",
      "Epoch 631/750, Train Loss: 5.449797256129562, Val Loss: 6.947951820935427, Val MAE: 1.809032678604126\n",
      "Epoch 632/750, Train Loss: 5.449913499917194, Val Loss: 6.9485915428440785, Val MAE: 1.8053642511367798\n",
      "Epoch 633/750, Train Loss: 5.449681893731379, Val Loss: 6.948257042170312, Val MAE: 1.8076506853103638\n",
      "Epoch 634/750, Train Loss: 5.449516092895702, Val Loss: 6.948002955941697, Val MAE: 1.8096261024475098\n",
      "Epoch 635/750, Train Loss: 5.449719850576607, Val Loss: 6.947899375145594, Val MAE: 1.8091861009597778\n",
      "Epoch 636/750, Train Loss: 5.449729481472331, Val Loss: 6.947348863979243, Val MAE: 1.8099894523620605\n",
      "Epoch 637/750, Train Loss: 5.450036924811685, Val Loss: 6.94780203613011, Val MAE: 1.8079358339309692\n",
      "Epoch 638/750, Train Loss: 5.449237808907867, Val Loss: 6.948077471823778, Val MAE: 1.8051074743270874\n",
      "Epoch 639/750, Train Loss: 5.449385840118311, Val Loss: 6.947073592689187, Val MAE: 1.8097789287567139\n",
      "Epoch 640/750, Train Loss: 5.449565453134524, Val Loss: 6.947858478431749, Val MAE: 1.803810954093933\n",
      "Epoch 641/750, Train Loss: 5.449158684918835, Val Loss: 6.947661847376246, Val MAE: 1.8059477806091309\n",
      "Epoch 642/750, Train Loss: 5.449413409506439, Val Loss: 6.947769067093332, Val MAE: 1.8064782619476318\n",
      "Epoch 643/750, Train Loss: 5.449004576464367, Val Loss: 6.947866911396171, Val MAE: 1.8090157508850098\n",
      "Epoch 644/750, Train Loss: 5.448493361169366, Val Loss: 6.948028993058308, Val MAE: 1.8071633577346802\n",
      "Epoch 645/750, Train Loss: 5.448775231914156, Val Loss: 6.9475099212861045, Val MAE: 1.8092832565307617\n",
      "Epoch 646/750, Train Loss: 5.448987531965705, Val Loss: 6.947373685257416, Val MAE: 1.8090671300888062\n",
      "Epoch 647/750, Train Loss: 5.449177641625617, Val Loss: 6.947737781545107, Val MAE: 1.8095473051071167\n",
      "Epoch 648/750, Train Loss: 5.449084881460591, Val Loss: 6.9472694271021265, Val MAE: 1.8070803880691528\n",
      "Epoch 649/750, Train Loss: 5.448221705977325, Val Loss: 6.94697795405456, Val MAE: 1.8115944862365723\n",
      "Epoch 650/750, Train Loss: 5.448482819575413, Val Loss: 6.947429848798985, Val MAE: 1.806272268295288\n",
      "Epoch 651/750, Train Loss: 5.448516120880273, Val Loss: 6.94741357491578, Val MAE: 1.808777928352356\n",
      "Epoch 652/750, Train Loss: 5.448522543451588, Val Loss: 6.9471908381372005, Val MAE: 1.8076590299606323\n",
      "Epoch 653/750, Train Loss: 5.44823362842487, Val Loss: 6.94668102049546, Val MAE: 1.8059954643249512\n",
      "Epoch 654/750, Train Loss: 5.448333061121072, Val Loss: 6.947431445418121, Val MAE: 1.8103259801864624\n",
      "Epoch 655/750, Train Loss: 5.44856292836985, Val Loss: 6.947614874730131, Val MAE: 1.8078469038009644\n",
      "Epoch 656/750, Train Loss: 5.447837328151532, Val Loss: 6.9477729901087955, Val MAE: 1.8046553134918213\n",
      "Epoch 657/750, Train Loss: 5.448337414613954, Val Loss: 6.9468178385307215, Val MAE: 1.808073878288269\n",
      "Epoch 658/750, Train Loss: 5.448319979382169, Val Loss: 6.946943634040909, Val MAE: 1.8096674680709839\n",
      "Epoch 659/750, Train Loss: 5.4477610996954, Val Loss: 6.946555273513605, Val MAE: 1.8070409297943115\n",
      "Epoch 660/750, Train Loss: 5.44773314591426, Val Loss: 6.94669633440678, Val MAE: 1.8061933517456055\n",
      "Epoch 661/750, Train Loss: 5.447806897740455, Val Loss: 6.946818217533282, Val MAE: 1.807080626487732\n",
      "Epoch 662/750, Train Loss: 5.447817431285882, Val Loss: 6.946601625182304, Val MAE: 1.8070721626281738\n",
      "Epoch 663/750, Train Loss: 5.447427852897887, Val Loss: 6.946420296480228, Val MAE: 1.8095718622207642\n",
      "Epoch 664/750, Train Loss: 5.447488144400773, Val Loss: 6.945824437144382, Val MAE: 1.808031439781189\n",
      "Epoch 665/750, Train Loss: 5.4480866738945055, Val Loss: 6.946270776654416, Val MAE: 1.808495283126831\n",
      "Epoch 666/750, Train Loss: 5.4472773606610145, Val Loss: 6.946321693394331, Val MAE: 1.8050090074539185\n",
      "Epoch 667/750, Train Loss: 5.447219744153843, Val Loss: 6.946454873138366, Val MAE: 1.8035091161727905\n",
      "Epoch 668/750, Train Loss: 5.447246074904302, Val Loss: 6.946045525626709, Val MAE: 1.8043441772460938\n",
      "Epoch 669/750, Train Loss: 5.447210134518374, Val Loss: 6.946388979036664, Val MAE: 1.803357720375061\n",
      "Epoch 670/750, Train Loss: 5.447650771991462, Val Loss: 6.945612237422344, Val MAE: 1.8069288730621338\n",
      "Epoch 671/750, Train Loss: 5.447011684004668, Val Loss: 6.94555483892736, Val MAE: 1.8066751956939697\n",
      "Epoch 672/750, Train Loss: 5.446585175489924, Val Loss: 6.945310140263597, Val MAE: 1.810678482055664\n",
      "Epoch 673/750, Train Loss: 5.44678463206929, Val Loss: 6.945354866492519, Val MAE: 1.806444525718689\n",
      "Epoch 674/750, Train Loss: 5.446987907142396, Val Loss: 6.945507088160944, Val MAE: 1.8040885925292969\n",
      "Epoch 675/750, Train Loss: 5.446800870956129, Val Loss: 6.945729549068927, Val MAE: 1.8063842058181763\n",
      "Epoch 676/750, Train Loss: 5.4466236706751925, Val Loss: 6.945431232896927, Val MAE: 1.8080716133117676\n",
      "Epoch 677/750, Train Loss: 5.4465393570578025, Val Loss: 6.945544705767302, Val MAE: 1.8070971965789795\n",
      "Epoch 678/750, Train Loss: 5.447180776535326, Val Loss: 6.9451667975757045, Val MAE: 1.8061190843582153\n",
      "Epoch 679/750, Train Loss: 5.446285619553487, Val Loss: 6.94544319709389, Val MAE: 1.8075755834579468\n",
      "Epoch 680/750, Train Loss: 5.446763768317593, Val Loss: 6.945283459016742, Val MAE: 1.8111764192581177\n",
      "Epoch 681/750, Train Loss: 5.446365572085046, Val Loss: 6.94494548460354, Val MAE: 1.8088288307189941\n",
      "Epoch 682/750, Train Loss: 5.446500781235422, Val Loss: 6.944965983415109, Val MAE: 1.8080542087554932\n",
      "Epoch 683/750, Train Loss: 5.446060835935508, Val Loss: 6.94464473164252, Val MAE: 1.8072322607040405\n",
      "Epoch 684/750, Train Loss: 5.445811218516842, Val Loss: 6.944583794246778, Val MAE: 1.8080089092254639\n",
      "Epoch 685/750, Train Loss: 5.446134349494983, Val Loss: 6.944903438990719, Val MAE: 1.8028918504714966\n",
      "Epoch 686/750, Train Loss: 5.445912980122172, Val Loss: 6.945226352358546, Val MAE: 1.8029358386993408\n",
      "Epoch 687/750, Train Loss: 5.446190408840301, Val Loss: 6.943971591532193, Val MAE: 1.8055193424224854\n",
      "Epoch 688/750, Train Loss: 5.446361079185632, Val Loss: 6.944729462219662, Val MAE: 1.808200716972351\n",
      "Epoch 689/750, Train Loss: 5.445706087161022, Val Loss: 6.944130354486243, Val MAE: 1.8066986799240112\n",
      "Epoch 690/750, Train Loss: 5.445812972487917, Val Loss: 6.944340612223017, Val MAE: 1.8056656122207642\n",
      "Epoch 691/750, Train Loss: 5.445768720177329, Val Loss: 6.944194829708453, Val MAE: 1.8043280839920044\n",
      "Epoch 692/750, Train Loss: 5.445975227720418, Val Loss: 6.944366517312932, Val MAE: 1.8022457361221313\n",
      "Epoch 693/750, Train Loss: 5.446146824101733, Val Loss: 6.943972525276554, Val MAE: 1.8099913597106934\n",
      "Epoch 694/750, Train Loss: 5.445560296401856, Val Loss: 6.943529094058286, Val MAE: 1.8084031343460083\n",
      "Epoch 695/750, Train Loss: 5.445887109732173, Val Loss: 6.943657516432964, Val MAE: 1.808371901512146\n",
      "Epoch 696/750, Train Loss: 5.445501624854507, Val Loss: 6.943792037040717, Val MAE: 1.804623007774353\n",
      "Epoch 697/750, Train Loss: 5.445295361500637, Val Loss: 6.944141818026417, Val MAE: 1.806858777999878\n",
      "Epoch 698/750, Train Loss: 5.445077956558033, Val Loss: 6.9435361957164785, Val MAE: 1.8067024946212769\n",
      "Epoch 699/750, Train Loss: 5.445904681333311, Val Loss: 6.943625349780652, Val MAE: 1.8050365447998047\n",
      "Epoch 700/750, Train Loss: 5.445474864570958, Val Loss: 6.943500910248351, Val MAE: 1.8073389530181885\n",
      "Epoch 701/750, Train Loss: 5.445036991994092, Val Loss: 6.943726842006295, Val MAE: 1.8059817552566528\n",
      "Epoch 702/750, Train Loss: 5.445295018603088, Val Loss: 6.943262992474216, Val MAE: 1.8070731163024902\n",
      "Epoch 703/750, Train Loss: 5.445119884818983, Val Loss: 6.9433990348341155, Val MAE: 1.8093560934066772\n",
      "Epoch 704/750, Train Loss: 5.444912527776827, Val Loss: 6.942967281643836, Val MAE: 1.809565782546997\n",
      "Epoch 705/750, Train Loss: 5.44554216846539, Val Loss: 6.942967298832497, Val MAE: 1.8066673278808594\n",
      "Epoch 706/750, Train Loss: 5.444823781699891, Val Loss: 6.943570077382237, Val MAE: 1.806315302848816\n",
      "Epoch 707/750, Train Loss: 5.444540151365244, Val Loss: 6.942747568513682, Val MAE: 1.8092420101165771\n",
      "Epoch 708/750, Train Loss: 5.444417250687909, Val Loss: 6.942568772471862, Val MAE: 1.8107503652572632\n",
      "Epoch 709/750, Train Loss: 5.4444689808377795, Val Loss: 6.942921779998865, Val MAE: 1.807934284210205\n",
      "Epoch 710/750, Train Loss: 5.444544696959721, Val Loss: 6.943322945152328, Val MAE: 1.8041229248046875\n",
      "Epoch 711/750, Train Loss: 5.444401065559144, Val Loss: 6.942484383556397, Val MAE: 1.8069137334823608\n",
      "Epoch 712/750, Train Loss: 5.444849846469369, Val Loss: 6.942928805011929, Val MAE: 1.8016910552978516\n",
      "Epoch 713/750, Train Loss: 5.44434547333201, Val Loss: 6.942713985500904, Val MAE: 1.808247447013855\n",
      "Epoch 714/750, Train Loss: 5.444288482180067, Val Loss: 6.942065169012569, Val MAE: 1.8062297105789185\n",
      "Epoch 715/750, Train Loss: 5.4445115927678005, Val Loss: 6.942556245864882, Val MAE: 1.8065232038497925\n",
      "Epoch 716/750, Train Loss: 5.444124045949073, Val Loss: 6.942395020116553, Val MAE: 1.8075922727584839\n",
      "Epoch 717/750, Train Loss: 5.444277185998905, Val Loss: 6.9424141895111635, Val MAE: 1.8055305480957031\n",
      "Epoch 718/750, Train Loss: 5.444246587328091, Val Loss: 6.942292836010271, Val MAE: 1.804871678352356\n",
      "Epoch 719/750, Train Loss: 5.444067814091968, Val Loss: 6.942384595230842, Val MAE: 1.8077783584594727\n",
      "Epoch 720/750, Train Loss: 5.444037883296894, Val Loss: 6.942954545068474, Val MAE: 1.8040140867233276\n",
      "Epoch 721/750, Train Loss: 5.4443237584108, Val Loss: 6.941860488885082, Val MAE: 1.8105908632278442\n",
      "Epoch 722/750, Train Loss: 5.4435764525346695, Val Loss: 6.941801356113498, Val MAE: 1.808183193206787\n",
      "Epoch 723/750, Train Loss: 5.443835521807337, Val Loss: 6.942592359833869, Val MAE: 1.8065427541732788\n",
      "Epoch 724/750, Train Loss: 5.443611661340021, Val Loss: 6.942861900780051, Val MAE: 1.8037391901016235\n",
      "Epoch 725/750, Train Loss: 5.443702565181027, Val Loss: 6.942513823101726, Val MAE: 1.806148886680603\n",
      "Epoch 726/750, Train Loss: 5.443845763479828, Val Loss: 6.942231700759708, Val MAE: 1.8087506294250488\n",
      "Epoch 727/750, Train Loss: 5.443409432575201, Val Loss: 6.942153896768319, Val MAE: 1.808620810508728\n",
      "Epoch 728/750, Train Loss: 5.4433635972867345, Val Loss: 6.941793354865999, Val MAE: 1.8099384307861328\n",
      "Epoch 729/750, Train Loss: 5.4434041756733205, Val Loss: 6.941614263060672, Val MAE: 1.8088592290878296\n",
      "Epoch 730/750, Train Loss: 5.443788684249683, Val Loss: 6.94156746511655, Val MAE: 1.8109025955200195\n",
      "Epoch 731/750, Train Loss: 5.443124055558709, Val Loss: 6.941625642139323, Val MAE: 1.8063551187515259\n",
      "Epoch 732/750, Train Loss: 5.442820197305862, Val Loss: 6.941513972411324, Val MAE: 1.8073827028274536\n",
      "Epoch 733/750, Train Loss: 5.443248332685726, Val Loss: 6.9418503847679345, Val MAE: 1.8054434061050415\n",
      "Epoch 734/750, Train Loss: 5.443115100131672, Val Loss: 6.941995286585901, Val MAE: 1.8038705587387085\n",
      "Epoch 735/750, Train Loss: 5.443106346980781, Val Loss: 6.941930574612116, Val MAE: 1.804980993270874\n",
      "Epoch 736/750, Train Loss: 5.442899467079503, Val Loss: 6.941517927766661, Val MAE: 1.8077889680862427\n",
      "Epoch 737/750, Train Loss: 5.443089475449483, Val Loss: 6.941907604634206, Val MAE: 1.8112825155258179\n",
      "Epoch 738/750, Train Loss: 5.443455996786713, Val Loss: 6.941981886024427, Val MAE: 1.8067712783813477\n",
      "Epoch 739/750, Train Loss: 5.442815524909147, Val Loss: 6.941789710203115, Val MAE: 1.8077781200408936\n",
      "Epoch 740/750, Train Loss: 5.442455976328273, Val Loss: 6.941227258844654, Val MAE: 1.8104912042617798\n",
      "Epoch 741/750, Train Loss: 5.442876066827471, Val Loss: 6.941112587139279, Val MAE: 1.8070231676101685\n",
      "Epoch 742/750, Train Loss: 5.442733334583841, Val Loss: 6.941221436556265, Val MAE: 1.807341456413269\n",
      "Epoch 743/750, Train Loss: 5.442540600649111, Val Loss: 6.941031255805026, Val MAE: 1.8063490390777588\n",
      "Epoch 744/750, Train Loss: 5.442403025536021, Val Loss: 6.941204037260313, Val MAE: 1.8052431344985962\n",
      "Epoch 745/750, Train Loss: 5.442392908084165, Val Loss: 6.9408526973297615, Val MAE: 1.8084568977355957\n",
      "Epoch 746/750, Train Loss: 5.442352005174965, Val Loss: 6.941335876325852, Val MAE: 1.8038381338119507\n",
      "Epoch 747/750, Train Loss: 5.4423275595258, Val Loss: 6.94119809213227, Val MAE: 1.8082395792007446\n",
      "Epoch 748/750, Train Loss: 5.442680442105433, Val Loss: 6.94088003415362, Val MAE: 1.8064121007919312\n",
      "Epoch 749/750, Train Loss: 5.442188480705212, Val Loss: 6.939954662560083, Val MAE: 1.807525873184204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 1/3 [08:45<17:30, 525.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 750/750, Train Loss: 5.441948582716049, Val Loss: 6.94066498648535, Val MAE: 1.8039360046386719\n",
      "Test Loss (MSE): 5.533347129821777\n",
      "Test Mean Absolute Error (MAE): 1.539722626032786\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 401, 'position': 'DEF', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'medium', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 401\n",
      "position DEF\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features medium\n",
      "stratify_by stdev\n",
      "Running Iteration:  1\n",
      "Epoch 1/750, Train Loss: 9.928848642345665, Val Loss: 9.529692681382423, Val MAE: 1.5676394701004028\n",
      "Epoch 2/750, Train Loss: 9.53886027005108, Val Loss: 9.153834059904627, Val MAE: 1.557379126548767\n",
      "Epoch 3/750, Train Loss: 9.141665164644529, Val Loss: 8.769759309476225, Val MAE: 1.5530095100402832\n",
      "Epoch 4/750, Train Loss: 8.736968941655734, Val Loss: 8.383147906359202, Val MAE: 1.546179175376892\n",
      "Epoch 5/750, Train Loss: 8.335284416790378, Val Loss: 8.003614906584888, Val MAE: 1.537574291229248\n",
      "Epoch 6/750, Train Loss: 7.950398266501841, Val Loss: 7.6489620531014175, Val MAE: 1.5331605672836304\n",
      "Epoch 7/750, Train Loss: 7.598638909395372, Val Loss: 7.332051913611586, Val MAE: 1.5407588481903076\n",
      "Epoch 8/750, Train Loss: 7.291518124585751, Val Loss: 7.069595813190047, Val MAE: 1.5592570304870605\n",
      "Epoch 9/750, Train Loss: 7.035132206254187, Val Loss: 6.8527861649988475, Val MAE: 1.5827423334121704\n",
      "Epoch 10/750, Train Loss: 6.831587921462259, Val Loss: 6.687480756959101, Val MAE: 1.606911301612854\n",
      "Epoch 11/750, Train Loss: 6.67729216609022, Val Loss: 6.5650933817399775, Val MAE: 1.6295324563980103\n",
      "Epoch 12/750, Train Loss: 6.567420561064624, Val Loss: 6.482151965607551, Val MAE: 1.6494439840316772\n",
      "Epoch 13/750, Train Loss: 6.491317372086497, Val Loss: 6.425065383465401, Val MAE: 1.6658570766448975\n",
      "Epoch 14/750, Train Loss: 6.438922215283402, Val Loss: 6.385883862409482, Val MAE: 1.674736738204956\n",
      "Epoch 15/750, Train Loss: 6.403334174177064, Val Loss: 6.35740764596435, Val MAE: 1.6831163167953491\n",
      "Epoch 16/750, Train Loss: 6.377610396861732, Val Loss: 6.336669146653864, Val MAE: 1.6887561082839966\n",
      "Epoch 17/750, Train Loss: 6.358917858393361, Val Loss: 6.32050072978468, Val MAE: 1.688096523284912\n",
      "Epoch 18/750, Train Loss: 6.3443810541679, Val Loss: 6.306336335872914, Val MAE: 1.6900783777236938\n",
      "Epoch 19/750, Train Loss: 6.331803354939645, Val Loss: 6.293510023426833, Val MAE: 1.6886200904846191\n",
      "Epoch 20/750, Train Loss: 6.3210981132240125, Val Loss: 6.282477637305705, Val MAE: 1.6857167482376099\n",
      "Epoch 21/750, Train Loss: 6.311764195533452, Val Loss: 6.2724728629183435, Val MAE: 1.6841241121292114\n",
      "Epoch 22/750, Train Loss: 6.3027840805769415, Val Loss: 6.262384769013846, Val MAE: 1.6826467514038086\n",
      "Epoch 23/750, Train Loss: 6.294683970683362, Val Loss: 6.252779974661662, Val MAE: 1.6821233034133911\n",
      "Epoch 24/750, Train Loss: 6.2870258411219, Val Loss: 6.2443900453859955, Val MAE: 1.6796343326568604\n",
      "Epoch 25/750, Train Loss: 6.279822438713012, Val Loss: 6.235000792813125, Val MAE: 1.676824688911438\n",
      "Epoch 26/750, Train Loss: 6.273146970187074, Val Loss: 6.226969144692258, Val MAE: 1.6753779649734497\n",
      "Epoch 27/750, Train Loss: 6.266154237506239, Val Loss: 6.219228230231387, Val MAE: 1.6739716529846191\n",
      "Epoch 28/750, Train Loss: 6.259444321745108, Val Loss: 6.211883133100677, Val MAE: 1.6737440824508667\n",
      "Epoch 29/750, Train Loss: 6.253072324732529, Val Loss: 6.204336994673954, Val MAE: 1.6721912622451782\n",
      "Epoch 30/750, Train Loss: 6.2467493876730975, Val Loss: 6.197018564909837, Val MAE: 1.6697885990142822\n",
      "Epoch 31/750, Train Loss: 6.240777312404592, Val Loss: 6.190313101696663, Val MAE: 1.6701619625091553\n",
      "Epoch 32/750, Train Loss: 6.234657866795262, Val Loss: 6.183985857412009, Val MAE: 1.6696280241012573\n",
      "Epoch 33/750, Train Loss: 6.229155391957925, Val Loss: 6.1770319076055795, Val MAE: 1.668528437614441\n",
      "Epoch 34/750, Train Loss: 6.224009478964457, Val Loss: 6.171191061913927, Val MAE: 1.6666339635849\n",
      "Epoch 35/750, Train Loss: 6.218518924534209, Val Loss: 6.165985885529047, Val MAE: 1.668900966644287\n",
      "Epoch 36/750, Train Loss: 6.21352408661404, Val Loss: 6.1601520899405156, Val MAE: 1.6670609712600708\n",
      "Epoch 37/750, Train Loss: 6.208715959516743, Val Loss: 6.154254439378955, Val MAE: 1.6655409336090088\n",
      "Epoch 38/750, Train Loss: 6.20381148864956, Val Loss: 6.149173159926388, Val MAE: 1.6643770933151245\n",
      "Epoch 39/750, Train Loss: 6.199505078934221, Val Loss: 6.144061350068873, Val MAE: 1.6633069515228271\n",
      "Epoch 40/750, Train Loss: 6.194937630248413, Val Loss: 6.138692636175611, Val MAE: 1.6628682613372803\n",
      "Epoch 41/750, Train Loss: 6.190671453481916, Val Loss: 6.133801085402201, Val MAE: 1.6613636016845703\n",
      "Epoch 42/750, Train Loss: 6.186725139021501, Val Loss: 6.129584288228336, Val MAE: 1.6619787216186523\n",
      "Epoch 43/750, Train Loss: 6.181959478761197, Val Loss: 6.125525797944624, Val MAE: 1.6615554094314575\n",
      "Epoch 44/750, Train Loss: 6.177867664852465, Val Loss: 6.12106332693953, Val MAE: 1.6610974073410034\n",
      "Epoch 45/750, Train Loss: 6.174138629786889, Val Loss: 6.1164970482125245, Val MAE: 1.6582978963851929\n",
      "Epoch 46/750, Train Loss: 6.169988762221537, Val Loss: 6.112340928005867, Val MAE: 1.6580579280853271\n",
      "Epoch 47/750, Train Loss: 6.166112257436785, Val Loss: 6.107781472587714, Val MAE: 1.6555203199386597\n",
      "Epoch 48/750, Train Loss: 6.162486176255198, Val Loss: 6.104410467327403, Val MAE: 1.6575316190719604\n",
      "Epoch 49/750, Train Loss: 6.159016193487705, Val Loss: 6.101241602702035, Val MAE: 1.6587706804275513\n",
      "Epoch 50/750, Train Loss: 6.155261748279908, Val Loss: 6.097629278557527, Val MAE: 1.6575182676315308\n",
      "Epoch 51/750, Train Loss: 6.151854128596035, Val Loss: 6.09402334826379, Val MAE: 1.6563266515731812\n",
      "Epoch 52/750, Train Loss: 6.148606617350814, Val Loss: 6.089769834388241, Val MAE: 1.6547101736068726\n",
      "Epoch 53/750, Train Loss: 6.144897334496627, Val Loss: 6.086560609121008, Val MAE: 1.6545484066009521\n",
      "Epoch 54/750, Train Loss: 6.141422172722927, Val Loss: 6.083551785652479, Val MAE: 1.6539242267608643\n",
      "Epoch 55/750, Train Loss: 6.138177980252398, Val Loss: 6.0806759434734, Val MAE: 1.6542878150939941\n",
      "Epoch 56/750, Train Loss: 6.134943588142323, Val Loss: 6.077274849932708, Val MAE: 1.6531888246536255\n",
      "Epoch 57/750, Train Loss: 6.131884965544719, Val Loss: 6.073816202756295, Val MAE: 1.6522849798202515\n",
      "Epoch 58/750, Train Loss: 6.128802679418548, Val Loss: 6.071289835716946, Val MAE: 1.6537516117095947\n",
      "Epoch 59/750, Train Loss: 6.125748907796587, Val Loss: 6.0680859816018256, Val MAE: 1.6509050130844116\n",
      "Epoch 60/750, Train Loss: 6.122462253856838, Val Loss: 6.065640091495219, Val MAE: 1.6526683568954468\n",
      "Epoch 61/750, Train Loss: 6.119512037905848, Val Loss: 6.062266143360792, Val MAE: 1.650699257850647\n",
      "Epoch 62/750, Train Loss: 6.1167926758509115, Val Loss: 6.0602537565128705, Val MAE: 1.6509602069854736\n",
      "Epoch 63/750, Train Loss: 6.114005176479776, Val Loss: 6.057623178348015, Val MAE: 1.650999903678894\n",
      "Epoch 64/750, Train Loss: 6.1108185509132396, Val Loss: 6.054968252146733, Val MAE: 1.6509541273117065\n",
      "Epoch 65/750, Train Loss: 6.108533856867253, Val Loss: 6.052541306937551, Val MAE: 1.651179313659668\n",
      "Epoch 66/750, Train Loss: 6.105363087179961, Val Loss: 6.050850811867242, Val MAE: 1.6530497074127197\n",
      "Epoch 67/750, Train Loss: 6.102903581097992, Val Loss: 6.047003030937312, Val MAE: 1.6495023965835571\n",
      "Epoch 68/750, Train Loss: 6.099900840892875, Val Loss: 6.045033103039222, Val MAE: 1.6494910717010498\n",
      "Epoch 69/750, Train Loss: 6.097555876225512, Val Loss: 6.043003165890245, Val MAE: 1.6489211320877075\n",
      "Epoch 70/750, Train Loss: 6.0942427731812785, Val Loss: 6.040454893843312, Val MAE: 1.6487213373184204\n",
      "Epoch 71/750, Train Loss: 6.091823184393285, Val Loss: 6.038040865646085, Val MAE: 1.6485623121261597\n",
      "Epoch 72/750, Train Loss: 6.089577511446859, Val Loss: 6.035694595143229, Val MAE: 1.647734522819519\n",
      "Epoch 73/750, Train Loss: 6.087139083416183, Val Loss: 6.033254547013319, Val MAE: 1.6474109888076782\n",
      "Epoch 74/750, Train Loss: 6.08473373309309, Val Loss: 6.031159903110075, Val MAE: 1.6460703611373901\n",
      "Epoch 75/750, Train Loss: 6.081765767706417, Val Loss: 6.029297184767784, Val MAE: 1.6461927890777588\n",
      "Epoch 76/750, Train Loss: 6.079334931048548, Val Loss: 6.026978018144352, Val MAE: 1.6450395584106445\n",
      "Epoch 77/750, Train Loss: 6.077194066849852, Val Loss: 6.024301066363347, Val MAE: 1.6436623334884644\n",
      "Epoch 78/750, Train Loss: 6.07465071317328, Val Loss: 6.022284715064997, Val MAE: 1.6440443992614746\n",
      "Epoch 79/750, Train Loss: 6.072355280524869, Val Loss: 6.020878341026409, Val MAE: 1.6436570882797241\n",
      "Epoch 80/750, Train Loss: 6.069912226965012, Val Loss: 6.018647542798287, Val MAE: 1.6431337594985962\n",
      "Epoch 81/750, Train Loss: 6.067573747163716, Val Loss: 6.017446030525049, Val MAE: 1.6443063020706177\n",
      "Epoch 82/750, Train Loss: 6.065253001887624, Val Loss: 6.015389372697996, Val MAE: 1.6435394287109375\n",
      "Epoch 83/750, Train Loss: 6.062803436921044, Val Loss: 6.013600224525077, Val MAE: 1.6441208124160767\n",
      "Epoch 84/750, Train Loss: 6.061056005872735, Val Loss: 6.011939598316303, Val MAE: 1.6442292928695679\n",
      "Epoch 85/750, Train Loss: 6.058313143708096, Val Loss: 6.01041883939693, Val MAE: 1.6439040899276733\n",
      "Epoch 86/750, Train Loss: 6.0562153697535726, Val Loss: 6.007308327678714, Val MAE: 1.6406174898147583\n",
      "Epoch 87/750, Train Loss: 6.054184299025258, Val Loss: 6.005489773852919, Val MAE: 1.6411943435668945\n",
      "Epoch 88/750, Train Loss: 6.051510155983759, Val Loss: 6.003797409673946, Val MAE: 1.6406759023666382\n",
      "Epoch 89/750, Train Loss: 6.049613190412969, Val Loss: 6.001487718946435, Val MAE: 1.6384738683700562\n",
      "Epoch 90/750, Train Loss: 6.047319191556338, Val Loss: 5.999956407158986, Val MAE: 1.6382784843444824\n",
      "Epoch 91/750, Train Loss: 6.045032349730223, Val Loss: 5.998839732056587, Val MAE: 1.6412572860717773\n",
      "Epoch 92/750, Train Loss: 6.043200836992771, Val Loss: 5.99710742747455, Val MAE: 1.6405409574508667\n",
      "Epoch 93/750, Train Loss: 6.040868272775408, Val Loss: 5.9952727178639025, Val MAE: 1.639835000038147\n",
      "Epoch 94/750, Train Loss: 6.039193542544882, Val Loss: 5.993329133455114, Val MAE: 1.6390552520751953\n",
      "Epoch 95/750, Train Loss: 6.037058416048089, Val Loss: 5.991866097325195, Val MAE: 1.6383789777755737\n",
      "Epoch 96/750, Train Loss: 6.034318937369627, Val Loss: 5.99081687295525, Val MAE: 1.6401842832565308\n",
      "Epoch 97/750, Train Loss: 6.032591025332796, Val Loss: 5.987962389401164, Val MAE: 1.6375216245651245\n",
      "Epoch 98/750, Train Loss: 6.03050779714817, Val Loss: 5.98816684828401, Val MAE: 1.6407232284545898\n",
      "Epoch 99/750, Train Loss: 6.028617281254714, Val Loss: 5.987727530385795, Val MAE: 1.6433491706848145\n",
      "Epoch 100/750, Train Loss: 6.026506515500544, Val Loss: 5.984968008013804, Val MAE: 1.6404247283935547\n",
      "Epoch 101/750, Train Loss: 6.024999042687527, Val Loss: 5.984226670374226, Val MAE: 1.6419832706451416\n",
      "Epoch 102/750, Train Loss: 6.022282615909732, Val Loss: 5.981061152226988, Val MAE: 1.6380411386489868\n",
      "Epoch 103/750, Train Loss: 6.020634644325261, Val Loss: 5.979590452871791, Val MAE: 1.6377402544021606\n",
      "Epoch 104/750, Train Loss: 6.018569136053566, Val Loss: 5.977023395942054, Val MAE: 1.634307861328125\n",
      "Epoch 105/750, Train Loss: 6.016620655071743, Val Loss: 5.97697994255266, Val MAE: 1.637911319732666\n",
      "Epoch 106/750, Train Loss: 6.0149156911586354, Val Loss: 5.974481977146588, Val MAE: 1.6353648900985718\n",
      "Epoch 107/750, Train Loss: 6.012397821282058, Val Loss: 5.974397312496522, Val MAE: 1.6371452808380127\n",
      "Epoch 108/750, Train Loss: 6.010515143231648, Val Loss: 5.973295248629025, Val MAE: 1.6369595527648926\n",
      "Epoch 109/750, Train Loss: 6.008705597955633, Val Loss: 5.9712759928167785, Val MAE: 1.6355429887771606\n",
      "Epoch 110/750, Train Loss: 6.0073586392954335, Val Loss: 5.970199244729448, Val MAE: 1.6363893747329712\n",
      "Epoch 111/750, Train Loss: 6.004781041315304, Val Loss: 5.968601376720138, Val MAE: 1.634984016418457\n",
      "Epoch 112/750, Train Loss: 6.002804411732457, Val Loss: 5.967165915254145, Val MAE: 1.6347156763076782\n",
      "Epoch 113/750, Train Loss: 6.001140605217968, Val Loss: 5.966220059567998, Val MAE: 1.635919451713562\n",
      "Epoch 114/750, Train Loss: 5.999491913159092, Val Loss: 5.965330564462666, Val MAE: 1.6355414390563965\n",
      "Epoch 115/750, Train Loss: 5.997491086550696, Val Loss: 5.962754974849447, Val MAE: 1.6321041584014893\n",
      "Epoch 116/750, Train Loss: 5.995397079281691, Val Loss: 5.962066595315452, Val MAE: 1.632627248764038\n",
      "Epoch 117/750, Train Loss: 5.993490168420578, Val Loss: 5.960469260901994, Val MAE: 1.6326693296432495\n",
      "Epoch 118/750, Train Loss: 5.992102670252062, Val Loss: 5.9600817525475, Val MAE: 1.63308846950531\n",
      "Epoch 119/750, Train Loss: 5.989863610103624, Val Loss: 5.958032741631609, Val MAE: 1.6328585147857666\n",
      "Epoch 120/750, Train Loss: 5.988101354459437, Val Loss: 5.958393218658398, Val MAE: 1.6352834701538086\n",
      "Epoch 121/750, Train Loss: 5.986441736298848, Val Loss: 5.956882541902769, Val MAE: 1.6346755027770996\n",
      "Epoch 122/750, Train Loss: 5.984323360235561, Val Loss: 5.955490196480715, Val MAE: 1.633967399597168\n",
      "Epoch 123/750, Train Loss: 5.98263606047019, Val Loss: 5.9536745455253826, Val MAE: 1.6321853399276733\n",
      "Epoch 124/750, Train Loss: 5.9811113875235815, Val Loss: 5.9526759897131365, Val MAE: 1.6324721574783325\n",
      "Epoch 125/750, Train Loss: 5.97912678441232, Val Loss: 5.950937954387011, Val MAE: 1.6307374238967896\n",
      "Epoch 126/750, Train Loss: 5.977473757578627, Val Loss: 5.950360060459668, Val MAE: 1.6313358545303345\n",
      "Epoch 127/750, Train Loss: 5.975807694213848, Val Loss: 5.948310085567918, Val MAE: 1.6292935609817505\n",
      "Epoch 128/750, Train Loss: 5.974010776474447, Val Loss: 5.946853527544967, Val MAE: 1.6290708780288696\n",
      "Epoch 129/750, Train Loss: 5.972274833951166, Val Loss: 5.946472585682269, Val MAE: 1.629896640777588\n",
      "Epoch 130/750, Train Loss: 5.971223848771125, Val Loss: 5.945055981570631, Val MAE: 1.6283483505249023\n",
      "Epoch 131/750, Train Loss: 5.969395466936313, Val Loss: 5.945109689243704, Val MAE: 1.630486011505127\n",
      "Epoch 132/750, Train Loss: 5.967557557602835, Val Loss: 5.944434700752956, Val MAE: 1.630617380142212\n",
      "Epoch 133/750, Train Loss: 5.965880346417501, Val Loss: 5.941646581699805, Val MAE: 1.6263130903244019\n",
      "Epoch 134/750, Train Loss: 5.964267300098221, Val Loss: 5.940998391409248, Val MAE: 1.6272807121276855\n",
      "Epoch 135/750, Train Loss: 5.962455097327313, Val Loss: 5.941467504575097, Val MAE: 1.6303579807281494\n",
      "Epoch 136/750, Train Loss: 5.961150994518535, Val Loss: 5.940551161685884, Val MAE: 1.6303890943527222\n",
      "Epoch 137/750, Train Loss: 5.959135118464219, Val Loss: 5.938699184926308, Val MAE: 1.6271811723709106\n",
      "Epoch 138/750, Train Loss: 5.957878612592267, Val Loss: 5.937890484658519, Val MAE: 1.6271353960037231\n",
      "Epoch 139/750, Train Loss: 5.956482591444138, Val Loss: 5.936745845798526, Val MAE: 1.626607060432434\n",
      "Epoch 140/750, Train Loss: 5.954810526611061, Val Loss: 5.936362066442083, Val MAE: 1.6278326511383057\n",
      "Epoch 141/750, Train Loss: 5.9532622318852315, Val Loss: 5.9355990125095595, Val MAE: 1.6280862092971802\n",
      "Epoch 142/750, Train Loss: 5.951961464252674, Val Loss: 5.933766775910455, Val MAE: 1.6256580352783203\n",
      "Epoch 143/750, Train Loss: 5.950470063446312, Val Loss: 5.933646668743269, Val MAE: 1.6265374422073364\n",
      "Epoch 144/750, Train Loss: 5.948913912090232, Val Loss: 5.933120937979133, Val MAE: 1.6278525590896606\n",
      "Epoch 145/750, Train Loss: 5.947490359933172, Val Loss: 5.932266866511439, Val MAE: 1.627381682395935\n",
      "Epoch 146/750, Train Loss: 5.946334411234614, Val Loss: 5.92975316772384, Val MAE: 1.623008131980896\n",
      "Epoch 147/750, Train Loss: 5.94475996799958, Val Loss: 5.930108377976793, Val MAE: 1.6257542371749878\n",
      "Epoch 148/750, Train Loss: 5.943339776068348, Val Loss: 5.929713237662402, Val MAE: 1.6266911029815674\n",
      "Epoch 149/750, Train Loss: 5.942396390356072, Val Loss: 5.928257168742259, Val MAE: 1.6264845132827759\n",
      "Epoch 150/750, Train Loss: 5.9414039096510205, Val Loss: 5.927972665791874, Val MAE: 1.6267341375350952\n",
      "Epoch 151/750, Train Loss: 5.9396294488841255, Val Loss: 5.9275828611954955, Val MAE: 1.6272765398025513\n",
      "Epoch 152/750, Train Loss: 5.938654377133941, Val Loss: 5.926944334954324, Val MAE: 1.627821683883667\n",
      "Epoch 153/750, Train Loss: 5.936891587545456, Val Loss: 5.926399322748986, Val MAE: 1.6269705295562744\n",
      "Epoch 154/750, Train Loss: 5.935991729774499, Val Loss: 5.925571664396195, Val MAE: 1.6261354684829712\n",
      "Epoch 155/750, Train Loss: 5.934882231098626, Val Loss: 5.925312114627713, Val MAE: 1.6275346279144287\n",
      "Epoch 156/750, Train Loss: 5.9337219242157975, Val Loss: 5.924487320624828, Val MAE: 1.6265095472335815\n",
      "Epoch 157/750, Train Loss: 5.932195462458875, Val Loss: 5.923357143687497, Val MAE: 1.6261337995529175\n",
      "Epoch 158/750, Train Loss: 5.931008398271934, Val Loss: 5.921931183610735, Val MAE: 1.6238538026809692\n",
      "Epoch 159/750, Train Loss: 5.929938852526084, Val Loss: 5.921787375079439, Val MAE: 1.6249656677246094\n",
      "Epoch 160/750, Train Loss: 5.928977264024974, Val Loss: 5.921391474775764, Val MAE: 1.6254949569702148\n",
      "Epoch 161/750, Train Loss: 5.927631696959896, Val Loss: 5.919491009737873, Val MAE: 1.623225450515747\n",
      "Epoch 162/750, Train Loss: 5.926429724678388, Val Loss: 5.918769214004178, Val MAE: 1.6218183040618896\n",
      "Epoch 163/750, Train Loss: 5.925650205218546, Val Loss: 5.918431621112516, Val MAE: 1.6235216856002808\n",
      "Epoch 164/750, Train Loss: 5.924161458179457, Val Loss: 5.917977039214342, Val MAE: 1.623392105102539\n",
      "Epoch 165/750, Train Loss: 5.9228329226104375, Val Loss: 5.917331258923557, Val MAE: 1.6226038932800293\n",
      "Epoch 166/750, Train Loss: 5.921926837775616, Val Loss: 5.916858059652876, Val MAE: 1.6231722831726074\n",
      "Epoch 167/750, Train Loss: 5.921004216025366, Val Loss: 5.915372659154762, Val MAE: 1.6215262413024902\n",
      "Epoch 168/750, Train Loss: 5.919964603068607, Val Loss: 5.915155567895187, Val MAE: 1.6217539310455322\n",
      "Epoch 169/750, Train Loss: 5.918873917840286, Val Loss: 5.9154899989348095, Val MAE: 1.62348210811615\n",
      "Epoch 170/750, Train Loss: 5.917820487639694, Val Loss: 5.914039824100333, Val MAE: 1.6223710775375366\n",
      "Epoch 171/750, Train Loss: 5.916507179696833, Val Loss: 5.914340449365579, Val MAE: 1.6235204935073853\n",
      "Epoch 172/750, Train Loss: 5.915943197640424, Val Loss: 5.913264818694981, Val MAE: 1.621767520904541\n",
      "Epoch 173/750, Train Loss: 5.914861015560778, Val Loss: 5.912020061748815, Val MAE: 1.6196314096450806\n",
      "Epoch 174/750, Train Loss: 5.913885909814698, Val Loss: 5.912939647940396, Val MAE: 1.6233927011489868\n",
      "Epoch 175/750, Train Loss: 5.912962630810478, Val Loss: 5.912472062649628, Val MAE: 1.6224029064178467\n",
      "Epoch 176/750, Train Loss: 5.912337487455157, Val Loss: 5.912554057035658, Val MAE: 1.6218889951705933\n",
      "Epoch 177/750, Train Loss: 5.910951016171415, Val Loss: 5.9114608074405, Val MAE: 1.6220721006393433\n",
      "Epoch 178/750, Train Loss: 5.9102754387130885, Val Loss: 5.910710244108224, Val MAE: 1.6212137937545776\n",
      "Epoch 179/750, Train Loss: 5.909601125663485, Val Loss: 5.909593175062014, Val MAE: 1.620941400527954\n",
      "Epoch 180/750, Train Loss: 5.908366812848538, Val Loss: 5.910181144940108, Val MAE: 1.6225619316101074\n",
      "Epoch 181/750, Train Loss: 5.907549303274291, Val Loss: 5.910250691134575, Val MAE: 1.6235077381134033\n",
      "Epoch 182/750, Train Loss: 5.906614249911138, Val Loss: 5.907403170260793, Val MAE: 1.617881178855896\n",
      "Epoch 183/750, Train Loss: 5.906025168148706, Val Loss: 5.907852652334253, Val MAE: 1.6188234090805054\n",
      "Epoch 184/750, Train Loss: 5.905133946975221, Val Loss: 5.90776215661029, Val MAE: 1.619189739227295\n",
      "Epoch 185/750, Train Loss: 5.903959150833216, Val Loss: 5.9062450527744, Val MAE: 1.617335557937622\n",
      "Epoch 186/750, Train Loss: 5.903474491637077, Val Loss: 5.906647652487829, Val MAE: 1.6189777851104736\n",
      "Epoch 187/750, Train Loss: 5.902718035857181, Val Loss: 5.906752116742676, Val MAE: 1.6212139129638672\n",
      "Epoch 188/750, Train Loss: 5.9021299280473185, Val Loss: 5.9071332436723845, Val MAE: 1.6213730573654175\n",
      "Epoch 189/750, Train Loss: 5.900862217247672, Val Loss: 5.906148907516174, Val MAE: 1.6197954416275024\n",
      "Epoch 190/750, Train Loss: 5.899764539004118, Val Loss: 5.905809335686171, Val MAE: 1.6198463439941406\n",
      "Epoch 191/750, Train Loss: 5.899466318067272, Val Loss: 5.905687009999635, Val MAE: 1.620503306388855\n",
      "Epoch 192/750, Train Loss: 5.899088443555707, Val Loss: 5.904948766229934, Val MAE: 1.619581937789917\n",
      "Epoch 193/750, Train Loss: 5.898120322176783, Val Loss: 5.903850076217818, Val MAE: 1.6174637079238892\n",
      "Epoch 194/750, Train Loss: 5.897237916973847, Val Loss: 5.904879938738412, Val MAE: 1.6220942735671997\n",
      "Epoch 195/750, Train Loss: 5.89661613593182, Val Loss: 5.904545152035029, Val MAE: 1.6200735569000244\n",
      "Epoch 196/750, Train Loss: 5.896071618910951, Val Loss: 5.902935710823929, Val MAE: 1.617876410484314\n",
      "Epoch 197/750, Train Loss: 5.895177162461462, Val Loss: 5.903927276378201, Val MAE: 1.619860053062439\n",
      "Epoch 198/750, Train Loss: 5.894582245035273, Val Loss: 5.9030055210906225, Val MAE: 1.6192660331726074\n",
      "Epoch 199/750, Train Loss: 5.893953716404517, Val Loss: 5.903494299219692, Val MAE: 1.6216119527816772\n",
      "Epoch 200/750, Train Loss: 5.893423131438178, Val Loss: 5.901433209244164, Val MAE: 1.6164014339447021\n",
      "Epoch 201/750, Train Loss: 5.892554485924621, Val Loss: 5.900118085707925, Val MAE: 1.6148282289505005\n",
      "Epoch 202/750, Train Loss: 5.892108698946897, Val Loss: 5.901312427786909, Val MAE: 1.6161826848983765\n",
      "Epoch 203/750, Train Loss: 5.891201005271258, Val Loss: 5.9006652747053545, Val MAE: 1.6181514263153076\n",
      "Epoch 204/750, Train Loss: 5.890965440110761, Val Loss: 5.9004948114338704, Val MAE: 1.6186749935150146\n",
      "Epoch 205/750, Train Loss: 5.89043632084463, Val Loss: 5.900374770805831, Val MAE: 1.6190139055252075\n",
      "Epoch 206/750, Train Loss: 5.889153138632473, Val Loss: 5.900521429337025, Val MAE: 1.619883418083191\n",
      "Epoch 207/750, Train Loss: 5.888568499149421, Val Loss: 5.899632120821888, Val MAE: 1.6178836822509766\n",
      "Epoch 208/750, Train Loss: 5.888444096241391, Val Loss: 5.89912955582182, Val MAE: 1.6168164014816284\n",
      "Epoch 209/750, Train Loss: 5.88754598448767, Val Loss: 5.898866926292966, Val MAE: 1.6160681247711182\n",
      "Epoch 210/750, Train Loss: 5.886972846949078, Val Loss: 5.897606263805254, Val MAE: 1.6151920557022095\n",
      "Epoch 211/750, Train Loss: 5.886858936844206, Val Loss: 5.898178625203013, Val MAE: 1.6161024570465088\n",
      "Epoch 212/750, Train Loss: 5.886057907525564, Val Loss: 5.897262462250081, Val MAE: 1.6154670715332031\n",
      "Epoch 213/750, Train Loss: 5.885720248368473, Val Loss: 5.897377390066248, Val MAE: 1.6161755323410034\n",
      "Epoch 214/750, Train Loss: 5.8846907863175595, Val Loss: 5.8962366277528595, Val MAE: 1.6138043403625488\n",
      "Epoch 215/750, Train Loss: 5.8848292214785465, Val Loss: 5.89743943168721, Val MAE: 1.6167875528335571\n",
      "Epoch 216/750, Train Loss: 5.883868431284548, Val Loss: 5.8965896855846855, Val MAE: 1.615425944328308\n",
      "Epoch 217/750, Train Loss: 5.883314967080904, Val Loss: 5.896450050621122, Val MAE: 1.616058111190796\n",
      "Epoch 218/750, Train Loss: 5.882916153781931, Val Loss: 5.896424123722992, Val MAE: 1.6155019998550415\n",
      "Epoch 219/750, Train Loss: 5.882338532214615, Val Loss: 5.89625117176719, Val MAE: 1.6152645349502563\n",
      "Epoch 220/750, Train Loss: 5.882267895007297, Val Loss: 5.897825514340289, Val MAE: 1.6194342374801636\n",
      "Epoch 221/750, Train Loss: 5.88194433967347, Val Loss: 5.897009409668467, Val MAE: 1.6184349060058594\n",
      "Epoch 222/750, Train Loss: 5.880883057613981, Val Loss: 5.8963038577437, Val MAE: 1.6176291704177856\n",
      "Epoch 223/750, Train Loss: 5.880661991851787, Val Loss: 5.896522178374125, Val MAE: 1.6191145181655884\n",
      "Epoch 224/750, Train Loss: 5.880211594553573, Val Loss: 5.896454014029372, Val MAE: 1.6188690662384033\n",
      "Epoch 225/750, Train Loss: 5.879768465219847, Val Loss: 5.894844058382487, Val MAE: 1.6157552003860474\n",
      "Epoch 226/750, Train Loss: 5.8794598267777705, Val Loss: 5.893983377733725, Val MAE: 1.6130409240722656\n",
      "Epoch 227/750, Train Loss: 5.8790819067892395, Val Loss: 5.894359800706554, Val MAE: 1.6149795055389404\n",
      "Epoch 228/750, Train Loss: 5.878142305729611, Val Loss: 5.89503100861137, Val MAE: 1.616154670715332\n",
      "Epoch 229/750, Train Loss: 5.877906658263263, Val Loss: 5.894345276974637, Val MAE: 1.6163471937179565\n",
      "Epoch 230/750, Train Loss: 5.877345698262991, Val Loss: 5.893763910184871, Val MAE: 1.6146211624145508\n",
      "Epoch 231/750, Train Loss: 5.8768754889623604, Val Loss: 5.894277679174637, Val MAE: 1.615738034248352\n",
      "Epoch 232/750, Train Loss: 5.8764676478149145, Val Loss: 5.893886152136302, Val MAE: 1.6151394844055176\n",
      "Epoch 233/750, Train Loss: 5.876447851766714, Val Loss: 5.8940413708484805, Val MAE: 1.6159237623214722\n",
      "Epoch 234/750, Train Loss: 5.875643833940517, Val Loss: 5.893725877851949, Val MAE: 1.6153689622879028\n",
      "Epoch 235/750, Train Loss: 5.875323665074962, Val Loss: 5.893020712415404, Val MAE: 1.6139708757400513\n",
      "Epoch 236/750, Train Loss: 5.874798659759436, Val Loss: 5.8920024970913385, Val MAE: 1.6129164695739746\n",
      "Epoch 237/750, Train Loss: 5.874530335677423, Val Loss: 5.8916508343769705, Val MAE: 1.610746145248413\n",
      "Epoch 238/750, Train Loss: 5.87434986638158, Val Loss: 5.892201801394967, Val MAE: 1.612645149230957\n",
      "Epoch 239/750, Train Loss: 5.874029754175851, Val Loss: 5.89113645751603, Val MAE: 1.61098051071167\n",
      "Epoch 240/750, Train Loss: 5.873344856772742, Val Loss: 5.8921400121417555, Val MAE: 1.6143839359283447\n",
      "Epoch 241/750, Train Loss: 5.872873762982424, Val Loss: 5.8917706272469, Val MAE: 1.6143953800201416\n",
      "Epoch 242/750, Train Loss: 5.872711097545516, Val Loss: 5.8928839163886035, Val MAE: 1.6154567003250122\n",
      "Epoch 243/750, Train Loss: 5.87217472224328, Val Loss: 5.890834397009418, Val MAE: 1.612060546875\n",
      "Epoch 244/750, Train Loss: 5.871735376593022, Val Loss: 5.891719053235724, Val MAE: 1.6143187284469604\n",
      "Epoch 245/750, Train Loss: 5.8718408937376685, Val Loss: 5.892339511012262, Val MAE: 1.6160334348678589\n",
      "Epoch 246/750, Train Loss: 5.871770758566221, Val Loss: 5.892841605187745, Val MAE: 1.6159170866012573\n",
      "Epoch 247/750, Train Loss: 5.8706385561493954, Val Loss: 5.890136482455864, Val MAE: 1.6115814447402954\n",
      "Epoch 248/750, Train Loss: 5.8704150043628305, Val Loss: 5.890796205223047, Val MAE: 1.6132149696350098\n",
      "Epoch 249/750, Train Loss: 5.870274115682319, Val Loss: 5.8903889494128245, Val MAE: 1.6115509271621704\n",
      "Epoch 250/750, Train Loss: 5.869669741284035, Val Loss: 5.8897246061432815, Val MAE: 1.6095080375671387\n",
      "Epoch 251/750, Train Loss: 5.869324387424509, Val Loss: 5.888931688119199, Val MAE: 1.6105679273605347\n",
      "Epoch 252/750, Train Loss: 5.869094967022026, Val Loss: 5.889302067493703, Val MAE: 1.611918330192566\n",
      "Epoch 253/750, Train Loss: 5.868882702319901, Val Loss: 5.890266961673, Val MAE: 1.6137330532073975\n",
      "Epoch 254/750, Train Loss: 5.868100046738749, Val Loss: 5.889013553675823, Val MAE: 1.6124645471572876\n",
      "Epoch 255/750, Train Loss: 5.867939339793422, Val Loss: 5.889888876143445, Val MAE: 1.6143008470535278\n",
      "Epoch 256/750, Train Loss: 5.8681658404256645, Val Loss: 5.890270276685329, Val MAE: 1.6158435344696045\n",
      "Epoch 257/750, Train Loss: 5.8674608848481125, Val Loss: 5.888027601259703, Val MAE: 1.6099286079406738\n",
      "Epoch 258/750, Train Loss: 5.866806339218588, Val Loss: 5.8886428325065605, Val MAE: 1.6121418476104736\n",
      "Epoch 259/750, Train Loss: 5.866638380262985, Val Loss: 5.8880393946419325, Val MAE: 1.6104204654693604\n",
      "Epoch 260/750, Train Loss: 5.866200369995337, Val Loss: 5.888968795918745, Val MAE: 1.612883448600769\n",
      "Epoch 261/750, Train Loss: 5.865768173771251, Val Loss: 5.888273247874015, Val MAE: 1.612363338470459\n",
      "Epoch 262/750, Train Loss: 5.865416840958252, Val Loss: 5.888862969816012, Val MAE: 1.61403226852417\n",
      "Epoch 263/750, Train Loss: 5.8653579682689525, Val Loss: 5.889276058303484, Val MAE: 1.614086389541626\n",
      "Epoch 264/750, Train Loss: 5.865047274118964, Val Loss: 5.8885224575794375, Val MAE: 1.6124012470245361\n",
      "Epoch 265/750, Train Loss: 5.864742691625723, Val Loss: 5.887157872850332, Val MAE: 1.6113255023956299\n",
      "Epoch 266/750, Train Loss: 5.864377927303016, Val Loss: 5.888535807737185, Val MAE: 1.6143966913223267\n",
      "Epoch 267/750, Train Loss: 5.864082094876001, Val Loss: 5.887773266717261, Val MAE: 1.6114885807037354\n",
      "Epoch 268/750, Train Loss: 5.863811437303831, Val Loss: 5.8881654179809715, Val MAE: 1.6129200458526611\n",
      "Epoch 269/750, Train Loss: 5.864330572065075, Val Loss: 5.888046998422901, Val MAE: 1.6123557090759277\n",
      "Epoch 270/750, Train Loss: 5.862794263576104, Val Loss: 5.887781979737862, Val MAE: 1.6120827198028564\n",
      "Epoch 271/750, Train Loss: 5.862960903401521, Val Loss: 5.888200533093425, Val MAE: 1.6129153966903687\n",
      "Epoch 272/750, Train Loss: 5.862946848484634, Val Loss: 5.887220646922991, Val MAE: 1.6107789278030396\n",
      "Epoch 273/750, Train Loss: 5.862464022382936, Val Loss: 5.887556658828507, Val MAE: 1.6128877401351929\n",
      "Epoch 274/750, Train Loss: 5.86266094360447, Val Loss: 5.887499883340041, Val MAE: 1.6133153438568115\n",
      "Epoch 275/750, Train Loss: 5.861987389349207, Val Loss: 5.887441462290392, Val MAE: 1.6140122413635254\n",
      "Epoch 276/750, Train Loss: 5.861527656301101, Val Loss: 5.887639757924699, Val MAE: 1.613429307937622\n",
      "Epoch 277/750, Train Loss: 5.861209108950273, Val Loss: 5.886279867973276, Val MAE: 1.6114381551742554\n",
      "Epoch 278/750, Train Loss: 5.861576876243701, Val Loss: 5.886018370571919, Val MAE: 1.6111267805099487\n",
      "Epoch 279/750, Train Loss: 5.8611313567599925, Val Loss: 5.886482201812246, Val MAE: 1.6119369268417358\n",
      "Epoch 280/750, Train Loss: 5.860280758295303, Val Loss: 5.886499105962074, Val MAE: 1.6122872829437256\n",
      "Epoch 281/750, Train Loss: 5.860397707752469, Val Loss: 5.8871328818293005, Val MAE: 1.612818956375122\n",
      "Epoch 282/750, Train Loss: 5.859761736406394, Val Loss: 5.88550310649564, Val MAE: 1.611267328262329\n",
      "Epoch 283/750, Train Loss: 5.859698286646974, Val Loss: 5.885867676722097, Val MAE: 1.6118676662445068\n",
      "Epoch 284/750, Train Loss: 5.859960532173505, Val Loss: 5.886979130986719, Val MAE: 1.6119736433029175\n",
      "Epoch 285/750, Train Loss: 5.8592088587810425, Val Loss: 5.887017727900289, Val MAE: 1.6137869358062744\n",
      "Epoch 286/750, Train Loss: 5.859379968470227, Val Loss: 5.88585582549891, Val MAE: 1.6132398843765259\n",
      "Epoch 287/750, Train Loss: 5.859067131833332, Val Loss: 5.886227464707971, Val MAE: 1.612602710723877\n",
      "Epoch 288/750, Train Loss: 5.858672462007714, Val Loss: 5.885373048840069, Val MAE: 1.6112463474273682\n",
      "Epoch 289/750, Train Loss: 5.858234850148099, Val Loss: 5.88607592693334, Val MAE: 1.6121124029159546\n",
      "Epoch 290/750, Train Loss: 5.858004047693202, Val Loss: 5.8848206155959115, Val MAE: 1.6116715669631958\n",
      "Epoch 291/750, Train Loss: 5.858524466843215, Val Loss: 5.884953189553554, Val MAE: 1.611664056777954\n",
      "Epoch 292/750, Train Loss: 5.857950951920367, Val Loss: 5.88503935502212, Val MAE: 1.6114505529403687\n",
      "Epoch 293/750, Train Loss: 5.85759715395767, Val Loss: 5.884554924464659, Val MAE: 1.6121835708618164\n",
      "Epoch 294/750, Train Loss: 5.8571581458806845, Val Loss: 5.885010067456187, Val MAE: 1.6115431785583496\n",
      "Epoch 295/750, Train Loss: 5.8571597493537295, Val Loss: 5.885628020835548, Val MAE: 1.6129945516586304\n",
      "Epoch 296/750, Train Loss: 5.856937571717024, Val Loss: 5.885246648272172, Val MAE: 1.6120190620422363\n",
      "Epoch 297/750, Train Loss: 5.8570693369728835, Val Loss: 5.884484882032463, Val MAE: 1.6110756397247314\n",
      "Epoch 298/750, Train Loss: 5.856511363541804, Val Loss: 5.884981297652995, Val MAE: 1.612741470336914\n",
      "Epoch 299/750, Train Loss: 5.856176614314038, Val Loss: 5.883178812109069, Val MAE: 1.6097501516342163\n",
      "Epoch 300/750, Train Loss: 5.856145320198102, Val Loss: 5.883166575960931, Val MAE: 1.609757423400879\n",
      "Epoch 301/750, Train Loss: 5.856091542867215, Val Loss: 5.882587437511051, Val MAE: 1.607848048210144\n",
      "Epoch 302/750, Train Loss: 5.85559343918925, Val Loss: 5.882695642749014, Val MAE: 1.608253836631775\n",
      "Epoch 303/750, Train Loss: 5.855711508125272, Val Loss: 5.883447199518759, Val MAE: 1.6097139120101929\n",
      "Epoch 304/750, Train Loss: 5.85516040454886, Val Loss: 5.882869693121817, Val MAE: 1.609384536743164\n",
      "Epoch 305/750, Train Loss: 5.855058146387879, Val Loss: 5.882410696070709, Val MAE: 1.6077771186828613\n",
      "Epoch 306/750, Train Loss: 5.854957141303658, Val Loss: 5.882631406739164, Val MAE: 1.6068896055221558\n",
      "Epoch 307/750, Train Loss: 5.854349043907561, Val Loss: 5.882039898100842, Val MAE: 1.6084727048873901\n",
      "Epoch 308/750, Train Loss: 5.854556112009112, Val Loss: 5.882972001869084, Val MAE: 1.6103150844573975\n",
      "Epoch 309/750, Train Loss: 5.854339649410379, Val Loss: 5.882963447820924, Val MAE: 1.6103013753890991\n",
      "Epoch 310/750, Train Loss: 5.854089710696031, Val Loss: 5.882444734692013, Val MAE: 1.6086982488632202\n",
      "Epoch 311/750, Train Loss: 5.854234370684311, Val Loss: 5.883192428984574, Val MAE: 1.6111876964569092\n",
      "Epoch 312/750, Train Loss: 5.853640364884883, Val Loss: 5.882165962365464, Val MAE: 1.6083078384399414\n",
      "Epoch 313/750, Train Loss: 5.85360680377953, Val Loss: 5.883092025822252, Val MAE: 1.6100624799728394\n",
      "Epoch 314/750, Train Loss: 5.853397463022581, Val Loss: 5.883452450138815, Val MAE: 1.6121046543121338\n",
      "Epoch 315/750, Train Loss: 5.852752681073731, Val Loss: 5.88221384766283, Val MAE: 1.6089707612991333\n",
      "Epoch 316/750, Train Loss: 5.852808160883251, Val Loss: 5.882166918885732, Val MAE: 1.6101288795471191\n",
      "Epoch 317/750, Train Loss: 5.85284907553925, Val Loss: 5.88192636724599, Val MAE: 1.610437273979187\n",
      "Epoch 318/750, Train Loss: 5.852050772601921, Val Loss: 5.88251112292257, Val MAE: 1.6122597455978394\n",
      "Epoch 319/750, Train Loss: 5.852587702275813, Val Loss: 5.882979217839065, Val MAE: 1.6111267805099487\n",
      "Epoch 320/750, Train Loss: 5.851867039103744, Val Loss: 5.882285279881337, Val MAE: 1.6098685264587402\n",
      "Epoch 321/750, Train Loss: 5.85178554587397, Val Loss: 5.882004458750345, Val MAE: 1.610858678817749\n",
      "Epoch 322/750, Train Loss: 5.85194621345563, Val Loss: 5.8804847013252886, Val MAE: 1.608521580696106\n",
      "Epoch 323/750, Train Loss: 5.851823389120144, Val Loss: 5.8813847940723285, Val MAE: 1.6097575426101685\n",
      "Epoch 324/750, Train Loss: 5.851403093844969, Val Loss: 5.880541904708565, Val MAE: 1.6070009469985962\n",
      "Epoch 325/750, Train Loss: 5.851353925343526, Val Loss: 5.880783952557167, Val MAE: 1.6073822975158691\n",
      "Epoch 326/750, Train Loss: 5.851262030860944, Val Loss: 5.880168302153772, Val MAE: 1.608393669128418\n",
      "Epoch 327/750, Train Loss: 5.8510577903828676, Val Loss: 5.88219275118604, Val MAE: 1.6122210025787354\n",
      "Epoch 328/750, Train Loss: 5.850830341444081, Val Loss: 5.881238468317033, Val MAE: 1.6089580059051514\n",
      "Epoch 329/750, Train Loss: 5.850770213739659, Val Loss: 5.880991149646128, Val MAE: 1.6095651388168335\n",
      "Epoch 330/750, Train Loss: 5.850910535672816, Val Loss: 5.881576328808298, Val MAE: 1.6091960668563843\n",
      "Epoch 331/750, Train Loss: 5.850600610008979, Val Loss: 5.880393762675082, Val MAE: 1.6082844734191895\n",
      "Epoch 332/750, Train Loss: 5.850048852160694, Val Loss: 5.879843585629069, Val MAE: 1.6064701080322266\n",
      "Epoch 333/750, Train Loss: 5.8504159317827735, Val Loss: 5.879296216535023, Val MAE: 1.6055350303649902\n",
      "Epoch 334/750, Train Loss: 5.849500673125877, Val Loss: 5.880081149781399, Val MAE: 1.607724666595459\n",
      "Epoch 335/750, Train Loss: 5.849689421018561, Val Loss: 5.880397755424689, Val MAE: 1.6082125902175903\n",
      "Epoch 336/750, Train Loss: 5.849794185034852, Val Loss: 5.8788868586890395, Val MAE: 1.606871247291565\n",
      "Epoch 337/750, Train Loss: 5.849793516448917, Val Loss: 5.8799752872424715, Val MAE: 1.6082253456115723\n",
      "Epoch 338/750, Train Loss: 5.849063836611235, Val Loss: 5.878542596811244, Val MAE: 1.6054316759109497\n",
      "Epoch 339/750, Train Loss: 5.849213760297249, Val Loss: 5.878784082764816, Val MAE: 1.6062467098236084\n",
      "Epoch 340/750, Train Loss: 5.849561911959287, Val Loss: 5.879553935915813, Val MAE: 1.608173131942749\n",
      "Epoch 341/750, Train Loss: 5.848856940874835, Val Loss: 5.878924839554333, Val MAE: 1.6070821285247803\n",
      "Epoch 342/750, Train Loss: 5.84873105556686, Val Loss: 5.879110645590489, Val MAE: 1.6079189777374268\n",
      "Epoch 343/750, Train Loss: 5.848322619938567, Val Loss: 5.879491774925227, Val MAE: 1.6099271774291992\n",
      "Epoch 344/750, Train Loss: 5.848451943901496, Val Loss: 5.88027161411896, Val MAE: 1.6115869283676147\n",
      "Epoch 345/750, Train Loss: 5.848360236992756, Val Loss: 5.880641552210656, Val MAE: 1.6102845668792725\n",
      "Epoch 346/750, Train Loss: 5.848054603161552, Val Loss: 5.879640624638604, Val MAE: 1.6102054119110107\n",
      "Epoch 347/750, Train Loss: 5.848453514124171, Val Loss: 5.882172970460739, Val MAE: 1.6138391494750977\n",
      "Epoch 348/750, Train Loss: 5.847733925848622, Val Loss: 5.879686066746792, Val MAE: 1.6099870204925537\n",
      "Epoch 349/750, Train Loss: 5.84778003352668, Val Loss: 5.878690058141245, Val MAE: 1.6054723262786865\n",
      "Epoch 350/750, Train Loss: 5.847625357572402, Val Loss: 5.878529706895953, Val MAE: 1.6067721843719482\n",
      "Epoch 351/750, Train Loss: 5.847658056255577, Val Loss: 5.880803861109779, Val MAE: 1.6106326580047607\n",
      "Epoch 352/750, Train Loss: 5.8472795492414384, Val Loss: 5.879723432445655, Val MAE: 1.6090978384017944\n",
      "Epoch 353/750, Train Loss: 5.846983671635669, Val Loss: 5.878257500860783, Val MAE: 1.6063766479492188\n",
      "Epoch 354/750, Train Loss: 5.847161388158649, Val Loss: 5.878588016272072, Val MAE: 1.6087995767593384\n",
      "Epoch 355/750, Train Loss: 5.8467051217972, Val Loss: 5.878646797755459, Val MAE: 1.6083813905715942\n",
      "Epoch 356/750, Train Loss: 5.846467935569291, Val Loss: 5.878918594374461, Val MAE: 1.6087536811828613\n",
      "Epoch 357/750, Train Loss: 5.846702115844383, Val Loss: 5.87875326331542, Val MAE: 1.6082417964935303\n",
      "Epoch 358/750, Train Loss: 5.846322321459381, Val Loss: 5.87815059481969, Val MAE: 1.6080267429351807\n",
      "Epoch 359/750, Train Loss: 5.846278462878162, Val Loss: 5.879111216945302, Val MAE: 1.6101853847503662\n",
      "Epoch 360/750, Train Loss: 5.8460779631413695, Val Loss: 5.878124227039591, Val MAE: 1.6075336933135986\n",
      "Epoch 361/750, Train Loss: 5.845875535479481, Val Loss: 5.877217934647902, Val MAE: 1.6061251163482666\n",
      "Epoch 362/750, Train Loss: 5.845939630564486, Val Loss: 5.876888464502464, Val MAE: 1.6051788330078125\n",
      "Epoch 363/750, Train Loss: 5.845882348674324, Val Loss: 5.877378311266255, Val MAE: 1.607032060623169\n",
      "Epoch 364/750, Train Loss: 5.845295817200432, Val Loss: 5.877220582585248, Val MAE: 1.6066606044769287\n",
      "Epoch 365/750, Train Loss: 5.845646543604198, Val Loss: 5.878187737011156, Val MAE: 1.6092591285705566\n",
      "Epoch 366/750, Train Loss: 5.845294273667517, Val Loss: 5.877698105978501, Val MAE: 1.6081886291503906\n",
      "Epoch 367/750, Train Loss: 5.845376292939034, Val Loss: 5.878699756742886, Val MAE: 1.609989047050476\n",
      "Epoch 368/750, Train Loss: 5.84497317886114, Val Loss: 5.877940008402993, Val MAE: 1.6091783046722412\n",
      "Epoch 369/750, Train Loss: 5.845273768178667, Val Loss: 5.877363978613922, Val MAE: 1.6083470582962036\n",
      "Epoch 370/750, Train Loss: 5.84488279958156, Val Loss: 5.877487271603274, Val MAE: 1.606660008430481\n",
      "Epoch 371/750, Train Loss: 5.844725653258915, Val Loss: 5.876212633289422, Val MAE: 1.6047781705856323\n",
      "Epoch 372/750, Train Loss: 5.845520850939629, Val Loss: 5.876930449100333, Val MAE: 1.6059074401855469\n",
      "Epoch 373/750, Train Loss: 5.844555182632914, Val Loss: 5.876653724896803, Val MAE: 1.6064928770065308\n",
      "Epoch 374/750, Train Loss: 5.844367207103106, Val Loss: 5.87717930205973, Val MAE: 1.6066820621490479\n",
      "Epoch 375/750, Train Loss: 5.844424333327856, Val Loss: 5.876726647712435, Val MAE: 1.606360912322998\n",
      "Epoch 376/750, Train Loss: 5.843690720105484, Val Loss: 5.876755063815973, Val MAE: 1.6052054166793823\n",
      "Epoch 377/750, Train Loss: 5.844390650702089, Val Loss: 5.8763233683377045, Val MAE: 1.6059900522232056\n",
      "Epoch 378/750, Train Loss: 5.843959398013193, Val Loss: 5.875676279670854, Val MAE: 1.60520601272583\n",
      "Epoch 379/750, Train Loss: 5.843929885997856, Val Loss: 5.87744308688453, Val MAE: 1.6085002422332764\n",
      "Epoch 380/750, Train Loss: 5.843544288453942, Val Loss: 5.876585448397512, Val MAE: 1.607028841972351\n",
      "Epoch 381/750, Train Loss: 5.843489631404721, Val Loss: 5.876226593362619, Val MAE: 1.6061917543411255\n",
      "Epoch 382/750, Train Loss: 5.843812719444099, Val Loss: 5.876002967237384, Val MAE: 1.6053798198699951\n",
      "Epoch 383/750, Train Loss: 5.843070187368864, Val Loss: 5.87572179096372, Val MAE: 1.6048611402511597\n",
      "Epoch 384/750, Train Loss: 5.843282235869025, Val Loss: 5.876502658828602, Val MAE: 1.6076254844665527\n",
      "Epoch 385/750, Train Loss: 5.84285538669226, Val Loss: 5.876336728396156, Val MAE: 1.6062119007110596\n",
      "Epoch 386/750, Train Loss: 5.843267003769722, Val Loss: 5.876939408636446, Val MAE: 1.6094423532485962\n",
      "Epoch 387/750, Train Loss: 5.842846635433195, Val Loss: 5.87541798232142, Val MAE: 1.604868769645691\n",
      "Epoch 388/750, Train Loss: 5.842625791837157, Val Loss: 5.875837893556571, Val MAE: 1.6040821075439453\n",
      "Epoch 389/750, Train Loss: 5.842981119615127, Val Loss: 5.875702138426485, Val MAE: 1.606109380722046\n",
      "Epoch 390/750, Train Loss: 5.842573958236475, Val Loss: 5.876640590309856, Val MAE: 1.608109474182129\n",
      "Epoch 391/750, Train Loss: 5.8426089087003765, Val Loss: 5.877309103218917, Val MAE: 1.6099295616149902\n",
      "Epoch 392/750, Train Loss: 5.842175389171168, Val Loss: 5.876238055171466, Val MAE: 1.6080111265182495\n",
      "Epoch 393/750, Train Loss: 5.842330541217081, Val Loss: 5.875383705682376, Val MAE: 1.605222463607788\n",
      "Epoch 394/750, Train Loss: 5.84259141870705, Val Loss: 5.876193853858019, Val MAE: 1.6086417436599731\n",
      "Epoch 395/750, Train Loss: 5.841658543615956, Val Loss: 5.876662809253942, Val MAE: 1.608777642250061\n",
      "Epoch 396/750, Train Loss: 5.841655788159207, Val Loss: 5.876888377239982, Val MAE: 1.608601689338684\n",
      "Epoch 397/750, Train Loss: 5.841645608177925, Val Loss: 5.876154960364244, Val MAE: 1.6072245836257935\n",
      "Epoch 398/750, Train Loss: 5.841592000230094, Val Loss: 5.875932586217135, Val MAE: 1.6064740419387817\n",
      "Epoch 399/750, Train Loss: 5.841213435363293, Val Loss: 5.876642008355262, Val MAE: 1.6079375743865967\n",
      "Epoch 400/750, Train Loss: 5.841404852828359, Val Loss: 5.874610991468346, Val MAE: 1.6047238111495972\n",
      "Epoch 401/750, Train Loss: 5.841204965315885, Val Loss: 5.874889930102676, Val MAE: 1.6052950620651245\n",
      "Epoch 402/750, Train Loss: 5.840754568912298, Val Loss: 5.874656483160223, Val MAE: 1.6047775745391846\n",
      "Epoch 403/750, Train Loss: 5.841230389175749, Val Loss: 5.874430958144041, Val MAE: 1.603816032409668\n",
      "Epoch 404/750, Train Loss: 5.841052877820977, Val Loss: 5.874994589285155, Val MAE: 1.6055811643600464\n",
      "Epoch 405/750, Train Loss: 5.841004402358656, Val Loss: 5.876143930338338, Val MAE: 1.6085734367370605\n",
      "Epoch 406/750, Train Loss: 5.840522539623087, Val Loss: 5.87495487776072, Val MAE: 1.6065435409545898\n",
      "Epoch 407/750, Train Loss: 5.84050567273873, Val Loss: 5.8742546291261535, Val MAE: 1.6046961545944214\n",
      "Epoch 408/750, Train Loss: 5.84048034370952, Val Loss: 5.874227120400117, Val MAE: 1.6050362586975098\n",
      "Epoch 409/750, Train Loss: 5.840915923121574, Val Loss: 5.875638227777026, Val MAE: 1.60826575756073\n",
      "Epoch 410/750, Train Loss: 5.840125900272133, Val Loss: 5.874914993675427, Val MAE: 1.6054917573928833\n",
      "Epoch 411/750, Train Loss: 5.839999367402001, Val Loss: 5.875066308784421, Val MAE: 1.605560541152954\n",
      "Epoch 412/750, Train Loss: 5.840088949269098, Val Loss: 5.873906306957509, Val MAE: 1.604909896850586\n",
      "Epoch 413/750, Train Loss: 5.8400777473235, Val Loss: 5.874990235781141, Val MAE: 1.6073509454727173\n",
      "Epoch 414/750, Train Loss: 5.8396579329113125, Val Loss: 5.873150183197309, Val MAE: 1.6036936044692993\n",
      "Epoch 415/750, Train Loss: 5.839546170810225, Val Loss: 5.8745092120680455, Val MAE: 1.6066155433654785\n",
      "Epoch 416/750, Train Loss: 5.839871799595435, Val Loss: 5.874187621239776, Val MAE: 1.6058499813079834\n",
      "Epoch 417/750, Train Loss: 5.839549318114394, Val Loss: 5.872540020830454, Val MAE: 1.6017496585845947\n",
      "Epoch 418/750, Train Loss: 5.83970324138763, Val Loss: 5.872436745461796, Val MAE: 1.6028255224227905\n",
      "Epoch 419/750, Train Loss: 5.8398026719251375, Val Loss: 5.873099193688439, Val MAE: 1.602562665939331\n",
      "Epoch 420/750, Train Loss: 5.839238142952314, Val Loss: 5.873526125685353, Val MAE: 1.6039576530456543\n",
      "Epoch 421/750, Train Loss: 5.8392107246666125, Val Loss: 5.873601685976453, Val MAE: 1.6048555374145508\n",
      "Epoch 422/750, Train Loss: 5.83905415344119, Val Loss: 5.87593275633288, Val MAE: 1.6094590425491333\n",
      "Epoch 423/750, Train Loss: 5.83879738259569, Val Loss: 5.87362770069824, Val MAE: 1.6052924394607544\n",
      "Epoch 424/750, Train Loss: 5.838557382312844, Val Loss: 5.87367739249334, Val MAE: 1.605899691581726\n",
      "Epoch 425/750, Train Loss: 5.8394325949983195, Val Loss: 5.874420001832286, Val MAE: 1.607330560684204\n",
      "Epoch 426/750, Train Loss: 5.838820191753738, Val Loss: 5.8734679690285985, Val MAE: 1.605371117591858\n",
      "Epoch 427/750, Train Loss: 5.838205035139874, Val Loss: 5.873549491106942, Val MAE: 1.6062086820602417\n",
      "Epoch 428/750, Train Loss: 5.838426258953159, Val Loss: 5.873438410095184, Val MAE: 1.606068730354309\n",
      "Epoch 429/750, Train Loss: 5.838135538584296, Val Loss: 5.872705835741321, Val MAE: 1.6040087938308716\n",
      "Epoch 430/750, Train Loss: 5.838788217719903, Val Loss: 5.871914470764961, Val MAE: 1.6021220684051514\n",
      "Epoch 431/750, Train Loss: 5.8385969144691146, Val Loss: 5.872427956562517, Val MAE: 1.6032123565673828\n",
      "Epoch 432/750, Train Loss: 5.838519976987475, Val Loss: 5.87135464045211, Val MAE: 1.6020750999450684\n",
      "Epoch 433/750, Train Loss: 5.838025835173215, Val Loss: 5.872298644225551, Val MAE: 1.602707862854004\n",
      "Epoch 434/750, Train Loss: 5.837916093293095, Val Loss: 5.873219079152, Val MAE: 1.605067491531372\n",
      "Epoch 435/750, Train Loss: 5.8378044597203465, Val Loss: 5.8732099374749955, Val MAE: 1.6056610345840454\n",
      "Epoch 436/750, Train Loss: 5.837635544257435, Val Loss: 5.873482033720575, Val MAE: 1.6056733131408691\n",
      "Epoch 437/750, Train Loss: 5.837708702677857, Val Loss: 5.873556955756961, Val MAE: 1.605450987815857\n",
      "Epoch 438/750, Train Loss: 5.837649784660697, Val Loss: 5.874319083512671, Val MAE: 1.606729507446289\n",
      "Epoch 439/750, Train Loss: 5.837513921706657, Val Loss: 5.872862262860512, Val MAE: 1.603570818901062\n",
      "Epoch 440/750, Train Loss: 5.837243514332345, Val Loss: 5.872427361959087, Val MAE: 1.6034049987792969\n",
      "Epoch 441/750, Train Loss: 5.83742537775809, Val Loss: 5.873376561598082, Val MAE: 1.6051806211471558\n",
      "Epoch 442/750, Train Loss: 5.837963546194682, Val Loss: 5.873584343429853, Val MAE: 1.6059366464614868\n",
      "Epoch 443/750, Train Loss: 5.837071589039891, Val Loss: 5.873849397108069, Val MAE: 1.605629324913025\n",
      "Epoch 444/750, Train Loss: 5.836945740709907, Val Loss: 5.873582757593564, Val MAE: 1.6065340042114258\n",
      "Epoch 445/750, Train Loss: 5.836949286422109, Val Loss: 5.8724902609329375, Val MAE: 1.603833794593811\n",
      "Epoch 446/750, Train Loss: 5.836929275662993, Val Loss: 5.874378502970185, Val MAE: 1.60774827003479\n",
      "Epoch 447/750, Train Loss: 5.836767016238462, Val Loss: 5.874273100793963, Val MAE: 1.6079148054122925\n",
      "Epoch 448/750, Train Loss: 5.836603349264597, Val Loss: 5.871926987580028, Val MAE: 1.6033074855804443\n",
      "Epoch 449/750, Train Loss: 5.836760454732526, Val Loss: 5.872564693145008, Val MAE: 1.605334758758545\n",
      "Epoch 450/750, Train Loss: 5.836446386340858, Val Loss: 5.871074989848916, Val MAE: 1.602108120918274\n",
      "Epoch 451/750, Train Loss: 5.8366813650721685, Val Loss: 5.872507776601569, Val MAE: 1.6045832633972168\n",
      "Epoch 452/750, Train Loss: 5.83604596986705, Val Loss: 5.871666341599155, Val MAE: 1.603026032447815\n",
      "Epoch 453/750, Train Loss: 5.836343475100843, Val Loss: 5.871134347817845, Val MAE: 1.6017972230911255\n",
      "Epoch 454/750, Train Loss: 5.836265284542444, Val Loss: 5.872418702771059, Val MAE: 1.6048595905303955\n",
      "Epoch 455/750, Train Loss: 5.836231442225434, Val Loss: 5.872303227409871, Val MAE: 1.605534315109253\n",
      "Epoch 456/750, Train Loss: 5.836096146913377, Val Loss: 5.872059648760069, Val MAE: 1.6039713621139526\n",
      "Epoch 457/750, Train Loss: 5.836213522810277, Val Loss: 5.871751352332948, Val MAE: 1.6039983034133911\n",
      "Epoch 458/750, Train Loss: 5.835948660345358, Val Loss: 5.871288770064461, Val MAE: 1.6025210618972778\n",
      "Epoch 459/750, Train Loss: 5.835861831549334, Val Loss: 5.8717553642025395, Val MAE: 1.602941632270813\n",
      "Epoch 460/750, Train Loss: 5.835571911724155, Val Loss: 5.871655117006372, Val MAE: 1.6032735109329224\n",
      "Epoch 461/750, Train Loss: 5.835413638616518, Val Loss: 5.872524256744718, Val MAE: 1.6057997941970825\n",
      "Epoch 462/750, Train Loss: 5.835411436636796, Val Loss: 5.872116833143613, Val MAE: 1.6036021709442139\n",
      "Epoch 463/750, Train Loss: 5.8354314812427015, Val Loss: 5.87249386474921, Val MAE: 1.60452401638031\n",
      "Epoch 464/750, Train Loss: 5.83498364765246, Val Loss: 5.872024622834122, Val MAE: 1.60284423828125\n",
      "Epoch 465/750, Train Loss: 5.835385766903112, Val Loss: 5.871100449129168, Val MAE: 1.6017870903015137\n",
      "Epoch 466/750, Train Loss: 5.835191717738282, Val Loss: 5.871407922835822, Val MAE: 1.6026383638381958\n",
      "Epoch 467/750, Train Loss: 5.835250900416467, Val Loss: 5.872188546910892, Val MAE: 1.603541374206543\n",
      "Epoch 468/750, Train Loss: 5.8352760555894765, Val Loss: 5.871526395304538, Val MAE: 1.6035940647125244\n",
      "Epoch 469/750, Train Loss: 5.835190566649207, Val Loss: 5.871550030249754, Val MAE: 1.603473424911499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 2/3 [14:13<06:49, 409.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470/750, Train Loss: 5.8351006847832485, Val Loss: 5.872705077235139, Val MAE: 1.6056280136108398\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.146683692932129\n",
      "Test Mean Absolute Error (MAE): 1.5759015268741767\n",
      "===== Running Experiment for Parameters: =====\n",
      " {'seed': 402, 'position': 'DEF', 'window_size': 6, 'num_dense': 64, 'tolerance': 0.0001, 'amt_num_features': 'medium', 'stratify_by': 'stdev'}\n",
      "\n",
      "seed 402\n",
      "position DEF\n",
      "window_size 6\n",
      "num_dense 64\n",
      "tolerance 0.0001\n",
      "amt_num_features medium\n",
      "stratify_by stdev\n",
      "Running Iteration:  2\n",
      "Epoch 1/750, Train Loss: 9.518013095855713, Val Loss: 9.048218669484577, Val MAE: 1.600227952003479\n",
      "Epoch 2/750, Train Loss: 9.064367193567293, Val Loss: 8.638148304733297, Val MAE: 1.6095678806304932\n",
      "Epoch 3/750, Train Loss: 8.647602329763531, Val Loss: 8.266403611479536, Val MAE: 1.6169049739837646\n",
      "Epoch 4/750, Train Loss: 8.270590930310485, Val Loss: 7.936605381820359, Val MAE: 1.6237409114837646\n",
      "Epoch 5/750, Train Loss: 7.940794065232093, Val Loss: 7.652679343924138, Val MAE: 1.6353061199188232\n",
      "Epoch 6/750, Train Loss: 7.658077769180434, Val Loss: 7.4134748655150045, Val MAE: 1.6557540893554688\n",
      "Epoch 7/750, Train Loss: 7.42518352853792, Val Loss: 7.221115779295594, Val MAE: 1.6851372718811035\n",
      "Epoch 8/750, Train Loss: 7.239564507934389, Val Loss: 7.069532140009938, Val MAE: 1.712878942489624\n",
      "Epoch 9/750, Train Loss: 7.090375797543398, Val Loss: 6.944704987507215, Val MAE: 1.7368775606155396\n",
      "Epoch 10/750, Train Loss: 6.969337122659655, Val Loss: 6.8428673799084745, Val MAE: 1.7522897720336914\n",
      "Epoch 11/750, Train Loss: 6.868872604879498, Val Loss: 6.7524304354441025, Val MAE: 1.7606029510498047\n",
      "Epoch 12/750, Train Loss: 6.7811528732231885, Val Loss: 6.670194476055953, Val MAE: 1.7625696659088135\n",
      "Epoch 13/750, Train Loss: 6.701527839173902, Val Loss: 6.591702532267845, Val MAE: 1.7584346532821655\n",
      "Epoch 14/750, Train Loss: 6.6283635040419036, Val Loss: 6.519254909791734, Val MAE: 1.7514708042144775\n",
      "Epoch 15/750, Train Loss: 6.56169042700476, Val Loss: 6.451238168601499, Val MAE: 1.7417290210723877\n",
      "Epoch 16/750, Train Loss: 6.502182720818222, Val Loss: 6.391846220317368, Val MAE: 1.7304790019989014\n",
      "Epoch 17/750, Train Loss: 6.450732759481368, Val Loss: 6.338726156050075, Val MAE: 1.7165682315826416\n",
      "Epoch 18/750, Train Loss: 6.406235082637663, Val Loss: 6.292060675779225, Val MAE: 1.7062376737594604\n",
      "Epoch 19/750, Train Loss: 6.369624415491031, Val Loss: 6.253055193493636, Val MAE: 1.696892261505127\n",
      "Epoch 20/750, Train Loss: 6.338839833036021, Val Loss: 6.221056499106705, Val MAE: 1.6881628036499023\n",
      "Epoch 21/750, Train Loss: 6.31380115701466, Val Loss: 6.193296743068811, Val MAE: 1.6801432371139526\n",
      "Epoch 22/750, Train Loss: 6.293792802006979, Val Loss: 6.170760520826584, Val MAE: 1.671906590461731\n",
      "Epoch 23/750, Train Loss: 6.277240089280669, Val Loss: 6.151763208346667, Val MAE: 1.667414665222168\n",
      "Epoch 24/750, Train Loss: 6.263144296425386, Val Loss: 6.1358169853969775, Val MAE: 1.6618305444717407\n",
      "Epoch 25/750, Train Loss: 6.251049811196256, Val Loss: 6.122341680946428, Val MAE: 1.6581132411956787\n",
      "Epoch 26/750, Train Loss: 6.2407013359692405, Val Loss: 6.110115519389964, Val MAE: 1.6536411046981812\n",
      "Epoch 27/750, Train Loss: 6.231585985121458, Val Loss: 6.099815119418569, Val MAE: 1.6520367860794067\n",
      "Epoch 28/750, Train Loss: 6.223843574382431, Val Loss: 6.0904435288203915, Val MAE: 1.6509674787521362\n",
      "Epoch 29/750, Train Loss: 6.215436873733113, Val Loss: 6.0812426798232275, Val MAE: 1.6444405317306519\n",
      "Epoch 30/750, Train Loss: 6.208363624708589, Val Loss: 6.073615942601828, Val MAE: 1.6440582275390625\n",
      "Epoch 31/750, Train Loss: 6.2011552425803345, Val Loss: 6.066356622454437, Val MAE: 1.6443613767623901\n",
      "Epoch 32/750, Train Loss: 6.195061738130247, Val Loss: 6.059543301305339, Val MAE: 1.641683578491211\n",
      "Epoch 33/750, Train Loss: 6.189101599232026, Val Loss: 6.0531345188577355, Val MAE: 1.6393136978149414\n",
      "Epoch 34/750, Train Loss: 6.18303688032337, Val Loss: 6.047047375179903, Val MAE: 1.6373062133789062\n",
      "Epoch 35/750, Train Loss: 6.177467659460684, Val Loss: 6.041352329176668, Val MAE: 1.6372047662734985\n",
      "Epoch 36/750, Train Loss: 6.172520526014379, Val Loss: 6.036080726837902, Val MAE: 1.6357365846633911\n",
      "Epoch 37/750, Train Loss: 6.168023629712068, Val Loss: 6.030831773134205, Val MAE: 1.6386024951934814\n",
      "Epoch 38/750, Train Loss: 6.162999612995001, Val Loss: 6.025674493230511, Val MAE: 1.6356439590454102\n",
      "Epoch 39/750, Train Loss: 6.158063617445949, Val Loss: 6.021027371943118, Val MAE: 1.633991003036499\n",
      "Epoch 40/750, Train Loss: 6.153782705878646, Val Loss: 6.016499846386764, Val MAE: 1.6341756582260132\n",
      "Epoch 41/750, Train Loss: 6.149681535035991, Val Loss: 6.011287422218917, Val MAE: 1.632645606994629\n",
      "Epoch 42/750, Train Loss: 6.1458847689699345, Val Loss: 6.007372691490881, Val MAE: 1.632850170135498\n",
      "Epoch 43/750, Train Loss: 6.142210718715226, Val Loss: 6.003025710623088, Val MAE: 1.6318405866622925\n",
      "Epoch 44/750, Train Loss: 6.138013751428983, Val Loss: 5.999379164799695, Val MAE: 1.6298490762710571\n",
      "Epoch 45/750, Train Loss: 6.135184377591051, Val Loss: 5.995304055365664, Val MAE: 1.629448413848877\n",
      "Epoch 46/750, Train Loss: 6.130834546641002, Val Loss: 5.9911929603840095, Val MAE: 1.6267482042312622\n",
      "Epoch 47/750, Train Loss: 6.1280030471281055, Val Loss: 5.987450768516608, Val MAE: 1.6282752752304077\n",
      "Epoch 48/750, Train Loss: 6.124565947161938, Val Loss: 5.984301825350979, Val MAE: 1.6283776760101318\n",
      "Epoch 49/750, Train Loss: 6.121364755941782, Val Loss: 5.980920037467817, Val MAE: 1.628494143486023\n",
      "Epoch 50/750, Train Loss: 6.118429057463694, Val Loss: 5.9776745233167725, Val MAE: 1.6274443864822388\n",
      "Epoch 51/750, Train Loss: 6.115011550835403, Val Loss: 5.974182874347684, Val MAE: 1.627258062362671\n",
      "Epoch 52/750, Train Loss: 6.112241713866282, Val Loss: 5.971310278492839, Val MAE: 1.6264946460723877\n",
      "Epoch 53/750, Train Loss: 6.109373990859759, Val Loss: 5.968049308942151, Val MAE: 1.6252412796020508\n",
      "Epoch 54/750, Train Loss: 6.106626650416886, Val Loss: 5.964882964287243, Val MAE: 1.626310110092163\n",
      "Epoch 55/750, Train Loss: 6.10389802859162, Val Loss: 5.962041507157911, Val MAE: 1.6250324249267578\n",
      "Epoch 56/750, Train Loss: 6.101607010272561, Val Loss: 5.958872582352314, Val MAE: 1.628012776374817\n",
      "Epoch 57/750, Train Loss: 6.098276051710904, Val Loss: 5.95534436801195, Val MAE: 1.627762794494629\n",
      "Epoch 58/750, Train Loss: 6.095805948636652, Val Loss: 5.9519408283479, Val MAE: 1.6238088607788086\n",
      "Epoch 59/750, Train Loss: 6.0930163287267485, Val Loss: 5.949582676509787, Val MAE: 1.6251983642578125\n",
      "Epoch 60/750, Train Loss: 6.090614001333536, Val Loss: 5.946533812327246, Val MAE: 1.6252367496490479\n",
      "Epoch 61/750, Train Loss: 6.087833073160414, Val Loss: 5.943298230723263, Val MAE: 1.621664047241211\n",
      "Epoch 62/750, Train Loss: 6.085420175546709, Val Loss: 5.941038354474625, Val MAE: 1.6231414079666138\n",
      "Epoch 63/750, Train Loss: 6.083058417515514, Val Loss: 5.938531098853274, Val MAE: 1.6226880550384521\n",
      "Epoch 64/750, Train Loss: 6.080846910731375, Val Loss: 5.93487012200688, Val MAE: 1.6198384761810303\n",
      "Epoch 65/750, Train Loss: 6.078646409971424, Val Loss: 5.933042989845767, Val MAE: 1.6215564012527466\n",
      "Epoch 66/750, Train Loss: 6.076150647398272, Val Loss: 5.930768289030123, Val MAE: 1.6237488985061646\n",
      "Epoch 67/750, Train Loss: 6.074089826249935, Val Loss: 5.928035234710858, Val MAE: 1.6214574575424194\n",
      "Epoch 68/750, Train Loss: 6.0720601322389145, Val Loss: 5.925054383907434, Val MAE: 1.6177419424057007\n",
      "Epoch 69/750, Train Loss: 6.070158527159196, Val Loss: 5.922944061483332, Val MAE: 1.6178544759750366\n",
      "Epoch 70/750, Train Loss: 6.067796234561003, Val Loss: 5.920588141449739, Val MAE: 1.62172532081604\n",
      "Epoch 71/750, Train Loss: 6.065891539802891, Val Loss: 5.918435566152522, Val MAE: 1.621141791343689\n",
      "Epoch 72/750, Train Loss: 6.063665871096648, Val Loss: 5.916268984369111, Val MAE: 1.6207876205444336\n",
      "Epoch 73/750, Train Loss: 6.0615745896755415, Val Loss: 5.913686371688998, Val MAE: 1.6201281547546387\n",
      "Epoch 74/750, Train Loss: 6.059839254882994, Val Loss: 5.911543138461413, Val MAE: 1.6205320358276367\n",
      "Epoch 75/750, Train Loss: 6.058240525446589, Val Loss: 5.909098011783917, Val MAE: 1.6195961236953735\n",
      "Epoch 76/750, Train Loss: 6.055956501493822, Val Loss: 5.90667564512187, Val MAE: 1.619235873222351\n",
      "Epoch 77/750, Train Loss: 6.0537780695216235, Val Loss: 5.904477512989806, Val MAE: 1.6202164888381958\n",
      "Epoch 78/750, Train Loss: 6.052198808483271, Val Loss: 5.902193299321283, Val MAE: 1.6159980297088623\n",
      "Epoch 79/750, Train Loss: 6.049974022986981, Val Loss: 5.899644532071309, Val MAE: 1.6179898977279663\n",
      "Epoch 80/750, Train Loss: 6.0477661871414865, Val Loss: 5.897673193635211, Val MAE: 1.6182857751846313\n",
      "Epoch 81/750, Train Loss: 6.046044105026064, Val Loss: 5.895661137438176, Val MAE: 1.6175342798233032\n",
      "Epoch 82/750, Train Loss: 6.0444852969059255, Val Loss: 5.893619590622392, Val MAE: 1.6195528507232666\n",
      "Epoch 83/750, Train Loss: 6.042586476866498, Val Loss: 5.891647865099122, Val MAE: 1.618835687637329\n",
      "Epoch 84/750, Train Loss: 6.040701539014143, Val Loss: 5.889186914336786, Val MAE: 1.617152452468872\n",
      "Epoch 85/750, Train Loss: 6.0386895375011225, Val Loss: 5.88680901459608, Val MAE: 1.6170952320098877\n",
      "Epoch 86/750, Train Loss: 6.037221962903303, Val Loss: 5.885424725333015, Val MAE: 1.618387222290039\n",
      "Epoch 87/750, Train Loss: 6.0353845551035175, Val Loss: 5.8832997268490805, Val MAE: 1.6180357933044434\n",
      "Epoch 88/750, Train Loss: 6.033693598993454, Val Loss: 5.881054535699843, Val MAE: 1.6177362203598022\n",
      "Epoch 89/750, Train Loss: 6.031707503604606, Val Loss: 5.8784850232893335, Val MAE: 1.6138437986373901\n",
      "Epoch 90/750, Train Loss: 6.030107853603646, Val Loss: 5.876460250035766, Val MAE: 1.6136785745620728\n",
      "Epoch 91/750, Train Loss: 6.028721229541903, Val Loss: 5.874279045138556, Val MAE: 1.6115264892578125\n",
      "Epoch 92/750, Train Loss: 6.027514751021162, Val Loss: 5.87237760483129, Val MAE: 1.6128677129745483\n",
      "Epoch 93/750, Train Loss: 6.025166866121377, Val Loss: 5.8705366615031975, Val MAE: 1.6133766174316406\n",
      "Epoch 94/750, Train Loss: 6.023316057923993, Val Loss: 5.868818285663788, Val MAE: 1.6137243509292603\n",
      "Epoch 95/750, Train Loss: 6.022381403608916, Val Loss: 5.86718723268709, Val MAE: 1.6161898374557495\n",
      "Epoch 96/750, Train Loss: 6.01996127986059, Val Loss: 5.865047690508409, Val MAE: 1.6129652261734009\n",
      "Epoch 97/750, Train Loss: 6.018710115189369, Val Loss: 5.86295655779364, Val MAE: 1.6125962734222412\n",
      "Epoch 98/750, Train Loss: 6.017199965683572, Val Loss: 5.8608850231829255, Val MAE: 1.6127777099609375\n",
      "Epoch 99/750, Train Loss: 6.016041064616127, Val Loss: 5.859239390030372, Val MAE: 1.612783670425415\n",
      "Epoch 100/750, Train Loss: 6.014649903668141, Val Loss: 5.858150276204744, Val MAE: 1.6171116828918457\n",
      "Epoch 101/750, Train Loss: 6.012541474079166, Val Loss: 5.856221668447444, Val MAE: 1.6159470081329346\n",
      "Epoch 102/750, Train Loss: 6.010803586439138, Val Loss: 5.85443303225084, Val MAE: 1.6155468225479126\n",
      "Epoch 103/750, Train Loss: 6.009494222448558, Val Loss: 5.8524055019293435, Val MAE: 1.6138709783554077\n",
      "Epoch 104/750, Train Loss: 6.008350670585293, Val Loss: 5.850436955105233, Val MAE: 1.613011121749878\n",
      "Epoch 105/750, Train Loss: 6.006217281330233, Val Loss: 5.848403021541897, Val MAE: 1.612145185470581\n",
      "Epoch 106/750, Train Loss: 6.005400176784051, Val Loss: 5.847129911850172, Val MAE: 1.615505576133728\n",
      "Epoch 107/750, Train Loss: 6.003609935183199, Val Loss: 5.845182488879886, Val MAE: 1.6117384433746338\n",
      "Epoch 108/750, Train Loss: 6.002084949108543, Val Loss: 5.842937309030108, Val MAE: 1.608958125114441\n",
      "Epoch 109/750, Train Loss: 6.000737081295659, Val Loss: 5.841742509752815, Val MAE: 1.60903000831604\n",
      "Epoch 110/750, Train Loss: 5.9995355253757285, Val Loss: 5.839974086673662, Val MAE: 1.6100317239761353\n",
      "Epoch 111/750, Train Loss: 5.997936782780316, Val Loss: 5.838761573719188, Val MAE: 1.6116077899932861\n",
      "Epoch 112/750, Train Loss: 5.996453175841878, Val Loss: 5.8371796084967675, Val MAE: 1.6108137369155884\n",
      "Epoch 113/750, Train Loss: 5.995232545020672, Val Loss: 5.835285339794534, Val MAE: 1.6095012426376343\n",
      "Epoch 114/750, Train Loss: 5.993681346485806, Val Loss: 5.83417569646661, Val MAE: 1.611034870147705\n",
      "Epoch 115/750, Train Loss: 5.992288815303089, Val Loss: 5.832236356160571, Val MAE: 1.6096957921981812\n",
      "Epoch 116/750, Train Loss: 5.990828715021603, Val Loss: 5.830544931951534, Val MAE: 1.6075844764709473\n",
      "Epoch 117/750, Train Loss: 5.989519242363089, Val Loss: 5.8291715159961806, Val MAE: 1.609262466430664\n",
      "Epoch 118/750, Train Loss: 5.988402843192352, Val Loss: 5.827436560138142, Val MAE: 1.6074013710021973\n",
      "Epoch 119/750, Train Loss: 5.987635782705924, Val Loss: 5.826405484206626, Val MAE: 1.6093515157699585\n",
      "Epoch 120/750, Train Loss: 5.985772442463952, Val Loss: 5.824795147011873, Val MAE: 1.6098833084106445\n",
      "Epoch 121/750, Train Loss: 5.984676986490584, Val Loss: 5.823300367444451, Val MAE: 1.6084630489349365\n",
      "Epoch 122/750, Train Loss: 5.983613909316699, Val Loss: 5.821908637649236, Val MAE: 1.6072683334350586\n",
      "Epoch 123/750, Train Loss: 5.982087920610558, Val Loss: 5.820217772117723, Val MAE: 1.606768250465393\n",
      "Epoch 124/750, Train Loss: 5.981975817751106, Val Loss: 5.819715093258837, Val MAE: 1.6097221374511719\n",
      "Epoch 125/750, Train Loss: 5.979966469263818, Val Loss: 5.818114873922267, Val MAE: 1.6085875034332275\n",
      "Epoch 126/750, Train Loss: 5.978651829501642, Val Loss: 5.817380059467592, Val MAE: 1.612836241722107\n",
      "Epoch 127/750, Train Loss: 5.97767276763916, Val Loss: 5.815947218851698, Val MAE: 1.6100108623504639\n",
      "Epoch 128/750, Train Loss: 5.97620430813345, Val Loss: 5.8146615982378504, Val MAE: 1.611094355583191\n",
      "Epoch 129/750, Train Loss: 5.975149394072128, Val Loss: 5.812569815319942, Val MAE: 1.608102560043335\n",
      "Epoch 130/750, Train Loss: 5.974278729888735, Val Loss: 5.811287358231696, Val MAE: 1.606506109237671\n",
      "Epoch 131/750, Train Loss: 5.973477724397926, Val Loss: 5.810377214434346, Val MAE: 1.6089253425598145\n",
      "Epoch 132/750, Train Loss: 5.971715841378232, Val Loss: 5.809061944686487, Val MAE: 1.6087874174118042\n",
      "Epoch 133/750, Train Loss: 5.970856420647143, Val Loss: 5.807766118240099, Val MAE: 1.6082782745361328\n",
      "Epoch 134/750, Train Loss: 5.9696168432603605, Val Loss: 5.80629656648991, Val MAE: 1.6060916185379028\n",
      "Epoch 135/750, Train Loss: 5.968291174268864, Val Loss: 5.80544056776172, Val MAE: 1.6069279909133911\n",
      "Epoch 136/750, Train Loss: 5.967157495481678, Val Loss: 5.80356969136036, Val MAE: 1.60238516330719\n",
      "Epoch 137/750, Train Loss: 5.966168365987897, Val Loss: 5.8022162499424565, Val MAE: 1.6028977632522583\n",
      "Epoch 138/750, Train Loss: 5.965605390814716, Val Loss: 5.801386036740499, Val MAE: 1.6039268970489502\n",
      "Epoch 139/750, Train Loss: 5.964581884616207, Val Loss: 5.800255985460171, Val MAE: 1.6058707237243652\n",
      "Epoch 140/750, Train Loss: 5.963409852840073, Val Loss: 5.799745557916753, Val MAE: 1.6074705123901367\n",
      "Epoch 141/750, Train Loss: 5.962093426565742, Val Loss: 5.79852286850833, Val MAE: 1.6069164276123047\n",
      "Epoch 142/750, Train Loss: 5.961209904758442, Val Loss: 5.797230583796569, Val MAE: 1.6062284708023071\n",
      "Epoch 143/750, Train Loss: 5.960232892305278, Val Loss: 5.796143115817815, Val MAE: 1.6075583696365356\n",
      "Epoch 144/750, Train Loss: 5.959169538226255, Val Loss: 5.795147831567494, Val MAE: 1.6064423322677612\n",
      "Epoch 145/750, Train Loss: 5.958484078302582, Val Loss: 5.79401016316062, Val MAE: 1.6062532663345337\n",
      "Epoch 146/750, Train Loss: 5.957578595385, Val Loss: 5.792687145858176, Val MAE: 1.603389024734497\n",
      "Epoch 147/750, Train Loss: 5.956322302521159, Val Loss: 5.791387283729393, Val MAE: 1.6025069952011108\n",
      "Epoch 148/750, Train Loss: 5.955336027400076, Val Loss: 5.790859023380215, Val MAE: 1.605265736579895\n",
      "Epoch 149/750, Train Loss: 5.954302835605972, Val Loss: 5.789614622384693, Val MAE: 1.6034828424453735\n",
      "Epoch 150/750, Train Loss: 5.953702047104652, Val Loss: 5.7888088184348945, Val MAE: 1.60429847240448\n",
      "Epoch 151/750, Train Loss: 5.952925168442089, Val Loss: 5.787739228621155, Val MAE: 1.6036853790283203\n",
      "Epoch 152/750, Train Loss: 5.951893427817687, Val Loss: 5.78655630265367, Val MAE: 1.6019922494888306\n",
      "Epoch 153/750, Train Loss: 5.951056747832709, Val Loss: 5.7857869173436045, Val MAE: 1.6032859086990356\n",
      "Epoch 154/750, Train Loss: 5.950101536883799, Val Loss: 5.785199002632033, Val MAE: 1.6039255857467651\n",
      "Epoch 155/750, Train Loss: 5.949639193452784, Val Loss: 5.784505357593803, Val MAE: 1.6057239770889282\n",
      "Epoch 156/750, Train Loss: 5.948733553334584, Val Loss: 5.783640862964018, Val MAE: 1.6043189764022827\n",
      "Epoch 157/750, Train Loss: 5.948270081200302, Val Loss: 5.78307011422872, Val MAE: 1.6057108640670776\n",
      "Epoch 158/750, Train Loss: 5.947450161546204, Val Loss: 5.78144250290573, Val MAE: 1.6035513877868652\n",
      "Epoch 159/750, Train Loss: 5.946491338661941, Val Loss: 5.781302983000542, Val MAE: 1.6063205003738403\n",
      "Epoch 160/750, Train Loss: 5.945896654100729, Val Loss: 5.779599556829289, Val MAE: 1.60269296169281\n",
      "Epoch 161/750, Train Loss: 5.945112861157878, Val Loss: 5.778868982900053, Val MAE: 1.6018805503845215\n",
      "Epoch 162/750, Train Loss: 5.944204527404966, Val Loss: 5.777996892451272, Val MAE: 1.6015803813934326\n",
      "Epoch 163/750, Train Loss: 5.9434816767978385, Val Loss: 5.777260605999967, Val MAE: 1.600951075553894\n",
      "Epoch 164/750, Train Loss: 5.9425480638837955, Val Loss: 5.777026111238618, Val MAE: 1.6035168170928955\n",
      "Epoch 165/750, Train Loss: 5.942324551630445, Val Loss: 5.7758366527312015, Val MAE: 1.6029114723205566\n",
      "Epoch 166/750, Train Loss: 5.941759761221685, Val Loss: 5.775462712156829, Val MAE: 1.6033633947372437\n",
      "Epoch 167/750, Train Loss: 5.940665619734133, Val Loss: 5.775264662508231, Val MAE: 1.6054290533065796\n",
      "Epoch 168/750, Train Loss: 5.939882669816739, Val Loss: 5.77394398161101, Val MAE: 1.6032980680465698\n",
      "Epoch 169/750, Train Loss: 5.939573381350373, Val Loss: 5.773032662024269, Val MAE: 1.602849006652832\n",
      "Epoch 170/750, Train Loss: 5.938953906350744, Val Loss: 5.7724103301299445, Val MAE: 1.602560043334961\n",
      "Epoch 171/750, Train Loss: 5.938084609713682, Val Loss: 5.771931096195125, Val MAE: 1.6029549837112427\n",
      "Epoch 172/750, Train Loss: 5.937350481412531, Val Loss: 5.771078746869263, Val MAE: 1.6026630401611328\n",
      "Epoch 173/750, Train Loss: 5.93675124114628, Val Loss: 5.770638723831849, Val MAE: 1.6030277013778687\n",
      "Epoch 174/750, Train Loss: 5.93597112352841, Val Loss: 5.769734720483211, Val MAE: 1.6028395891189575\n",
      "Epoch 175/750, Train Loss: 5.935216610226504, Val Loss: 5.7694399116324995, Val MAE: 1.603944182395935\n",
      "Epoch 176/750, Train Loss: 5.934876195754779, Val Loss: 5.768282800892679, Val MAE: 1.6020750999450684\n",
      "Epoch 177/750, Train Loss: 5.934553716868958, Val Loss: 5.767630282010109, Val MAE: 1.6006265878677368\n",
      "Epoch 178/750, Train Loss: 5.933854822447462, Val Loss: 5.766924586099148, Val MAE: 1.6002540588378906\n",
      "Epoch 179/750, Train Loss: 5.9332513349346305, Val Loss: 5.767007544626638, Val MAE: 1.6029953956604004\n",
      "Epoch 180/750, Train Loss: 5.932509805186919, Val Loss: 5.766042166815146, Val MAE: 1.600631833076477\n",
      "Epoch 181/750, Train Loss: 5.931879729828424, Val Loss: 5.765300648714452, Val MAE: 1.6007027626037598\n",
      "Epoch 182/750, Train Loss: 5.931185509047805, Val Loss: 5.764842425961711, Val MAE: 1.6002371311187744\n",
      "Epoch 183/750, Train Loss: 5.930832652309882, Val Loss: 5.7640221257748925, Val MAE: 1.598624348640442\n",
      "Epoch 184/750, Train Loss: 5.930356250247899, Val Loss: 5.763633493482865, Val MAE: 1.5993049144744873\n",
      "Epoch 185/750, Train Loss: 5.930033990820367, Val Loss: 5.762924106845843, Val MAE: 1.5982327461242676\n",
      "Epoch 186/750, Train Loss: 5.929452280899183, Val Loss: 5.762310199827945, Val MAE: 1.5980563163757324\n",
      "Epoch 187/750, Train Loss: 5.929681935013225, Val Loss: 5.7623814322614315, Val MAE: 1.6000853776931763\n",
      "Epoch 188/750, Train Loss: 5.928273660422079, Val Loss: 5.76178020646464, Val MAE: 1.601096510887146\n",
      "Epoch 189/750, Train Loss: 5.927632746993611, Val Loss: 5.761018452486156, Val MAE: 1.6000216007232666\n",
      "Epoch 190/750, Train Loss: 5.927173036637575, Val Loss: 5.760921340256667, Val MAE: 1.602331280708313\n",
      "Epoch 191/750, Train Loss: 5.927035113116754, Val Loss: 5.760696192246155, Val MAE: 1.603257417678833\n",
      "Epoch 192/750, Train Loss: 5.926292279070846, Val Loss: 5.759356561349548, Val MAE: 1.5984989404678345\n",
      "Epoch 193/750, Train Loss: 5.925775736092814, Val Loss: 5.758878934843442, Val MAE: 1.5995666980743408\n",
      "Epoch 194/750, Train Loss: 5.9251543407270395, Val Loss: 5.758007621636, Val MAE: 1.598239779472351\n",
      "Epoch 195/750, Train Loss: 5.924864208379909, Val Loss: 5.758855990001133, Val MAE: 1.6030364036560059\n",
      "Epoch 196/750, Train Loss: 5.924976193374271, Val Loss: 5.757763862448442, Val MAE: 1.6009962558746338\n",
      "Epoch 197/750, Train Loss: 5.923954922226133, Val Loss: 5.757278315183191, Val MAE: 1.5996975898742676\n",
      "Epoch 198/750, Train Loss: 5.923340273186432, Val Loss: 5.75684760725813, Val MAE: 1.6001882553100586\n",
      "Epoch 199/750, Train Loss: 5.923659787758168, Val Loss: 5.757444010445938, Val MAE: 1.6043219566345215\n",
      "Epoch 200/750, Train Loss: 5.92325878185759, Val Loss: 5.757129457566733, Val MAE: 1.6046359539031982\n",
      "Epoch 201/750, Train Loss: 5.922162080589317, Val Loss: 5.75552408099578, Val MAE: 1.5994793176651\n",
      "Epoch 202/750, Train Loss: 5.921702937768545, Val Loss: 5.7550369903858885, Val MAE: 1.5991714000701904\n",
      "Epoch 203/750, Train Loss: 5.921252217844615, Val Loss: 5.754343744702814, Val MAE: 1.597673773765564\n",
      "Epoch 204/750, Train Loss: 5.92094383749127, Val Loss: 5.753805486269714, Val MAE: 1.5971567630767822\n",
      "Epoch 205/750, Train Loss: 5.920925835968832, Val Loss: 5.754297144927927, Val MAE: 1.6016788482666016\n",
      "Epoch 206/750, Train Loss: 5.920042302558967, Val Loss: 5.753248570816051, Val MAE: 1.5987197160720825\n",
      "Epoch 207/750, Train Loss: 5.920215204844489, Val Loss: 5.752427270465714, Val MAE: 1.5964763164520264\n",
      "Epoch 208/750, Train Loss: 5.919239270015003, Val Loss: 5.752106642674657, Val MAE: 1.596991777420044\n",
      "Epoch 209/750, Train Loss: 5.918833167050642, Val Loss: 5.752129761044367, Val MAE: 1.5986673831939697\n",
      "Epoch 210/750, Train Loss: 5.918565181030364, Val Loss: 5.751375149938006, Val MAE: 1.5971107482910156\n",
      "Epoch 211/750, Train Loss: 5.918120721822676, Val Loss: 5.750754729266415, Val MAE: 1.5964378118515015\n",
      "Epoch 212/750, Train Loss: 5.9177514410160414, Val Loss: 5.750598137654722, Val MAE: 1.5963761806488037\n",
      "Epoch 213/750, Train Loss: 5.917170105206861, Val Loss: 5.750794542909233, Val MAE: 1.6000633239746094\n",
      "Epoch 214/750, Train Loss: 5.916943378476785, Val Loss: 5.749965104365882, Val MAE: 1.5974946022033691\n",
      "Epoch 215/750, Train Loss: 5.9164983721090705, Val Loss: 5.749490792313861, Val MAE: 1.596669316291809\n",
      "Epoch 216/750, Train Loss: 5.916325111841943, Val Loss: 5.749660581370504, Val MAE: 1.598515510559082\n",
      "Epoch 217/750, Train Loss: 5.915845819892091, Val Loss: 5.749556385833182, Val MAE: 1.6000887155532837\n",
      "Epoch 218/750, Train Loss: 5.9152838334839135, Val Loss: 5.748909039197133, Val MAE: 1.5981217622756958\n",
      "Epoch 219/750, Train Loss: 5.915359689432365, Val Loss: 5.748576731478433, Val MAE: 1.5991343259811401\n",
      "Epoch 220/750, Train Loss: 5.914925940595678, Val Loss: 5.7483855131597394, Val MAE: 1.5995144844055176\n",
      "Epoch 221/750, Train Loss: 5.9151243805531575, Val Loss: 5.747280037233615, Val MAE: 1.595935583114624\n",
      "Epoch 222/750, Train Loss: 5.914126762367498, Val Loss: 5.748097488504047, Val MAE: 1.6007025241851807\n",
      "Epoch 223/750, Train Loss: 5.913885458711347, Val Loss: 5.747262789578396, Val MAE: 1.5991673469543457\n",
      "Epoch 224/750, Train Loss: 5.913010101148566, Val Loss: 5.7463118423056425, Val MAE: 1.5955755710601807\n",
      "Epoch 225/750, Train Loss: 5.91278853572087, Val Loss: 5.746107659956532, Val MAE: 1.5953991413116455\n",
      "Epoch 226/750, Train Loss: 5.912665412121782, Val Loss: 5.7459905497835715, Val MAE: 1.5963279008865356\n",
      "Epoch 227/750, Train Loss: 5.912667505818941, Val Loss: 5.745216490048852, Val MAE: 1.5932300090789795\n",
      "Epoch 228/750, Train Loss: 5.912362812888374, Val Loss: 5.745275675967972, Val MAE: 1.5950497388839722\n",
      "Epoch 229/750, Train Loss: 5.9116571613164615, Val Loss: 5.74444684355987, Val MAE: 1.591978669166565\n",
      "Epoch 230/750, Train Loss: 5.911713703732816, Val Loss: 5.7446782755222205, Val MAE: 1.5941118001937866\n",
      "Epoch 231/750, Train Loss: 5.911109461770213, Val Loss: 5.744363518122813, Val MAE: 1.594881296157837\n",
      "Epoch 232/750, Train Loss: 5.9109725946488645, Val Loss: 5.743970219477173, Val MAE: 1.5927708148956299\n",
      "Epoch 233/750, Train Loss: 5.911165987348698, Val Loss: 5.744394559995823, Val MAE: 1.597109079360962\n",
      "Epoch 234/750, Train Loss: 5.910448953305932, Val Loss: 5.743845952872739, Val MAE: 1.5958023071289062\n",
      "Epoch 235/750, Train Loss: 5.910139386377986, Val Loss: 5.743588577837418, Val MAE: 1.595140814781189\n",
      "Epoch 236/750, Train Loss: 5.909887568816233, Val Loss: 5.74278389011272, Val MAE: 1.5925853252410889\n",
      "Epoch 237/750, Train Loss: 5.909598009239672, Val Loss: 5.743319895595171, Val MAE: 1.5964112281799316\n",
      "Epoch 238/750, Train Loss: 5.9093166883338455, Val Loss: 5.743266654482708, Val MAE: 1.5965222120285034\n",
      "Epoch 239/750, Train Loss: 5.909213847151849, Val Loss: 5.742439177848232, Val MAE: 1.5939847230911255\n",
      "Epoch 240/750, Train Loss: 5.909301009305507, Val Loss: 5.742328972451657, Val MAE: 1.5939445495605469\n",
      "Epoch 241/750, Train Loss: 5.908810521587066, Val Loss: 5.742028090464721, Val MAE: 1.5933271646499634\n",
      "Epoch 242/750, Train Loss: 5.908039344878154, Val Loss: 5.742104890302944, Val MAE: 1.5944911241531372\n",
      "Epoch 243/750, Train Loss: 5.907932262746092, Val Loss: 5.7420442958255835, Val MAE: 1.5948774814605713\n",
      "Epoch 244/750, Train Loss: 5.9076828116125455, Val Loss: 5.741729373131237, Val MAE: 1.5948591232299805\n",
      "Epoch 245/750, Train Loss: 5.907766678071518, Val Loss: 5.741917492976928, Val MAE: 1.596655249595642\n",
      "Epoch 246/750, Train Loss: 5.907574250082588, Val Loss: 5.741423038994047, Val MAE: 1.5953127145767212\n",
      "Epoch 247/750, Train Loss: 5.907168427985217, Val Loss: 5.741392379042742, Val MAE: 1.5952951908111572\n",
      "Epoch 248/750, Train Loss: 5.906831550315155, Val Loss: 5.7416125869299, Val MAE: 1.5981745719909668\n",
      "Epoch 249/750, Train Loss: 5.906347254546531, Val Loss: 5.740897882767602, Val MAE: 1.5951260328292847\n",
      "Epoch 250/750, Train Loss: 5.906085107729767, Val Loss: 5.74047939310868, Val MAE: 1.594271183013916\n",
      "Epoch 251/750, Train Loss: 5.906236308406298, Val Loss: 5.73952078076739, Val MAE: 1.5896950960159302\n",
      "Epoch 252/750, Train Loss: 5.905545367685199, Val Loss: 5.7398742988614835, Val MAE: 1.593040108680725\n",
      "Epoch 253/750, Train Loss: 5.905474410778691, Val Loss: 5.7394940961270855, Val MAE: 1.5920209884643555\n",
      "Epoch 254/750, Train Loss: 5.905316380577201, Val Loss: 5.739820236439792, Val MAE: 1.5938979387283325\n",
      "Epoch 255/750, Train Loss: 5.905450721593568, Val Loss: 5.740416110974352, Val MAE: 1.59749174118042\n",
      "Epoch 256/750, Train Loss: 5.904936234392115, Val Loss: 5.739353511005781, Val MAE: 1.5935596227645874\n",
      "Epoch 257/750, Train Loss: 5.904347788615468, Val Loss: 5.7391623026951795, Val MAE: 1.594224214553833\n",
      "Epoch 258/750, Train Loss: 5.904372681354556, Val Loss: 5.738161289457865, Val MAE: 1.5895460844039917\n",
      "Epoch 259/750, Train Loss: 5.90487718751947, Val Loss: 5.738847101711307, Val MAE: 1.5927679538726807\n",
      "Epoch 260/750, Train Loss: 5.904120988874124, Val Loss: 5.7384134853055695, Val MAE: 1.5921496152877808\n",
      "Epoch 261/750, Train Loss: 5.904179696866949, Val Loss: 5.738499137624019, Val MAE: 1.593240737915039\n",
      "Epoch 262/750, Train Loss: 5.9038159511917065, Val Loss: 5.73880983206091, Val MAE: 1.5952343940734863\n",
      "Epoch 263/750, Train Loss: 5.903313430480504, Val Loss: 5.73829474575066, Val MAE: 1.594232201576233\n",
      "Epoch 264/750, Train Loss: 5.903361436238275, Val Loss: 5.738014839756061, Val MAE: 1.5941954851150513\n",
      "Epoch 265/750, Train Loss: 5.902808964217098, Val Loss: 5.7377696460215075, Val MAE: 1.5924924612045288\n",
      "Epoch 266/750, Train Loss: 5.9030071377400475, Val Loss: 5.737560212491086, Val MAE: 1.5919990539550781\n",
      "Epoch 267/750, Train Loss: 5.9023222544072995, Val Loss: 5.737421592022812, Val MAE: 1.5928291082382202\n",
      "Epoch 268/750, Train Loss: 5.902382384846401, Val Loss: 5.737542821494163, Val MAE: 1.5939116477966309\n",
      "Epoch 269/750, Train Loss: 5.90224708930675, Val Loss: 5.737686683781339, Val MAE: 1.594825267791748\n",
      "Epoch 270/750, Train Loss: 5.901677754795516, Val Loss: 5.736887777989671, Val MAE: 1.5915801525115967\n",
      "Epoch 271/750, Train Loss: 5.901932249154111, Val Loss: 5.736819753795357, Val MAE: 1.5918891429901123\n",
      "Epoch 272/750, Train Loss: 5.901347136851235, Val Loss: 5.73694323492599, Val MAE: 1.5938612222671509\n",
      "Epoch 273/750, Train Loss: 5.901141694179275, Val Loss: 5.7372107549381965, Val MAE: 1.5949242115020752\n",
      "Epoch 274/750, Train Loss: 5.901238056035707, Val Loss: 5.736885381697317, Val MAE: 1.5949143171310425\n",
      "Epoch 275/750, Train Loss: 5.90095259556077, Val Loss: 5.736588018846092, Val MAE: 1.5933672189712524\n",
      "Epoch 276/750, Train Loss: 5.9009085018486225, Val Loss: 5.736922479757362, Val MAE: 1.5954170227050781\n",
      "Epoch 277/750, Train Loss: 5.901326432779918, Val Loss: 5.736670748801673, Val MAE: 1.5951677560806274\n",
      "Epoch 278/750, Train Loss: 5.900674269814873, Val Loss: 5.736195879202076, Val MAE: 1.593706727027893\n",
      "Epoch 279/750, Train Loss: 5.900526065430231, Val Loss: 5.735302704770418, Val MAE: 1.5894145965576172\n",
      "Epoch 280/750, Train Loss: 5.900157883188491, Val Loss: 5.735586872920848, Val MAE: 1.590776801109314\n",
      "Epoch 281/750, Train Loss: 5.8997160359730705, Val Loss: 5.73543882337711, Val MAE: 1.5909297466278076\n",
      "Epoch 282/750, Train Loss: 5.899728406006221, Val Loss: 5.73591236416036, Val MAE: 1.5935378074645996\n",
      "Epoch 283/750, Train Loss: 5.899375516512274, Val Loss: 5.735668216917106, Val MAE: 1.5927609205245972\n",
      "Epoch 284/750, Train Loss: 5.899356244933357, Val Loss: 5.73571856230114, Val MAE: 1.5935542583465576\n",
      "Epoch 285/750, Train Loss: 5.899306333383396, Val Loss: 5.735430333286665, Val MAE: 1.593220591545105\n",
      "Epoch 286/750, Train Loss: 5.899390990373289, Val Loss: 5.735091149040227, Val MAE: 1.592146635055542\n",
      "Epoch 287/750, Train Loss: 5.89869057799659, Val Loss: 5.735591888427734, Val MAE: 1.595218539237976\n",
      "Epoch 288/750, Train Loss: 5.8988998962085395, Val Loss: 5.734807051753545, Val MAE: 1.5918986797332764\n",
      "Epoch 289/750, Train Loss: 5.898670970226254, Val Loss: 5.734588765581476, Val MAE: 1.5915312767028809\n",
      "Epoch 290/750, Train Loss: 5.898814826195601, Val Loss: 5.735360018046385, Val MAE: 1.5955654382705688\n",
      "Epoch 291/750, Train Loss: 5.898164382391231, Val Loss: 5.735102171500793, Val MAE: 1.5949066877365112\n",
      "Epoch 292/750, Train Loss: 5.897768798001796, Val Loss: 5.734172066563921, Val MAE: 1.5911312103271484\n",
      "Epoch 293/750, Train Loss: 5.897853458812046, Val Loss: 5.73457479347792, Val MAE: 1.5933001041412354\n",
      "Epoch 294/750, Train Loss: 5.897465287047253, Val Loss: 5.733818137008109, Val MAE: 1.5902730226516724\n",
      "Epoch 295/750, Train Loss: 5.897670043752879, Val Loss: 5.733887526984141, Val MAE: 1.5909274816513062\n",
      "Epoch 296/750, Train Loss: 5.897389456921586, Val Loss: 5.733628344681106, Val MAE: 1.589966058731079\n",
      "Epoch 297/750, Train Loss: 5.897824810378983, Val Loss: 5.733475993756273, Val MAE: 1.5906970500946045\n",
      "Epoch 298/750, Train Loss: 5.89716661429193, Val Loss: 5.733134606115383, Val MAE: 1.587835431098938\n",
      "Epoch 299/750, Train Loss: 5.897159252534634, Val Loss: 5.733241288824992, Val MAE: 1.589477777481079\n",
      "Epoch 300/750, Train Loss: 5.896736184496554, Val Loss: 5.7338679986138885, Val MAE: 1.5936999320983887\n",
      "Epoch 301/750, Train Loss: 5.8965175713558935, Val Loss: 5.733563772955696, Val MAE: 1.5921063423156738\n",
      "Epoch 302/750, Train Loss: 5.896624489283349, Val Loss: 5.734240881074661, Val MAE: 1.595913052558899\n",
      "Epoch 303/750, Train Loss: 5.89603216188244, Val Loss: 5.73375631121259, Val MAE: 1.5942120552062988\n",
      "Epoch 304/750, Train Loss: 5.896530061376555, Val Loss: 5.732687836332586, Val MAE: 1.5894678831100464\n",
      "Epoch 305/750, Train Loss: 5.8962450613253905, Val Loss: 5.732593732180431, Val MAE: 1.5894272327423096\n",
      "Epoch 306/750, Train Loss: 5.896125212055286, Val Loss: 5.7324807538966835, Val MAE: 1.5889592170715332\n",
      "Epoch 307/750, Train Loss: 5.895914109312108, Val Loss: 5.732650722776141, Val MAE: 1.5902798175811768\n",
      "Epoch 308/750, Train Loss: 5.895453815573401, Val Loss: 5.732365363824182, Val MAE: 1.5898317098617554\n",
      "Epoch 309/750, Train Loss: 5.895653944553183, Val Loss: 5.732357446132032, Val MAE: 1.589015245437622\n",
      "Epoch 310/750, Train Loss: 5.89545183733592, Val Loss: 5.733188650619362, Val MAE: 1.5944584608078003\n",
      "Epoch 311/750, Train Loss: 5.895279613234523, Val Loss: 5.732974570912934, Val MAE: 1.5938243865966797\n",
      "Epoch 312/750, Train Loss: 5.894844129319007, Val Loss: 5.732250873350016, Val MAE: 1.5909442901611328\n",
      "Epoch 313/750, Train Loss: 5.894875557557058, Val Loss: 5.732038031741422, Val MAE: 1.590012550354004\n",
      "Epoch 314/750, Train Loss: 5.894716130519833, Val Loss: 5.732392473646976, Val MAE: 1.5922740697860718\n",
      "Epoch 315/750, Train Loss: 5.8950053050893, Val Loss: 5.732460302199232, Val MAE: 1.5927258729934692\n",
      "Epoch 316/750, Train Loss: 5.894569881450528, Val Loss: 5.731045681637691, Val MAE: 1.5854436159133911\n",
      "Epoch 317/750, Train Loss: 5.894471552251709, Val Loss: 5.73170404750589, Val MAE: 1.589781641960144\n",
      "Epoch 318/750, Train Loss: 5.89449474330472, Val Loss: 5.731731515844367, Val MAE: 1.5900166034698486\n",
      "Epoch 319/750, Train Loss: 5.894104783966562, Val Loss: 5.7320333374620045, Val MAE: 1.5922847986221313\n",
      "Epoch 320/750, Train Loss: 5.893860093068652, Val Loss: 5.731531263931264, Val MAE: 1.590348720550537\n",
      "Epoch 321/750, Train Loss: 5.893845853041471, Val Loss: 5.731448588051664, Val MAE: 1.5902715921401978\n",
      "Epoch 322/750, Train Loss: 5.8938260460465886, Val Loss: 5.731692902688328, Val MAE: 1.591787576675415\n",
      "Epoch 323/750, Train Loss: 5.893306770664294, Val Loss: 5.7316546530520185, Val MAE: 1.5921608209609985\n",
      "Epoch 324/750, Train Loss: 5.893450523059517, Val Loss: 5.73139190641544, Val MAE: 1.5907484292984009\n",
      "Epoch 325/750, Train Loss: 5.893635850844114, Val Loss: 5.731660374452233, Val MAE: 1.5927151441574097\n",
      "Epoch 326/750, Train Loss: 5.893248556346497, Val Loss: 5.731068580152218, Val MAE: 1.58998441696167\n",
      "Epoch 327/750, Train Loss: 5.893108400746693, Val Loss: 5.730543775984622, Val MAE: 1.5877676010131836\n",
      "Epoch 328/750, Train Loss: 5.892969334656124, Val Loss: 5.730726860307243, Val MAE: 1.5889430046081543\n",
      "Epoch 329/750, Train Loss: 5.892577573453637, Val Loss: 5.730627383423885, Val MAE: 1.5893020629882812\n",
      "Epoch 330/750, Train Loss: 5.8930423544139465, Val Loss: 5.730400916120856, Val MAE: 1.5880472660064697\n",
      "Epoch 331/750, Train Loss: 5.8926504732239495, Val Loss: 5.730181069654956, Val MAE: 1.5877999067306519\n",
      "Epoch 332/750, Train Loss: 5.893051693347512, Val Loss: 5.730532602756994, Val MAE: 1.5894235372543335\n",
      "Epoch 333/750, Train Loss: 5.892471344039419, Val Loss: 5.73023220777673, Val MAE: 1.5882210731506348\n",
      "Epoch 334/750, Train Loss: 5.892480968580048, Val Loss: 5.730836052142371, Val MAE: 1.5920435190200806\n",
      "Epoch 335/750, Train Loss: 5.891978311538696, Val Loss: 5.730927366458167, Val MAE: 1.5923885107040405\n",
      "Epoch 336/750, Train Loss: 5.892060987885698, Val Loss: 5.730252649466089, Val MAE: 1.5893263816833496\n",
      "Epoch 337/750, Train Loss: 5.892183797649531, Val Loss: 5.729648637868459, Val MAE: 1.586401343345642\n",
      "Epoch 338/750, Train Loss: 5.892218410650417, Val Loss: 5.730173772625933, Val MAE: 1.5896073579788208\n",
      "Epoch 339/750, Train Loss: 5.891698978774979, Val Loss: 5.730144086849391, Val MAE: 1.5896871089935303\n",
      "Epoch 340/750, Train Loss: 5.891960319496404, Val Loss: 5.730001760158655, Val MAE: 1.5892754793167114\n",
      "Epoch 341/750, Train Loss: 5.89139663430279, Val Loss: 5.729867077390326, Val MAE: 1.5891203880310059\n",
      "Epoch 342/750, Train Loss: 5.891418179418637, Val Loss: 5.730008602465175, Val MAE: 1.5895822048187256\n",
      "Epoch 343/750, Train Loss: 5.891546758063115, Val Loss: 5.729507944595192, Val MAE: 1.5875067710876465\n",
      "Epoch 344/750, Train Loss: 5.8913644718487115, Val Loss: 5.72991854093651, Val MAE: 1.5901392698287964\n",
      "Epoch 345/750, Train Loss: 5.8911353793271575, Val Loss: 5.730274890997802, Val MAE: 1.5921595096588135\n",
      "Epoch 346/750, Train Loss: 5.8911744813890765, Val Loss: 5.730276862430508, Val MAE: 1.59291410446167\n",
      "Epoch 347/750, Train Loss: 5.891587224558482, Val Loss: 5.729414656909641, Val MAE: 1.5888813734054565\n",
      "Epoch 348/750, Train Loss: 5.890615742567385, Val Loss: 5.730292198540153, Val MAE: 1.5931334495544434\n",
      "Epoch 349/750, Train Loss: 5.890693639930703, Val Loss: 5.7298091484552796, Val MAE: 1.5908482074737549\n",
      "Epoch 350/750, Train Loss: 5.890424952521169, Val Loss: 5.72980026482083, Val MAE: 1.5913128852844238\n",
      "Epoch 351/750, Train Loss: 5.890593843714774, Val Loss: 5.729344677908967, Val MAE: 1.5894060134887695\n",
      "Epoch 352/750, Train Loss: 5.890277422500294, Val Loss: 5.729295168846301, Val MAE: 1.5894590616226196\n",
      "Epoch 353/750, Train Loss: 5.890593759666919, Val Loss: 5.729426939582695, Val MAE: 1.590093970298767\n",
      "Epoch 354/750, Train Loss: 5.89028533799712, Val Loss: 5.729108841256185, Val MAE: 1.5893230438232422\n",
      "Epoch 355/750, Train Loss: 5.890335119796436, Val Loss: 5.729202643067109, Val MAE: 1.5897433757781982\n",
      "Epoch 356/750, Train Loss: 5.890069005043641, Val Loss: 5.729222524626157, Val MAE: 1.590295672416687\n",
      "Epoch 357/750, Train Loss: 5.88994046389526, Val Loss: 5.729118105682394, Val MAE: 1.5900249481201172\n",
      "Epoch 358/750, Train Loss: 5.8899278303989675, Val Loss: 5.7292188639243715, Val MAE: 1.5903971195220947\n",
      "Epoch 359/750, Train Loss: 5.8898121764468865, Val Loss: 5.728926114319302, Val MAE: 1.5890181064605713\n",
      "Epoch 360/750, Train Loss: 5.88976849349387, Val Loss: 5.728477664629834, Val MAE: 1.587529182434082\n",
      "Epoch 361/750, Train Loss: 5.8894339332240975, Val Loss: 5.728764762891331, Val MAE: 1.5882582664489746\n",
      "Epoch 362/750, Train Loss: 5.889197949485892, Val Loss: 5.728553937913278, Val MAE: 1.5877858400344849\n",
      "Epoch 363/750, Train Loss: 5.889654747597896, Val Loss: 5.728751347689671, Val MAE: 1.5894443988800049\n",
      "Epoch 364/750, Train Loss: 5.889155239308977, Val Loss: 5.728566313241895, Val MAE: 1.5887680053710938\n",
      "Epoch 365/750, Train Loss: 5.8888541331984525, Val Loss: 5.728700914976963, Val MAE: 1.5891779661178589\n",
      "Epoch 366/750, Train Loss: 5.888896746592988, Val Loss: 5.728235167914657, Val MAE: 1.5874278545379639\n",
      "Epoch 367/750, Train Loss: 5.888702644438701, Val Loss: 5.72833738749949, Val MAE: 1.5879706144332886\n",
      "Epoch 368/750, Train Loss: 5.888536629690968, Val Loss: 5.72916416841015, Val MAE: 1.5918292999267578\n",
      "Epoch 369/750, Train Loss: 5.888717446482854, Val Loss: 5.728645969421862, Val MAE: 1.5900952816009521\n",
      "Epoch 370/750, Train Loss: 5.88885658615067, Val Loss: 5.728545293343092, Val MAE: 1.5893081426620483\n",
      "Epoch 371/750, Train Loss: 5.888528643305294, Val Loss: 5.728789807657334, Val MAE: 1.5911307334899902\n",
      "Epoch 372/750, Train Loss: 5.888885408622221, Val Loss: 5.728244948564709, Val MAE: 1.5877646207809448\n",
      "Epoch 373/750, Train Loss: 5.888467892128919, Val Loss: 5.727911542700042, Val MAE: 1.5867290496826172\n",
      "Epoch 374/750, Train Loss: 5.888399284317515, Val Loss: 5.728131695219852, Val MAE: 1.588594913482666\n",
      "Epoch 375/750, Train Loss: 5.887933050631061, Val Loss: 5.728096228801647, Val MAE: 1.5884588956832886\n",
      "Epoch 376/750, Train Loss: 5.888128887934926, Val Loss: 5.727696426026145, Val MAE: 1.5865005254745483\n",
      "Epoch 377/750, Train Loss: 5.888045816251715, Val Loss: 5.727884680173974, Val MAE: 1.5876100063323975\n",
      "Epoch 378/750, Train Loss: 5.887716911100846, Val Loss: 5.727686611154875, Val MAE: 1.5867031812667847\n",
      "Epoch 379/750, Train Loss: 5.888186615793924, Val Loss: 5.7280146276619925, Val MAE: 1.5894508361816406\n",
      "Epoch 380/750, Train Loss: 5.887895188487248, Val Loss: 5.727815962515736, Val MAE: 1.5877021551132202\n",
      "Epoch 381/750, Train Loss: 5.887639050101668, Val Loss: 5.728213355923796, Val MAE: 1.5904525518417358\n",
      "Epoch 382/750, Train Loss: 5.887392562503985, Val Loss: 5.727110226733828, Val MAE: 1.5843526124954224\n",
      "Epoch 383/750, Train Loss: 5.887946742366259, Val Loss: 5.7273201587318, Val MAE: 1.5852402448654175\n",
      "Epoch 384/750, Train Loss: 5.8874587462280905, Val Loss: 5.7270424096424515, Val MAE: 1.5843610763549805\n",
      "Epoch 385/750, Train Loss: 5.887236830883988, Val Loss: 5.727154227794629, Val MAE: 1.5851281881332397\n",
      "Epoch 386/750, Train Loss: 5.8874604058194935, Val Loss: 5.727864074642463, Val MAE: 1.5885844230651855\n",
      "Epoch 387/750, Train Loss: 5.887326320908544, Val Loss: 5.727650856923314, Val MAE: 1.5885846614837646\n",
      "Epoch 388/750, Train Loss: 5.887109034988222, Val Loss: 5.7279359661572355, Val MAE: 1.5904392004013062\n",
      "Epoch 389/750, Train Loss: 5.886968163849692, Val Loss: 5.7274930257609995, Val MAE: 1.5880653858184814\n",
      "Epoch 390/750, Train Loss: 5.886671312201978, Val Loss: 5.7276589321622025, Val MAE: 1.5890707969665527\n",
      "Epoch 391/750, Train Loss: 5.886984344052278, Val Loss: 5.727703797631596, Val MAE: 1.5894495248794556\n",
      "Epoch 392/750, Train Loss: 5.8866094628851915, Val Loss: 5.727827973194677, Val MAE: 1.5894423723220825\n",
      "Epoch 393/750, Train Loss: 5.886976972178111, Val Loss: 5.727252171987121, Val MAE: 1.587777853012085\n",
      "Epoch 394/750, Train Loss: 5.886961412288315, Val Loss: 5.727544807629078, Val MAE: 1.5892235040664673\n",
      "Epoch 395/750, Train Loss: 5.886604851006754, Val Loss: 5.727989102507574, Val MAE: 1.5910645723342896\n",
      "Epoch 396/750, Train Loss: 5.886329130744368, Val Loss: 5.727533059743263, Val MAE: 1.5892221927642822\n",
      "Epoch 397/750, Train Loss: 5.886224425935604, Val Loss: 5.727236782770022, Val MAE: 1.5875768661499023\n",
      "Epoch 398/750, Train Loss: 5.886666121610195, Val Loss: 5.727424056931937, Val MAE: 1.5891523361206055\n",
      "Epoch 399/750, Train Loss: 5.886066595383143, Val Loss: 5.727687308170058, Val MAE: 1.5903809070587158\n",
      "Epoch 400/750, Train Loss: 5.886214779251054, Val Loss: 5.727477180045766, Val MAE: 1.5892717838287354\n",
      "Epoch 401/750, Train Loss: 5.88577923166292, Val Loss: 5.727222303320769, Val MAE: 1.5880414247512817\n",
      "Epoch 402/750, Train Loss: 5.886202835965935, Val Loss: 5.726627942752257, Val MAE: 1.5852935314178467\n",
      "Epoch 403/750, Train Loss: 5.885959484598403, Val Loss: 5.727390308192879, Val MAE: 1.5892300605773926\n",
      "Epoch 404/750, Train Loss: 5.885779671428465, Val Loss: 5.727132935256028, Val MAE: 1.5882165431976318\n",
      "Epoch 405/750, Train Loss: 5.885610397350187, Val Loss: 5.727776783418236, Val MAE: 1.5914877653121948\n",
      "Epoch 406/750, Train Loss: 5.886137911403215, Val Loss: 5.728793382321812, Val MAE: 1.5958317518234253\n",
      "Epoch 407/750, Train Loss: 5.885589720162864, Val Loss: 5.726915796786123, Val MAE: 1.5878632068634033\n",
      "Epoch 408/750, Train Loss: 5.885629035457305, Val Loss: 5.727419154419637, Val MAE: 1.5902100801467896\n",
      "Epoch 409/750, Train Loss: 5.885481068359285, Val Loss: 5.727548771317778, Val MAE: 1.5909565687179565\n",
      "Epoch 410/750, Train Loss: 5.885475924460754, Val Loss: 5.727138688294728, Val MAE: 1.589241862297058\n",
      "Epoch 411/750, Train Loss: 5.885323983483923, Val Loss: 5.72691458403136, Val MAE: 1.588072419166565\n",
      "Epoch 412/750, Train Loss: 5.885238984218337, Val Loss: 5.727153007775929, Val MAE: 1.5895594358444214\n",
      "Epoch 413/750, Train Loss: 5.884857734657537, Val Loss: 5.726563005892909, Val MAE: 1.5863057374954224\n",
      "Epoch 414/750, Train Loss: 5.8850199934283065, Val Loss: 5.7269038479313625, Val MAE: 1.5886033773422241\n",
      "Epoch 415/750, Train Loss: 5.885241299354711, Val Loss: 5.727167711277305, Val MAE: 1.5902314186096191\n",
      "Epoch 416/750, Train Loss: 5.8850325642424455, Val Loss: 5.727465708189586, Val MAE: 1.5912718772888184\n",
      "Epoch 417/750, Train Loss: 5.884974579004576, Val Loss: 5.7266937759815235, Val MAE: 1.5876972675323486\n",
      "Epoch 418/750, Train Loss: 5.88487796896643, Val Loss: 5.726783490455385, Val MAE: 1.588323712348938\n",
      "Epoch 419/750, Train Loss: 5.885013660408269, Val Loss: 5.726244264950039, Val MAE: 1.585666537284851\n",
      "Epoch 420/750, Train Loss: 5.885041981564077, Val Loss: 5.726340454820853, Val MAE: 1.5863733291625977\n",
      "Epoch 421/750, Train Loss: 5.884386909184895, Val Loss: 5.726883416250096, Val MAE: 1.5892242193222046\n",
      "Epoch 422/750, Train Loss: 5.884762142815293, Val Loss: 5.727297565948342, Val MAE: 1.5913078784942627\n",
      "Epoch 423/750, Train Loss: 5.885086039761053, Val Loss: 5.7260316151094015, Val MAE: 1.58509361743927\n",
      "Epoch 424/750, Train Loss: 5.88483519426793, Val Loss: 5.7268128979424, Val MAE: 1.5888053178787231\n",
      "Epoch 425/750, Train Loss: 5.884732143956759, Val Loss: 5.727237999237463, Val MAE: 1.5915395021438599\n",
      "Epoch 426/750, Train Loss: 5.884192637661444, Val Loss: 5.726957047074408, Val MAE: 1.589924693107605\n",
      "Epoch 427/750, Train Loss: 5.884061101353133, Val Loss: 5.725975432909253, Val MAE: 1.585367202758789\n",
      "Epoch 428/750, Train Loss: 5.884572491093984, Val Loss: 5.725569530847352, Val MAE: 1.5824730396270752\n",
      "Epoch 429/750, Train Loss: 5.884176352186797, Val Loss: 5.725699880111839, Val MAE: 1.5842549800872803\n",
      "Epoch 430/750, Train Loss: 5.88403916514592, Val Loss: 5.7260358343272895, Val MAE: 1.5864882469177246\n",
      "Epoch 431/750, Train Loss: 5.884295223445496, Val Loss: 5.7261366950715, Val MAE: 1.5869570970535278\n",
      "Epoch 432/750, Train Loss: 5.883962364366571, Val Loss: 5.727306483158326, Val MAE: 1.5919667482376099\n",
      "Epoch 433/750, Train Loss: 5.884092415829441, Val Loss: 5.726073290095914, Val MAE: 1.5861804485321045\n",
      "Epoch 434/750, Train Loss: 5.883637870030163, Val Loss: 5.726362049216424, Val MAE: 1.5885884761810303\n",
      "Epoch 435/750, Train Loss: 5.883448938799895, Val Loss: 5.726681042460065, Val MAE: 1.5898228883743286\n",
      "Epoch 436/750, Train Loss: 5.883595164664068, Val Loss: 5.725465863640436, Val MAE: 1.5836620330810547\n",
      "Epoch 437/750, Train Loss: 5.883529425657821, Val Loss: 5.726167673344699, Val MAE: 1.58778977394104\n",
      "Epoch 438/750, Train Loss: 5.883601133759722, Val Loss: 5.725424427879608, Val MAE: 1.5837101936340332\n",
      "Epoch 439/750, Train Loss: 5.883238437267722, Val Loss: 5.726458710597509, Val MAE: 1.5894291400909424\n",
      "Epoch 440/750, Train Loss: 5.8834380987489965, Val Loss: 5.726652419478326, Val MAE: 1.5900280475616455\n",
      "Epoch 441/750, Train Loss: 5.884294967198584, Val Loss: 5.726747867781808, Val MAE: 1.5906459093093872\n",
      "Epoch 442/750, Train Loss: 5.883103610924514, Val Loss: 5.726089859299346, Val MAE: 1.5878512859344482\n",
      "Epoch 443/750, Train Loss: 5.883562932453099, Val Loss: 5.726240042503734, Val MAE: 1.5882017612457275\n",
      "Epoch 444/750, Train Loss: 5.8828128926478085, Val Loss: 5.7264114799899835, Val MAE: 1.5892583131790161\n",
      "Epoch 445/750, Train Loss: 5.883378049103372, Val Loss: 5.725501439501969, Val MAE: 1.5848604440689087\n",
      "Epoch 446/750, Train Loss: 5.883311392220969, Val Loss: 5.726101166990534, Val MAE: 1.587449312210083\n",
      "Epoch 447/750, Train Loss: 5.88256639316457, Val Loss: 5.725481438975764, Val MAE: 1.584592342376709\n",
      "Epoch 448/750, Train Loss: 5.882891829572729, Val Loss: 5.725619395197286, Val MAE: 1.5857282876968384\n",
      "Epoch 449/750, Train Loss: 5.882490999691564, Val Loss: 5.725747672302463, Val MAE: 1.586417317390442\n",
      "Epoch 450/750, Train Loss: 5.882651078311909, Val Loss: 5.725307236673385, Val MAE: 1.584044098854065\n",
      "Epoch 451/750, Train Loss: 5.882548874988046, Val Loss: 5.725572431902669, Val MAE: 1.5863345861434937\n",
      "Epoch 452/750, Train Loss: 5.882486087278372, Val Loss: 5.726413129388115, Val MAE: 1.5902070999145508\n",
      "Epoch 453/750, Train Loss: 5.882492622918828, Val Loss: 5.7254809585873465, Val MAE: 1.5860991477966309\n",
      "Epoch 454/750, Train Loss: 5.8822344772893524, Val Loss: 5.72574366373877, Val MAE: 1.58739173412323\n",
      "Epoch 455/750, Train Loss: 5.882485077006173, Val Loss: 5.72539443508063, Val MAE: 1.5852049589157104\n",
      "Epoch 456/750, Train Loss: 5.88199683594067, Val Loss: 5.724907214526962, Val MAE: 1.5832622051239014\n",
      "Epoch 457/750, Train Loss: 5.882302415972294, Val Loss: 5.725380160474422, Val MAE: 1.5853692293167114\n",
      "Epoch 458/750, Train Loss: 5.882051034921709, Val Loss: 5.725409722441181, Val MAE: 1.586249589920044\n",
      "Epoch 459/750, Train Loss: 5.882471198922449, Val Loss: 5.725639902100663, Val MAE: 1.587409257888794\n",
      "Epoch 460/750, Train Loss: 5.882099702591712, Val Loss: 5.726144740608954, Val MAE: 1.5898977518081665\n",
      "Epoch 461/750, Train Loss: 5.882025359858392, Val Loss: 5.725478831867794, Val MAE: 1.5867258310317993\n",
      "Epoch 462/750, Train Loss: 5.882321236890572, Val Loss: 5.725015685940886, Val MAE: 1.584064245223999\n",
      "Epoch 463/750, Train Loss: 5.8818262507724475, Val Loss: 5.72649608888413, Val MAE: 1.5911842584609985\n",
      "Epoch 464/750, Train Loss: 5.882134486659698, Val Loss: 5.725925363716275, Val MAE: 1.5883941650390625\n",
      "Epoch 465/750, Train Loss: 5.882217882082795, Val Loss: 5.725996419716785, Val MAE: 1.5891224145889282\n",
      "Epoch 466/750, Train Loss: 5.88173447787231, Val Loss: 5.725803682589418, Val MAE: 1.589017629623413\n",
      "Epoch 467/750, Train Loss: 5.8814191676742595, Val Loss: 5.725615626182117, Val MAE: 1.5879249572753906\n",
      "Epoch 468/750, Train Loss: 5.88198609960539, Val Loss: 5.724900655836856, Val MAE: 1.5836782455444336\n",
      "Epoch 469/750, Train Loss: 5.881410039284816, Val Loss: 5.725849688981301, Val MAE: 1.58901846408844\n",
      "Epoch 470/750, Train Loss: 5.881584127578962, Val Loss: 5.72593228576469, Val MAE: 1.5895689725875854\n",
      "Epoch 471/750, Train Loss: 5.8812374709268, Val Loss: 5.725182638671968, Val MAE: 1.585338830947876\n",
      "Epoch 472/750, Train Loss: 5.881624540164846, Val Loss: 5.726223318336296, Val MAE: 1.5907350778579712\n",
      "Epoch 473/750, Train Loss: 5.881475081995616, Val Loss: 5.725743872294494, Val MAE: 1.5885319709777832\n",
      "Epoch 474/750, Train Loss: 5.881077151057982, Val Loss: 5.725200955740245, Val MAE: 1.5855622291564941\n",
      "Epoch 475/750, Train Loss: 5.8809630269466595, Val Loss: 5.725456246671115, Val MAE: 1.5876853466033936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [19:57<00:00, 399.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 476/750, Train Loss: 5.881282282866073, Val Loss: 5.725703580845992, Val MAE: 1.5887141227722168\n",
      "Early stopping\n",
      "Test Loss (MSE): 6.178062915802002\n",
      "Test Mean Absolute Error (MAE): 1.5808556030738672\n",
      "Logging experiment results to gridsearch\\results\\cnn_deep_def_12-11.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from final_project.cnn_deep.experiment import gridsearch_cnn\n",
    "\n",
    "gridsearch_cnn(experiment_name = \"cnn_deep_def\", verbose=False)\n",
    "\n",
    "#PERFORMING VIA COMMAND LINE SCRIPT NOW FOR EFFICIENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate GridSearch Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve, Filter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "        params.pop('stratify_by')  #don't need this, we have the pickled split data \n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized_2d.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(f\"Averaged 1D Convolution Filter (Normalized)  {position}\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V12 (overfits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model('gridsearch_v12', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V11 (stratified by stdev score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Model (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='stdev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse Stability with 'Skill' instead of 'stdev'? \n",
    "### Ans: No Significant Diff. -> Skill the better stratification for performance based on top 1 and top 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', drop_low_playtime=True, stratify_by='skill', eval_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n ========= Interesting Model (DROP BENCHWARMERS) ==========\")\n",
    "best_models = investigate_model('gridsearch_v11', drop_low_playtime=True, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"\\n ========= Easier Model (FULL DATA) ==========\")\n",
    "#best_models = investigate_model('gridsearch_v11', drop_low_playtime=False, stratify_by='skill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 and Top 5 Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_analysis('gridsearch_v11', \n",
    "                    stratify_by='skill', \n",
    "                    eval_top=2, \n",
    "                    drop_low_playtime = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model_v0(expt_name: str = 'gridsearch', \n",
    "                      drop_low_playtime: bool = True,\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Investigate the best model for the given experiment and drop status.\n",
    "\n",
    "    Retrains the model on the same exact data split (to avoid data leakage), and\n",
    "    generates learning curves.\n",
    "    \"\"\"\n",
    "    season =  \"['2020-21', '2021-22']\"\n",
    "    best_params = gridsearch_analysis(expt_name, \n",
    "                        season=season, \n",
    "                        eval_top=1,\n",
    "                        drop_low_playtime=drop_low_playtime,\n",
    "                        **kwargs)\n",
    "    \n",
    "    POSITIONS = ['GK', 'DEF', 'MID', 'FWD']\n",
    "    best_models = {}\n",
    "    for position in POSITIONS: \n",
    "\n",
    "        print(f\"\\n======= Retraining and Filter Analysis for {position} model: =======\\n\")\n",
    "        # Clean up and consolidate parameters in the 'params' dictionary\n",
    "        params = best_params.loc[position, :].to_dict()\n",
    "\n",
    "        serialized_dataset = params.pop('dataset')\n",
    "        dataset = pickle.loads(ast.literal_eval(serialized_dataset))\n",
    "\n",
    "        # Retrieve individual datasets\n",
    "        X_train = dataset['X_train']\n",
    "        d_train = dataset['d_train']\n",
    "        y_train = dataset['y_train']\n",
    "        X_val = dataset['X_val']\n",
    "        d_val = dataset['d_val']\n",
    "        y_val = dataset['y_val']\n",
    "        X_test = dataset['X_test']\n",
    "        d_test = dataset['d_test']\n",
    "        y_test = dataset['y_test']\n",
    "\n",
    "        serialized_pipeline = params.pop('pipeline')\n",
    "        pipeline = pickle.loads(ast.literal_eval(serialized_pipeline))\n",
    "\n",
    "        params['season'] = ['2020-21', '2021-22']\n",
    "        params['metrics'] = ['mae']\n",
    "        params['num_features'] = NUM_FEATURES_DICT[params['position']][params.pop('amt_num_features')]\n",
    "\n",
    "        # Add the datasets to params\n",
    "        params.update({\n",
    "            'X_train': X_train,\n",
    "            'd_train': d_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'd_val': d_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'd_test': d_test,\n",
    "            'y_test': y_test,\n",
    "            'plot': True,\n",
    "        })\n",
    "\n",
    "        # =========== Qualitative Analysis of Best & Worst Ex =============\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        model, expt_res = build_train_cnn(**params)\n",
    "        best_models[position] = model\n",
    "\n",
    "        y_pred = model.predict([X_test, d_test])\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        y_pred_flattened = y_pred.flatten()\n",
    "        mse_per_example = np.square(y_test_flattened - y_pred_flattened)\n",
    "\n",
    "        results_df = pd.DataFrame({'Actual Score': y_test_flattened, 'Predicted Score': y_pred_flattened, 'MSE': mse_per_example})\n",
    "        results_df['d_test'] = d_test\n",
    "\n",
    "        # Add X_test features to the DataFrame\n",
    "        # unstandardize the features\n",
    "        X_test_original = X_test.copy()\n",
    "\n",
    "        # Calculate unstandardized pts\n",
    "        numerical_transformer = pipeline.named_steps['preprocessor'].named_transformers_['num']\n",
    "        X_test_reshaped = X_test_original.reshape(-1, X_test_original.shape[-1])\n",
    "        X_test_unstandardized = numerical_transformer.inverse_transform(X_test_reshaped).astype(int)\n",
    "        X_test_unstandardized = X_test_unstandardized.reshape(X_test_original.shape)\n",
    "        X_test_unstandardized_2d = X_test_unstandardized.reshape(X_test_unstandardized.shape[0], -1)\n",
    "        column_names = [f'pts_week{i}' for i in range(X_test_unstandardized.shape[1])]\n",
    "        unstandardized_df = pd.DataFrame(X_test_unstandardized_2d, columns=column_names)\n",
    "\n",
    "        results_df = pd.concat([results_df, unstandardized_df], axis=1)\n",
    "\n",
    "        results_df.sort_values(by='MSE', ascending=False, inplace=True)\n",
    "\n",
    "        print(f\"\\nWorst two examples for {position}:\\n\")\n",
    "        display(results_df.head(2))\n",
    "\n",
    "        print(f\"\\nBest two examples for {position}:\\n\")\n",
    "        display(results_df.tail(2))\n",
    "\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if 'conv' not in layer.name:\n",
    "                continue\n",
    "            filters, biases = layer.get_weights()\n",
    "\n",
    "        # retrieve filter weights from the second hidden layer\n",
    "        filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "        # normalize filter values by z-score normalization\n",
    "        mean_value, std_dev = filters.mean(), filters.std()\n",
    "        normalized_filters = (filters - mean_value) / std_dev\n",
    "        mean_filter = normalized_filters.mean(axis=-1)\n",
    "\n",
    "        # set greyscale color map\n",
    "        cmap = plt.cm.Greys_r\n",
    "        cmap.set_bad('0.5')  # Set the color for NaN values (if any) to medium-grey\n",
    "\n",
    "        # plot the normalized average filter with numeric values inside pixels\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(mean_filter, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "        # add text annotations with normalized values inside each pixel\n",
    "        for i, value in enumerate(mean_filter):\n",
    "            value = value[0]\n",
    "            # set text color based on brightness\n",
    "            text_color = 'white' if value < 0 else 'black'\n",
    "            ax.text(0, i, f\"{value:.4f}\", ha='center', va='center', fontsize=8, color=text_color)\n",
    "        ax.set_title(\"Averaged 1D Convolution Filter (Normalized)\")\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier model\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with drop benched players\n",
    "#best_models = investigate_model_v0('gridsearch_v10', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models = investigate_model_v0('gridsearch_v9', drop_low_playtime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_params = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "best_hyperparams = gridsearch_analysis('gridsearch_v8', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without player dropping\n",
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v7', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    drop_low_playtime = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6  With Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=5,\n",
    "                    drop_low_playtime = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V6 Best Models Without Player Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v6', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1,\n",
    "                    num_dense=64,\n",
    "                    num_filters=64,\n",
    "                    amt_num_features = 'ptsonly',\n",
    "                    drop_low_playtime = True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('_gridsearch_v4', \n",
    "                    season=\"['2020-21', '2021-22']\", \n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2020-21',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v4_singleyear_drop', \n",
    "                    season='2021-22',\n",
    "                    eval_top=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridsearch_analysis('gridsearch_v5', eval_top=3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_hyperparams = gridsearch_analysis('gridsearch_v4_optimal_drop', \n",
    "                    eval_top=1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
